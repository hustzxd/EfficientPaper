paper {
  title: "Efficient Streaming Language Models with Attention Sinks"
  abbr: "StreamingLLM"
  url: "http://arxiv.org/abs/2309.17453v4"
  authors: "Guangxuan Xiao"
  authors: "Yuandong Tian"
  authors: "Beidi Chen"
  authors: "Song Han"
  authors: "Mike Lewis"
  institutions: "Massachusetts Institute of Technology"
  institutions: "Meta AI"
  institutions: "Carnegie Mellon University"
  institutions: "NVIDIA"
}
pub {
  where: "ICLR"
  year: 2024
}
code {
  type: "Pytorch"
  url: "https://github.com/mit-han-lab/streaming-llm"
}
keyword {
  words: attention_sparsity
  words: sparse_pruning
  words: kv_cache_management
}
cover {
  url: "cover.jpg"
}
baseline {
  methods: "None"
}
