{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EfficientPaper Pruning, Quantization and efficient-inference/training paper list. Table of Contents EfficientPaper Getting Started Paper List keyword year publication institution author References Getting Started git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper } Recent Changes Page Date Author Description docs/cls_author.md 2025-09-03 22:03 hustzxd demo docs/cls_institution.md 2025-09-03 22:03 hustzxd demo docs/cls_keyword.md 2025-09-03 22:03 hustzxd demo docs/cls_publication.md 2025-09-03 22:03 hustzxd demo docs/cls_year.md 2025-09-03 22:03 hustzxd demo docs/index.md 2025-09-03 22:03 hustzxd demo docs/meta 2025-08-29 15:01 hustzxd del docs/notes 2025-08-29 15:01 hustzxd del docs/weekly_paper/2025-07-25.md 2025-08-29 10:59 hustzxd change_dir Only the most recent 9 entries are displayed. Paper List year 2026 FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ] 2025 AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] Training LLMs with MXFP4 [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] PQCache: Product Quantization-based KVCache for Long Context LLM Inference [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DReSS: Data-driven Regularized Structured Streamlining for Large Language Models [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KV Cache Compression for Inference Efficiency in LLMs: A Review [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] MiniMax-01: Scaling Foundation Models with Lightning Attention [ ] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Retrospective Sparse Attention for Efficient Long-Context Generation [ ] RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning [ ] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Attention-Gym: Triton-Based Sparse and Quantization Attention [ ] DeepEP: an efficient expert-parallel communication library [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress: LLM KV cache compression made easy [ ] 2024 Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention [ ] Splitwise: Efficient generative LLM inference using phase splitting [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference [ ] Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models [ ] Massive Activations in Large Language Models [ ] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization [ ] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization [ ] SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification [ ] 2023 Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ] 2022 Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ] 2021 Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ] 2020 Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ] 2019 ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ] 2018 A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ] 2017 DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Attention Is All You Need [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ] 2016 Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ] 1993 Optimal Brain Surgeon and general network pruning [ ] 1989 Optimal Brain Damage [ ] References https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling [ ] https://github.com/weigao266/Awesome-Efficient-Arch [ ] https://github.com/horseee/Awesome-Efficient-LLM [ ] https://github.com/DefTruth/Awesome-Diffusion-Inference [ ] https://github.com/DefTruth/Awesome-LLM-Inference [ ] https://github.com/AmberLJC/LLMSys-PaperList [ ] https://github.com/Hannibal046/Awesome-LLM [ ] https://github.com/AmadeusChan/Awesome-LLM-System-Papers [ ] https://github.com/KnowingNothing/compiler-and-arch [ ] https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management [ ] https://github.com/October2001/Awesome-KV-Cache-Compression [ ] https://github.com/he-y/Awesome-Pruning [ ] https://github.com/htqin/awesome-model-quantization [ ] https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression [ ] https://github.com/AojunZhou/Efficient-Deep-Learning [ ] https://github.com/chester256/Model-Compression-Papers [ ]","title":"Home"},{"location":"#efficientpaper","text":"Pruning, Quantization and efficient-inference/training paper list.","title":"EfficientPaper"},{"location":"#table-of-contents","text":"EfficientPaper Getting Started Paper List keyword year publication institution author References","title":"Table of Contents"},{"location":"#getting-started","text":"git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper }","title":"Getting Started"},{"location":"#recent-changes","text":"Page Date Author Description docs/cls_author.md 2025-09-03 22:03 hustzxd demo docs/cls_institution.md 2025-09-03 22:03 hustzxd demo docs/cls_keyword.md 2025-09-03 22:03 hustzxd demo docs/cls_publication.md 2025-09-03 22:03 hustzxd demo docs/cls_year.md 2025-09-03 22:03 hustzxd demo docs/index.md 2025-09-03 22:03 hustzxd demo docs/meta 2025-08-29 15:01 hustzxd del docs/notes 2025-08-29 15:01 hustzxd del docs/weekly_paper/2025-07-25.md 2025-08-29 10:59 hustzxd change_dir Only the most recent 9 entries are displayed.","title":"Recent Changes"},{"location":"#paper-list","text":"","title":"Paper List"},{"location":"#year","text":"","title":"year"},{"location":"#2026","text":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ]","title":"2026"},{"location":"#2025","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] Training LLMs with MXFP4 [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] PQCache: Product Quantization-based KVCache for Long Context LLM Inference [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DReSS: Data-driven Regularized Structured Streamlining for Large Language Models [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KV Cache Compression for Inference Efficiency in LLMs: A Review [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] MiniMax-01: Scaling Foundation Models with Lightning Attention [ ] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Retrospective Sparse Attention for Efficient Long-Context Generation [ ] RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning [ ] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Attention-Gym: Triton-Based Sparse and Quantization Attention [ ] DeepEP: an efficient expert-parallel communication library [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress: LLM KV cache compression made easy [ ]","title":"2025"},{"location":"#2024","text":"Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention [ ] Splitwise: Efficient generative LLM inference using phase splitting [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference [ ] Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models [ ] Massive Activations in Large Language Models [ ] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization [ ] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization [ ] SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification [ ]","title":"2024"},{"location":"#2023","text":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ]","title":"2023"},{"location":"#2022","text":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ]","title":"2022"},{"location":"#2021","text":"Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ]","title":"2021"},{"location":"#2020","text":"Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ]","title":"2020"},{"location":"#2019","text":"ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ]","title":"2019"},{"location":"#2018","text":"A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ]","title":"2018"},{"location":"#2017","text":"DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Attention Is All You Need [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ]","title":"2017"},{"location":"#2016","text":"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ]","title":"2016"},{"location":"#1993","text":"Optimal Brain Surgeon and general network pruning [ ]","title":"1993"},{"location":"#1989","text":"Optimal Brain Damage [ ]","title":"1989"},{"location":"#references","text":"https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling [ ] https://github.com/weigao266/Awesome-Efficient-Arch [ ] https://github.com/horseee/Awesome-Efficient-LLM [ ] https://github.com/DefTruth/Awesome-Diffusion-Inference [ ] https://github.com/DefTruth/Awesome-LLM-Inference [ ] https://github.com/AmberLJC/LLMSys-PaperList [ ] https://github.com/Hannibal046/Awesome-LLM [ ] https://github.com/AmadeusChan/Awesome-LLM-System-Papers [ ] https://github.com/KnowingNothing/compiler-and-arch [ ] https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management [ ] https://github.com/October2001/Awesome-KV-Cache-Compression [ ] https://github.com/he-y/Awesome-Pruning [ ] https://github.com/htqin/awesome-model-quantization [ ] https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression [ ] https://github.com/AojunZhou/Efficient-Deep-Learning [ ] https://github.com/chester256/Model-Compression-Papers [ ]","title":"References"},{"location":"cls_author/","text":"author Aaron Courville Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Abhay Gupta Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Adam Fisch Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Aixin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ajay Jaiswal Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note Amir Gholami Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Amir H. Abdi Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Andr\u00e9 F. T. Martins Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Aojun Zhou Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Aonian Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Arvind Krishnamurthy Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ashish Panwar Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Bairu Hou Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Bangwei Gong Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Baris Kasikci Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Bei Feng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Beidi Chen Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note Bin Cui Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note Bin Gao Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Bin Lin Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Bin Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Bing Xue Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bingxuan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bo Yang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Bochao Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bohan Zhuang Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Boji Shan Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Chang Chen Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Chang Gao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note Chao Wang Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Chao Yang Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Chaojun Xiao Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Chen Chen Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Chen Zhang Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Cheng Zhu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Chengda Lu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chenggang Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengqi Deng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengquan Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note Chengruidong Zhang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Chenyang Song Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Chenyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chong Ruan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Christos Kozyrakis Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note Chuang Gan Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note Chunhao Zhang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Clark Barrett Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Cody Hao Yu Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Coleman Hooper Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Congchao Guo Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Da Chen Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Damai Dai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dan Alistarh Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Daxin Jiang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Daya Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dejian Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Deli Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dianhai Yu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Dong Li Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Dongjie Ji Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dongsheng Li Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Dongwon Jo Meta Title Cover Publish Code Note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note Dongyang Wang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Eldar Kurtic Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Elias Frantar Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Emad Barsoum Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Enwei Jiao Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Erhang Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fan Yang Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Fangcheng Fu Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note Fangyun Lin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fei Huang Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Fucong Dai Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fuli Luo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Furu Wei Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note Genghan Zhang Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Gongfan Fang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Guanchen Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Guangbo Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guangxuan Xiao Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Guanting Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guohao Dai Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Guowei Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note H. Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hai Zhao Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Haibin Lin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Haibo Chen Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haifeng Wang Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Hailin Zhang Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note Han Bao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hanshi Sun Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Hanwei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hao Zhang Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Haocheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Haocheng Xi Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Haofeng Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Haohai Sun Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Haoli Bai Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Haotian Tang Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Haotong Xie Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haowei Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hayden Kwok-Hay So Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Heung-Yeung Shum Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Hong Zhou Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Honghui Ding Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hongsheng Li Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Hrayr Harutyunyan Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Huajian Xin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huazuo Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Qu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huiqiang Jiang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Iman Mirzadeh Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Ion Stoica Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note J. L. Cai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note J. Zico Kolter Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Jae-Joon Kim Meta Title Cover Publish Code Note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note Jan Kautz Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Jason D. Lee Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Jayashree Mohan Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Jeff Pool Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Jia Wei Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Jiaming Tang Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Jiaming Xu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note Jian Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jianfei Chen Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Jianfeng Gao Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Jiangfei Duan Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Jianxi Ye Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jianyong Wang Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note Jianzhong Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiaqi Ni Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiaqi Zhuang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Jiashi Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiawei Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiayuan Song Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Jie Zhou Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Jin Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jin Fang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jin Zhu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Jing Liu Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Jingchang Chen Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jingyang Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Jingyang Yuan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jintao Zhang Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Joseph E. Gonzalez Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note Jun Zhu Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Junhao Xu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Junjie Qiu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junjie Yan Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Junlong Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junxian Guo Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Junxiao Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junyang Lin Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Kai Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kai Hu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kaige Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kan Zhu Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Kang Guan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kang Zhao Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Ke Hong Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Kecheng Xiao Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Kehong Yuan Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Kexin Huang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kuai Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kurt Keutzer Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Le Han Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Lean Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lecong Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lefei Zhang Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Lei Chen Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Lei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Leyang Wang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Leyi Xia Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Li Dong Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Li-Wen Chang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Lian Liu Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Lianfei Yu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Liang Zhao Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lianmin Zheng Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Liheng Feng Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Lili Qiu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Lin Li Meta Title Cover Publish Code Note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Lin Zheng Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Litong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Liyue Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lu Hou Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Mao Yang Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Maosong Sun Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Marcos Treviso Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Mark Kurtz Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Mehrdad Farajtabar Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Meng Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mengdi Wang Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Miaojun Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Michael Goin Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Michael Hassid Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Michael W. Mahoney Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Mingchuan Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghua Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghui Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mingjie Sun Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Mingming Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mingyuan Chi Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note MiniMax Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Minmin Sun Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Minsik Cho Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note Mohammad Rastegari Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note Mozhi Zhang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Ning Tian Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ningxin Zheng Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Nipun Kwatra Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Panpan Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pavlo Molchanov Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Peiyi Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Peng Gao Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Peng Sun Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Peng Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pengcheng He Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Pengfei Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Pengfei Zuo Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Pengle Zhang Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note Pengyu Zhao Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Ping Luo Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Qi Hou Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Qianchao Zhu Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Qiancheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qianhui Wu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Qidi Xu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Qihao Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qin Wang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Qingru Zhang Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note Qinyu Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qiushi Du Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. J. Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. L. Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ramachandran Ramjee Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Ramya Prabhu Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Rayan Saab Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Roy Schwartz Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Ruihang Lai Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ruiqi Ge Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruisong Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruitao Leng Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Ruizhe Pan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runji Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runxin Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruoyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruyi Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note S. S. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Saeed Maleki Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note Sangmin Bae Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Saurav Muralidharan Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Sean Lie Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Sehoon Kim Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Shang Yang Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Shanghao Lu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shangyan Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shanhuang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shaoqing Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shengen Yan Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shengfeng Ye Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None Shengmin Shi Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Shijie Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Shirong Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shiwei Liu Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Shiyao Li Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shiyu Chang Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Shiyu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shreyas Saxena Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note Shuang Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuiping Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shunfeng Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuo Yang Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Shuqi Yu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Shuting Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Sichen Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Size Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Song Han Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Songquan Zhu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Stephanie Wang Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Surin Ahn Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note T. Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tal Schuster Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Tao Xie Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Tao Yu Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models MXFP4Train Training LLMs with MXFP4 note Tao Yuan Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Tao Yun Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tian Pei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianle Cai Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Tianlong Chen Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Tianqi Chen Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Tianqi Wu Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Tianrun Liang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Tianyu Fu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Tianyu Gao Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note Tianyu Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianzhu Ye Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tim Dettmers Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression Ting Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tong Yang Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Torsten Hoefler Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Tri Dao Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note Tuo Zhao Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note Vithursan Thangarasa Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note W. L. Xiao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wangding Zeng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wanjia Zhao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei An Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei Lin Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Wei Wang Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Weigao Sun Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Weilin Zhao Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Weixuan Sun Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note Weiyu Cheng Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Weiyu Huang Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Weizhu Chen Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Wen Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenfeng Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenjun Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenkai Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Wenlei Bao Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Wenqi Shao Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Wenqin Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wentao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Woosuk Kwon Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Wulong Liu Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note X. Q. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiafei Qiu Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Xiandong Zhao Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiang Liu Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note Xiangjun Song Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xiangyu Zhang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Xiangyue Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xianzhi Yu Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Xianzu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiao Bi Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiao Su Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xiaodong Han Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xiaodong Ji Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note Xiaodong Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaohan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaojin Shen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaosha Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaotao Nie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaowei Li Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiaowen Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaoxiang Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Chen Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Xin Cheng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Jin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Xin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xin Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinchao Wang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Xingchao Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xingcheng Zhang Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Xingkai Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinnan Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinxia Shan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyi Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Zhou Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Xinyuan Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinzhu Hou Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xiuhong Li Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Xu Han Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Xu Owen He Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Xuan Lu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xuecheng Su Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xuefei Ning Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Xuegui Zheng Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xufang Luo Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Xuheng Lin Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xun Zou Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Xuyang Shen Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Y. K. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Q. Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Y. X. Wei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. X. Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yan Gong Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yang Li Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note Yang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yanhong Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None Yankai Lin Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yanping Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaofeng Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yefei He Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Yehui Tang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yi Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yi Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yichao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yifan Shi Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yikai Zhang Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yiliang Xiong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yilong Zhao Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Ying He Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ying Sheng Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Ying Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yingfa Chen Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Yinhe Han Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Yiran Zhong Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yishi Piao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yisong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiwu Yao Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note Yixiao Li Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Yixin Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note Yixin Song Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Yixuan Tan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyang Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Ma Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Yizhao Gao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yong Li Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Yongqiang Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yongyi Hu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yu Cheng Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Yu Wang Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Yu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuan Ou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuandong Tian Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Yuanxiang Fan Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yuchen Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yucheng Li Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yuduan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yue Gong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuezhou Hu Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Yufeng Yang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yuhao Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yuheng Zou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuhui Xu Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Yujia He Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yujun Lin Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note Yukun Zha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yulhwa Kim Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note Yunan Huang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yunfan Xiong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yunhe Wang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yunji Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yunxian Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yunzhi Xu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yuqing Xia Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuqing Yang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Yutao Sun Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuting Yan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang Luo Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang You Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxin Mao Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Yuxin Wu Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Yuxiong He Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Yuxuan Li Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yuxuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuyang Zhou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. F. Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. Z. Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zefan Cai Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note Zehan Li Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Zehui Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zeping Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Zeyu Mi Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Zhangli Sha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhangyang Wang Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhe Fu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhean Xu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Dong Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Zhen Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Qin Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note Zhen Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenda Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhengyan Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenhua Fan Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Zhenyu Zhang Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhewei Yao Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Zhewen Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhibin Gou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhicheng Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhigang Yan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhihang Yu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Zhihang Yuan Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note Zhihong Shao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhilin Yang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipeng Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhixuan Lin Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Zhiyu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhiyuan Liu Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Zhongyu Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhou Yu Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Zhuang Liu Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Zhuo Jiang Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Zhuomin He Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Zhuoshu Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zihan Wang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Zihao Ye Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ziheng Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Zihui Gu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijia Wu Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Zijia Zhu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijun Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zili Wang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Zilin Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziqing Yang Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note Ziwei Ji Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Ziwei Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zixiao Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Zixuan Zhou Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Ziyang Song Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziyi Gao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zizheng Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zuchao Li Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Zunhai Su Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"By Author"},{"location":"cls_author/#author","text":"","title":"author"},{"location":"cls_author/#aaron-courville","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Aaron Courville"},{"location":"cls_author/#abhay-gupta","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Abhay Gupta"},{"location":"cls_author/#adam-fisch","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Adam Fisch"},{"location":"cls_author/#aixin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Aixin Liu"},{"location":"cls_author/#ajay-jaiswal","text":"Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note","title":"Ajay Jaiswal"},{"location":"cls_author/#amir-gholami","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Amir Gholami"},{"location":"cls_author/#amir-h-abdi","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Amir H. Abdi"},{"location":"cls_author/#andre-f-t-martins","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Andr\u00e9 F. T. Martins"},{"location":"cls_author/#aojun-zhou","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Aojun Zhou"},{"location":"cls_author/#aonian-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Aonian Li"},{"location":"cls_author/#arvind-krishnamurthy","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Arvind Krishnamurthy"},{"location":"cls_author/#ashish-panwar","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ashish Panwar"},{"location":"cls_author/#bairu-hou","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Bairu Hou"},{"location":"cls_author/#bangwei-gong","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Bangwei Gong"},{"location":"cls_author/#baris-kasikci","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Baris Kasikci"},{"location":"cls_author/#bei-feng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bei Feng"},{"location":"cls_author/#beidi-chen","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note","title":"Beidi Chen"},{"location":"cls_author/#bin-cui","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note","title":"Bin Cui"},{"location":"cls_author/#bin-gao","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Bin Gao"},{"location":"cls_author/#bin-lin","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Bin Lin"},{"location":"cls_author/#bin-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Bin Wang"},{"location":"cls_author/#bing-xue","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bing Xue"},{"location":"cls_author/#bingxuan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bingxuan Wang"},{"location":"cls_author/#bo-yang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Bo Yang"},{"location":"cls_author/#bochao-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bochao Wu"},{"location":"cls_author/#bohan-zhuang","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Bohan Zhuang"},{"location":"cls_author/#boji-shan","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Boji Shan"},{"location":"cls_author/#chang-chen","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Chang Chen"},{"location":"cls_author/#chang-gao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note","title":"Chang Gao"},{"location":"cls_author/#chao-wang","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Chao Wang"},{"location":"cls_author/#chao-yang","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Chao Yang"},{"location":"cls_author/#chaojun-xiao","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Chaojun Xiao"},{"location":"cls_author/#chen-chen","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Chen Chen"},{"location":"cls_author/#chen-zhang","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Chen Zhang"},{"location":"cls_author/#cheng-zhu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Cheng Zhu"},{"location":"cls_author/#chengda-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengda Lu"},{"location":"cls_author/#chenggang-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenggang Zhao"},{"location":"cls_author/#chengqi-deng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengqi Deng"},{"location":"cls_author/#chengquan-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note","title":"Chengquan Jiang"},{"location":"cls_author/#chengruidong-zhang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Chengruidong Zhang"},{"location":"cls_author/#chenyang-song","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Chenyang Song"},{"location":"cls_author/#chenyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenyu Zhang"},{"location":"cls_author/#chong-ruan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chong Ruan"},{"location":"cls_author/#christos-kozyrakis","text":"Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"Christos Kozyrakis"},{"location":"cls_author/#chuang-gan","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note","title":"Chuang Gan"},{"location":"cls_author/#chunhao-zhang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Chunhao Zhang"},{"location":"cls_author/#clark-barrett","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Clark Barrett"},{"location":"cls_author/#cody-hao-yu","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Cody Hao Yu"},{"location":"cls_author/#coleman-hooper","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Coleman Hooper"},{"location":"cls_author/#congchao-guo","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Congchao Guo"},{"location":"cls_author/#da-chen","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Da Chen"},{"location":"cls_author/#damai-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Damai Dai"},{"location":"cls_author/#dan-alistarh","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Dan Alistarh"},{"location":"cls_author/#daxin-jiang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Daxin Jiang"},{"location":"cls_author/#daya-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Daya Guo"},{"location":"cls_author/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_author/#dejian-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dejian Yang"},{"location":"cls_author/#deli-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Deli Chen"},{"location":"cls_author/#dianhai-yu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Dianhai Yu"},{"location":"cls_author/#dong-li","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"Dong Li"},{"location":"cls_author/#dongjie-ji","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dongjie Ji"},{"location":"cls_author/#dongsheng-li","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Dongsheng Li"},{"location":"cls_author/#dongwon-jo","text":"Meta Title Cover Publish Code Note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note","title":"Dongwon Jo"},{"location":"cls_author/#dongyang-wang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Dongyang Wang"},{"location":"cls_author/#eldar-kurtic","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Eldar Kurtic"},{"location":"cls_author/#elias-frantar","text":"Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models","title":"Elias Frantar"},{"location":"cls_author/#emad-barsoum","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"Emad Barsoum"},{"location":"cls_author/#enwei-jiao","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Enwei Jiao"},{"location":"cls_author/#erhang-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Erhang Li"},{"location":"cls_author/#fan-yang","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Fan Yang"},{"location":"cls_author/#fangcheng-fu","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note","title":"Fangcheng Fu"},{"location":"cls_author/#fangyun-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fangyun Lin"},{"location":"cls_author/#fei-huang","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Fei Huang"},{"location":"cls_author/#fucong-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fucong Dai"},{"location":"cls_author/#fuli-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fuli Luo"},{"location":"cls_author/#furu-wei","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note","title":"Furu Wei"},{"location":"cls_author/#genghan-zhang","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Genghan Zhang"},{"location":"cls_author/#gongfan-fang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Gongfan Fang"},{"location":"cls_author/#guanchen-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"Guanchen Li"},{"location":"cls_author/#guangbo-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guangbo Hao"},{"location":"cls_author/#guangxuan-xiao","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Guangxuan Xiao"},{"location":"cls_author/#guanting-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guanting Chen"},{"location":"cls_author/#guohao-dai","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Guohao Dai"},{"location":"cls_author/#guowei-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guowei Li"},{"location":"cls_author/#h-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"H. Zhang"},{"location":"cls_author/#hai-zhao","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note","title":"Hai Zhao"},{"location":"cls_author/#haibin-lin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Haibin Lin"},{"location":"cls_author/#haibo-chen","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haibo Chen"},{"location":"cls_author/#haifeng-wang","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Haifeng Wang"},{"location":"cls_author/#hailin-zhang","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note","title":"Hailin Zhang"},{"location":"cls_author/#han-bao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Han Bao"},{"location":"cls_author/#hanshi-sun","text":"Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Hanshi Sun"},{"location":"cls_author/#hanwei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hanwei Xu"},{"location":"cls_author/#hao-zhang","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Hao Zhang"},{"location":"cls_author/#haocheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haocheng Wang"},{"location":"cls_author/#haocheng-xi","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Haocheng Xi"},{"location":"cls_author/#haofeng-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Haofeng Huang"},{"location":"cls_author/#haohai-sun","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Haohai Sun"},{"location":"cls_author/#haoli-bai","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Haoli Bai"},{"location":"cls_author/#haotian-tang","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Haotian Tang"},{"location":"cls_author/#haotong-xie","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haotong Xie"},{"location":"cls_author/#haowei-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haowei Zhang"},{"location":"cls_author/#hayden-kwok-hay-so","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Hayden Kwok-Hay So"},{"location":"cls_author/#heung-yeung-shum","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Heung-Yeung Shum"},{"location":"cls_author/#hong-zhou","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Hong Zhou"},{"location":"cls_author/#honghui-ding","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Honghui Ding"},{"location":"cls_author/#hongsheng-li","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Hongsheng Li"},{"location":"cls_author/#hrayr-harutyunyan","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Hrayr Harutyunyan"},{"location":"cls_author/#huajian-xin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huajian Xin"},{"location":"cls_author/#huazuo-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huazuo Gao"},{"location":"cls_author/#hui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Li"},{"location":"cls_author/#hui-qu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Qu"},{"location":"cls_author/#huiqiang-jiang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Huiqiang Jiang"},{"location":"cls_author/#iman-mirzadeh","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Iman Mirzadeh"},{"location":"cls_author/#ion-stoica","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Ion Stoica"},{"location":"cls_author/#j-l-cai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"J. L. Cai"},{"location":"cls_author/#j-zico-kolter","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"J. Zico Kolter"},{"location":"cls_author/#jae-joon-kim","text":"Meta Title Cover Publish Code Note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note","title":"Jae-Joon Kim"},{"location":"cls_author/#jan-kautz","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Jan Kautz"},{"location":"cls_author/#jason-d-lee","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Jason D. Lee"},{"location":"cls_author/#jayashree-mohan","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Jayashree Mohan"},{"location":"cls_author/#jeff-pool","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Jeff Pool"},{"location":"cls_author/#jia-wei","text":"Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Jia Wei"},{"location":"cls_author/#jiaming-tang","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Jiaming Tang"},{"location":"cls_author/#jiaming-xu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"Jiaming Xu"},{"location":"cls_author/#jian-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jian Liang"},{"location":"cls_author/#jianfei-chen","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Jianfei Chen"},{"location":"cls_author/#jianfeng-gao","text":"Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Jianfeng Gao"},{"location":"cls_author/#jiangfei-duan","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Jiangfei Duan"},{"location":"cls_author/#jianxi-ye","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jianxi Ye"},{"location":"cls_author/#jianyong-wang","text":"Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note","title":"Jianyong Wang"},{"location":"cls_author/#jianzhong-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jianzhong Guo"},{"location":"cls_author/#jiaqi-ni","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiaqi Ni"},{"location":"cls_author/#jiaqi-zhuang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Jiaqi Zhuang"},{"location":"cls_author/#jiashi-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiashi Li"},{"location":"cls_author/#jiawei-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiawei Wang"},{"location":"cls_author/#jiayuan-song","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Jiayuan Song"},{"location":"cls_author/#jie-zhou","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Jie Zhou"},{"location":"cls_author/#jin-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jin Chen"},{"location":"cls_author/#jin-fang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jin Fang"},{"location":"cls_author/#jin-zhu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Jin Zhu"},{"location":"cls_author/#jing-liu","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Jing Liu"},{"location":"cls_author/#jingchang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingchang Chen"},{"location":"cls_author/#jingyang-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Jingyang Li"},{"location":"cls_author/#jingyang-yuan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingyang Yuan"},{"location":"cls_author/#jintao-zhang","text":"Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Jintao Zhang"},{"location":"cls_author/#joseph-e-gonzalez","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"Joseph E. Gonzalez"},{"location":"cls_author/#jun-zhu","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Jun Zhu"},{"location":"cls_author/#junhao-xu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Junhao Xu"},{"location":"cls_author/#junjie-qiu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junjie Qiu"},{"location":"cls_author/#junjie-yan","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Junjie Yan"},{"location":"cls_author/#junlong-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junlong Li"},{"location":"cls_author/#junxian-guo","text":"Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Junxian Guo"},{"location":"cls_author/#junxiao-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junxiao Song"},{"location":"cls_author/#junyang-lin","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Junyang Lin"},{"location":"cls_author/#kai-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Dong"},{"location":"cls_author/#kai-hu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Hu"},{"location":"cls_author/#kaige-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kaige Gao"},{"location":"cls_author/#kan-zhu","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note","title":"Kan Zhu"},{"location":"cls_author/#kang-guan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kang Guan"},{"location":"cls_author/#kang-zhao","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Kang Zhao"},{"location":"cls_author/#ke-hong","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Ke Hong"},{"location":"cls_author/#kecheng-xiao","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Kecheng Xiao"},{"location":"cls_author/#kehong-yuan","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Kehong Yuan"},{"location":"cls_author/#kexin-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kexin Huang"},{"location":"cls_author/#kuai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kuai Yu"},{"location":"cls_author/#kurt-keutzer","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Kurt Keutzer"},{"location":"cls_author/#le-han","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Le Han"},{"location":"cls_author/#lean-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lean Wang"},{"location":"cls_author/#lecong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lecong Zhang"},{"location":"cls_author/#lefei-zhang","text":"Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Lefei Zhang"},{"location":"cls_author/#lei-chen","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Lei Chen"},{"location":"cls_author/#lei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lei Xu"},{"location":"cls_author/#leyang-wang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Leyang Wang"},{"location":"cls_author/#leyi-xia","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Leyi Xia"},{"location":"cls_author/#li-dong","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Li Dong"},{"location":"cls_author/#li-wen-chang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Li-Wen Chang"},{"location":"cls_author/#lian-liu","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Lian Liu"},{"location":"cls_author/#lianfei-yu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Lianfei Yu"},{"location":"cls_author/#liang-zhao","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liang Zhao"},{"location":"cls_author/#lianmin-zheng","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Lianmin Zheng"},{"location":"cls_author/#liheng-feng","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Liheng Feng"},{"location":"cls_author/#lili-qiu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Lili Qiu"},{"location":"cls_author/#lin-li","text":"Meta Title Cover Publish Code Note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Lin Li"},{"location":"cls_author/#lin-zheng","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Lin Zheng"},{"location":"cls_author/#litong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Litong Wang"},{"location":"cls_author/#liyue-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liyue Zhang"},{"location":"cls_author/#lu-hou","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Lu Hou"},{"location":"cls_author/#mao-yang","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Mao Yang"},{"location":"cls_author/#maosong-sun","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Maosong Sun"},{"location":"cls_author/#marcos-treviso","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Marcos Treviso"},{"location":"cls_author/#mark-kurtz","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Mark Kurtz"},{"location":"cls_author/#mehrdad-farajtabar","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Mehrdad Farajtabar"},{"location":"cls_author/#meng-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Meng Li"},{"location":"cls_author/#mengdi-wang","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Mengdi Wang"},{"location":"cls_author/#miaojun-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Miaojun Wang"},{"location":"cls_author/#michael-goin","text":"Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Michael Goin"},{"location":"cls_author/#michael-hassid","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Michael Hassid"},{"location":"cls_author/#michael-w-mahoney","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Michael W. Mahoney"},{"location":"cls_author/#mingchuan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingchuan Zhang"},{"location":"cls_author/#minghua-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghua Zhang"},{"location":"cls_author/#minghui-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghui Tang"},{"location":"cls_author/#mingjie-sun","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Mingjie Sun"},{"location":"cls_author/#mingming-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingming Li"},{"location":"cls_author/#mingyuan-chi","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Mingyuan Chi"},{"location":"cls_author/#minimax","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"MiniMax"},{"location":"cls_author/#minmin-sun","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Minmin Sun"},{"location":"cls_author/#minsik-cho","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note","title":"Minsik Cho"},{"location":"cls_author/#mohammad-rastegari","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note","title":"Mohammad Rastegari"},{"location":"cls_author/#mozhi-zhang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Mozhi Zhang"},{"location":"cls_author/#ning-tian","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ning Tian"},{"location":"cls_author/#ningxin-zheng","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ningxin Zheng"},{"location":"cls_author/#nipun-kwatra","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Nipun Kwatra"},{"location":"cls_author/#panpan-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Panpan Huang"},{"location":"cls_author/#pavlo-molchanov","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Pavlo Molchanov"},{"location":"cls_author/#peiyi-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peiyi Wang"},{"location":"cls_author/#peng-gao","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Peng Gao"},{"location":"cls_author/#peng-sun","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Peng Sun"},{"location":"cls_author/#peng-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peng Zhang"},{"location":"cls_author/#pengcheng-he","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Pengcheng He"},{"location":"cls_author/#pengfei-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Pengfei Li"},{"location":"cls_author/#pengfei-zuo","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Pengfei Zuo"},{"location":"cls_author/#pengle-zhang","text":"Meta Title Cover Publish Code Note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note","title":"Pengle Zhang"},{"location":"cls_author/#pengyu-zhao","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Pengyu Zhao"},{"location":"cls_author/#ping-luo","text":"Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note","title":"Ping Luo"},{"location":"cls_author/#qi-hou","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Qi Hou"},{"location":"cls_author/#qianchao-zhu","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Qianchao Zhu"},{"location":"cls_author/#qiancheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiancheng Wang"},{"location":"cls_author/#qianhui-wu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Qianhui Wu"},{"location":"cls_author/#qidi-xu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Qidi Xu"},{"location":"cls_author/#qihao-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qihao Zhu"},{"location":"cls_author/#qin-wang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Qin Wang"},{"location":"cls_author/#qingru-zhang","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note","title":"Qingru Zhang"},{"location":"cls_author/#qinyu-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Qinyu Chen"},{"location":"cls_author/#qiushi-du","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiushi Du"},{"location":"cls_author/#r-j-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. J. Chen"},{"location":"cls_author/#r-l-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. L. Jin"},{"location":"cls_author/#ramachandran-ramjee","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Ramachandran Ramjee"},{"location":"cls_author/#ramya-prabhu","text":"Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ramya Prabhu"},{"location":"cls_author/#rayan-saab","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"Rayan Saab"},{"location":"cls_author/#roy-schwartz","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Roy Schwartz"},{"location":"cls_author/#ruihang-lai","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Ruihang Lai"},{"location":"cls_author/#ruiqi-ge","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruiqi Ge"},{"location":"cls_author/#ruisong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruisong Zhang"},{"location":"cls_author/#ruitao-leng","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Ruitao Leng"},{"location":"cls_author/#ruizhe-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruizhe Pan"},{"location":"cls_author/#runji-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runji Wang"},{"location":"cls_author/#runxin-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runxin Xu"},{"location":"cls_author/#ruoyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruoyu Zhang"},{"location":"cls_author/#ruyi-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruyi Chen"},{"location":"cls_author/#s-s-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"S. S. Li"},{"location":"cls_author/#saeed-maleki","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note","title":"Saeed Maleki"},{"location":"cls_author/#sangmin-bae","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Sangmin Bae"},{"location":"cls_author/#saurav-muralidharan","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Saurav Muralidharan"},{"location":"cls_author/#sean-lie","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Sean Lie"},{"location":"cls_author/#sehoon-kim","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Sehoon Kim"},{"location":"cls_author/#shang-yang","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Shang Yang"},{"location":"cls_author/#shanghao-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanghao Lu"},{"location":"cls_author/#shangyan-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shangyan Zhou"},{"location":"cls_author/#shanhuang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanhuang Chen"},{"location":"cls_author/#shaoqing-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shaoqing Wu"},{"location":"cls_author/#shengen-yan","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shengen Yan"},{"location":"cls_author/#shengfeng-ye","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None","title":"Shengfeng Ye"},{"location":"cls_author/#shengmin-shi","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Shengmin Shi"},{"location":"cls_author/#shijie-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Shijie Cao"},{"location":"cls_author/#shirong-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shirong Ma"},{"location":"cls_author/#shiwei-liu","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Shiwei Liu"},{"location":"cls_author/#shiyao-li","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shiyao Li"},{"location":"cls_author/#shiyu-chang","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Shiyu Chang"},{"location":"cls_author/#shiyu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shiyu Wang"},{"location":"cls_author/#shreyas-saxena","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Shreyas Saxena"},{"location":"cls_author/#shuang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuang Zhou"},{"location":"cls_author/#shuiping-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuiping Yu"},{"location":"cls_author/#shunfeng-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shunfeng Zhou"},{"location":"cls_author/#shuo-yang","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Shuo Yang"},{"location":"cls_author/#shuqi-yu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Shuqi Yu"},{"location":"cls_author/#shuting-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuting Pan"},{"location":"cls_author/#sichen-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Sichen Li"},{"location":"cls_author/#size-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Size Zheng"},{"location":"cls_author/#song-han","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Song Han"},{"location":"cls_author/#songquan-zhu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Songquan Zhu"},{"location":"cls_author/#stephanie-wang","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Stephanie Wang"},{"location":"cls_author/#surin-ahn","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Surin Ahn"},{"location":"cls_author/#t-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"T. Wang"},{"location":"cls_author/#tal-schuster","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Tal Schuster"},{"location":"cls_author/#tao-xie","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Tao Xie"},{"location":"cls_author/#tao-yu","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models MXFP4Train Training LLMs with MXFP4 note","title":"Tao Yu"},{"location":"cls_author/#tao-yuan","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Tao Yuan"},{"location":"cls_author/#tao-yun","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tao Yun"},{"location":"cls_author/#tian-pei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tian Pei"},{"location":"cls_author/#tianle-cai","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Tianle Cai"},{"location":"cls_author/#tianlong-chen","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Tianlong Chen"},{"location":"cls_author/#tianqi-chen","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Tianqi Chen"},{"location":"cls_author/#tianqi-wu","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Tianqi Wu"},{"location":"cls_author/#tianrun-liang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Tianrun Liang"},{"location":"cls_author/#tianyu-fu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Tianyu Fu"},{"location":"cls_author/#tianyu-gao","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note","title":"Tianyu Gao"},{"location":"cls_author/#tianyu-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tianyu Sun"},{"location":"cls_author/#tianzhu-ye","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Tianzhu Ye"},{"location":"cls_author/#tim-dettmers","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression","title":"Tim Dettmers"},{"location":"cls_author/#ting-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Ting Cao"},{"location":"cls_author/#tong-yang","text":"Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Tong Yang"},{"location":"cls_author/#torsten-hoefler","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"Torsten Hoefler"},{"location":"cls_author/#tri-dao","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"Tri Dao"},{"location":"cls_author/#tuo-zhao","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note","title":"Tuo Zhao"},{"location":"cls_author/#vithursan-thangarasa","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Vithursan Thangarasa"},{"location":"cls_author/#w-l-xiao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"W. L. Xiao"},{"location":"cls_author/#wangding-zeng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wangding Zeng"},{"location":"cls_author/#wanjia-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wanjia Zhao"},{"location":"cls_author/#wei-an","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wei An"},{"location":"cls_author/#wei-lin","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Wei Lin"},{"location":"cls_author/#wei-wang","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Wei Wang"},{"location":"cls_author/#weigao-sun","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Weigao Sun"},{"location":"cls_author/#weilin-zhao","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Weilin Zhao"},{"location":"cls_author/#weixuan-sun","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note","title":"Weixuan Sun"},{"location":"cls_author/#weiyu-cheng","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Weiyu Cheng"},{"location":"cls_author/#weiyu-huang","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Weiyu Huang"},{"location":"cls_author/#weizhu-chen","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Weizhu Chen"},{"location":"cls_author/#wen-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wen Liu"},{"location":"cls_author/#wenfeng-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenfeng Liang"},{"location":"cls_author/#wenjun-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenjun Gao"},{"location":"cls_author/#wenkai-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Wenkai Li"},{"location":"cls_author/#wenlei-bao","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Wenlei Bao"},{"location":"cls_author/#wenqi-shao","text":"Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Wenqi Shao"},{"location":"cls_author/#wenqin-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenqin Yu"},{"location":"cls_author/#wentao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wentao Zhang"},{"location":"cls_author/#woosuk-kwon","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"Woosuk Kwon"},{"location":"cls_author/#wulong-liu","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Wulong Liu"},{"location":"cls_author/#x-q-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"X. Q. Li"},{"location":"cls_author/#xiafei-qiu","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Xiafei Qiu"},{"location":"cls_author/#xiandong-zhao","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiandong Zhao"},{"location":"cls_author/#xiang-liu","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note","title":"Xiang Liu"},{"location":"cls_author/#xiangjun-song","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xiangjun Song"},{"location":"cls_author/#xiangyu-zhang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Xiangyu Zhang"},{"location":"cls_author/#xiangyue-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiangyue Jin"},{"location":"cls_author/#xianzhi-yu","text":"Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Xianzhi Yu"},{"location":"cls_author/#xianzu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xianzu Wang"},{"location":"cls_author/#xiao-bi","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiao Bi"},{"location":"cls_author/#xiao-su","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xiao Su"},{"location":"cls_author/#xiaodong-han","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xiaodong Han"},{"location":"cls_author/#xiaodong-ji","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note","title":"Xiaodong Ji"},{"location":"cls_author/#xiaodong-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaodong Liu"},{"location":"cls_author/#xiaohan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaohan Wang"},{"location":"cls_author/#xiaojin-shen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaojin Shen"},{"location":"cls_author/#xiaokang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Chen"},{"location":"cls_author/#xiaokang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Zhang"},{"location":"cls_author/#xiaosha-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaosha Chen"},{"location":"cls_author/#xiaotao-nie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaotao Nie"},{"location":"cls_author/#xiaowei-li","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiaowei Li"},{"location":"cls_author/#xiaowen-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaowen Sun"},{"location":"cls_author/#xiaoxiang-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaoxiang Wang"},{"location":"cls_author/#xin-chen","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note","title":"Xin Chen"},{"location":"cls_author/#xin-cheng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Cheng"},{"location":"cls_author/#xin-jin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Xin Jin"},{"location":"cls_author/#xin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xin Liu"},{"location":"cls_author/#xin-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Xie"},{"location":"cls_author/#xinchao-wang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Xinchao Wang"},{"location":"cls_author/#xingchao-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingchao Liu"},{"location":"cls_author/#xingcheng-zhang","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Xingcheng Zhang"},{"location":"cls_author/#xingkai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingkai Yu"},{"location":"cls_author/#xinnan-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinnan Song"},{"location":"cls_author/#xinxia-shan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinxia Shan"},{"location":"cls_author/#xinyi-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyi Zhou"},{"location":"cls_author/#xinyu-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyu Yang"},{"location":"cls_author/#xinyu-zhou","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Xinyu Zhou"},{"location":"cls_author/#xinyuan-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyuan Li"},{"location":"cls_author/#xinzhu-hou","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xinzhu Hou"},{"location":"cls_author/#xiuhong-li","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Xiuhong Li"},{"location":"cls_author/#xu-han","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Xu Han"},{"location":"cls_author/#xu-owen-he","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Xu Owen He"},{"location":"cls_author/#xuan-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xuan Lu"},{"location":"cls_author/#xuecheng-su","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuecheng Su"},{"location":"cls_author/#xuefei-ning","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Xuefei Ning"},{"location":"cls_author/#xuegui-zheng","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xuegui Zheng"},{"location":"cls_author/#xufang-luo","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Xufang Luo"},{"location":"cls_author/#xuheng-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuheng Lin"},{"location":"cls_author/#xun-zou","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xun Zou"},{"location":"cls_author/#xuyang-shen","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Xuyang Shen"},{"location":"cls_author/#y-k-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. K. Li"},{"location":"cls_author/#y-q-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. Q. Wang"},{"location":"cls_author/#y-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note","title":"Y. Wu"},{"location":"cls_author/#y-x-wei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Wei"},{"location":"cls_author/#y-x-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Zhu"},{"location":"cls_author/#yan-gong","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yan Gong"},{"location":"cls_author/#yang-li","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note","title":"Yang Li"},{"location":"cls_author/#yang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yang Zhang"},{"location":"cls_author/#yanhong-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None","title":"Yanhong Xu"},{"location":"cls_author/#yankai-lin","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yankai Lin"},{"location":"cls_author/#yanping-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yanping Huang"},{"location":"cls_author/#yao-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Li"},{"location":"cls_author/#yao-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Zhao"},{"location":"cls_author/#yaofeng-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaofeng Sun"},{"location":"cls_author/#yaohui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Li"},{"location":"cls_author/#yaohui-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Wang"},{"location":"cls_author/#yefei-he","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Yefei He"},{"location":"cls_author/#yehui-tang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yehui Tang"},{"location":"cls_author/#yi-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Yu"},{"location":"cls_author/#yi-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Zheng"},{"location":"cls_author/#yichao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yichao Zhang"},{"location":"cls_author/#yifan-shi","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yifan Shi"},{"location":"cls_author/#yikai-zhang","text":"Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yikai Zhang"},{"location":"cls_author/#yiliang-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiliang Xiong"},{"location":"cls_author/#yilong-zhao","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note","title":"Yilong Zhao"},{"location":"cls_author/#ying-he","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying He"},{"location":"cls_author/#ying-sheng","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Ying Sheng"},{"location":"cls_author/#ying-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying Tang"},{"location":"cls_author/#yingfa-chen","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Yingfa Chen"},{"location":"cls_author/#yinhe-han","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Yinhe Han"},{"location":"cls_author/#yiran-zhong","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yiran Zhong"},{"location":"cls_author/#yishi-piao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yishi Piao"},{"location":"cls_author/#yisong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yisong Wang"},{"location":"cls_author/#yiwu-yao","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note","title":"Yiwu Yao"},{"location":"cls_author/#yixiao-li","text":"Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Yixiao Li"},{"location":"cls_author/#yixin-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note","title":"Yixin Dong"},{"location":"cls_author/#yixin-song","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Yixin Song"},{"location":"cls_author/#yixuan-tan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yixuan Tan"},{"location":"cls_author/#yiyang-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyang Ma"},{"location":"cls_author/#yiyuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyuan Liu"},{"location":"cls_author/#yiyuan-ma","text":"Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Yiyuan Ma"},{"location":"cls_author/#yizhao-gao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yizhao Gao"},{"location":"cls_author/#yong-li","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Yong Li"},{"location":"cls_author/#yongqiang-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yongqiang Guo"},{"location":"cls_author/#yongyi-hu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yongyi Hu"},{"location":"cls_author/#yu-cheng","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Yu Cheng"},{"location":"cls_author/#yu-wang","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Yu Wang"},{"location":"cls_author/#yu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yu Wu"},{"location":"cls_author/#yuan-ou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuan Ou"},{"location":"cls_author/#yuandong-tian","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Yuandong Tian"},{"location":"cls_author/#yuanxiang-fan","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yuanxiang Fan"},{"location":"cls_author/#yuchen-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuchen Zhu"},{"location":"cls_author/#yucheng-li","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yucheng Li"},{"location":"cls_author/#yuduan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuduan Wang"},{"location":"cls_author/#yue-gong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yue Gong"},{"location":"cls_author/#yuezhou-hu","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Yuezhou Hu"},{"location":"cls_author/#yufeng-yang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yufeng Yang"},{"location":"cls_author/#yuhao-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yuhao Li"},{"location":"cls_author/#yuheng-zou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuheng Zou"},{"location":"cls_author/#yuhui-xu","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Yuhui Xu"},{"location":"cls_author/#yujia-he","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yujia He"},{"location":"cls_author/#yujun-lin","text":"Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"Yujun Lin"},{"location":"cls_author/#yukun-zha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yukun Zha"},{"location":"cls_author/#yulhwa-kim","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note","title":"Yulhwa Kim"},{"location":"cls_author/#yunan-huang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yunan Huang"},{"location":"cls_author/#yunfan-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunfan Xiong"},{"location":"cls_author/#yunhe-wang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yunhe Wang"},{"location":"cls_author/#yunji-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yunji Li"},{"location":"cls_author/#yunxian-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunxian Ma"},{"location":"cls_author/#yunzhi-xu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yunzhi Xu"},{"location":"cls_author/#yuqing-xia","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yuqing Xia"},{"location":"cls_author/#yuqing-yang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Yuqing Yang"},{"location":"cls_author/#yutao-sun","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yutao Sun"},{"location":"cls_author/#yuting-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuting Yan"},{"location":"cls_author/#yuxiang-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang Luo"},{"location":"cls_author/#yuxiang-you","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang You"},{"location":"cls_author/#yuxin-mao","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Yuxin Mao"},{"location":"cls_author/#yuxin-wu","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Yuxin Wu"},{"location":"cls_author/#yuxiong-he","text":"Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Yuxiong He"},{"location":"cls_author/#yuxuan-li","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yuxuan Li"},{"location":"cls_author/#yuxuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxuan Liu"},{"location":"cls_author/#yuyang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuyang Zhou"},{"location":"cls_author/#z-f-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. F. Wu"},{"location":"cls_author/#z-z-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. Z. Ren"},{"location":"cls_author/#zefan-cai","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note","title":"Zefan Cai"},{"location":"cls_author/#zehan-li","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Zehan Li"},{"location":"cls_author/#zehui-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zehui Ren"},{"location":"cls_author/#zeping-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"Zeping Li"},{"location":"cls_author/#zeyu-mi","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Zeyu Mi"},{"location":"cls_author/#zhangli-sha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhangli Sha"},{"location":"cls_author/#zhangyang-wang","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhangyang Wang"},{"location":"cls_author/#zhe-fu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhe Fu"},{"location":"cls_author/#zhean-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhean Xu"},{"location":"cls_author/#zhen-dong","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Zhen Dong"},{"location":"cls_author/#zhen-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Huang"},{"location":"cls_author/#zhen-qin","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note","title":"Zhen Qin"},{"location":"cls_author/#zhen-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Zhang"},{"location":"cls_author/#zhenda-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhenda Xie"},{"location":"cls_author/#zhengyan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhengyan Zhang"},{"location":"cls_author/#zhenhua-fan","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Zhenhua Fan"},{"location":"cls_author/#zhenyu-zhang","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhenyu Zhang"},{"location":"cls_author/#zhewei-yao","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Zhewei Yao"},{"location":"cls_author/#zhewen-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhewen Hao"},{"location":"cls_author/#zhibin-gou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhibin Gou"},{"location":"cls_author/#zhicheng-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhicheng Ma"},{"location":"cls_author/#zhigang-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhigang Yan"},{"location":"cls_author/#zhihang-yu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Zhihang Yu"},{"location":"cls_author/#zhihang-yuan","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note","title":"Zhihang Yuan"},{"location":"cls_author/#zhihong-shao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhihong Shao"},{"location":"cls_author/#zhilin-yang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhilin Yang"},{"location":"cls_author/#zhipeng-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhipeng Xu"},{"location":"cls_author/#zhixuan-lin","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Zhixuan Lin"},{"location":"cls_author/#zhiyu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhiyu Wu"},{"location":"cls_author/#zhiyuan-liu","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zhiyuan Liu"},{"location":"cls_author/#zhongyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhongyu Zhang"},{"location":"cls_author/#zhou-yu","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Zhou Yu"},{"location":"cls_author/#zhuang-liu","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Zhuang Liu"},{"location":"cls_author/#zhuo-jiang","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Zhuo Jiang"},{"location":"cls_author/#zhuomin-he","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Zhuomin He"},{"location":"cls_author/#zhuoshu-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhuoshu Li"},{"location":"cls_author/#zihan-wang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Zihan Wang"},{"location":"cls_author/#zihao-ye","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Zihao Ye"},{"location":"cls_author/#ziheng-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ziheng Jiang"},{"location":"cls_author/#zihui-gu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zihui Gu"},{"location":"cls_author/#zijia-wu","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"Zijia Wu"},{"location":"cls_author/#zijia-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijia Zhu"},{"location":"cls_author/#zijun-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijun Liu"},{"location":"cls_author/#zili-wang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Zili Wang"},{"location":"cls_author/#zilin-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zilin Li"},{"location":"cls_author/#ziqing-yang","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"Ziqing Yang"},{"location":"cls_author/#ziwei-ji","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Ziwei Ji"},{"location":"cls_author/#ziwei-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziwei Xie"},{"location":"cls_author/#zixiao-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Zixiao Huang"},{"location":"cls_author/#zixuan-zhou","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zixuan Zhou"},{"location":"cls_author/#ziyang-song","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyang Song"},{"location":"cls_author/#ziyi-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyi Gao"},{"location":"cls_author/#zizheng-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zizheng Pan"},{"location":"cls_author/#zuchao-li","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Zuchao Li"},{"location":"cls_author/#zunhai-su","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Zunhai Su"},{"location":"cls_institution/","text":"institution AWS AI Meta Title Cover Publish Code Note MXFP4Train Training LLMs with MXFP4 note AWS AI Labs Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Adobe Research Meta Title Cover Publish Code Note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note Advanced Micro Devices Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Alibaba Cloud Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Alibaba Group Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Apple Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note IFPruning Instruction-Following Pruning for Large Language Models note Baichuan Inc Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note Beihang University Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note ByteDance Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note 52A7RO95 Mixture of Experts in Large Language Models note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ByteDance Seed Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note CMU Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note CPII under InnoHK Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Carnegie Mellon University Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note CentML Meta Title Cover Publish Code Note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note Center for Advanced AI Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Central South University Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Cerebras Systems Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Chinese Academy of Sciences Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Chinese University of Hong Kong Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note Chongqing University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor City University of Hong Kong Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note Cohere Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Comenius University Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Computer Network Information Center, Chinese Academy of Sciences Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note Cornell University Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note MXFP4Train Training LLMs with MXFP4 note DENSO IT Lab Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note DeepAuto.ai Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeepMind Meta Title Cover Publish Code Note m Fast Sparse ConvNets DeepSeek Meta Title Cover Publish Code Note DeepEP DeepEP: an efficient expert-parallel communication library note DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSpeed Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note Delft University of Technology Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Duke University Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note ETH Zurich Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Eindhoven University of Technology Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Emory University Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note FAIR Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note Fairleigh Dickinson University Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note Fudan University Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Gaoling School of Artificial Intelligence, Renmin University of China Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Georgia Institute of Technology Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note Georgia Tech Meta Title Cover Publish Code Note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note Google Meta Title Cover Publish Code Note Transformer Attention Is All You Need note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note Google Cloud Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google DeepMind Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google Research Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Graphcore Research Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note HKUST Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Habana Labs Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients Harbin Institute of Technology Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Harvard University Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Heriot-Watt University Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Hong Kong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Houmo AI Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Huawei Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note Huawei Cloud Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Huawei Noah's Ark Lab Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Huawei Technologies Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note Huazhong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Hugging Face Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning IST Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey IST Austria Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Imperial College London Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Indian Institute of Science Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Infinigence-AI Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Institute for Advanced Algorithms Research Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Institute of Automation, Chinese Academy of Sciences Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Institute of Computing Technology Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Institute of Computing Technology, Chinese Academy of Sciences Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Institute of Information Engineering, Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Intel Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note Intel Corporation Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration Intellifusion Inc. Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KAIST Meta Title Cover Publish Code Note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note KAIST AI Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note KAUST Meta Title Cover Publish Code Note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note KTH Royal Institute of Technology Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Key Laboratory of Multimedia Trusted Perception and Efficient Computing Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Kyushu University Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Lanzhou University Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note Leiden University Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note MBZUAI Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note MIT Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note MIT-IBM Watson AI Lab Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note MakerMaker AI Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Massachusetts Institute of Technology Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note Megvii Technology Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Meituan Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Meta Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Meta AI Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Meta AI (FAIR) Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Meta Platforms Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Michigan State University Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Microsoft Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Microsoft Azure Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Microsoft Azure AI Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning Microsoft Research Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Microsoft Research India Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Mila & Universite de Montreal Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MiniCPM Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note Ministry of Education of China, Xiamen University Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Mohamed bin Zayed University of AI Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Monash University Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note Moonshot AI Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Multimedia Laboratory (MMLab) Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note NAVER Cloud Meta Title Cover Publish Code Note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note NVIDIA Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress: LLM KV cache compression made easy note NVIDIA Research Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note NanKai University Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Nanjing University Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Nanyang Technological University Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note National University of Singapore Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Neural Magic Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note New York University Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note Noah\u2019s Ark Lab, Huawei Technologies Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Normal Computing Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note North China Electric Power University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Northeastern University Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers Northwestern University Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Numenta Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note OPPO Research Institute Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note Ohio State University Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note OpenAI Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights OpenGVLab Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models OpenNLPLab Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note OpenTeams Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Oxford University Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note Peking University Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Perplexity AI Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Princeton University Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note Purdue University Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note 52A7RO95 Mixture of Experts in Large Language Models note PyTorch Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note Qwen Team Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note Renmin University of China Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Rice University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note RiseAI-Sys Meta Title Cover Publish Code Note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note SJTU Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Salesforce AI Research Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Salesforce Research Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note Samsung Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Samsung AI Center Meta Title Cover Publish Code Note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note Santa Clara University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note School of Cyber Security, University of Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models SenseTime Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SenseTime Research Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Seoul National University Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note Shanghai AI Lab Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note Shanghai AI Laboratory Meta Title Cover Publish Code Note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Shanghai Artificial Intelligence Laboratory Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Shanghai Artificial Intelligence Laboratorys Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Shanghai Jiao Tong University Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Shanghai Jiaotong University Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ShanghaiTech University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS) Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note Singapore University of Technology and Design Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Sogang University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note Soochow University Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Stanford Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Stanford University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note StepFun Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Stevens Institute of Technology Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note Sun Yat-sen University Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Sungkyunkwan University Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note Synthesia Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Tencent AI Lab Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Tencent Machine Learning Platform Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Tencent Youtu Lab Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Texas A&M University Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note The Chinese University of Hong Kong Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note The Hebrew University of Jerusalem Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note The Hebrew University of Jerusalem, Israel Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey The Hong Kong Polytechnic University Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note The Hong Kong University of Science and Technology Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note The Ohio State University Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note The University of Adelaide Meta Title Cover Publish Code Note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note The University of Hong Kong Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note The University of North Carolina Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note The University of Texas at Austin Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Together AI Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note Tongji University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Tsinghua University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note UC Berkeley Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note UC Santa Barbara Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note UCSD Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Univeristy of Sydney Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note Universidade da Coru\u00f1a Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Universidade de Lisboa Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note University College London Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note University of Basel Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers University of California Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note University of California, Berkeley Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note University of California, Riverside Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note University of California, San Diego Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note University of Cambridge, United Kingdom Meta Title Cover Publish Code Note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note University of Chinese Academy of Sciences Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note University of Connecticut Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm University of Edinburgh Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note University of Electronic Science and Technology of China Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction University of Hong Kong Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note University of Illinois Urbana-Champaign Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note University of Illinois at Urbana-Champaign Meta Title Cover Publish Code Note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note University of Macau Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note University of Maryland Meta Title Cover Publish Code Note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note University of Massachusetts Amherst Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note University of Oxford Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note University of Science and Technology Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note University of Science and Technology of China Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note University of Seoul Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note University of Southern California Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note University of St Andrews Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note University of Surrey Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Surrey, UK Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering University of Texas at Austin Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity University of Toronto Meta Title Cover Publish Code Note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note University of Washington Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Splitwise Splitwise: Efficient generative LLM inference using phase splitting note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note University of Waterloo Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note University of Wisconsin Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Wisconsin-Madison Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note VITA Group Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Vector Institute Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note Vizuara AI Labs Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note Vokram Group Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note WeChat AI Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note Wuhan University Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Xi'an Jiaotong University Meta Title Cover Publish Code Note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note Xiamen University Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Xiaohongshu Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Xiaomi Meta Title Cover Publish Code Note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Yale University Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences Zhe Jiang University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Zhejiang University Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipu.AI Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Zhongguancun Laboratory Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning baidu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note iFLYTEK Research Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note inst1 Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note inst2 Meta Title Cover Publish Code Note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"By Institution"},{"location":"cls_institution/#institution","text":"","title":"institution"},{"location":"cls_institution/#aws-ai","text":"Meta Title Cover Publish Code Note MXFP4Train Training LLMs with MXFP4 note","title":"AWS AI"},{"location":"cls_institution/#aws-ai-labs","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AWS AI Labs"},{"location":"cls_institution/#adobe-research","text":"Meta Title Cover Publish Code Note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note","title":"Adobe Research"},{"location":"cls_institution/#advanced-micro-devices","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"Advanced Micro Devices"},{"location":"cls_institution/#alibaba-cloud","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Alibaba Cloud"},{"location":"cls_institution/#alibaba-group","text":"Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Alibaba Group"},{"location":"cls_institution/#apple","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note IFPruning Instruction-Following Pruning for Large Language Models note","title":"Apple"},{"location":"cls_institution/#baichuan-inc","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note","title":"Baichuan Inc"},{"location":"cls_institution/#beihang-university","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note","title":"Beihang University"},{"location":"cls_institution/#bytedance","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note 52A7RO95 Mixture of Experts in Large Language Models note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"ByteDance"},{"location":"cls_institution/#bytedance-seed","text":"Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"ByteDance Seed"},{"location":"cls_institution/#cmu","text":"Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note","title":"CMU"},{"location":"cls_institution/#cpii-under-innohk","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"CPII under InnoHK"},{"location":"cls_institution/#carnegie-mellon-university","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Carnegie Mellon University"},{"location":"cls_institution/#centml","text":"Meta Title Cover Publish Code Note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note","title":"CentML"},{"location":"cls_institution/#center-for-advanced-ai","text":"Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Center for Advanced AI"},{"location":"cls_institution/#central-south-university","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Central South University"},{"location":"cls_institution/#cerebras-systems","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Cerebras Systems"},{"location":"cls_institution/#chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note","title":"Chinese Academy of Sciences"},{"location":"cls_institution/#chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note","title":"Chinese University of Hong Kong"},{"location":"cls_institution/#chongqing-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor","title":"Chongqing University"},{"location":"cls_institution/#city-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"City University of Hong Kong"},{"location":"cls_institution/#cohere","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Cohere"},{"location":"cls_institution/#comenius-university","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"Comenius University"},{"location":"cls_institution/#computer-network-information-center-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note","title":"Computer Network Information Center, Chinese Academy of Sciences"},{"location":"cls_institution/#cornell-university","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note MXFP4Train Training LLMs with MXFP4 note","title":"Cornell University"},{"location":"cls_institution/#denso-it-lab","text":"Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note","title":"DENSO IT Lab"},{"location":"cls_institution/#deepautoai","text":"Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"DeepAuto.ai"},{"location":"cls_institution/#deepmind","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets","title":"DeepMind"},{"location":"cls_institution/#deepseek","text":"Meta Title Cover Publish Code Note DeepEP DeepEP: an efficient expert-parallel communication library note","title":"DeepSeek"},{"location":"cls_institution/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_institution/#deepspeed","text":"Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note","title":"DeepSpeed"},{"location":"cls_institution/#delft-university-of-technology","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Delft University of Technology"},{"location":"cls_institution/#duke-university","text":"Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Duke University"},{"location":"cls_institution/#eth-zurich","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"ETH Zurich"},{"location":"cls_institution/#eindhoven-university-of-technology","text":"Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Eindhoven University of Technology"},{"location":"cls_institution/#emory-university","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note","title":"Emory University"},{"location":"cls_institution/#fair","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"FAIR"},{"location":"cls_institution/#fairleigh-dickinson-university","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"Fairleigh Dickinson University"},{"location":"cls_institution/#fudan-university","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Fudan University"},{"location":"cls_institution/#gaoling-school-of-artificial-intelligence-renmin-university-of-china","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Gaoling School of Artificial Intelligence, Renmin University of China"},{"location":"cls_institution/#georgia-institute-of-technology","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note","title":"Georgia Institute of Technology"},{"location":"cls_institution/#georgia-tech","text":"Meta Title Cover Publish Code Note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note","title":"Georgia Tech"},{"location":"cls_institution/#google","text":"Meta Title Cover Publish Code Note Transformer Attention Is All You Need note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note","title":"Google"},{"location":"cls_institution/#google-cloud","text":"Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Cloud"},{"location":"cls_institution/#google-deepmind","text":"Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google DeepMind"},{"location":"cls_institution/#google-research","text":"Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Research"},{"location":"cls_institution/#graphcore-research","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Graphcore Research"},{"location":"cls_institution/#hkust","text":"Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"HKUST"},{"location":"cls_institution/#habana-labs","text":"Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients","title":"Habana Labs"},{"location":"cls_institution/#harbin-institute-of-technology","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Harbin Institute of Technology"},{"location":"cls_institution/#harvard-university","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"Harvard University"},{"location":"cls_institution/#heriot-watt-university","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Heriot-Watt University"},{"location":"cls_institution/#hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note","title":"Hong Kong University of Science and Technology"},{"location":"cls_institution/#houmo-ai","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Houmo AI"},{"location":"cls_institution/#huawei","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note","title":"Huawei"},{"location":"cls_institution/#huawei-cloud","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Huawei Cloud"},{"location":"cls_institution/#huawei-noahs-ark-lab","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Huawei Noah's Ark Lab"},{"location":"cls_institution/#huawei-technologies","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"Huawei Technologies"},{"location":"cls_institution/#huazhong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Huazhong University of Science and Technology"},{"location":"cls_institution/#hugging-face","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning","title":"Hugging Face"},{"location":"cls_institution/#ist","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"IST"},{"location":"cls_institution/#ist-austria","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"IST Austria"},{"location":"cls_institution/#imperial-college-london","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Imperial College London"},{"location":"cls_institution/#indian-institute-of-science","text":"Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Indian Institute of Science"},{"location":"cls_institution/#infinigence-ai","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Infinigence-AI"},{"location":"cls_institution/#institute-for-advanced-algorithms-research","text":"Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Institute for Advanced Algorithms Research"},{"location":"cls_institution/#institute-of-automation-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Institute of Automation, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-computing-technology","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Institute of Computing Technology"},{"location":"cls_institution/#institute-of-computing-technology-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Institute of Computing Technology, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-information-engineering-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Institute of Information Engineering, Chinese Academy of Sciences"},{"location":"cls_institution/#intel","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note","title":"Intel"},{"location":"cls_institution/#intel-corporation","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"Intel Corporation"},{"location":"cls_institution/#intellifusion-inc","text":"Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note","title":"Intellifusion Inc."},{"location":"cls_institution/#kaist","text":"Meta Title Cover Publish Code Note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"KAIST"},{"location":"cls_institution/#kaist-ai","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"KAIST AI"},{"location":"cls_institution/#kaust","text":"Meta Title Cover Publish Code Note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note","title":"KAUST"},{"location":"cls_institution/#kth-royal-institute-of-technology","text":"Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"KTH Royal Institute of Technology"},{"location":"cls_institution/#key-laboratory-of-multimedia-trusted-perception-and-efficient-computing","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Key Laboratory of Multimedia Trusted Perception and Efficient Computing"},{"location":"cls_institution/#kyushu-university","text":"Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note","title":"Kyushu University"},{"location":"cls_institution/#lanzhou-university","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note","title":"Lanzhou University"},{"location":"cls_institution/#leiden-university","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Leiden University"},{"location":"cls_institution/#mbzuai","text":"Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"MBZUAI"},{"location":"cls_institution/#mit","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"MIT"},{"location":"cls_institution/#mit-ibm-watson-ai-lab","text":"Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note","title":"MIT-IBM Watson AI Lab"},{"location":"cls_institution/#makermaker-ai","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"MakerMaker AI"},{"location":"cls_institution/#massachusetts-institute-of-technology","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"Massachusetts Institute of Technology"},{"location":"cls_institution/#megvii-technology","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note","title":"Megvii Technology"},{"location":"cls_institution/#meituan","text":"Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Meituan"},{"location":"cls_institution/#meta","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Meta"},{"location":"cls_institution/#meta-ai","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Meta AI"},{"location":"cls_institution/#meta-ai-fair","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"Meta AI (FAIR)"},{"location":"cls_institution/#meta-platforms-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"Meta Platforms Inc"},{"location":"cls_institution/#michigan-state-university","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Michigan State University"},{"location":"cls_institution/#microsoft","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Microsoft"},{"location":"cls_institution/#microsoft-azure","text":"Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Microsoft Azure"},{"location":"cls_institution/#microsoft-azure-ai","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning","title":"Microsoft Azure AI"},{"location":"cls_institution/#microsoft-research","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Microsoft Research"},{"location":"cls_institution/#microsoft-research-india","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Microsoft Research India"},{"location":"cls_institution/#mila-universite-de-montreal","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Mila &amp; Universite de Montreal"},{"location":"cls_institution/#minicpm","text":"Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"MiniCPM"},{"location":"cls_institution/#minimax","text":"Meta Title Cover Publish Code Note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note","title":"MiniMax"},{"location":"cls_institution/#ministry-of-education-of-china-xiamen-university","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Ministry of Education of China, Xiamen University"},{"location":"cls_institution/#mohamed-bin-zayed-university-of-ai","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note","title":"Mohamed bin Zayed University of AI"},{"location":"cls_institution/#monash-university","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"Monash University"},{"location":"cls_institution/#moonshot-ai","text":"Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Moonshot AI"},{"location":"cls_institution/#multimedia-laboratory-mmlab","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Multimedia Laboratory (MMLab)"},{"location":"cls_institution/#naver-cloud","text":"Meta Title Cover Publish Code Note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note","title":"NAVER Cloud"},{"location":"cls_institution/#nvidia","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress: LLM KV cache compression made easy note","title":"NVIDIA"},{"location":"cls_institution/#nvidia-research","text":"Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"NVIDIA Research"},{"location":"cls_institution/#nankai-university","text":"Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note","title":"NanKai University"},{"location":"cls_institution/#nanjing-university","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Nanjing University"},{"location":"cls_institution/#nanyang-technological-university","text":"Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Nanyang Technological University"},{"location":"cls_institution/#national-university-of-singapore","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"National University of Singapore"},{"location":"cls_institution/#neural-magic","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Neural Magic"},{"location":"cls_institution/#new-york-university","text":"Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note","title":"New York University"},{"location":"cls_institution/#noahs-ark-lab-huawei-technologies","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Noah\u2019s Ark Lab, Huawei Technologies"},{"location":"cls_institution/#normal-computing","text":"Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note","title":"Normal Computing"},{"location":"cls_institution/#north-china-electric-power-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"North China Electric Power University"},{"location":"cls_institution/#northeastern-university","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"Northeastern University"},{"location":"cls_institution/#northwestern-university","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"Northwestern University"},{"location":"cls_institution/#numenta","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Numenta"},{"location":"cls_institution/#oppo-research-institute","text":"Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"OPPO Research Institute"},{"location":"cls_institution/#ohio-state-university","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Ohio State University"},{"location":"cls_institution/#openai","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights","title":"OpenAI"},{"location":"cls_institution/#opengvlab","text":"Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models","title":"OpenGVLab"},{"location":"cls_institution/#opennlplab","text":"Meta Title Cover Publish Code Note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note","title":"OpenNLPLab"},{"location":"cls_institution/#openteams-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"OpenTeams Inc"},{"location":"cls_institution/#oxford-university","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"Oxford University"},{"location":"cls_institution/#peking-university","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Peking University"},{"location":"cls_institution/#perplexity-ai","text":"Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Perplexity AI"},{"location":"cls_institution/#princeton-university","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note","title":"Princeton University"},{"location":"cls_institution/#purdue-university","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Purdue University"},{"location":"cls_institution/#pytorch","text":"Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"PyTorch"},{"location":"cls_institution/#qwen-team","text":"Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note","title":"Qwen Team"},{"location":"cls_institution/#renmin-university-of-china","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Renmin University of China"},{"location":"cls_institution/#rice-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note","title":"Rice University"},{"location":"cls_institution/#riseai-sys","text":"Meta Title Cover Publish Code Note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note","title":"RiseAI-Sys"},{"location":"cls_institution/#sjtu","text":"Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"SJTU"},{"location":"cls_institution/#salesforce-ai-research","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Salesforce AI Research"},{"location":"cls_institution/#salesforce-research","text":"Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"Salesforce Research"},{"location":"cls_institution/#samsung","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note","title":"Samsung"},{"location":"cls_institution/#samsung-ai-center","text":"Meta Title Cover Publish Code Note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note","title":"Samsung AI Center"},{"location":"cls_institution/#santa-clara-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Santa Clara University"},{"location":"cls_institution/#school-of-cyber-security-university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"School of Cyber Security, University of Chinese Academy of Sciences"},{"location":"cls_institution/#sensetime","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"SenseTime"},{"location":"cls_institution/#sensetime-research","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"SenseTime Research"},{"location":"cls_institution/#seoul-national-university","text":"Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note","title":"Seoul National University"},{"location":"cls_institution/#shanghai-ai-lab","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note","title":"Shanghai AI Lab"},{"location":"cls_institution/#shanghai-ai-laboratory","text":"Meta Title Cover Publish Code Note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Shanghai AI Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratory","text":"Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Shanghai Artificial Intelligence Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratorys","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Shanghai Artificial Intelligence Laboratorys"},{"location":"cls_institution/#shanghai-jiao-tong-university","text":"Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Shanghai Jiao Tong University"},{"location":"cls_institution/#shanghai-jiaotong-university","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"Shanghai Jiaotong University"},{"location":"cls_institution/#shanghaitech-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"ShanghaiTech University"},{"location":"cls_institution/#shenzhen-institutes-of-advanced-technologysiat-chinese-academy-of-sciencecas","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS)"},{"location":"cls_institution/#singapore-university-of-technology-and-design","text":"Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Singapore University of Technology and Design"},{"location":"cls_institution/#sogang-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Sogang University"},{"location":"cls_institution/#soochow-university","text":"Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"Soochow University"},{"location":"cls_institution/#stanford","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Stanford"},{"location":"cls_institution/#stanford-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note","title":"Stanford University"},{"location":"cls_institution/#stepfun","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"StepFun"},{"location":"cls_institution/#stevens-institute-of-technology","text":"Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note","title":"Stevens Institute of Technology"},{"location":"cls_institution/#sun-yat-sen-university","text":"Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Sun Yat-sen University"},{"location":"cls_institution/#sungkyunkwan-university","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"Sungkyunkwan University"},{"location":"cls_institution/#synthesia","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Synthesia"},{"location":"cls_institution/#tencent-ai-lab","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Tencent AI Lab"},{"location":"cls_institution/#tencent-machine-learning-platform","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Tencent Machine Learning Platform"},{"location":"cls_institution/#tencent-youtu-lab","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Tencent Youtu Lab"},{"location":"cls_institution/#texas-am-university","text":"Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note","title":"Texas A&amp;M University"},{"location":"cls_institution/#the-chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"The Chinese University of Hong Kong"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"The Hebrew University of Jerusalem"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem-israel","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"The Hebrew University of Jerusalem, Israel"},{"location":"cls_institution/#the-hong-kong-polytechnic-university","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note","title":"The Hong Kong Polytechnic University"},{"location":"cls_institution/#the-hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note","title":"The Hong Kong University of Science and Technology"},{"location":"cls_institution/#the-ohio-state-university","text":"Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note","title":"The Ohio State University"},{"location":"cls_institution/#the-university-of-adelaide","text":"Meta Title Cover Publish Code Note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"The University of Adelaide"},{"location":"cls_institution/#the-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"The University of Hong Kong"},{"location":"cls_institution/#the-university-of-north-carolina","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"The University of North Carolina"},{"location":"cls_institution/#the-university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"The University of Texas at Austin"},{"location":"cls_institution/#together-ai","text":"Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"Together AI"},{"location":"cls_institution/#tongji-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note","title":"Tongji University"},{"location":"cls_institution/#tsinghua-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Tsinghua University"},{"location":"cls_institution/#uc-berkeley","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"UC Berkeley"},{"location":"cls_institution/#uc-santa-barbara","text":"Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"UC Santa Barbara"},{"location":"cls_institution/#ucsd","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"UCSD"},{"location":"cls_institution/#univeristy-of-sydney","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"Univeristy of Sydney"},{"location":"cls_institution/#universidade-da-coruna","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"Universidade da Coru\u00f1a"},{"location":"cls_institution/#universidade-de-lisboa","text":"Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Universidade de Lisboa"},{"location":"cls_institution/#university-college-london","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"University College London"},{"location":"cls_institution/#university-of-basel","text":"Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers","title":"University of Basel"},{"location":"cls_institution/#university-of-california","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"University of California"},{"location":"cls_institution/#university-of-california-berkeley","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"University of California, Berkeley"},{"location":"cls_institution/#university-of-california-riverside","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note","title":"University of California, Riverside"},{"location":"cls_institution/#university-of-california-san-diego","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"University of California, San Diego"},{"location":"cls_institution/#university-of-cambridge-united-kingdom","text":"Meta Title Cover Publish Code Note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note","title":"University of Cambridge, United Kingdom"},{"location":"cls_institution/#university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"University of Chinese Academy of Sciences"},{"location":"cls_institution/#university-of-connecticut","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm","title":"University of Connecticut"},{"location":"cls_institution/#university-of-edinburgh","text":"Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"University of Edinburgh"},{"location":"cls_institution/#university-of-electronic-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction","title":"University of Electronic Science and Technology of China"},{"location":"cls_institution/#university-of-hong-kong","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note","title":"University of Hong Kong"},{"location":"cls_institution/#university-of-illinois-urbana-champaign","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note","title":"University of Illinois Urbana-Champaign"},{"location":"cls_institution/#university-of-illinois-at-urbana-champaign","text":"Meta Title Cover Publish Code Note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note","title":"University of Illinois at Urbana-Champaign"},{"location":"cls_institution/#university-of-macau","text":"Meta Title Cover Publish Code Note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note","title":"University of Macau"},{"location":"cls_institution/#university-of-maryland","text":"Meta Title Cover Publish Code Note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"University of Maryland"},{"location":"cls_institution/#university-of-massachusetts-amherst","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note","title":"University of Massachusetts Amherst"},{"location":"cls_institution/#university-of-oxford","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"University of Oxford"},{"location":"cls_institution/#university-of-science-and-technology","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"University of Science and Technology"},{"location":"cls_institution/#university-of-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"University of Science and Technology of China"},{"location":"cls_institution/#university-of-seoul","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"University of Seoul"},{"location":"cls_institution/#university-of-southern-california","text":"Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"University of Southern California"},{"location":"cls_institution/#university-of-st-andrews","text":"Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note","title":"University of St Andrews"},{"location":"cls_institution/#university-of-surrey","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Surrey"},{"location":"cls_institution/#university-of-surrey-uk","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering","title":"University of Surrey, UK"},{"location":"cls_institution/#university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"University of Texas at Austin"},{"location":"cls_institution/#university-of-toronto","text":"Meta Title Cover Publish Code Note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note","title":"University of Toronto"},{"location":"cls_institution/#university-of-washington","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Splitwise Splitwise: Efficient generative LLM inference using phase splitting note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"University of Washington"},{"location":"cls_institution/#university-of-waterloo","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note","title":"University of Waterloo"},{"location":"cls_institution/#university-of-wisconsin","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Wisconsin"},{"location":"cls_institution/#university-of-wisconsin-madison","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note","title":"University of Wisconsin-Madison"},{"location":"cls_institution/#vita-group","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter","title":"VITA Group"},{"location":"cls_institution/#vector-institute","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note","title":"Vector Institute"},{"location":"cls_institution/#vizuara-ai-labs","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"Vizuara AI Labs"},{"location":"cls_institution/#vokram-group","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Vokram Group"},{"location":"cls_institution/#wechat-ai","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note","title":"WeChat AI"},{"location":"cls_institution/#wuhan-university","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Wuhan University"},{"location":"cls_institution/#xian-jiaotong-university","text":"Meta Title Cover Publish Code Note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note","title":"Xi'an Jiaotong University"},{"location":"cls_institution/#xiamen-university","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note","title":"Xiamen University"},{"location":"cls_institution/#xiaohongshu","text":"Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Xiaohongshu"},{"location":"cls_institution/#xiaomi","text":"Meta Title Cover Publish Code Note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Xiaomi"},{"location":"cls_institution/#yale-university","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences","title":"Yale University"},{"location":"cls_institution/#zhe-jiang-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"Zhe Jiang University"},{"location":"cls_institution/#zhejiang-university","text":"Meta Title Cover Publish Code Note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhejiang University"},{"location":"cls_institution/#zhipuai","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Zhipu.AI"},{"location":"cls_institution/#zhongguancun-laboratory","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning","title":"Zhongguancun Laboratory"},{"location":"cls_institution/#baidu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note","title":"baidu"},{"location":"cls_institution/#iflytek-research","text":"Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"iFLYTEK Research"},{"location":"cls_institution/#inst1","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"inst1"},{"location":"cls_institution/#inst2","text":"Meta Title Cover Publish Code Note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note","title":"inst2"},{"location":"cls_keyword/","text":"keyword 01-Sparsity (Attention) Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note 02-Sparsity (Activation) Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note 03-Sparsity (Weight) Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note 04-Sparsity (Structured) Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note 05-Sparse/Pruning Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note 06-Quantization (KV Cache) Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note 07-Quantization Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note MXFP4Train Training LLMs with MXFP4 note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note 08-Communication-Computation Overlap Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note DeepEP DeepEP: an efficient expert-parallel communication library note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note 09-Performance Modeling Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note 10-LLM Deployment Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note 11-Survey Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note 52A7RO95 Mixture of Experts in Large Language Models note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note 12-Network Structure Design Meta Title Cover Publish Code Note Transformer Attention Is All You Need note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note 13-Low Rank Decomposition Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note 14-KV Cache Optimization/Efficient Attention Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note 15-Layer Fusion (Reduce IO) Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note 16-Efficient Training Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note MXFP4Train Training LLMs with MXFP4 note 17-Tool Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"By Keyword"},{"location":"cls_keyword/#keyword","text":"","title":"keyword"},{"location":"cls_keyword/#01-sparsity-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"01-Sparsity (Attention)"},{"location":"cls_keyword/#02-sparsity-activation","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note","title":"02-Sparsity (Activation)"},{"location":"cls_keyword/#03-sparsity-weight","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"03-Sparsity (Weight)"},{"location":"cls_keyword/#04-sparsity-structured","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note","title":"04-Sparsity (Structured)"},{"location":"cls_keyword/#05-sparsepruning","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note","title":"05-Sparse/Pruning"},{"location":"cls_keyword/#06-quantization-kv-cache","text":"Meta Title Cover Publish Code Note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note","title":"06-Quantization (KV Cache)"},{"location":"cls_keyword/#07-quantization","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note MXFP4Train Training LLMs with MXFP4 note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note","title":"07-Quantization"},{"location":"cls_keyword/#08-communication-computation-overlap","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note DeepEP DeepEP: an efficient expert-parallel communication library note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"08-Communication-Computation Overlap"},{"location":"cls_keyword/#09-performance-modeling","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"09-Performance Modeling"},{"location":"cls_keyword/#10-llm-deployment","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"10-LLM Deployment"},{"location":"cls_keyword/#11-survey","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note 52A7RO95 Mixture of Experts in Large Language Models note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"11-Survey"},{"location":"cls_keyword/#12-network-structure-design","text":"Meta Title Cover Publish Code Note Transformer Attention Is All You Need note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"12-Network Structure Design"},{"location":"cls_keyword/#13-low-rank-decomposition","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"13-Low Rank Decomposition"},{"location":"cls_keyword/#14-kv-cache-optimizationefficient-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"14-KV Cache Optimization/Efficient Attention"},{"location":"cls_keyword/#15-layer-fusion-reduce-io","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"15-Layer Fusion (Reduce IO)"},{"location":"cls_keyword/#16-efficient-training","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note MXFP4Train Training LLMs with MXFP4 note","title":"16-Efficient Training"},{"location":"cls_keyword/#17-tool","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"17-Tool"},{"location":"cls_publication/","text":"publication AAAI Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note ACL Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note AISTATS Meta Title Cover Publish Code Note MXFP4Train Training LLMs with MXFP4 note ASPLOS Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note ATC Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AutoML Workshop Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Blog Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note COLM Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note CVPR Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note CVPR workshop Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Coling Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note DATE Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note ECCV Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers ENLSP Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note EuroSys Meta Title Cover Publish Code Note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note ICCV Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration ICLR Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note ICLR oral Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models ICML Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note ICML Workshop Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note ISCA Meta Title Cover Publish Code Note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note JMLR Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks KDD Workshop Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note MICRO Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation MLSys Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note NIPS Meta Title Cover Publish Code Note Transformer Attention Is All You Need note NeurIPS Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note Neuromorphic Computing and Engineering Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note OSDI Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note SC Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SIGMOD Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note SOSP Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note TACL Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TC Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note TMLR Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note UAI Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models VLDB Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note VLSI Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers arXiv Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note massive-activations Massive Activations in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note github Meta Title Cover Publish Code Note FT FasterTransformer attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"By Publication"},{"location":"cls_publication/#publication","text":"","title":"publication"},{"location":"cls_publication/#aaai","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note","title":"AAAI"},{"location":"cls_publication/#acl","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note","title":"ACL"},{"location":"cls_publication/#aistats","text":"Meta Title Cover Publish Code Note MXFP4Train Training LLMs with MXFP4 note","title":"AISTATS"},{"location":"cls_publication/#asplos","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"ASPLOS"},{"location":"cls_publication/#atc","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note","title":"ATC"},{"location":"cls_publication/#automl-workshop","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AutoML Workshop"},{"location":"cls_publication/#blog","text":"Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"Blog"},{"location":"cls_publication/#colm","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note","title":"COLM"},{"location":"cls_publication/#cvpr","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note","title":"CVPR"},{"location":"cls_publication/#cvpr-workshop","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine","title":"CVPR workshop"},{"location":"cls_publication/#coling","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note","title":"Coling"},{"location":"cls_publication/#date","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"DATE"},{"location":"cls_publication/#eccv","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"ECCV"},{"location":"cls_publication/#enlsp","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note","title":"ENLSP"},{"location":"cls_publication/#eurosys","text":"Meta Title Cover Publish Code Note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"EuroSys"},{"location":"cls_publication/#iccv","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"ICCV"},{"location":"cls_publication/#iclr","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"ICLR"},{"location":"cls_publication/#iclr-oral","text":"Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"ICLR oral"},{"location":"cls_publication/#icml","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"ICML"},{"location":"cls_publication/#icml-workshop","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"ICML Workshop"},{"location":"cls_publication/#isca","text":"Meta Title Cover Publish Code Note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"ISCA"},{"location":"cls_publication/#jmlr","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks","title":"JMLR"},{"location":"cls_publication/#kdd-workshop","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"KDD Workshop"},{"location":"cls_publication/#micro","text":"Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation","title":"MICRO"},{"location":"cls_publication/#mlsys","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"MLSys"},{"location":"cls_publication/#nips","text":"Meta Title Cover Publish Code Note Transformer Attention Is All You Need note","title":"NIPS"},{"location":"cls_publication/#neurips","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note","title":"NeurIPS"},{"location":"cls_publication/#neuromorphic-computing-and-engineering","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Neuromorphic Computing and Engineering"},{"location":"cls_publication/#osdi","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note","title":"OSDI"},{"location":"cls_publication/#sc","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"SC"},{"location":"cls_publication/#sigmod","text":"Meta Title Cover Publish Code Note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note","title":"SIGMOD"},{"location":"cls_publication/#sosp","text":"Meta Title Cover Publish Code Note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note","title":"SOSP"},{"location":"cls_publication/#tacl","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"TACL"},{"location":"cls_publication/#tc","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"TC"},{"location":"cls_publication/#tmlr","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"TMLR"},{"location":"cls_publication/#uai","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models","title":"UAI"},{"location":"cls_publication/#vldb","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"VLDB"},{"location":"cls_publication/#vlsi","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers","title":"VLSI"},{"location":"cls_publication/#arxiv","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note massive-activations Massive Activations in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"arXiv"},{"location":"cls_publication/#github","text":"Meta Title Cover Publish Code Note FT FasterTransformer attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"github"},{"location":"cls_year/","text":"year 2026 Meta Title Cover Publish Code Note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note 2025 Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note MXFP4Train Training LLMs with MXFP4 note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note 2024 Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note massive-activations Massive Activations in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note 2023 Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer 2022 Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models 2021 Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 2020 Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights 2019 Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training 2018 Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers 2017 Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks Transformer Attention Is All You Need note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon 2016 Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding 1993 Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning 1989 Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"By Year"},{"location":"cls_year/#year","text":"","title":"year"},{"location":"cls_year/#2026","text":"Meta Title Cover Publish Code Note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"2026"},{"location":"cls_year/#2025","text":"Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note QJL QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note MXFP4Train Training LLMs with MXFP4 note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note CacheBlend CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TidalDecode TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note PQCache PQCache: Product Quantization-based KVCache for Long Context LLM Inference note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DReSS DReSS: Data-driven Regularized Structured Streamlining for Large Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FSA Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note 209M5GA7 KV Cache Compression for Inference Efficiency in LLMs: A Review note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KVmix KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MiniMax-01 MiniMax-01: Scaling Foundation Models with Lightning Attention note MiniMax-M1 MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note RetroAttention Retrospective Sparse Attention for Efficient Long-Context Generation note RotateKV RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SageAttention3 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SlimInfer SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning note Awesome-Efficient-Arch Speed Always Wins: A Survey on Efficient Architectures for Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note T\u00fdr-the-Pruner T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note attention-gym Attention-Gym: Triton-Based Sparse and Quantization Attention note DeepEP DeepEP: an efficient expert-parallel communication library note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress: LLM KV cache compression made easy note","title":"2025"},{"location":"cls_year/#2024","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note Centauri Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note KIVI KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note LightningAttention Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention note Splitwise Splitwise: Efficient generative LLM inference using phase splitting note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ZipCache ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note GEAR GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note LazyLLM LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference note LightningAttention-2 Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models note massive-activations Massive Activations in Large Language Models note MiniCache MiniCache: KV Cache Compression in Depth Dimension for Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note MiKV No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note PrefixQuant PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SageAttention2 SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization note SageAttention SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note ZipVL ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification note","title":"2024"},{"location":"cls_year/#2023","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note Dist-Einsum Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note PagedAttention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer","title":"2023"},{"location":"cls_year/#2022","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models","title":"2022"},{"location":"cls_year/#2021","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks","title":"2021"},{"location":"cls_year/#2020","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights","title":"2020"},{"location":"cls_year/#2019","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training","title":"2019"},{"location":"cls_year/#2018","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"2018"},{"location":"cls_year/#2017","text":"Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks Transformer Attention Is All You Need note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon","title":"2017"},{"location":"cls_year/#2016","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","title":"2016"},{"location":"cls_year/#1993","text":"Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning","title":"1993"},{"location":"cls_year/#1989","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"1989"},{"location":"contributors/","text":"Contributors \u00d7 View GitHub Profile :root { --progress-color: #4caf50; /* \u53ef\u8c03\u4e3b\u9898\u989c\u8272 */ } .contributor { text-align: center; transition: transform 0.2s; cursor: pointer; position: relative; border-radius: 12px; padding: 8px; } .contributor:hover { transform: scale(1.08); } .contributor img { border-radius: 50%; display: block; margin: 0 auto; transition: transform 0.2s; } .tooltip { visibility: hidden; background-color: rgba(0,0,0,0.8); color: #fff; text-align: center; border-radius: 5px; padding: 4px 8px; position: absolute; bottom: 100%; left: 50%; transform: translateX(-50%); white-space: nowrap; font-size: 12px; opacity: 0; transition: opacity 0.2s; z-index: 1; } .contributor:hover .tooltip { visibility: visible; opacity: 1; } /* \u7f51\u683c\u5e03\u5c40\u54cd\u5e94\u5f0f */ #contributors > div { display: grid; grid-template-columns: repeat(auto-fill, minmax(120px, 1fr)); gap: 20px; text-align: center; } /* \u8fdb\u5ea6\u6761\u6837\u5f0f */ .progress-bar { background:#eee; border-radius:5px; overflow:hidden; margin-top:4px; height:6px; } .progress-bar-fill { height:100%; background: var(--progress-color); } axios.get(\"https://api.github.com/repos/hustzxd/EfficientPaper/contributors\") .then(function (res) { let contributors = res.data.sort((a,b) => b.contributions - a.contributions); let totalCommits = contributors.reduce((sum,c) => sum + c.contributions, 0); document.getElementById(\"contributors-count\").innerText = \"Total Contributors: \" + contributors.length; const medalColors = [\"#FFD700\", \"#C0C0C0\", \"#CD7F32\"]; // Top3 \u8fb9\u6846\u989c\u8272 let html = '<div>'; contributors.forEach((c,index)=>{ let percent = ((c.contributions/totalCommits)*100).toFixed(1); let borderColor = index<3? medalColors[index]:\"#ddd\"; let shadow = index<3? \"0 4px 15px rgba(0,0,0,0.2)\":\"none\"; html += ` <div class=\"contributor\" data-login=\"${c.login}\" data-avatar=\"${c.avatar_url}\" data-contrib=\"${c.contributions}\" data-url=\"${c.html_url}\" style=\"border:3px solid ${borderColor}; box-shadow:${shadow};\"> <a class=\"glightbox\" href=\"${c.avatar_url}\" data-type=\"image\" data-width=\"auto\" data-height=\"auto\" data-desc-position=\"bottom\"><img src=\"${c.avatar_url}\" width=\"70\" height=\"70\"></a> <div style=\"font-size:13px; margin-top:6px;\">${c.login}</div> <div style=\"font-size:11px; color:#666; margin-top:4px;\">${c.contributions} commits (${percent}%)</div> <div class=\"progress-bar\"> <div class=\"progress-bar-fill\" style=\"width:${percent}%\"></div> </div> <div class=\"tooltip\">${c.login} - ${c.contributions} commits</div> </div> `; }); html += '</div>'; document.getElementById(\"contributors\").innerHTML = html; // \u70b9\u51fb\u663e\u793a Modal document.querySelectorAll('.contributor').forEach(el=>{ el.addEventListener('click', ()=>{ document.getElementById('modal-avatar').src = el.dataset.avatar; document.getElementById('modal-login').innerText = el.dataset.login; document.getElementById('modal-commits').innerText = el.dataset.contrib + ' commits'; document.getElementById('modal-link').href = el.dataset.url; document.getElementById('contributor-modal').style.display = 'flex'; }); }); // \u5173\u95ed Modal document.getElementById('modal-close').addEventListener('click', ()=>{ document.getElementById('contributor-modal').style.display = 'none'; }); document.getElementById('contributor-modal').addEventListener('click',(e)=>{ if(e.target.id==='contributor-modal') document.getElementById('contributor-modal').style.display='none'; }); });","title":"Contributors"},{"location":"contributors/#contributors","text":"\u00d7 View GitHub Profile :root { --progress-color: #4caf50; /* \u53ef\u8c03\u4e3b\u9898\u989c\u8272 */ } .contributor { text-align: center; transition: transform 0.2s; cursor: pointer; position: relative; border-radius: 12px; padding: 8px; } .contributor:hover { transform: scale(1.08); } .contributor img { border-radius: 50%; display: block; margin: 0 auto; transition: transform 0.2s; } .tooltip { visibility: hidden; background-color: rgba(0,0,0,0.8); color: #fff; text-align: center; border-radius: 5px; padding: 4px 8px; position: absolute; bottom: 100%; left: 50%; transform: translateX(-50%); white-space: nowrap; font-size: 12px; opacity: 0; transition: opacity 0.2s; z-index: 1; } .contributor:hover .tooltip { visibility: visible; opacity: 1; } /* \u7f51\u683c\u5e03\u5c40\u54cd\u5e94\u5f0f */ #contributors > div { display: grid; grid-template-columns: repeat(auto-fill, minmax(120px, 1fr)); gap: 20px; text-align: center; } /* \u8fdb\u5ea6\u6761\u6837\u5f0f */ .progress-bar { background:#eee; border-radius:5px; overflow:hidden; margin-top:4px; height:6px; } .progress-bar-fill { height:100%; background: var(--progress-color); } axios.get(\"https://api.github.com/repos/hustzxd/EfficientPaper/contributors\") .then(function (res) { let contributors = res.data.sort((a,b) => b.contributions - a.contributions); let totalCommits = contributors.reduce((sum,c) => sum + c.contributions, 0); document.getElementById(\"contributors-count\").innerText = \"Total Contributors: \" + contributors.length; const medalColors = [\"#FFD700\", \"#C0C0C0\", \"#CD7F32\"]; // Top3 \u8fb9\u6846\u989c\u8272 let html = '<div>'; contributors.forEach((c,index)=>{ let percent = ((c.contributions/totalCommits)*100).toFixed(1); let borderColor = index<3? medalColors[index]:\"#ddd\"; let shadow = index<3? \"0 4px 15px rgba(0,0,0,0.2)\":\"none\"; html += ` <div class=\"contributor\" data-login=\"${c.login}\" data-avatar=\"${c.avatar_url}\" data-contrib=\"${c.contributions}\" data-url=\"${c.html_url}\" style=\"border:3px solid ${borderColor}; box-shadow:${shadow};\"> <a class=\"glightbox\" href=\"${c.avatar_url}\" data-type=\"image\" data-width=\"auto\" data-height=\"auto\" data-desc-position=\"bottom\"><img src=\"${c.avatar_url}\" width=\"70\" height=\"70\"></a> <div style=\"font-size:13px; margin-top:6px;\">${c.login}</div> <div style=\"font-size:11px; color:#666; margin-top:4px;\">${c.contributions} commits (${percent}%)</div> <div class=\"progress-bar\"> <div class=\"progress-bar-fill\" style=\"width:${percent}%\"></div> </div> <div class=\"tooltip\">${c.login} - ${c.contributions} commits</div> </div> `; }); html += '</div>'; document.getElementById(\"contributors\").innerHTML = html; // \u70b9\u51fb\u663e\u793a Modal document.querySelectorAll('.contributor').forEach(el=>{ el.addEventListener('click', ()=>{ document.getElementById('modal-avatar').src = el.dataset.avatar; document.getElementById('modal-login').innerText = el.dataset.login; document.getElementById('modal-commits').innerText = el.dataset.contrib + ' commits'; document.getElementById('modal-link').href = el.dataset.url; document.getElementById('contributor-modal').style.display = 'flex'; }); }); // \u5173\u95ed Modal document.getElementById('modal-close').addEventListener('click', ()=>{ document.getElementById('contributor-modal').style.display = 'none'; }); document.getElementById('contributor-modal').addEventListener('click',(e)=>{ if(e.target.id==='contributor-modal') document.getElementById('contributor-modal').style.display='none'; }); });","title":"Contributors"},{"location":"notes/2017/Transformer/note/","text":"Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","title":"Attention Is All You Need"},{"location":"notes/2017/Transformer/note/#attention-is-all-you-need","text":"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","title":"Attention Is All You Need"},{"location":"notes/2017/Transformer/note/#abstract","text":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","title":"Abstract"},{"location":"notes/2022/CoCoNet/note/","text":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi Abstract Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2022/CoCoNet/note/#breaking-the-computation-and-communication-abstraction-barrier-in-distributed-machine-learning-workloads","text":"Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2022/CoCoNet/note/#abstract","text":"Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Abstract"},{"location":"notes/2022/ComplementarySparsity/note/","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Combine weight sparsity and activation sparsity Table of Contents Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation Method sparse weight and dense activation a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity. sparse weight and sparse activation a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#two-sparsities-are-better-than-one-unlocking-the-performance-benefits-of-sparse-sparse-networks","text":"Combine weight sparsity and activation sparsity","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#table-of-contents","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation","title":"Table of Contents"},{"location":"notes/2022/ComplementarySparsity/note/#method","text":"","title":"Method"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-dense-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity.","title":"sparse weight and dense activation"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-sparse-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"sparse weight and sparse activation"},{"location":"notes/2022/DSA/note/","text":"Transformer Acceleration with Dynamic Sparse Attention Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie Abstract Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#transformer-acceleration-with-dynamic-sparse-attention","text":"Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#abstract","text":"Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Abstract"},{"location":"notes/2022/fisherpruning/note/","text":"A Fast Post-Training Pruning Framework for Transformers Abstract Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#a-fast-post-training-pruning-framework-for-transformers","text":"","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#abstract","text":"Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"Abstract"},{"location":"notes/2022/spdy/","text":"SPDY: Accurate Pruning with Speedup Guarantees \"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2022/spdy/#spdy-accurate-pruning-with-speedup-guarantees","text":"\"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2023/CodeGeeX/note/","text":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang Abstract Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#codegeex-a-pre-trained-model-for-code-generation-with-multilingual-benchmarking-on-humaneval-x","text":"Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#abstract","text":"Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"Abstract"},{"location":"notes/2023/Compresso/note/","text":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance. Method Challenges Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection Training data for pruning We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset Efficient training-based structured pruning Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, \\alpha is the learnable parameter: Mask regularization for the expected pruning ratio: Experiments The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#compresso-structured-pruning-with-collaborative-prompting-learns-compact-large-language-models","text":"This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#method","text":"","title":"Method"},{"location":"notes/2023/Compresso/note/#challenges","text":"Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection","title":"Challenges"},{"location":"notes/2023/Compresso/note/#training-data-for-pruning","text":"We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset","title":"Training data for pruning"},{"location":"notes/2023/Compresso/note/#efficient-training-based-structured-pruning","text":"Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, \\alpha is the learnable parameter: Mask regularization for the expected pruning ratio:","title":"Efficient training-based structured pruning"},{"location":"notes/2023/Compresso/note/#experiments","text":"The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Experiments"},{"location":"notes/2023/Dist-Einsum/note/","text":"RIIWOI3F Abstract","title":"RIIWOI3F"},{"location":"notes/2023/Dist-Einsum/note/#riiwoi3f","text":"","title":"RIIWOI3F"},{"location":"notes/2023/Dist-Einsum/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/FlashDecoding/note/","text":"FlashDecoding Abstract","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#flashdecoding","text":"","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/GBLM-Pruner/note/","text":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models Abstract Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#beyond-size-how-gradients-shape-pruning-decisions-in-large-language-models","text":"","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#abstract","text":"Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Abstract"},{"location":"notes/2023/H2O/note/","text":"H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models Abstract Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H _2 ). Through a comprehensive investigation, we find that (i) the emergence of H _2 is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H _2 O), a KV cache eviction policy that dynamically retains a balance of recent and H _2 tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H _2 O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29 \\times , 29 \\times , and 3 \\times on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9 \\times . The code is available at https://github.com/FMInference/H2O.","title":"H_2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models","text":"","title":"H_2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#abstract","text":"Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H _2 ). Through a comprehensive investigation, we find that (i) the emergence of H _2 is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H _2 O), a KV cache eviction policy that dynamically retains a balance of recent and H _2 tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H _2 O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29 \\times , 29 \\times , and 3 \\times on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9 \\times . The code is available at https://github.com/FMInference/H2O.","title":"Abstract"},{"location":"notes/2023/IHOT8YP4/note/","text":"Efficient Guided Generation for Large Language Models Abstract In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#efficient-guided-generation-for-large-language-models","text":"","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#abstract","text":"In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Abstract"},{"location":"notes/2023/LLM-Pruner/note/","text":"LLM-Pruner: On the Structural Pruning of Large Language Models Abstract Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#llm-pruner-on-the-structural-pruning-of-large-language-models","text":"","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#abstract","text":"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"Abstract"},{"location":"notes/2023/LLM_in_a_flash/note/","text":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory Abstract Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory","text":"","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#abstract","text":"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"Abstract"},{"location":"notes/2023/LLM_shearing/note/","text":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning structured pruning\u7684\u4e24\u4e2a\u56f0\u96be \u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa Method targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#sheared-llama-accelerating-language-model-pre-training-via-structured-pruning","text":"","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#structured-pruning","text":"\u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa","title":"structured pruning\u7684\u4e24\u4e2a\u56f0\u96be"},{"location":"notes/2023/LLM_shearing/note/#method","text":"targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Method"},{"location":"notes/2023/MeZO/note/","text":"Fine-Tuning Language Models with Just Forward Passes Abstract Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#fine-tuning-language-models-with-just-forward-passes","text":"","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#abstract","text":"Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Abstract"},{"location":"notes/2023/PagedAttention/note/","text":"Efficient Memory Management for Large Language Model Serving with PagedAttention Abstract High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4 \\times with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#efficient-memory-management-for-large-language-model-serving-with-pagedattention","text":"","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#abstract","text":"High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4 \\times with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Abstract"},{"location":"notes/2023/PowerInfer/note/","text":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Abstract This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#powerinfer-fast-large-language-model-serving-with-a-consumer-grade-gpu","text":"","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#abstract","text":"This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"Abstract"},{"location":"notes/2023/SparseViT/","text":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer Method Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity Result \u223c50% latency reduction with 60% sparsity","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#sparsevit-revisiting-activation-sparsity-for-efficient-high-resolution-vision-transformer","text":"","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#method","text":"Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity","title":"Method"},{"location":"notes/2023/SparseViT/#result","text":"\u223c50% latency reduction with 60% sparsity","title":"Result"},{"location":"notes/2023/VENOM/note/","text":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores Abstract The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#venom-a-vectorized-nm-format-for-unleashing-the-power-of-sparse-tensor-cores","text":"","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#abstract","text":"The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"Abstract"},{"location":"notes/2023/grain/","text":"GRAIN","title":"GRAIN"},{"location":"notes/2023/grain/#grain","text":"","title":"GRAIN"},{"location":"notes/2023/k_pruning/note/","text":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining This is a retraning-free structured pruning approach. Method Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where \\lambda = \\\\{0.00025, 1\\\\} and \\mu = 64 . \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c \\lambda \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c \\lambda \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 \\mu \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002 Results","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#knowledge-preserving-pruning-for-pre-trained-language-models-without-retraining","text":"This is a retraning-free structured pruning approach.","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#method","text":"Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where \\lambda = \\\\{0.00025, 1\\\\} and \\mu = 64 . \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c \\lambda \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c \\lambda \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 \\mu \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002","title":"Method"},{"location":"notes/2023/k_pruning/note/#results","text":"","title":"Results"},{"location":"notes/2023/loftq/note/","text":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models Method \u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#loftq-lora-fine-tuning-aware-quantization-for-large-language-models","text":"","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#method","text":"\u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"Method"},{"location":"notes/2023/simple/","text":"Structured Pruning for Efficient Generative Pre-trained Language Models Method Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V \\ell_{hidden} is hidden state distillation loss: Results \u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#structured-pruning-for-efficient-generative-pre-trained-language-models","text":"","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#method","text":"Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V \\ell_{hidden} is hidden state distillation loss:","title":"Method"},{"location":"notes/2023/simple/#results","text":"\u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Results"},{"location":"notes/2024/068ZPAME/note/","text":"A Survey on Inference Optimization Techniques for Mixture of Experts Models Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li Abstract The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#a-survey-on-inference-optimization-techniques-for-mixture-of-experts-models","text":"Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#abstract","text":"The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"Abstract"},{"location":"notes/2024/0Y41U1N2/note/","text":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs Abstract To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ( \\leq 1.3 ) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#beyond-24-exploring-vnm-sparsity-for-efficient-transformer-inference-on-gpus","text":"","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#abstract","text":"To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ( \\leq 1.3 ) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Abstract"},{"location":"notes/2024/ADMM-pruning/note/","text":"Fast and Effective Weight Update for Pruned Large Language Models Abstract Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#fast-and-effective-weight-update-for-pruned-large-language-models","text":"","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#abstract","text":"Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Abstract"},{"location":"notes/2024/APEX/note/","text":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino Abstract Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#apex-an-extensible-and-dynamism-aware-simulator-for-automated-parallel-execution-in-llm-serving","text":"Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#abstract","text":"Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"Abstract"},{"location":"notes/2024/AVSS/note/","text":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis Abstract The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#avss-layer-importance-evaluation-in-large-language-models-via-activation-variance-sparsity-analysis","text":"","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#abstract","text":"The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"Abstract"},{"location":"notes/2024/AdaKV/note/","text":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference Abstract Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the L_1 eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the L_1 eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#ada-kv-optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference","text":"","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#abstract","text":"Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the L_1 eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the L_1 eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Abstract"},{"location":"notes/2024/Async-TP/note/","text":"Async-TP Abstract","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#async-tp","text":"","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/CATS/note/","text":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models Abstract Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#cats-contextually-aware-thresholding-for-sparsity-in-large-language-models","text":"","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#abstract","text":"Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"Abstract"},{"location":"notes/2024/CHESS/note/","text":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Abstract Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#chess-optimizing-llm-inference-via-channel-wise-thresholding-and-selective-sparsification","text":"","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#abstract","text":"Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"Abstract"},{"location":"notes/2024/CLA/note/","text":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention Abstract Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#reducing-transformer-key-value-cache-size-with-cross-layer-attention","text":"","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#abstract","text":"Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Abstract"},{"location":"notes/2024/CachedAttention/note/","text":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention Abstract Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8 \\times for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention","text":"","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#abstract","text":"Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8 \\times for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Abstract"},{"location":"notes/2024/Centauri/note/","text":"Centauri Abstract","title":"Centauri"},{"location":"notes/2024/Centauri/note/#centauri","text":"","title":"Centauri"},{"location":"notes/2024/Centauri/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/ChunkAttention/note/","text":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition Abstract Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8 \\times compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#chunkattention-efficient-self-attention-with-prefix-aware-kv-cache-and-two-phase-partition","text":"","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#abstract","text":"Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8 \\times compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/CoreInfer/note/","text":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation Abstract Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86 \\alpha \\beta \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#coreinfer-accelerating-large-language-model-inference-with-semantics-inspired-adaptive-sparse-activation","text":"","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#abstract","text":"Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86 \\alpha \\beta \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"Abstract"},{"location":"notes/2024/DHIB73MC/note/","text":"A Survey on Efficient Inference for Large Language Models Abstract Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#a-survey-on-efficient-inference-for-large-language-models","text":"","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#abstract","text":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"Abstract"},{"location":"notes/2024/DSnoT/note/","text":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs Abstract The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#dynamic-sparse-no-training-training-free-fine-tuning-for-sparse-llms","text":"","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#abstract","text":"The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/","text":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model Abstract We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1 Multi-head Latent Attention (MLA) Preliminaries: Standard Multi-Head Attention \u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002 Low-Rank Key-Value Joint Compression \u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58 C_t^{KV} \uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e C_t^{KV} \u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a Decoupled Rotary Position Embedding \u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#deepseek-v2-a-strong-economical-and-efficient-mixture-of-experts-language-model","text":"","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#abstract","text":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/#multi-head-latent-attention-mla","text":"","title":"Multi-head Latent Attention (MLA)"},{"location":"notes/2024/DeepSeek-V2/note/#preliminaries-standard-multi-head-attention","text":"\u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002","title":"Preliminaries: Standard Multi-Head Attention"},{"location":"notes/2024/DeepSeek-V2/note/#low-rank-key-value-joint-compression","text":"\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58 C_t^{KV} \uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e C_t^{KV} \u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"Low-Rank Key-Value Joint Compression"},{"location":"notes/2024/DeepSeek-V2/note/#decoupled-rotary-position-embedding","text":"\u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"Decoupled Rotary Position Embedding"},{"location":"notes/2024/DeepSeek-V3/note/","text":"DeepSeek-V3 Technical Report Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#deepseek-v3-technical-report","text":"","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#abstract","text":"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"Abstract"},{"location":"notes/2024/DeepSeekMoE/note/","text":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models Abstract In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top- K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating K_s experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#deepseekmoe-towards-ultimate-expert-specialization-in-mixture-of-experts-language-models","text":"","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#abstract","text":"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top- K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating K_s experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"Abstract"},{"location":"notes/2024/DistAttention/note/","text":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache Abstract Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#infinite-llm-efficient-llm-service-for-long-context-with-distattention-and-distributed-kvcache","text":"","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#abstract","text":"Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Abstract"},{"location":"notes/2024/DistGEMM/note/","text":"DistributedGEMM Abstract","title":"DistributedGEMM"},{"location":"notes/2024/DistGEMM/note/#distributedgemm","text":"","title":"DistributedGEMM"},{"location":"notes/2024/DistGEMM/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/Domino/note/","text":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase Abstract Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs. LLM Training \u8ba1\u7b97\u901a\u4fe1\u91cd\u53e0\u7684\u4f18\u5316\uff0c\u5207\u5206\u65b9\u5f0f\u6309\u7167batch\u7b49\u5207\u5206\u3002","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#domino-eliminating-communication-in-llm-training-via-generic-tensor-slicing-and-overlapping","text":"Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#abstract","text":"Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs. LLM Training \u8ba1\u7b97\u901a\u4fe1\u91cd\u53e0\u7684\u4f18\u5316\uff0c\u5207\u5206\u65b9\u5f0f\u6309\u7167batch\u7b49\u5207\u5206\u3002","title":"Abstract"},{"location":"notes/2024/DoubleSparsity/note/","text":"Post-Training Sparse Attention with Double Sparsity Abstract The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve \\frac{1}{16} token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1 \\times acceleration in attention operations and a 1.9 \\times improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3 \\times compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#post-training-sparse-attention-with-double-sparsity","text":"","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#abstract","text":"The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve \\frac{1}{16} token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1 \\times acceleration in attention operations and a 1.9 \\times improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3 \\times compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Abstract"},{"location":"notes/2024/DuoAttention/note/","text":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads Abstract Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a \\alpha \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 \\alpha \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#duoattention-efficient-long-context-llm-inference-with-retrieval-and-streaming-heads","text":"","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#abstract","text":"Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a \\alpha \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 \\alpha \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"Abstract"},{"location":"notes/2024/Eagle/note/","text":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty Abstract Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#eagle-speculative-sampling-requires-rethinking-feature-uncertainty","text":"","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#abstract","text":"Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"Abstract"},{"location":"notes/2024/FLUX/note/","text":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu Abstract Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion","text":"Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#abstract","text":"Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"Abstract"},{"location":"notes/2024/FlashMask/note/","text":"FlashMask: Efficient and Rich Mask Extension of FlashAttention Abstract The computational and memory demands of vanilla attention scale quadratically with the sequence length N , posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the O(N^2) memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with O(N^2) memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity O(N) , suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#flashmask-efficient-and-rich-mask-extension-of-flashattention","text":"","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#abstract","text":"The computational and memory demands of vanilla attention scale quadratically with the sequence length N , posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the O(N^2) memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with O(N^2) memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity O(N) , suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"Abstract"},{"location":"notes/2024/FrameQuant/note/","text":"FrameQuant: Flexible Low-Bit Quantization for Transformers Abstract Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#framequant-flexible-low-bit-quantization-for-transformers","text":"","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#abstract","text":"Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/GEAR/note/","text":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao Abstract Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR. quant + \u4f4e\u79e9\u5206\u89e3","title":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"},{"location":"notes/2024/GEAR/note/#gear-an-efficient-kv-cache-compression-recipe-for-near-lossless-generative-inference-of-llm","text":"Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao","title":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"},{"location":"notes/2024/GEAR/note/#abstract","text":"Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR. quant + \u4f4e\u79e9\u5206\u89e3","title":"Abstract"},{"location":"notes/2024/HYPL7G37/note/","text":"Accelerating Transformer Pre-training with 2:4 Sparsity Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#accelerating-transformer-pre-training-with-24-sparsity","text":"","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#abstract","text":"Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Abstract"},{"location":"notes/2024/JSHWEV0S/note/","text":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption Abstract Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#keep-the-cost-down-a-review-on-methods-to-optimize-llm-s-kv-cache-consumption","text":"","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#abstract","text":"Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Abstract"},{"location":"notes/2024/KIVI/note/","text":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using \\mathbf{2.6\\times} less peak memory (including model weight). This reduction in memory usage enables up to \\mathbf{4\\times} larger batch size, bringing \\mathbf{2.35\\times \\sim 3.47\\times} throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.","title":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"},{"location":"notes/2024/KIVI/note/#kivi-a-tuning-free-asymmetric-2bit-quantization-for-kv-cache","text":"Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu","title":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"},{"location":"notes/2024/KIVI/note/#abstract","text":"Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using \\mathbf{2.6\\times} less peak memory (including model weight). This reduction in memory usage enables up to \\mathbf{4\\times} larger batch size, bringing \\mathbf{2.35\\times \\sim 3.47\\times} throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.","title":"Abstract"},{"location":"notes/2024/KVQuant/note/","text":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Abstract LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve <0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. Per-Channel Key Quantization \u51fa\u73b0outlier channel, channel-wise quant\u6bd4 token-wise quant\u8981\u597d Pre-RoPE Key Quantization RoPE\u524dKey\u7684\u5206\u5e03\u66f4\u96c6\u4e2d non-uniform quantization Per-Vector Dense-and-Sparse Quantization Outlier\u4f7f\u7528\u7a00\u758f\u683c\u5f0f\u7684\u9ad8\u4f4d\u5bbd\u6570\u636e\u683c\u5f0f\u5b58\u50a8 Attention Sink-Aware Quantization \u524d\u51e0\u4e2atoken\u5c5e\u4e8eSink\uff0c\u91c7\u7528fp16\u683c\u5f0f","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#kvquant-towards-10-million-context-length-llm-inference-with-kv-cache-quantization","text":"","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#abstract","text":"LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve <0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. Per-Channel Key Quantization \u51fa\u73b0outlier channel, channel-wise quant\u6bd4 token-wise quant\u8981\u597d Pre-RoPE Key Quantization RoPE\u524dKey\u7684\u5206\u5e03\u66f4\u96c6\u4e2d non-uniform quantization Per-Vector Dense-and-Sparse Quantization Outlier\u4f7f\u7528\u7a00\u758f\u683c\u5f0f\u7684\u9ad8\u4f4d\u5bbd\u6570\u636e\u683c\u5f0f\u5b58\u50a8 Attention Sink-Aware Quantization \u524d\u51e0\u4e2atoken\u5c5e\u4e8eSink\uff0c\u91c7\u7528fp16\u683c\u5f0f","title":"Abstract"},{"location":"notes/2024/L4Q/note/","text":"L4Q Method Step1: warm up Q(W) + \\alpha AB Step2: quantize(W + sAB) Experiments memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"L4Q"},{"location":"notes/2024/L4Q/note/#l4q","text":"Method Step1: warm up Q(W) + \\alpha AB Step2: quantize(W + sAB)","title":"L4Q"},{"location":"notes/2024/L4Q/note/#experiments","text":"memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"Experiments"},{"location":"notes/2024/LISA/note/","text":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LISA/note/#lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning","text":"","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LazyLLM/note/","text":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi Abstract The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy. Prefill \u968f\u7740\u6bcf\u5c42\u7684\u8fd0\u7b97\uff0cToken\u9010\u6e10\u7f29\u5c0f Token\u7684\u91cd\u8981\u6027\u8bc4\u4f30\u6839\u636e\u7b2c l \u5c42\u7684attention score\u4f30\u8ba1\uff0c\u5e76\u5bf9 l+1 \u5c42\u7684Token\u8fdb\u884cpruning \u540e\u7eed\u7684layer\u9009\u62e9\u53ea\u80fd\u65f6\u4e4b\u524d\u5c42\u7684subset Decode Decode\u65f6\u6bcf\u5c42\u53ef\u4ee5\u9009\u62e9\u4e0d\u540c\u7684subset\uff0c\u901a\u8fc7AuxCache\u5b9e\u73b0\uff08\u4f46\u8bba\u6587\u8bf4\u7684\u6bd4\u8f83\u6a21\u7cca\uff09 ICLR2025 reject","title":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"},{"location":"notes/2024/LazyLLM/note/#lazyllm-dynamic-token-pruning-for-efficient-long-context-llm-inference","text":"Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi","title":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"},{"location":"notes/2024/LazyLLM/note/#abstract","text":"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy. Prefill \u968f\u7740\u6bcf\u5c42\u7684\u8fd0\u7b97\uff0cToken\u9010\u6e10\u7f29\u5c0f Token\u7684\u91cd\u8981\u6027\u8bc4\u4f30\u6839\u636e\u7b2c l \u5c42\u7684attention score\u4f30\u8ba1\uff0c\u5e76\u5bf9 l+1 \u5c42\u7684Token\u8fdb\u884cpruning \u540e\u7eed\u7684layer\u9009\u62e9\u53ea\u80fd\u65f6\u4e4b\u524d\u5c42\u7684subset Decode Decode\u65f6\u6bcf\u5c42\u53ef\u4ee5\u9009\u62e9\u4e0d\u540c\u7684subset\uff0c\u901a\u8fc7AuxCache\u5b9e\u73b0\uff08\u4f46\u8bba\u6587\u8bf4\u7684\u6bd4\u8f83\u6a21\u7cca\uff09 ICLR2025 reject","title":"Abstract"},{"location":"notes/2024/LightningAttention/note/","text":"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong Abstract We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.","title":"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention"},{"location":"notes/2024/LightningAttention/note/#various-lengths-constant-speed-efficient-language-modeling-with-lightning-attention","text":"Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong","title":"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention"},{"location":"notes/2024/LightningAttention/note/#abstract","text":"We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.","title":"Abstract"},{"location":"notes/2024/LightningAttention-2/note/","text":"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong Abstract Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.","title":"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"},{"location":"notes/2024/LightningAttention-2/note/#lightning-attention-2-a-free-lunch-for-handling-unlimited-sequence-lengths-in-large-language-models","text":"Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong","title":"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"},{"location":"notes/2024/LightningAttention-2/note/#abstract","text":"Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.","title":"Abstract"},{"location":"notes/2024/LoRA%2B/note/","text":"LoRA+: Efficient Low Rank Adaptation of Large Models Abstract In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA + . In our extensive experiments, LoRA + improves performance (1-2 \\% improvements) and finetuning speed (up to \\sim 2X SpeedUp), at the same computational cost as LoRA.","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#lora-efficient-low-rank-adaptation-of-large-models","text":"","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#abstract","text":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA + . In our extensive experiments, LoRA + improves performance (1-2 \\% improvements) and finetuning speed (up to \\sim 2X SpeedUp), at the same computational cost as LoRA.","title":"Abstract"},{"location":"notes/2024/MFA/note/","text":"Multi-matrix Factorization Attention Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang Abstract We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#multi-matrix-factorization-attention","text":"Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#abstract","text":"We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Abstract"},{"location":"notes/2024/MInference/note/","text":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention Abstract The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#minference-10-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention","text":"","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#abstract","text":"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"Abstract"},{"location":"notes/2024/MaskLLM/note/","text":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models Abstract Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#maskllm-learnable-semi-structured-sparsity-for-large-language-models","text":"","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#abstract","text":"Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"Abstract"},{"location":"notes/2024/MiKV/note/","text":"No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee Abstract Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.","title":"No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization"},{"location":"notes/2024/MiKV/note/#no-token-left-behind-reliable-kv-cache-compression-via-importance-aware-mixed-precision-quantization","text":"June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee","title":"No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization"},{"location":"notes/2024/MiKV/note/#abstract","text":"Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.","title":"Abstract"},{"location":"notes/2024/MiniCache/note/","text":"MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang Abstract A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. kv cache merge","title":"MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"},{"location":"notes/2024/MiniCache/note/#minicache-kv-cache-compression-in-depth-dimension-for-large-language-models","text":"Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang","title":"MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"},{"location":"notes/2024/MiniCache/note/#abstract","text":"A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. kv cache merge","title":"Abstract"},{"location":"notes/2024/MiniKV/note/","text":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache Abstract How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#minikv-pushing-the-limits-of-llm-inference-via-2-bit-layer-discriminative-kv-cache","text":"","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#abstract","text":"How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"Abstract"},{"location":"notes/2024/Minitron/note/","text":"Compact Language Models via Pruning and Knowledge Distillation Abstract Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#compact-language-models-via-pruning-and-knowledge-distillation","text":"","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#abstract","text":"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Abstract"},{"location":"notes/2024/MoA/note/","text":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression Abstract Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by 3.9\\times with the same average attention span, boosting retrieval accuracy by 1.5-7.1\\times over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from 9\\%-36\\% to within 5\\% across two long-context understanding benchmarks. MoA achieves a 1.2-1.4\\times GPU memory reduction, boosting decode throughput by 6.6-8.2\\times and 1.7-1.9\\times compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#moa-mixture-of-sparse-attention-for-automatic-large-language-model-compression","text":"","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#abstract","text":"Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by 3.9\\times with the same average attention span, boosting retrieval accuracy by 1.5-7.1\\times over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from 9\\%-36\\% to within 5\\% across two long-context understanding benchmarks. MoA achieves a 1.2-1.4\\times GPU memory reduction, boosting decode throughput by 6.6-8.2\\times and 1.7-1.9\\times compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"Abstract"},{"location":"notes/2024/MoD/note/","text":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Abstract Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ( k ) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top- k routing mechanism. Since k is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the k tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling. Method Defining a compute budget \u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a T/2 \uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684 T^2 \u53d8\u4e3a \\frac{T}{2}^2 \uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684 T \u53d8\u4e3a \\frac{T}{2} \uff0c\u5373\u539f\u6765\u768450%\u3002 Routing schemes Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9 Routing implementation \u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa f_i(x) \u589e\u52a0\u4e86\u4e0e r_i \u4f5c\u4e3a\u6743\u91cd\u3002 Non-causal problem during sampling \u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684 r_i \u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk Training Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49 Results \u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models","text":"","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#abstract","text":"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ( k ) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top- k routing mechanism. Since k is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the k tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.","title":"Abstract"},{"location":"notes/2024/MoD/note/#method","text":"","title":"Method"},{"location":"notes/2024/MoD/note/#defining-a-compute-budget","text":"\u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a T/2 \uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684 T^2 \u53d8\u4e3a \\frac{T}{2}^2 \uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684 T \u53d8\u4e3a \\frac{T}{2} \uff0c\u5373\u539f\u6765\u768450%\u3002","title":"Defining a compute budget"},{"location":"notes/2024/MoD/note/#routing-schemes","text":"Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9","title":"Routing schemes"},{"location":"notes/2024/MoD/note/#routing-implementation","text":"\u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa f_i(x) \u589e\u52a0\u4e86\u4e0e r_i \u4f5c\u4e3a\u6743\u91cd\u3002","title":"Routing implementation"},{"location":"notes/2024/MoD/note/#non-causal-problem-during-sampling","text":"\u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684 r_i \u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk","title":"Non-causal problem during sampling"},{"location":"notes/2024/MoD/note/#training","text":"Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49","title":"Training"},{"location":"notes/2024/MoD/note/#results","text":"\u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Results"},{"location":"notes/2024/OSSCAR/note/","text":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization Abstract Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to 125\\times lower test perplexity on WikiText with 2\\times inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also 6\\times -- 8\\times faster. Notably, our work considers models with tens of billions of parameters, which is up to 100\\times larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 \\hat{w}","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#osscar-one-shot-structured-pruning-in-vision-and-language-models-with-combinatorial-optimization","text":"","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#abstract","text":"Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to 125\\times lower test perplexity on WikiText with 2\\times inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also 6\\times -- 8\\times faster. Notably, our work considers models with tens of billions of parameters, which is up to 100\\times larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 \\hat{w}","title":"Abstract"},{"location":"notes/2024/PWGG5HBE/note/","text":"A Survey on Large Language Model Acceleration based on KV Cache Management Abstract Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#a-survey-on-large-language-model-acceleration-based-on-kv-cache-management","text":"","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#abstract","text":"Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"Abstract"},{"location":"notes/2024/PowerInfer-2/note/","text":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone Abstract This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#powerinfer-2-fast-large-language-model-inference-on-a-smartphone","text":"","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#abstract","text":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"Abstract"},{"location":"notes/2024/PrefixQuant/note/","text":"PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo Abstract Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant. \u5c06\u7279\u5b9a\u7684\u4e00\u4e9btoken\u6807\u8bb0\u4e3aoutlier","title":"PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization"},{"location":"notes/2024/PrefixQuant/note/#prefixquant-eliminating-outliers-by-prefixed-tokens-for-large-language-models-quantization","text":"Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo","title":"PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization"},{"location":"notes/2024/PrefixQuant/note/#abstract","text":"Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant. \u5c06\u7279\u5b9a\u7684\u4e00\u4e9btoken\u6807\u8bb0\u4e3aoutlier","title":"Abstract"},{"location":"notes/2024/ProSparse/note/","text":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models Method There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface. Experiment Real accelerate on hardware Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization Training Dataset Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#prosparse-introducing-and-enhancing-intrinsic-activation-sparsity-within-large-language-models","text":"","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#method","text":"There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface.","title":"Method"},{"location":"notes/2024/ProSparse/note/#experiment","text":"","title":"Experiment"},{"location":"notes/2024/ProSparse/note/#real-accelerate-on-hardware","text":"Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization","title":"Real accelerate on hardware"},{"location":"notes/2024/ProSparse/note/#training-dataset","text":"Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"Training Dataset"},{"location":"notes/2024/Q-Sparse/note/","text":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated Abstract We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#q-sparse-all-large-language-models-can-be-fully-sparsely-activated","text":"","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#abstract","text":"We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Abstract"},{"location":"notes/2024/QA-LoRA/note/","text":"QA-LoRA QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QA-LoRA/note/#qa-lora","text":"QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QServer/note/","text":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Abstract Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving","text":"","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#abstract","text":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"Abstract"},{"location":"notes/2024/Quest/note/","text":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference Abstract As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#quest-query-aware-sparsity-for-efficient-long-context-llm-inference","text":"","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#abstract","text":"As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Abstract"},{"location":"notes/2024/ReLU2/note/","text":"ReLU^2 Wins: Discovering Efficient Activation Functions for Sparse LLMs The paper indicates through experiments that models employing ReLU^2 excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"ReLU^2 Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReLU2/note/#relu2-wins-discovering-efficient-activation-functions-for-sparse-llms","text":"The paper indicates through experiments that models employing ReLU^2 excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"ReLU^2 Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReMoE/note/","text":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing Abstract Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#remoe-fully-differentiable-mixture-of-experts-with-relu-routing","text":"","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"Abstract"},{"location":"notes/2024/RecycledAttention/note/","text":"Recycled Attention: Efficient inference for long-context language models Abstract Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#recycled-attention-efficient-inference-for-long-context-language-models","text":"","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#abstract","text":"Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Abstract"},{"location":"notes/2024/SAS/note/","text":"SAS Abstract \u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"SAS"},{"location":"notes/2024/SAS/note/#sas","text":"","title":"SAS"},{"location":"notes/2024/SAS/note/#abstract","text":"\u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"Abstract"},{"location":"notes/2024/SCAP/note/","text":"Post-Training Statistical Calibration for Higher Activation Sparsity Abstract We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#post-training-statistical-calibration-for-higher-activation-sparsity","text":"","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#abstract","text":"We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Abstract"},{"location":"notes/2024/SCBench/note/","text":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods Abstract Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#scbench-a-kv-cache-centric-analysis-of-long-context-methods","text":"","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#abstract","text":"Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"Abstract"},{"location":"notes/2024/SGLang/note/","text":"SGLang: Efficient Execution of Structured Language Model Programs Abstract Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#sglang-efficient-execution-of-structured-language-model-programs","text":"","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#abstract","text":"Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"Abstract"},{"location":"notes/2024/SIFT/note/","text":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models Abstract With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#sparse-is-enough-in-fine-tuning-pre-trained-large-language-models","text":"","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#abstract","text":"With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Abstract"},{"location":"notes/2024/SMAT/note/","text":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei Abstract Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#unleashing-the-power-of-meta-tuning-for-few-shot-generalization-through-sparse-interpolated-experts","text":"Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#abstract","text":"Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Abstract"},{"location":"notes/2024/SN1PK7EK/note/","text":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark Abstract In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#revisiting-zeroth-order-optimization-for-memory-efficient-llm-fine-tuning-a-benchmark","text":"","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#abstract","text":"In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Abstract"},{"location":"notes/2024/SPP/note/","text":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models Abstract Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#spp-sparsity-preserved-parameter-efficient-fine-tuning-for-large-language-models","text":"","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#abstract","text":"Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"Abstract"},{"location":"notes/2024/SageAttention/note/","text":"SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen Abstract The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2) , compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.","title":"SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration"},{"location":"notes/2024/SageAttention/note/#sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration","text":"Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen","title":"SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration"},{"location":"notes/2024/SageAttention/note/#abstract","text":"The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2) , compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.","title":"Abstract"},{"location":"notes/2024/SageAttention2/note/","text":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen Abstract Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices (Q, K) to INT4 in a hardware-friendly thread-level granularity and quantize matrices (\\widetilde P, V) to FP8. Second, we propose a method to smooth Q , enhancing the accuracy of INT4 QK^\\top . Third, we propose a two-level accumulation strategy for \\widetilde PV to enhance the accuracy of FP8 \\widetilde PV . The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization"},{"location":"notes/2024/SageAttention2/note/#sageattention2-efficient-attention-with-thorough-outlier-smoothing-and-per-thread-int4-quantization","text":"Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization"},{"location":"notes/2024/SageAttention2/note/#abstract","text":"Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices (Q, K) to INT4 in a hardware-friendly thread-level granularity and quantize matrices (\\widetilde P, V) to FP8. Second, we propose a method to smooth Q , enhancing the accuracy of INT4 QK^\\top . Third, we propose a two-level accumulation strategy for \\widetilde PV to enhance the accuracy of FP8 \\widetilde PV . The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.","title":"Abstract"},{"location":"notes/2024/SampleAttention/note/","text":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention Abstract Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to 2.42\\times compared with FlashAttention.","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#sampleattention-near-lossless-acceleration-of-long-context-llm-inference-with-adaptive-structured-sparse-attention","text":"","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#abstract","text":"Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to 2.42\\times compared with FlashAttention.","title":"Abstract"},{"location":"notes/2024/SeerAttention/note/","text":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs Abstract Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#seerattention-learning-intrinsic-sparse-attention-in-your-llms","text":"","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#abstract","text":"Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"Abstract"},{"location":"notes/2024/ShadowLLM/note/","text":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models Abstract The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#shadowllm-predictor-based-contextual-sparsity-for-large-language-models","text":"","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#abstract","text":"The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"Abstract"},{"location":"notes/2024/SharedAttention/note/","text":"Beyond KV Caching: Shared Attention for Efficient LLMs Abstract The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#beyond-kv-caching-shared-attention-for-efficient-llms","text":"","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#abstract","text":"The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Abstract"},{"location":"notes/2024/SliceGPT/note/","text":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns Abstract Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#slicegpt-compress-large-language-models-by-deleting-rows-and-columns","text":"","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#abstract","text":"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"Abstract"},{"location":"notes/2024/SlimGPT/note/","text":"SlimGPT: Layer-wise Structured Pruning for Large Language Models Abstract Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#slimgpt-layer-wise-structured-pruning-for-large-language-models","text":"","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#abstract","text":"Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"Abstract"},{"location":"notes/2024/SnapKV/note/","text":"SnapKV: LLM Knows What You are Looking for Before Generation Abstract Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#snapkv-llm-knows-what-you-are-looking-for-before-generation","text":"","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#abstract","text":"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"Abstract"},{"location":"notes/2024/SparQ/note/","text":"SparQ Attention: Bandwidth-Efficient LLM Inference Abstract The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#sparq-attention-bandwidth-efficient-llm-inference","text":"","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#abstract","text":"The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"Abstract"},{"location":"notes/2024/Sparse-IFT/note/","text":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Abstract Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#sparse-ift-sparse-iso-flop-transformations-for-maximizing-training-efficiency","text":"","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#abstract","text":"Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Abstract"},{"location":"notes/2024/SparseInfer/note/","text":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference Abstract Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#sparseinfer-training-free-prediction-of-activation-sparsity-for-fast-llm-inference","text":"","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#abstract","text":"Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"Abstract"},{"location":"notes/2024/SparseLLM/note/","text":"SparseLLM: Towards Global Pruning for Pre-trained Language Models Abstract The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#sparsellm-towards-global-pruning-for-pre-trained-language-models","text":"","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#abstract","text":"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"Abstract"},{"location":"notes/2024/Splitwise/note/","text":"Splitwise: Efficient generative LLM inference using phase splitting Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, \u00cd\u00f1igo Goiri, Saeed Maleki, Ricardo Bianchini Abstract Recent innovations in generative large language models (LLMs) have made their applications and use-cases ubiquitous. This has led to large-scale deployments of these models, using complex, expensive, and power-hungry AI accelerators, most commonly GPUs. These developments make LLM inference efficiency an important challenge. Based on our extensive characterization, we find that there are two main phases during an LLM inference request: a compute-intensive prompt computation, and a memory-intensive token generation, each with distinct latency, throughput, memory, and power characteristics. Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Specifically, unlike compute-intensive prompt computation phases, token generation phases do not require the compute capability of the latest GPUs, and can be run with lower power and cost. With Splitwise, we propose splitting the two phases of a LLM inference request on to separate machines. This allows us to use hardware that is well-suited for each phase, and provision resources independently per phase. However, splitting an inference request across machines requires state transfer from the machine running prompt computation over to the machine generating tokens. We implement and optimize this state transfer using the fast back-plane interconnects available in today's GPU clusters. We use the Splitwise technique to design LLM inference clusters using the same or different types of machines for the prompt computation and token generation phases. Our clusters are optimized for three key objectives: throughput, cost, and power. In particular, we show that we can achieve 1.4x higher throughput at 20% lower cost than current designs. Alternatively, we can achieve 2.35x more throughput with the same cost and power budgets.","title":"Splitwise: Efficient generative LLM inference using phase splitting"},{"location":"notes/2024/Splitwise/note/#splitwise-efficient-generative-llm-inference-using-phase-splitting","text":"Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, \u00cd\u00f1igo Goiri, Saeed Maleki, Ricardo Bianchini","title":"Splitwise: Efficient generative LLM inference using phase splitting"},{"location":"notes/2024/Splitwise/note/#abstract","text":"Recent innovations in generative large language models (LLMs) have made their applications and use-cases ubiquitous. This has led to large-scale deployments of these models, using complex, expensive, and power-hungry AI accelerators, most commonly GPUs. These developments make LLM inference efficiency an important challenge. Based on our extensive characterization, we find that there are two main phases during an LLM inference request: a compute-intensive prompt computation, and a memory-intensive token generation, each with distinct latency, throughput, memory, and power characteristics. Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Specifically, unlike compute-intensive prompt computation phases, token generation phases do not require the compute capability of the latest GPUs, and can be run with lower power and cost. With Splitwise, we propose splitting the two phases of a LLM inference request on to separate machines. This allows us to use hardware that is well-suited for each phase, and provision resources independently per phase. However, splitting an inference request across machines requires state transfer from the machine running prompt computation over to the machine generating tokens. We implement and optimize this state transfer using the fast back-plane interconnects available in today's GPU clusters. We use the Splitwise technique to design LLM inference clusters using the same or different types of machines for the prompt computation and token generation phases. Our clusters are optimized for three key objectives: throughput, cost, and power. In particular, we show that we can achieve 1.4x higher throughput at 20% lower cost than current designs. Alternatively, we can achieve 2.35x more throughput with the same cost and power budgets.","title":"Abstract"},{"location":"notes/2024/SqueezeLLM/note/","text":"SqueezeLLM: Dense-and-Sparse Quantization Abstract Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#squeezellm-dense-and-sparse-quantization","text":"","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#abstract","text":"Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"Abstract"},{"location":"notes/2024/T3/note/","text":"T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair Abstract Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in \\sim 500-billion parameter models, PALM and MT-NLG. track & trigger \u5b83\u4f7f\u7528\u8ddf\u8e2a\u548c\u89e6\u53d1\u673a\u5236\u6765\u76d1\u63a7 GEMM/collective \u7684\u8fdb\u5ea6\u5e76\u534f\u8c03\u901a\u4fe1\uff0c\u65e0\u9700\u989d\u5916\u7684\u8ba1\u7b97\u5355\u5143 (CU) \u5b83\u5229\u7528\u8fd1\u5185\u5b58\u8ba1\u7b97\u8fdb\u884c\u7ea6\u7b80\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f15\u8d77\u7684\u5185\u5b58\u6d41\u91cf\u3002 transparently \u4ee5\u900f\u660e\u7684\u65b9\u5f0f\u5b8c\u6210\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u53ea\u9700\u5bf9\u5185\u6838\u8fdb\u884c\u5c11\u91cf\u4fee\u6539","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#t3-transparent-tracking-triggering-for-fine-grained-overlap-of-compute-collectives","text":"Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#abstract","text":"Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in \\sim 500-billion parameter models, PALM and MT-NLG. track & trigger \u5b83\u4f7f\u7528\u8ddf\u8e2a\u548c\u89e6\u53d1\u673a\u5236\u6765\u76d1\u63a7 GEMM/collective \u7684\u8fdb\u5ea6\u5e76\u534f\u8c03\u901a\u4fe1\uff0c\u65e0\u9700\u989d\u5916\u7684\u8ba1\u7b97\u5355\u5143 (CU) \u5b83\u5229\u7528\u8fd1\u5185\u5b58\u8ba1\u7b97\u8fdb\u884c\u7ea6\u7b80\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f15\u8d77\u7684\u5185\u5b58\u6d41\u91cf\u3002 transparently \u4ee5\u900f\u660e\u7684\u65b9\u5f0f\u5b8c\u6210\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u53ea\u9700\u5bf9\u5185\u6838\u8fdb\u884c\u5c11\u91cf\u4fee\u6539","title":"Abstract"},{"location":"notes/2024/TOVA/note/","text":"Transformers are Multi-State RNNs Abstract Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into \\textit{bounded} multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - \\textbf{T} oken \\textbf{O} mission \\textbf{V} ia \\textbf{A} ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only \\frac{1}{8} of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#transformers-are-multi-state-rnns","text":"","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#abstract","text":"Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into \\textit{bounded} multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - \\textbf{T} oken \\textbf{O} mission \\textbf{V} ia \\textbf{A} ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only \\frac{1}{8} of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Abstract"},{"location":"notes/2024/TinyTrain/note/","text":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge Abstract On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#tinytrain-resource-aware-task-adaptive-sparse-training-of-dnns-at-the-data-scarce-edge","text":"","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#abstract","text":"On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"Abstract"},{"location":"notes/2024/TurboSparse/note/","text":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Abstract Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#turbo-sparse-achieving-llm-sota-performance-with-minimal-activated-parameters","text":"","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#abstract","text":"Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Abstract"},{"location":"notes/2024/ULY1AZGY/note/","text":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment Abstract Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#enabling-high-sparsity-foundational-llama-models-with-efficient-pretraining-and-deployment","text":"","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#abstract","text":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Abstract"},{"location":"notes/2024/VB8C61V6/note/","text":"Compressing LLMs: The Truth is Rarely Pure and Never Simple Abstract Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at \\geq 50 % sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#compressing-llms-the-truth-is-rarely-pure-and-never-simple","text":"","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#abstract","text":"Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at \\geq 50 % sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Abstract"},{"location":"notes/2024/Vidur/note/","text":"Vidur: A Large-Scale Simulation Framework For LLM Inference Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov Abstract Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#vidur-a-large-scale-simulation-framework-for-llm-inference","text":"Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#abstract","text":"Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Abstract"},{"location":"notes/2024/Wanda/note/","text":"A Simple and Effective Pruning Approach for Large Language Models Abstract As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#a-simple-and-effective-pruning-approach-for-large-language-models","text":"","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#abstract","text":"As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"Abstract"},{"location":"notes/2024/XGrammar/note/","text":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models Abstract The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#xgrammar-flexible-and-efficient-structured-generation-engine-for-large-language-models","text":"","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#abstract","text":"The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"Abstract"},{"location":"notes/2024/YS9YTT55/note/","text":"LLM Inference Serving: Survey of Recent Advances and Opportunities Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari Abstract This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#llm-inference-serving-survey-of-recent-advances-and-opportunities","text":"Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#abstract","text":"This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"Abstract"},{"location":"notes/2024/ZigZagKV/note/","text":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty Abstract Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only \\sim 20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#zigzagkv-dynamic-kv-cache-compression-for-long-context-modeling-based-on-layer-uncertainty","text":"","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#abstract","text":"Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only \\sim 20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"Abstract"},{"location":"notes/2024/ZipCache/note/","text":"ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, Bohan Zhuang Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs. First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98\\times , with only a 0.38\\% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3\\% reduction in prefill-phase latency, a 56.9\\% reduction in decoding-phase latency, and a 19.8\\% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096 .","title":"ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"},{"location":"notes/2024/ZipCache/note/#zipcache-accurate-and-efficient-kv-cache-quantization-with-salient-token-identification","text":"Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, Bohan Zhuang","title":"ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"},{"location":"notes/2024/ZipCache/note/#abstract","text":"KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs. First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98\\times , with only a 0.38\\% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3\\% reduction in prefill-phase latency, a 56.9\\% reduction in decoding-phase latency, and a 19.8\\% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096 .","title":"Abstract"},{"location":"notes/2024/ZipVL/note/","text":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang Abstract The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3 \\times and improve decoding throughput by 2.8 \\times , with a minimal accuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.","title":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification"},{"location":"notes/2024/ZipVL/note/#zipvl-efficient-large-vision-language-models-with-dynamic-token-sparsification","text":"Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang","title":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification"},{"location":"notes/2024/ZipVL/note/#abstract","text":"The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3 \\times and improve decoding throughput by 2.8 \\times , with a minimal accuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.","title":"Abstract"},{"location":"notes/2024/flash_llm/","text":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/flash_llm/#flash-llm-enabling-cost-effective-and-highly-efficient-large-generative-model-inference-with-unstructured-sparsity","text":"Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/massive-activations/note/","text":"Massive Activations in Large Language Models Abstract We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#massive-activations-in-large-language-models","text":"","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#abstract","text":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Abstract"},{"location":"notes/2024/streaming-llm/note/","text":"Efficient Streaming Language Models with Attention Sinks Abstract Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#efficient-streaming-language-models-with-attention-sinks","text":"","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#abstract","text":"Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Abstract"},{"location":"notes/2025/07NWF4VE/note/","text":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching Abstract Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#accelerating-llm-inference-throughput-via-asynchronous-kv-cache-prefetching","text":"","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#abstract","text":"Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Abstract"},{"location":"notes/2025/0VRXJQ3F/note/","text":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving Abstract Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}. KV cache\u538b\u7f29\u540e\u7f3a\u4e4f\u6027\u80fd\u7684\u8bc4\u4f30 PagedAttention FalshAttention\u6ca1\u6709\u5bf9KV \u538b\u7f29\u8fdb\u884c\u9002\u914d KV \u538b\u7f29\u540e\u53ef\u80fd\u5bfc\u81f4output\u53d8\u957f\uff0c\u4ece\u800c\u589e\u52a0end-to-end\u7684latency","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#rethinking-key-value-cache-compression-techniques-for-large-language-model-serving","text":"","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#abstract","text":"Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}. KV cache\u538b\u7f29\u540e\u7f3a\u4e4f\u6027\u80fd\u7684\u8bc4\u4f30 PagedAttention FalshAttention\u6ca1\u6709\u5bf9KV \u538b\u7f29\u8fdb\u884c\u9002\u914d KV \u538b\u7f29\u540e\u53ef\u80fd\u5bfc\u81f4output\u53d8\u957f\uff0c\u4ece\u800c\u589e\u52a0end-to-end\u7684latency","title":"Abstract"},{"location":"notes/2025/1DZIJVBI/note/","text":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan Abstract This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#characterizing-compute-communication-overlap-in-gpu-accelerated-distributed-deep-learning-performance-and-power-implications","text":"Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#abstract","text":"This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Abstract"},{"location":"notes/2025/209M5GA7/note/","text":"KV Cache Compression for Inference Efficiency in LLMs: A Review Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang Abstract Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.","title":"KV Cache Compression for Inference Efficiency in LLMs: A Review"},{"location":"notes/2025/209M5GA7/note/#kv-cache-compression-for-inference-efficiency-in-llms-a-review","text":"Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang","title":"KV Cache Compression for Inference Efficiency in LLMs: A Review"},{"location":"notes/2025/209M5GA7/note/#abstract","text":"Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.","title":"Abstract"},{"location":"notes/2025/2ZU1IWL6/note/","text":"Fast and Simplex: 2-Simplicial Attention in Triton Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil Abstract Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2 -simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#fast-and-simplex-2-simplicial-attention-in-triton","text":"Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#abstract","text":"Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2 -simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Abstract"},{"location":"notes/2025/52A7RO95/note/","text":"Mixture of Experts in Large Language Models Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao Abstract This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#mixture-of-experts-in-large-language-models","text":"Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#abstract","text":"This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Abstract"},{"location":"notes/2025/ACP/note/","text":"Adaptive Computation Pruning for the Forgetting Transformer Abstract The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#adaptive-computation-pruning-for-the-forgetting-transformer","text":"","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#abstract","text":"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Abstract"},{"location":"notes/2025/AMALI/note/","text":"AMALI Abstract GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a cycle-accurate simulators \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 analytical models \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"AMALI"},{"location":"notes/2025/AMALI/note/#amali","text":"","title":"AMALI"},{"location":"notes/2025/AMALI/note/#abstract","text":"GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a cycle-accurate simulators \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 analytical models \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"Abstract"},{"location":"notes/2025/Acc-SpMM/note/","text":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores Abstract General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#acc-spmm-accelerating-general-purpose-sparse-matrix-matrix-multiplication-with-gpu-tensor-cores","text":"","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#abstract","text":"General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/AdaSkip/note/","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference Abstract Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#adaskip-adaptive-sublayer-skipping-for-accelerating-long-context-llm-inference","text":"","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#abstract","text":"Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"Abstract"},{"location":"notes/2025/AdaSplash/note/","text":"AdaSplash: Adaptive Sparse Flash Attention Abstract The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which \\alpha -entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of \\alpha -entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the \\alpha -entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing \\alpha -entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#adasplash-adaptive-sparse-flash-attention","text":"","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#abstract","text":"The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which \\alpha -entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of \\alpha -entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the \\alpha -entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing \\alpha -entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"Abstract"},{"location":"notes/2025/AdaptiveSparseTrainer/note/","text":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training Abstract The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#pruning-large-language-models-with-semi-structural-adaptive-sparse-training","text":"","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#abstract","text":"The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Abstract"},{"location":"notes/2025/Adrenaline/note/","text":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation Abstract In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#injecting-adrenaline-into-llm-serving-boosting-resource-utilization-and-throughput-via-attention-disaggregation","text":"","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#abstract","text":"In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Abstract"},{"location":"notes/2025/AhaKV/note/","text":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu Abstract Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#ahakv-adaptive-holistic-attention-driven-kv-cache-eviction-for-efficient-inference-of-large-language-models","text":"Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#abstract","text":"Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"Abstract"},{"location":"notes/2025/AmberPruner/note/","text":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang Abstract In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#amber-pruner-leveraging-nm-activation-sparsity-for-efficient-prefill-in-large-language-models","text":"Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#abstract","text":"In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Abstract"},{"location":"notes/2025/AttentionPredictor/note/","text":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference Abstract With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16 \\times KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#attentionpredictor-temporal-pattern-matters-for-efficient-llm-inference","text":"","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#abstract","text":"With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16 \\times KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"Abstract"},{"location":"notes/2025/Awesome-Efficient-Arch/note/","text":"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng Abstract Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.","title":"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"},{"location":"notes/2025/Awesome-Efficient-Arch/note/#speed-always-wins-a-survey-on-efficient-architectures-for-large-language-models","text":"Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng","title":"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"},{"location":"notes/2025/Awesome-Efficient-Arch/note/#abstract","text":"Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.","title":"Abstract"},{"location":"notes/2025/BaWA/note/","text":"BaWA","title":"BaWA"},{"location":"notes/2025/BaWA/note/#bawa","text":"","title":"BaWA"},{"location":"notes/2025/BlockFFN/note/","text":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun Abstract To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67 \\times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#blockffn-towards-end-side-acceleration-friendly-mixture-of-experts-with-chunk-level-activation-sparsity","text":"Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#abstract","text":"To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67 \\times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"Abstract"},{"location":"notes/2025/CCQ/note/","text":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang Abstract The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#ccq-convolutional-code-for-extreme-low-bit-quantization-in-llms","text":"Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#abstract","text":"The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"Abstract"},{"location":"notes/2025/COMET/note/","text":"COMET: Towards Partical W4A4KV4 LLMs Serving Abstract Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{ 2.88\\times } over cuBLAS and a \\textbf{ 2.02 \\times } throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#comet-towards-partical-w4a4kv4-llms-serving","text":"","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#abstract","text":"Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{ 2.88\\times } over cuBLAS and a \\textbf{ 2.02 \\times } throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"Abstract"},{"location":"notes/2025/CacheBlend/note/","text":"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang Abstract Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.","title":"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion"},{"location":"notes/2025/CacheBlend/note/#cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion","text":"Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang","title":"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion"},{"location":"notes/2025/CacheBlend/note/#abstract","text":"Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.","title":"Abstract"},{"location":"notes/2025/CateKV/note/","text":"CateKV","title":"CateKV"},{"location":"notes/2025/CateKV/note/#catekv","text":"","title":"CateKV"},{"location":"notes/2025/ChunkKV/note/","text":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Abstract To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#chunkkv-semantic-preserving-kv-cache-compression-for-efficient-long-context-llm-inference","text":"","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#abstract","text":"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"Abstract"},{"location":"notes/2025/CometSeed/note/","text":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu Abstract Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96\\times and for end-to-end execution, COMET delivers a 1.71\\times speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts","text":"Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#abstract","text":"Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96\\times and for end-to-end execution, COMET delivers a 1.71\\times speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Abstract"},{"location":"notes/2025/Cus-Prun/note/","text":"Pruning General Large Language Models into Customized Expert Models Abstract Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a \\underline{Cus} tom \\underline{Prun} ing method ( \\texttt{Cus-Prun} ) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, \\texttt{Cus-Prun} creates expert models without any post-training. Our experiments demonstrate that \\texttt{Cus-Prun} consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#pruning-general-large-language-models-into-customized-expert-models","text":"","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#abstract","text":"Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a \\underline{Cus} tom \\underline{Prun} ing method ( \\texttt{Cus-Prun} ) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, \\texttt{Cus-Prun} creates expert models without any post-training. Our experiments demonstrate that \\texttt{Cus-Prun} consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Abstract"},{"location":"notes/2025/DBudgetKV/note/","text":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance Abstract To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#dbudgetkv-dynamic-budget-in-kv-cache-compression-for-ensuring-optimal-performance","text":"","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#abstract","text":"To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"Abstract"},{"location":"notes/2025/DReSS/note/","text":"DReSS: Data-driven Regularized Structured Streamlining for Large Language Models Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che Abstract Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput. channel pruning mask\u76f4\u63a5\u521d\u59cb\u5316\u56fa\u5b9a\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u5c06mask\u7684channel \u4fe1\u606f\u4f20\u9012\u5230\u5176\u4ed6channel \u6839\u636e mask pruning \u518d\u6b21\u8fdb\u884cfinetuning step1\u7684\u6b63\u5219\u5316\u4f1a\u6539\u53d8weight\u7684\u5206\u5e03\uff0c\u800c\u4e14\u662f\u6309\u7167\u4eba\u5de5\u8bbe\u7f6e\u7684mask\u6539\u53d8\uff0c\u662f\u5426\u4f1a\u6539\u53d8\u6a21\u578b\u7684\u57fa\u7840\u80fd\u529b\u5462\uff1f","title":"DReSS: Data-driven Regularized Structured Streamlining for Large Language Models"},{"location":"notes/2025/DReSS/note/#dress-data-driven-regularized-structured-streamlining-for-large-language-models","text":"Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che","title":"DReSS: Data-driven Regularized Structured Streamlining for Large Language Models"},{"location":"notes/2025/DReSS/note/#abstract","text":"Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput. channel pruning mask\u76f4\u63a5\u521d\u59cb\u5316\u56fa\u5b9a\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u5c06mask\u7684channel \u4fe1\u606f\u4f20\u9012\u5230\u5176\u4ed6channel \u6839\u636e mask pruning \u518d\u6b21\u8fdb\u884cfinetuning step1\u7684\u6b63\u5219\u5316\u4f1a\u6539\u53d8weight\u7684\u5206\u5e03\uff0c\u800c\u4e14\u662f\u6309\u7167\u4eba\u5de5\u8bbe\u7f6e\u7684mask\u6539\u53d8\uff0c\u662f\u5426\u4f1a\u6539\u53d8\u6a21\u578b\u7684\u57fa\u7840\u80fd\u529b\u5462\uff1f","title":"Abstract"},{"location":"notes/2025/DeepEP/note/","text":"DeepEP Abstract","title":"DeepEP"},{"location":"notes/2025/DeepEP/note/#deepep","text":"","title":"DeepEP"},{"location":"notes/2025/DeepEP/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/DeepSeek-R1/note/","text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning","text":"","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#abstract","text":"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"Abstract"},{"location":"notes/2025/DeltaAttention/note/","text":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction Abstract The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#delta-attention-fast-and-accurate-sparse-attention-inference-by-delta-correction","text":"","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#abstract","text":"The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Abstract"},{"location":"notes/2025/DeltaLLM/note/","text":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen Abstract Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#deltallm-a-training-free-framework-exploiting-temporal-sparsity-for-efficient-edge-llm-inference","text":"Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#abstract","text":"Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"Abstract"},{"location":"notes/2025/FSA/note/","text":"Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel Ran Yan, Youhe Jiang, Binhang Yuan Abstract Recent progress in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), a state-of-the-art approach, introduces natively trainable, hardware-aligned sparse attention that delivers substantial system-level performance gains while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA relies on a query-grouping strategy that is efficient only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs typically adopt much smaller GQA groups, which limits the applicability of this sparse algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), which includes an alternative kernel design that enables efficient NSA computation across a wide range of popular LLMs with varied smaller GQA group sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5 \\times and on average 1.6 \\times kernel-level latency reduction, (ii) up to 1.25 \\times and 1.09 \\times on average end-to-end training speedup on state-of-the-art LLMs, and (iii) up to 1.36 \\times and 1.11 \\times on average end-to-end prefill speedup on state-of-the-art LLMs. The source code is open-sourced and publicly available at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.","title":"Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel"},{"location":"notes/2025/FSA/note/#flash-sparse-attention-an-alternative-efficient-implementation-of-native-sparse-attention-kernel","text":"Ran Yan, Youhe Jiang, Binhang Yuan","title":"Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel"},{"location":"notes/2025/FSA/note/#abstract","text":"Recent progress in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), a state-of-the-art approach, introduces natively trainable, hardware-aligned sparse attention that delivers substantial system-level performance gains while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA relies on a query-grouping strategy that is efficient only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs typically adopt much smaller GQA groups, which limits the applicability of this sparse algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), which includes an alternative kernel design that enables efficient NSA computation across a wide range of popular LLMs with varied smaller GQA group sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5 \\times and on average 1.6 \\times kernel-level latency reduction, (ii) up to 1.25 \\times and 1.09 \\times on average end-to-end training speedup on state-of-the-art LLMs, and (iii) up to 1.36 \\times and 1.11 \\times on average end-to-end prefill speedup on state-of-the-art LLMs. The source code is open-sourced and publicly available at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.","title":"Abstract"},{"location":"notes/2025/FastKV/note/","text":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Abstract While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00 \\times and 1.40 \\times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#fastkv-kv-cache-compression-for-fast-long-context-processing-with-token-selective-propagation","text":"","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#abstract","text":"While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00 \\times and 1.40 \\times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"Abstract"},{"location":"notes/2025/FlashInfer/note/","text":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving Abstract Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#flashinfer-efficient-and-customizable-attention-engine-for-llm-inference-serving","text":"","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#abstract","text":"Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"Abstract"},{"location":"notes/2025/FlexPrefill/note/","text":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference Abstract Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference","text":"","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#abstract","text":"Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"Abstract"},{"location":"notes/2025/FlexiDepth/note/","text":"Adaptive Layer-skipping in Pre-trained LLMs Abstract Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#adaptive-layer-skipping-in-pre-trained-llms","text":"","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#abstract","text":"Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Abstract"},{"location":"notes/2025/FoX/note/","text":"Forgetting Transformer: Softmax Attention with a Forget Gate Abstract An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#forgetting-transformer-softmax-attention-with-a-forget-gate","text":"","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#abstract","text":"An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Abstract"},{"location":"notes/2025/FreqKV/note/","text":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension Abstract Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#freqkv-frequency-domain-key-value-compression-for-efficient-context-window-extension","text":"","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#abstract","text":"Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"Abstract"},{"location":"notes/2025/GLA/note/","text":"Hardware-Efficient Attention for Fast Decoding Abstract LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2 \\times faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2 \\times . MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#hardware-efficient-attention-for-fast-decoding","text":"","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#abstract","text":"LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2 \\times faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2 \\times . MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Abstract"},{"location":"notes/2025/HATA/note/","text":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li Abstract Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top- k attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top- k Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top- k attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2 \\times speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top- k attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#hata-trainable-and-hardware-efficient-hash-aware-top-k-attention-for-scalable-large-model-inference","text":"Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#abstract","text":"Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top- k attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top- k Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top- k attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2 \\times speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top- k attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"Abstract"},{"location":"notes/2025/HCAttention/note/","text":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao Abstract Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#hcattention-extreme-kv-cache-compression-via-heterogeneous-attention-computing-for-llms","text":"Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#abstract","text":"Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"Abstract"},{"location":"notes/2025/HashAttention/note/","text":"HashAttention: Semantic Sparsity for Faster Inference Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica Abstract Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to 16\\times with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to 32\\times through task-specific fine-tuning. On A100 GPU, at 32\\times sparsity, incorporating HashAttention reduces attention latency by up to 4.3\\times in GPT-FAST and 2.54\\times in FlashDecode, and achieves up to 3.12\\times higher throughput for GPT-FAST.","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#hashattention-semantic-sparsity-for-faster-inference","text":"Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#abstract","text":"Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to 16\\times with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to 32\\times through task-specific fine-tuning. On A100 GPU, at 32\\times sparsity, incorporating HashAttention reduces attention latency by up to 4.3\\times in GPT-FAST and 2.54\\times in FlashDecode, and achieves up to 3.12\\times higher throughput for GPT-FAST.","title":"Abstract"},{"location":"notes/2025/HelixParallelism/note/","text":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani Abstract As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#helix-parallelism-rethinking-sharding-strategies-for-interactive-multi-million-token-llm-decoding","text":"Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#abstract","text":"As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Abstract"},{"location":"notes/2025/IFPruning/note/","text":"Instruction-Following Pruning for Large Language Models Abstract With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#instruction-following-pruning-for-large-language-models","text":"","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#abstract","text":"With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Abstract"},{"location":"notes/2025/KVCache-Factory/note/","text":"KVCache-Factory Abstract","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#kvcache-factory","text":"","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/KVLink/note/","text":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse Abstract We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Motivation \u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898 KVLink KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#kvlink-accelerating-large-language-models-via-efficient-kv-cache-reuse","text":"","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#abstract","text":"We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.","title":"Abstract"},{"location":"notes/2025/KVLink/note/#motivation","text":"\u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898","title":"Motivation"},{"location":"notes/2025/KVLink/note/#kvlink","text":"KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink"},{"location":"notes/2025/KVSink/note/","text":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs Zunhai Su, Kehong Yuan Abstract Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#kvsink-understanding-and-enhancing-the-preservation-of-attention-sinks-in-kv-cache-quantization-for-llms","text":"Zunhai Su, Kehong Yuan","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#abstract","text":"Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"Abstract"},{"location":"notes/2025/KVmix/note/","text":"KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang Abstract The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput. \u4ee5KIVI\u548cKVQuant\u4f5c\u4e3abaseline","title":"KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache"},{"location":"notes/2025/KVmix/note/#kvmix-gradient-based-layer-importance-aware-mixed-precision-quantization-for-kv-cache","text":"Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang","title":"KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache"},{"location":"notes/2025/KVmix/note/#abstract","text":"The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput. \u4ee5KIVI\u548cKVQuant\u4f5c\u4e3abaseline","title":"Abstract"},{"location":"notes/2025/KeepKV/note/","text":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference Abstract Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#keepkv-eliminating-output-perturbation-in-kv-cache-compression-for-efficient-llms-inference","text":"","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#abstract","text":"Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"Abstract"},{"location":"notes/2025/LIMINAL/note/","text":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis Abstract This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need","text":"Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#abstract","text":"This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Abstract"},{"location":"notes/2025/LServer/note/","text":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Abstract Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention","text":"","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#abstract","text":"Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"Abstract"},{"location":"notes/2025/LaRoSA/note/","text":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu Abstract Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#la-rosa-enhancing-llm-efficiency-via-layerwise-rotated-sparse-activation","text":"Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#abstract","text":"Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"Abstract"},{"location":"notes/2025/LeanK/note/","text":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu Abstract Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#leank-learnable-k-cache-channel-pruning-for-efficient-decoding","text":"Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#abstract","text":"Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"Abstract"},{"location":"notes/2025/LinearPatch/note/","text":"A Simple Linear Patch Revives Layer-Pruned Large Language Models Abstract Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#a-simple-linear-patch-revives-layer-pruned-large-language-models","text":"","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#abstract","text":"Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"Abstract"},{"location":"notes/2025/MIRAGE/note/","text":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar Abstract KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#mirage-kv-cache-optimization-through-parameter-remapping-for-multi-tenant-llm-serving","text":"Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#abstract","text":"KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"Abstract"},{"location":"notes/2025/MMInference/note/","text":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu Abstract The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#mminference-accelerating-pre-filling-for-long-context-vlms-via-modality-aware-permutation-sparse-attention","text":"Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#abstract","text":"The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"Abstract"},{"location":"notes/2025/MXFP4Train/note/","text":"Training LLMs with MXFP4 Albert Tseng, Tao Yu, Youngsuk Park Abstract Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications (GEMMs) and reduce training costs. However, directly using MXFP4 instead of BF16 during training significantly degrades model quality. In this work, we present the first near-lossless training recipe that uses MXFP4 GEMMs, which are 2\\times faster than FP8 on supported hardware. Our key insight is to compute unbiased gradient estimates with stochastic rounding (SR), resulting in more accurate model updates. However, directly applying SR to MXFP4 can result in high variance from block-level outliers, harming convergence. To overcome this, we use the random Hadamard tranform to theoretically bound the variance of SR. We train GPT models up to 6.7B parameters and find that our method induces minimal degradation over mixed-precision BF16 training. Our recipe computes >1/2 the training FLOPs in MXFP4, enabling an estimated speedup of >1.3\\times over FP8 and >1.7\\times over BF16 during backpropagation.","title":"Training LLMs with MXFP4"},{"location":"notes/2025/MXFP4Train/note/#training-llms-with-mxfp4","text":"Albert Tseng, Tao Yu, Youngsuk Park","title":"Training LLMs with MXFP4"},{"location":"notes/2025/MXFP4Train/note/#abstract","text":"Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications (GEMMs) and reduce training costs. However, directly using MXFP4 instead of BF16 during training significantly degrades model quality. In this work, we present the first near-lossless training recipe that uses MXFP4 GEMMs, which are 2\\times faster than FP8 on supported hardware. Our key insight is to compute unbiased gradient estimates with stochastic rounding (SR), resulting in more accurate model updates. However, directly applying SR to MXFP4 can result in high variance from block-level outliers, harming convergence. To overcome this, we use the random Hadamard tranform to theoretically bound the variance of SR. We train GPT models up to 6.7B parameters and find that our method induces minimal degradation over mixed-precision BF16 training. Our recipe computes >1/2 the training FLOPs in MXFP4, enabling an estimated speedup of >1.3\\times over FP8 and >1.7\\times over BF16 during backpropagation.","title":"Abstract"},{"location":"notes/2025/MegaScale-MoE/note/","text":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu Abstract We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88 \\times compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#megascale-moe-large-scale-communication-efficient-training-of-mixture-of-experts-models-in-production","text":"Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#abstract","text":"We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88 \\times compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"Abstract"},{"location":"notes/2025/MiniCPM4/note/","text":"MiniCPM4: Ultra-Efficient LLMs on End Devices MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun Abstract This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#minicpm4-ultra-efficient-llms-on-end-devices","text":"MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#abstract","text":"This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"Abstract"},{"location":"notes/2025/MiniMax-01/note/","text":"MiniMax-01: Scaling Foundation Models with Lightning Attention MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu Abstract We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.","title":"MiniMax-01: Scaling Foundation Models with Lightning Attention"},{"location":"notes/2025/MiniMax-01/note/#minimax-01-scaling-foundation-models-with-lightning-attention","text":"MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu","title":"MiniMax-01: Scaling Foundation Models with Lightning Attention"},{"location":"notes/2025/MiniMax-01/note/#abstract","text":"We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.","title":"Abstract"},{"location":"notes/2025/MiniMax-M1/note/","text":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention MiniMax, :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun Abstract We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.","title":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"},{"location":"notes/2025/MiniMax-M1/note/#minimax-m1-scaling-test-time-compute-efficiently-with-lightning-attention","text":"MiniMax, :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun","title":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"},{"location":"notes/2025/MiniMax-M1/note/#abstract","text":"We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.","title":"Abstract"},{"location":"notes/2025/MoBA/note/","text":"MoBA: Mixture of Block Attention for Long-Context LLMs Abstract Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#moba-mixture-of-block-attention-for-long-context-llms","text":"","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#abstract","text":"Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"Abstract"},{"location":"notes/2025/MoE-MLA-RoPE/note/","text":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat Abstract We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top- k selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#unifying-mixture-of-experts-and-multi-head-latent-attention-for-efficient-language-models","text":"Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#abstract","text":"We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top- k selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Abstract"},{"location":"notes/2025/MoR/note/","text":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun Abstract Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation","text":"Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#abstract","text":"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/MoSA/note/","text":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing Abstract Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting k tokens from a sequence of length T , MoSA reduces the computational complexity of each attention head from O(T^2) to O(k^2 + T) . This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#mixture-of-sparse-attention-content-based-learnable-sparse-attention-via-expert-choice-routing","text":"","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#abstract","text":"Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting k tokens from a sequence of length T , MoSA reduces the computational complexity of each attention head from O(T^2) to O(k^2 + T) . This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Abstract"},{"location":"notes/2025/Mosaic/note/","text":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs Abstract Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#mosaic-composite-projection-pruning-for-resource-efficient-llms","text":"","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#abstract","text":"Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Abstract"},{"location":"notes/2025/NSA/note/","text":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention Abstract Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention","text":"","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#abstract","text":"Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/NanoFlow/note/","text":"NanoFlow: Towards Optimal Large Language Model Serving Throughput Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci Abstract Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2025/NanoFlow/note/#nanoflow-towards-optimal-large-language-model-serving-throughput","text":"Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2025/NanoFlow/note/#abstract","text":"Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"Abstract"},{"location":"notes/2025/POD-Attention/note/","text":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference Abstract Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to 59\\% (mean 28\\% ), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#pod-attention-unlocking-full-prefill-decode-overlap-for-faster-llm-inference","text":"","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#abstract","text":"Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to 59\\% (mean 28\\% ), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"Abstract"},{"location":"notes/2025/PQCache/note/","text":"PQCache: Product Quantization-based KVCache for Long Context LLM Inference Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, Bin Cui Abstract As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.","title":"PQCache: Product Quantization-based KVCache for Long Context LLM Inference"},{"location":"notes/2025/PQCache/note/#pqcache-product-quantization-based-kvcache-for-long-context-llm-inference","text":"Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, Bin Cui","title":"PQCache: Product Quantization-based KVCache for Long Context LLM Inference"},{"location":"notes/2025/PQCache/note/#abstract","text":"As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.","title":"Abstract"},{"location":"notes/2025/PSA/note/","text":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving Abstract Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top- k KV cache selection, which results in a trade-off between accuracy and efficiency. A larger k improves accuracy but decreases efficiency, while a smaller k boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a \\underline{P} rogressive \\underline{S} parse \\underline{A} ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget k . This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4 \\times and 8.8 \\times , and increases end-to-end serving throughput by up to 1.4 \\times and 2.0 \\times , compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#progressive-sparse-attention-algorithm-and-system-co-design-for-efficient-attention-in-llm-serving","text":"","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#abstract","text":"Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top- k KV cache selection, which results in a trade-off between accuracy and efficiency. A larger k improves accuracy but decreases efficiency, while a smaller k boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a \\underline{P} rogressive \\underline{S} parse \\underline{A} ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget k . This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4 \\times and 8.8 \\times , and increases end-to-end serving throughput by up to 1.4 \\times and 2.0 \\times , compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Abstract"},{"location":"notes/2025/PanguUltra/note/","text":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu Abstract We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#pangu-ultra-pushing-the-limits-of-dense-large-language-models-on-ascend-npus","text":"Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#abstract","text":"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Abstract"},{"location":"notes/2025/PoD/note/","text":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity Abstract The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves 35\\% KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#compressing-kv-cache-for-long-context-llm-inference-with-inter-layer-attention-similarity","text":"","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#abstract","text":"The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves 35\\% KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Abstract"},{"location":"notes/2025/PowerAttention/note/","text":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention Abstract Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in d -layer LLMs, allowing each output token to attend to 2^d tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by 5\\sim 40\\% , especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ( 3.0\\times faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#powerattention-exponentially-scaling-of-receptive-fields-for-effective-sparse-attention","text":"","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#abstract","text":"Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in d -layer LLMs, allowing each output token to attend to 2^d tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by 5\\sim 40\\% , especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ( 3.0\\times faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"Abstract"},{"location":"notes/2025/QJL/note/","text":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead Amir Zandieh, Majid Daliri, Insu Han Abstract Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead"},{"location":"notes/2025/QJL/note/#qjl-1-bit-quantized-jl-transform-for-kv-cache-quantization-with-zero-overhead","text":"Amir Zandieh, Majid Daliri, Insu Han","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead"},{"location":"notes/2025/QJL/note/#abstract","text":"Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.","title":"Abstract"},{"location":"notes/2025/QuickSilver/note/","text":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh Abstract Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#quicksilver-speeding-up-llm-inference-through-dynamic-token-halting-kv-skipping-contextual-token-fusion-and-adaptive-matryoshka-quantization","text":"Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#abstract","text":"Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"Abstract"},{"location":"notes/2025/Qwen3/note/","text":"Qwen3 Technical Report An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu Abstract In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#qwen3-technical-report","text":"An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#abstract","text":"In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Abstract"},{"location":"notes/2025/R-KV/note/","text":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu Abstract Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 \\alpha \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#r-kv-redundancy-aware-kv-cache-compression-for-training-free-reasoning-models-acceleration","text":"Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#abstract","text":"Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 \\alpha \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"Abstract"},{"location":"notes/2025/R-Sparse/note/","text":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference Abstract Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#r-sparse-rank-aware-activation-sparsity-for-efficient-llm-inference","text":"","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#abstract","text":"Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"Abstract"},{"location":"notes/2025/RaaS/note/","text":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity Abstract Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur O(N) time and memory consumption, where N is the chain length. To mitigate O(N) time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with O(L) time but O(N) memory ( L is the cache budget, L \\ll N ). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with O(L) time and O(L) memory complexity.","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#efficient-long-decoding-inference-with-reasoning-aware-attention-sparsity","text":"","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#abstract","text":"Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur O(N) time and memory consumption, where N is the chain length. To mitigate O(N) time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with O(L) time but O(N) memory ( L is the cache budget, L \\ll N ). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with O(L) time and O(L) memory complexity.","title":"Abstract"},{"location":"notes/2025/RadialAttention/note/","text":"Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han Abstract Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n \\log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9 \\times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4 \\times longer while reducing training costs by up to 4.4 \\times compared to direct fine-tuning and accelerating inference by up to 3.7 \\times compared to dense attention inference.","title":"Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation","text":"Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han","title":"Radial Attention: O(n\\log n) Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#abstract","text":"Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n \\log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9 \\times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4 \\times longer while reducing training costs by up to 4.4 \\times compared to direct fine-tuning and accelerating inference by up to 3.7 \\times compared to dense attention inference.","title":"Abstract"},{"location":"notes/2025/ReAttention/note/","text":"ReAttention: Training-Free Infinite Context with Finite Attention Scope Abstract The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top- k attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128 \\times to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#reattention-training-free-infinite-context-with-finite-attention-scope","text":"","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#abstract","text":"The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top- k attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128 \\times to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"Abstract"},{"location":"notes/2025/ReSA/note/","text":"Rectified Sparse Attention Abstract Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42 \\times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#rectified-sparse-attention","text":"","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#abstract","text":"Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42 \\times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Abstract"},{"location":"notes/2025/RecursiveTransformers/note/","text":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster Abstract Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#relaxed-recursive-transformers-effective-parameter-sharing-with-layer-wise-lora","text":"Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#abstract","text":"Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Abstract"},{"location":"notes/2025/RetroAttention/note/","text":"Retrospective Sparse Attention for Efficient Long-Context Generation Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim Abstract Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6 \\times and accuracy by up to 21.9\\%. Quest\u7684\u6539\u8fdb\uff0c\u538b\u7f29kv cache\uff0cdecoding \u9636\u6bb5","title":"Retrospective Sparse Attention for Efficient Long-Context Generation"},{"location":"notes/2025/RetroAttention/note/#retrospective-sparse-attention-for-efficient-long-context-generation","text":"Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim","title":"Retrospective Sparse Attention for Efficient Long-Context Generation"},{"location":"notes/2025/RetroAttention/note/#abstract","text":"Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6 \\times and accuracy by up to 21.9\\%. Quest\u7684\u6539\u8fdb\uff0c\u538b\u7f29kv cache\uff0cdecoding \u9636\u6bb5","title":"Abstract"},{"location":"notes/2025/RotateKV/note/","text":"RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan Abstract Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage. \u5bf9 kv\u8fdb\u884cHadamard\u53d8\u6362","title":"RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations"},{"location":"notes/2025/RotateKV/note/#rotatekv-accurate-and-robust-2-bit-kv-cache-quantization-for-llms-via-outlier-aware-adaptive-rotations","text":"Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan","title":"RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations"},{"location":"notes/2025/RotateKV/note/#abstract","text":"Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage. \u5bf9 kv\u8fdb\u884cHadamard\u53d8\u6362","title":"Abstract"},{"location":"notes/2025/SALE/note/","text":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling Abstract Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#sale-low-bit-estimation-for-efficient-sparse-attention-in-long-context-llm-prefilling","text":"","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#abstract","text":"Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"Abstract"},{"location":"notes/2025/SDS/note/","text":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism Abstract Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#enhancing-one-shot-pruned-pre-trained-language-models-through-sparse-dense-sparse-mechanism","text":"","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#abstract","text":"Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Abstract"},{"location":"notes/2025/SEAP/note/","text":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models Abstract Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#seap-training-free-sparse-expert-activation-pruning-unlock-the-brainpower-of-large-language-models","text":"","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#abstract","text":"Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"Abstract"},{"location":"notes/2025/SageAttention3/note/","text":"SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen Abstract The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.","title":"SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training"},{"location":"notes/2025/SageAttention3/note/#sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training","text":"Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen","title":"SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training"},{"location":"notes/2025/SageAttention3/note/#abstract","text":"The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.","title":"Abstract"},{"location":"notes/2025/SeerAttention-R/note/","text":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning Abstract We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#seerattention-r-sparse-attention-adaptation-for-long-reasoning","text":"","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#abstract","text":"We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"Abstract"},{"location":"notes/2025/Seesaw/note/","text":"Seesaw: High-throughput LLM Inference via Model Re-sharding Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko Abstract To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#seesaw-high-throughput-llm-inference-via-model-re-sharding","text":"Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#abstract","text":"To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/","text":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference Abstract With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6 \\times larger batch sizes and boost throughput by up to 3.04 \\times on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV. \u4e24\u4e2a Observation Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks ShadowKV Pre-filling \u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002 Decoding \u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#shadowkv-kv-cache-in-shadows-for-high-throughput-long-context-llm-inference","text":"","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#abstract","text":"With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6 \\times larger batch sizes and boost throughput by up to 3.04 \\times on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/#observation","text":"Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks","title":"\u4e24\u4e2a Observation"},{"location":"notes/2025/ShadowKV/note/#shadowkv","text":"","title":"ShadowKV"},{"location":"notes/2025/ShadowKV/note/#pre-filling","text":"\u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002","title":"Pre-filling"},{"location":"notes/2025/ShadowKV/note/#decoding","text":"\u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"Decoding"},{"location":"notes/2025/SharePrefill/note/","text":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing Abstract Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#accelerating-prefilling-for-long-context-llms-via-sparse-pattern-sharing","text":"","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#abstract","text":"Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/SlimInfer/note/","text":"SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang Abstract Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to \\mathbf{2.53\\times} time-to-first-token (TTFT) speedup and \\mathbf{1.88\\times} end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.","title":"SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning"},{"location":"notes/2025/SlimInfer/note/#sliminfer-accelerating-long-context-llm-inference-via-dynamic-token-pruning","text":"Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang","title":"SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning"},{"location":"notes/2025/SlimInfer/note/#abstract","text":"Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to \\mathbf{2.53\\times} time-to-first-token (TTFT) speedup and \\mathbf{1.88\\times} end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.","title":"Abstract"},{"location":"notes/2025/SlimLLM/note/","text":"SlimLLM: Accurate Structured Pruning for Large Language Models Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang Abstract Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#slimllm-accurate-structured-pruning-for-large-language-models","text":"Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#abstract","text":"Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"Abstract"},{"location":"notes/2025/SpargeAttn/note/","text":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Abstract An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#spargeattn-accurate-sparse-attention-accelerating-any-model-inference","text":"","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#abstract","text":"An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"Abstract"},{"location":"notes/2025/SparsingLaw/note/","text":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity Abstract Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL- p\\% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-\\mathrm{sparsity\\ ratio} ) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#sparsing-law-towards-large-language-models-with-greater-activation-sparsity","text":"","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#abstract","text":"Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL- p\\% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-\\mathrm{sparsity\\ ratio} ) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Abstract"},{"location":"notes/2025/SpecEE/note/","text":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting Abstract Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#specee-accelerating-large-language-model-inference-with-speculative-early-exiting","text":"","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#abstract","text":"Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"Abstract"},{"location":"notes/2025/SpindleKV/note/","text":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang Abstract Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#spindlekv-a-novel-kv-cache-reduction-method-balancing-both-shallow-and-deep-layers","text":"Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#abstract","text":"Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"Abstract"},{"location":"notes/2025/StarAttention/note/","text":"Star Attention: Efficient LLM Inference over Long Sequences Abstract Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#star-attention-efficient-llm-inference-over-long-sequences","text":"","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#abstract","text":"Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Abstract"},{"location":"notes/2025/Step-3/note/","text":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang Abstract Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding","text":"StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#abstract","text":"Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Abstract"},{"location":"notes/2025/Super-Experts-Profilling/note/","text":"Unveiling Super Experts in Mixture-of-Experts Large Language Models Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan Abstract Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#unveiling-super-experts-in-mixture-of-experts-large-language-models","text":"Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Abstract"},{"location":"notes/2025/TEAL/note/","text":"Training-Free Activation Sparsity in Large Language Models Abstract Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53 \\times and 1.8 \\times at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#training-free-activation-sparsity-in-large-language-models","text":"","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#abstract","text":"Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53 \\times and 1.8 \\times at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Abstract"},{"location":"notes/2025/Task-KV/note/","text":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads Abstract KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#task-kv-task-aware-kv-cache-optimization-via-semantic-differentiation-of-attention-heads","text":"","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#abstract","text":"KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Abstract"},{"location":"notes/2025/TidalDecode/note/","text":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia Abstract Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"},{"location":"notes/2025/TidalDecode/note/#tidaldecode-fast-and-accurate-llm-decoding-with-position-persistent-sparse-attention","text":"Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"},{"location":"notes/2025/TidalDecode/note/#abstract","text":"Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.","title":"Abstract"},{"location":"notes/2025/TileLink/note/","text":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu Abstract Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from 1.17\\times to 20.76\\times speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives","text":"Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#abstract","text":"Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from 1.17\\times to 20.76\\times speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"Abstract"},{"location":"notes/2025/TokenWeave/note/","text":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference Raja Gond, Nipun Kwatra, Ramachandran Ramjee Abstract Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#tokenweave-efficient-compute-communication-overlap-for-distributed-llm-inference","text":"Raja Gond, Nipun Kwatra, Ramachandran Ramjee","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#abstract","text":"Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"Abstract"},{"location":"notes/2025/TorchAO/note/","text":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 Abstract We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#torchao-pytorch-native-training-to-serving-model-optimization","text":"Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#abstract","text":"We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"Abstract"},{"location":"notes/2025/Triton-distributed/note/","text":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu Abstract In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#triton-distributed-programming-overlapping-kernels-on-distributed-ai-systems-with-the-triton-compiler","text":"Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#abstract","text":"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Abstract"},{"location":"notes/2025/T%C3%BDr-the-Pruner/note/","text":"T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization Guanchen Li, Yixing Xu, Zeping Li, Ji Liu, Xuanwu Yin, Dong Li, Emad Barsoum Abstract Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) but often struggles to maintain performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Global pruning has the potential to find the optimal solution although resource-intensive. However, existing methods tend to rank structural saliency uniformly, ignoring inter-structure dependencies and failing to achieve end-to-end optimization. To address these limitations, we propose T\\'yr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that T\\'yr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters. evolutionary search\u5f97\u5230\u6bcf\u4e00\u5c42\u7684\u7a00\u758f\u5ea6","title":"T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization"},{"location":"notes/2025/T%C3%BDr-the-Pruner/note/#tyr-the-pruner-unlocking-accurate-50-structural-pruning-for-llms-via-global-sparsity-distribution-optimization","text":"Guanchen Li, Yixing Xu, Zeping Li, Ji Liu, Xuanwu Yin, Dong Li, Emad Barsoum","title":"T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization"},{"location":"notes/2025/T%C3%BDr-the-Pruner/note/#abstract","text":"Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) but often struggles to maintain performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Global pruning has the potential to find the optimal solution although resource-intensive. However, existing methods tend to rank structural saliency uniformly, ignoring inter-structure dependencies and failing to achieve end-to-end optimization. To address these limitations, we propose T\\'yr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that T\\'yr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters. evolutionary search\u5f97\u5230\u6bcf\u4e00\u5c42\u7684\u7a00\u758f\u5ea6","title":"Abstract"},{"location":"notes/2025/UC0D8DJ6/note/","text":"Characterizing Communication Patterns in Distributed Large Language Model Inference Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda Abstract Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#characterizing-communication-patterns-in-distributed-large-language-model-inference","text":"Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#abstract","text":"Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Abstract"},{"location":"notes/2025/XAttention/note/","text":"XAttention: Block Sparse Attention with Antidiagonal Scoring Abstract Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#xattention-block-sparse-attention-with-antidiagonal-scoring","text":"","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#abstract","text":"Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"Abstract"},{"location":"notes/2025/attention-gym/note/","text":"attention-gym Abstract","title":"attention-gym"},{"location":"notes/2025/attention-gym/note/#attention-gym","text":"","title":"attention-gym"},{"location":"notes/2025/attention-gym/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/kvpress/note/","text":"kvpress Abstract","title":"kvpress"},{"location":"notes/2025/kvpress/note/#kvpress","text":"","title":"kvpress"},{"location":"notes/2025/kvpress/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/sparse-frontier/note/","text":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs Abstract Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#the-sparse-frontier-sparse-attention-trade-offs-in-transformer-llms","text":"","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#abstract","text":"Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"Abstract"},{"location":"notes/2025/topk-decoding/note/","text":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs Abstract There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#exploiting-sparsity-for-long-context-inference-million-token-contexts-on-commodity-gpus","text":"","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#abstract","text":"There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/vAttention/note/","text":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention Abstract PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#vattention-dynamic-memory-management-for-serving-llms-without-pagedattention","text":"","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#abstract","text":"PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"Abstract"},{"location":"notes/2026/FlashOverlap/note/","text":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang Abstract Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2026/FlashOverlap/note/#flashoverlap-a-lightweight-design-for-efficiently-overlapping-communication-and-computation","text":"Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2026/FlashOverlap/note/#abstract","text":"Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"Abstract"},{"location":"weekly_paper/2025-07-25/","text":"2025-07-25 Table of Contents Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding ICWLM A Multi-Task Wireless Large Model via In-Context Learning NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models LoRA is All You Need for Safety Alignment of Reasoning LLMs Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Benchmarking LLM Privacy Recognition for Social Robot Decision Making TorchAO PyTorch-Native Training-to-Serving Model Optimization On the transferability of Sparse Autoencoders for interpreting compressed models Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Reservoir Computing as a Language Model Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Scaling Decentralized Learning with FLock IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Tiny language models An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Linear Relational Decoding of Morphology in Language Models KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme k_{cat} and K_{M} Prediction Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Characterizing Communication Patterns in Distributed Large Language Model Inference DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration KROMA Ontology Matching with Knowledge Retrieval and Large Language Models LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Authors: Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang 2025-07-24 http://arxiv.org/abs/2507.18607v1 Large language models ( s) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods . However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable -based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods. Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Authors: Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci 2025-07-24 http://arxiv.org/abs/2507.18504v1 Large Language Models ( s) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as s' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates dependency graphs into s' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing -based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with s. Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks Authors: Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief 2025-07-24 http://arxiv.org/abs/2507.18328v1 In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for driving assistance.Nevertheless, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwelltime and duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model ( )-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI. StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Authors: Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu 2025-07-24 http://arxiv.org/abs/2507.18294v1 Adapting s to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in s. Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Authors: Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan 2025-07-24 http://arxiv.org/abs/2507.18224v1 Multi-agent systems (MAS) based on large language models ( s) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer. Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation Authors: Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan 2025-07-24 http://arxiv.org/abs/2507.18212v1 Layer has emerged as a promising technique for compressing large language models ( s) while achieving proportional to the ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the original model's question-answering performance, outperforming the baseline by 4.01%. SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding Authors: Linye Wei, Shuzhang Zhong, Songqiang Xu, Runsheng Wang, Ru Huang, Meng Li 2025-07-24 http://arxiv.org/abs/2507.18181v1 Large language model ( )-based automatic speech recognition (ASR) has recently attracted a lot of attention due to its high recognition accuracy and enhanced multi-dialect support. However, the high decoding latency of s challenges the real-time ASR requirements. Although speculative decoding has been explored for better decoding efficiency, they usually ignore the key characteristics of the ASR task and achieve limited speedup. To further reduce the real-time ASR latency, in this paper, we propose a novel speculative decoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed based on our core observation that ASR decoding is audio-conditioned, which results in high output alignment between small and large ASR models, even given output mismatches in intermediate decoding steps. Therefore, SpecASR features an adaptive draft sequence generation process that dynamically modifies the draft sequence length to maximize the token acceptance length. SpecASR further proposes a draft sequence recycling strategy that reuses the previously generated draft sequence to reduce the draft ASR model latency. Moreover, a two-pass token tree generation algorithm is also proposed to balance the latency of draft and target ASR models. With extensive experimental results, we demonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the baseline autoregressive decoding and speculative decoding, respectively, without any loss in recognition accuracy. ICWLM A Multi-Task Wireless Large Model via In-Context Learning Authors: Yuxuan Wen, Xiaoming Chen, Maojun Zhang, Zhaoyang Zhang 2025-07-24 http://arxiv.org/abs/2507.18167v1 The rapid evolution of wireless technologies, particularly massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave), introduces significant network complexity and computational demands. Significant research efforts have been made to improve physical layer performance by resorting to deep learning (DL) methods, which, however, are usually task-specific and struggle with data scarcity and generalization. To address these challenges, we propose a novel In-Context Wireless Large Model (ICWLM), a wireless-native foundation model designed for simultaneous multi-task learning at the physical layer. Unlike conventional methods that adapt wireless data to pre-trained large language models ( s), ICWLM is trained directly on large-scale, mixed wireless datasets from scratch. It jointly solves multiple classical physical layer problems, including multi-user precoding (sum-rate maximization and max-min SINR) and channel prediction. A key innovation of ICWLM is its utilization of in-context learning (ICL), enabling the model to adapt to varying system configurations and channel conditions with minimal demonstration pairs, eliminating the need for extensive retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm to dynamically balance the individual task losses during multi-task training, ensuring efficient and stable learning across diverse objectives. Extensive simulation results demonstrate that ICWLM achieves competitive performance compared to task-specific methods while exhibiting remarkable generalization capabilities to unseen system configurations. This work offers a promising paradigm for developing unified and adaptive AI models for future wireless networks, potentially reducing deployment complexity and enhancing intelligent resource management. NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Authors: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu 2025-07-24 http://arxiv.org/abs/2507.18028v1 Efficiently editing knowledge stored in large language models ( s) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of s and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value ( ) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of s. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work). Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries Authors: Victor Hartman, Petter T\u00f6rnberg 2025-07-23 http://arxiv.org/abs/2507.17636v1 Negative campaigning is a central feature of political competition, yet empirical research has been limited by the high cost and limited scalability of existing classification methods. This study makes two key contributions. First, it introduces zero-shot Large Language Models ( s) as a novel approach for cross-lingual classification of negative campaigning. Using benchmark datasets in ten languages, we demonstrate that s achieve performance on par with native-speaking human coders and outperform conventional supervised machine learning approaches. Second, we leverage this novel method to conduct the largest cross-national study of negative campaigning to date, analyzing 18 million tweets posted by parliamentarians in 19 European countries between 2017 and 2022. The results reveal consistent cross-national patterns: governing parties are less likely to use negative messaging, while ideologically extreme and populist parties -- particularly those on the radical right -- engage in significantly higher levels of negativity. These findings advance our understanding of how party-level characteristics shape strategic in multiparty systems. More broadly, the study demonstrates the potential of s to enable scalable, transparent, and replicable research in political across linguistic and cultural contexts. R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang 2025-07-23 http://arxiv.org/abs/2507.17307v1 Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model ( ) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning. EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models Authors: Haochen Luo, Yuan Zhang, Chen Liu 2025-07-23 http://arxiv.org/abs/2507.17211v1 Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models ( s) to automate the generation and evolution of alpha factors for portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by -generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints. LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Authors: Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato 2025-07-23 http://arxiv.org/abs/2507.17188v1 This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and , we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model ( )-guided heuristic multi-agent reinforcement learning approach ( -HeMARL) for trajectory optimization. -HeMARL efficiently incorporates expert heuristics policy generated by the , enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds. Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Authors: Mariam ALMutairi, Hyungmin Kim 2025-07-23 http://arxiv.org/abs/2507.17134v1 Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model ( ) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by s, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of -driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty. Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models Authors: Andrii Balashov 2025-07-23 http://arxiv.org/abs/2507.17107v1 Reinforcement learning (RL) is a key post-pretraining step for aligning large language models ( s) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update . It arises naturally, without any constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source s). Moreover, the subnetworks updated by RL show substantial across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes through the lens of the lottery ticket hypothesis. LoRA is All You Need for Safety Alignment of Reasoning LLMs Authors: Yihao Xue, Baharan Mirzasoleiman 2025-07-22 http://arxiv.org/abs/2507.17075v1 Reasoning s have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure s do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the \"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe s -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off. Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Authors: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao 2025-07-22 http://arxiv.org/abs/2507.17061v1 Large language model ( ) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent , reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent systems. Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Authors: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass 2025-07-22 http://arxiv.org/abs/2507.16784v1 To break the context limits of large language models ( s) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of s trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask- mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use. Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges Authors: Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li 2025-07-22 http://arxiv.org/abs/2507.16731v1 As large language models ( s) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based s and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, , bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for -SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence. ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Authors: Shreya Saxena, Siva Prasad, Zishan Ahmad, Vishal Vaddina 2025-07-22 http://arxiv.org/abs/2507.16478v1 Code translation is a crucial process in software development and migration projects, enabling interoperability between different programming languages and enhancing software adaptability and thus longevity. Traditional automated translation methods rely heavily on handcrafted transformation rules, which often lack flexibility and scalability. Meanwhile, advanced language models present promising alternatives but are often limited by proprietary, API-based implementations that raise concerns over data security and reliance. In this paper, we present Auto-Train for Code Translation (ACT), an innovative framework that aims to improve code translation capabilities by enabling in-house finetuning of open-source Large Language Models ( s). ACT's automated pipeline significantly boosts the performance of these models, narrowing the gap between open-source accessibility and the high performance of closed-source solutions. Central to ACT is its synthetic data generation module, which builds extensive, high-quality datasets from initial code samples, incorporating unit tests to ensure functional accuracy and diversity. ACT's evaluation framework incorporates execution-level checks, offering a comprehensive assessment of translation quality. A key feature in ACT is its controller module, which manages the entire pipeline by dynamically adjusting hyperparameters, orchestrating iterative data generation, and finetuning based on real-time evaluations. This enables ACT to intelligently optimize when to continue training, generate additional targeted training data, or stop the process. Our results demonstrate that ACT consistently enhances the effectiveness of open-source models, offering businesses and developers a secure and reliable alternative. Additionally, applying our data generation pipeline to industry-scale migration projects has led to a notable increase in developer . Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video Authors: Alessandro Sebastiano Catinello, Giovanni Maria Farinella, Antonino Furnari 2025-07-22 http://arxiv.org/abs/2507.16342v1 This work tackles the problem of Online detection of Take and Release (OTR) of an object in untrimmed egocentric videos. This task is challenging due to severe label imbalance, with temporally positive annotations, and the need for precise temporal predictions. Furthermore, methods need to be computationally efficient in order to be deployed in real-world online settings. To address these challenges, we propose Mamba-OTR, a model based on the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence during inference while being trained on short video clips. To address label imbalance, our training pipeline incorporates the focal loss and a novel regularization scheme that aligns model predictions with the evaluation metric. Extensive experiments on EPIC-KITCHENS-100, the comparisons with -based approach, and the evaluation of different training and test schemes demonstrate the superiority of Mamba-OTR in both accuracy and efficiency. These finding are particularly evident when evaluating full-length videos or high frame-rate sequences, even when trained on short video snippets for computational convenience. The proposed Mamba-OTR achieves a noteworthy mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in streaming mode, versus the 20.32 of a vanilla and 25.16 of a vanilla Mamba, thus providing a strong baseline for OTR. We will publicly release the source code of Mamba-OTR to support future research. CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Authors: Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu 2025-07-22 http://arxiv.org/abs/2507.16872v1 Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models ( s). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood. In this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are , quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets. Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Authors: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov 2025-07-22 http://arxiv.org/abs/2507.16289v1 Modern sequential recommender systems, ranging from lightweight -based variants to large language models, have become increasingly prominent in academia and industry due to their strong performance in the next-item prediction task. Yet common evaluation protocols for sequential recommendations remain insufficiently developed: they often fail to reflect the corresponding recommendation task accurately, or are not aligned with real-world scenarios. Although the widely used leave-one-out split matches next-item prediction, it permits the between training and test periods, which leads to temporal leakage and unrealistically long test horizon, limiting real-world relevance. Global temporal splitting addresses these issues by evaluating on distinct future periods. However, its applications to sequential recommendations remain loosely defined, particularly in terms of selecting target interactions and constructing a validation subset that provides necessary consistency between validation and test metrics. In this paper, we demonstrate that evaluation outcomes can vary significantly across splitting strategies, influencing model rankings and practical deployment decisions. To improve reproducibility in both academic and industrial settings, we systematically compare different splitting strategies for sequential recommendations across multiple datasets and established baselines. Our findings show that prevalent splits, such as leave-one-out, may be insufficiently aligned with more realistic evaluation strategies. Code: https://github.com/monkey0head/time-to-split Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Authors: Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang 2025-07-22 http://arxiv.org/abs/2507.16274v1 The rapid scaling of large language models ( s) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Default GPU memory allocators of popular deep learning frameworks like PyTorch use online strategies without knowledge of tensor lifespans, which can waste up to 43\\% of memory and cause out-of-memory errors, rendering optimization techniques ineffective or even unusable. To address this, we introduce STWeaver, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STWeaver introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by 79.2\\% (up to 100\\%) across both dense and models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves performance by up to 32.5\\%. Benchmarking LLM Privacy Recognition for Social Robot Decision Making Authors: Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz 2025-07-22 http://arxiv.org/abs/2507.16124v1 Social robots are embodied agents that interact with people while following human norms. These robots interact using verbal and non-verbal cues, and share the physical environments of people. While social robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models ( s) presents new opportunities to develop -empowered social robots for enhanced human-robot interaction. To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, s often process sensitive personal information, particularly within home environments. Given the tension between utility and privacy risks, evaluating how current s manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box s are privacy-aware in the context of household social robots. In this study, we present a set of privacy-relevant scenarios crafted through the lens of Contextual Integrity (CI). We first survey users' privacy preferences regarding in-home social robot behaviors and then examine how their privacy orientation affects their choices of these behaviors (N = 450). We then provide the same set of scenarios and questions to state-of-the-art s (N = 10) and find that the agreement between humans and s is low. To further investigate the capabilities of s as a potential privacy controller, we implement four additional prompting strategies and compare their results. Finally, we discuss the implications and potential of AI privacy awareness in human-robot interaction. TorchAO PyTorch-Native Training-to-Serving Model Optimization Authors: Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 2025-07-21 http://arxiv.org/abs/2507.16099v1 We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 , and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, v , SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/. On the transferability of Sparse Autoencoders for interpreting compressed models Authors: Suchit Gupte, Vishnu Kabir Chhabra, Mohammad Mahdi Khalili 2025-07-21 http://arxiv.org/abs/2507.15977v1 Modern s face inference efficiency challenges due to their scale. To address this, many compression methods have been proposed, such as and quantization. However, the effect of compression on a model's interpretability remains elusive. While several model interpretation approaches exist, such as circuit discovery, Sparse Autoencoders (SAEs) have proven particularly effective in decomposing a model's activation space into its feature basis. In this work, we explore the differences in SAEs for the original and compressed models. We find that SAEs trained on the original model can interpret the compressed model albeit with slight performance degradation compared to the trained SAE on the compressed model. Furthermore, simply the original SAE itself achieves performance comparable to training a new SAE on the pruned model. This finding enables us to mitigate the extensive training costs of SAEs. Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Authors: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl 2025-07-21 http://arxiv.org/abs/2507.15826v1 Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models ( s) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment. In this paper, we present JAM (Just Ask for Music), a lightweight and intuitive framework for natural language music recommendation. JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and mixture-of-experts. We also introduce JAMSessions, a new dataset of over 100k user-query-item triples with anonymized user/item embeddings, uniquely combining conversational queries and user long-term preferences. Our results show that JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks. Reservoir Computing as a Language Model Authors: Felix K\u00f6ster, Atsushi Uchida 2025-07-21 http://arxiv.org/abs/2507.15779v1 Large Language Models ( ) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain . In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known -based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that s excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance. Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Authors: Gordon Hew, Ian McCulloh 2025-07-21 http://arxiv.org/abs/2507.16858v1 Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and -based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments. Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Authors: Jun Kang Yap, Vishnu Monn Baskaran, Wen Shan Tan, Ze Yang Ding, Hao Wang, David L. Dowe 2025-07-21 http://arxiv.org/abs/2507.15385v1 The growing integration of renewable energy sources in modern power systems has introduced significant operational challenges due to their intermittent and uncertain outputs. In recent years, mobile energy storage systems (ESSs) have emerged as a popular flexible resource for mitigating these challenges. Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility, enabling cost-effective energy delivery through the transportation network. However, the widespread deployment of mobile ESSs is often hindered by the high investment cost, which has motivated researchers to investigate utilising more readily available alternatives, such as electric vehicles (EVs) as mobile energy storage units instead. Hence, we explore this opportunity with a MIP-based day-ahead electric vehicle joint routing and scheduling problem in this work. However, solving the problem in a practical setting can often be computationally intractable since the existence of binary variables makes it combinatorial challenging. Therefore, we proposed to simplify the problem's solution process for a MIP solver by the solution search space with a -based deep learning (DL) model. This is done by training the model to rapidly predict the optimal binary solutions. In addition, unlike many existing DL approaches that assume fixed problem structures, the proposed model is designed to accommodate problems with EV fleets of any sizes. This flexibility is essential since frequent re-training can introduce significant computational overhead. We evaluated the approach with simulations on the IEEE 33-bus system coupled with the Nguyen-Dupuis transportation network. Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Authors: Elisa Sanchez-Bayona, Rodrigo Agerri 2025-07-21 http://arxiv.org/abs/2507.15357v1 This paper presents a comprehensive evaluation of the capabilities of Large Language Models ( s) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that s' performance is more influenced by features like lexical and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of s to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of s in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available. Scaling Decentralized Learning with FLock Authors: Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo 2025-07-21 http://arxiv.org/abs/2507.15349v1 Fine-tuning the large language models ( s) are prevented by the deficiency of centralized control and the massive computing and overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data. IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry Authors: Junhyeong Lee, Joon-Young Kim, Heekyu Kim, Inhyo Lee, Seunghwa Ryu 2025-07-21 http://arxiv.org/abs/2507.15268v1 The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective . This study introduces IM-Chat, a multi-agent framework based on large language models ( s), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. Overall, these findings demonstrate the viability of multi-agent systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing. CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Authors: Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung 2025-07-21 http://arxiv.org/abs/2507.15189v1 Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion ), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight -based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets. Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence Authors: Andres Navarro, Carlos de Quinto, Jos\u00e9 Alberto Hern\u00e1ndez 2025-07-20 http://arxiv.org/abs/2507.15049v1 Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as agile, intelligent nodes capable of advanced analytics and instantaneous situational awareness. This article introduces a budget-friendly quadcopter platform that unites 5G s, edge-based processing, and AI to tackle core challenges in NTN scenarios. Outfitted with a panoramic camera, robust onboard computation, and s, the drone system delivers seamless object recognition, contextual analysis, and immersive operator experiences through virtual reality VR technology. Field evaluations confirm the platform's ability to process visual streams with low latency and sustain robust 5G links. Adding s further streamlines operations by extracting actionable insights and refining collected data for decision support. Demonstrated use cases, including emergency response, infrastructure assessment, and environmental surveillance, underscore the system's adaptability in demanding contexts. From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Authors: Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi 2025-07-20 http://arxiv.org/abs/2507.14900v2 Large language models ( s) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates ping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of s, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual s (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual s. Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Authors: Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng 2025-07-20 http://arxiv.org/abs/2507.14894v1 Large Language Models ( s) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses. However, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness. In this paper, we first provide an in-depth analysis of unexpected code-switching using autoencoders and find that when s switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose \\textbf{S} parse \\textbf{A} utoencoder-guided \\textbf{S} upervised \\textbf{F} ine \\textbf{t} uning (SASFT), which teaches s to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities. Tiny language models Authors: Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter 2025-07-20 http://arxiv.org/abs/2507.14871v2 A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward block architectures pre-trained on large language models ( s). However, pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of s. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language. The data and code that support the findings of this study are openly available on https://github.com/Rg32601/Tiny-Language-Models . An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks Authors: Xinyi Wu, Steven Landgraf, Markus Ulrich, Rongjun Qin 2025-07-20 http://arxiv.org/abs/2507.14798v1 State-of-the-art 3D computer vision algorithms continue to advance in handling , unordered image sets. Recently developed foundational models for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction (DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry Grounded Transformer (VGGT), have attracted attention due to their ability to handle very image s. Evaluating DUSt3R/MASt3R/VGGT on typical aerial images matters, as these models may handle extremely low image s, stereo occlusions, and textureless regions. For redundant collections, they can accelerate 3D reconstruction by using extremely sparsified image sets. Despite tests on various computer vision benchmarks, their potential on photogrammetric aerial blocks remains unexplored. This paper conducts a comprehensive evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of the UseGeo dataset for pose estimation and dense 3D reconstruction. Results show these methods can accurately reconstruct dense point clouds from very image sets (fewer than 10 images, up to 518 pixels resolution), with completeness gains up to +50% over COLMAP. VGGT also demonstrates higher computational efficiency, scalability, and more reliable camera pose estimation. However, all exhibit limitations with high-resolution images and large sets, as pose reliability declines with more images and geometric complexity. These findings suggest -based methods cannot fully replace traditional SfM and MVS, but offer promise as complementary approaches, especially in challenging, low-resolution, and scenarios. LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering Authors: Xinxin Dong, Baoyun Peng, Haokai Ma, Yufei Wang, Zixuan Dong, Fei Hu, Xiaodong Wang 2025-07-20 http://arxiv.org/abs/2507.14784v1 Video Question Answering (VideoQA) requires identifying critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages s to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an M to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency. CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories Authors: Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran 2025-07-19 http://arxiv.org/abs/2507.14766v1 In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes. GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Authors: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan 2025-07-19 http://arxiv.org/abs/2507.14758v1 Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of s and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences. Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Authors: Xuetao Lin, Tianhao Peng, Peihong Dai, Yu Liang, Wenjun Wu 2025-07-19 http://arxiv.org/abs/2507.14698v1 EEG-based emotion recognition plays an important role in developing adaptive brain-computer systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal s with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism. Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Authors: Joren Dumoulin, Pouya Houshmand, Vikram Jain, Marian Verhelst 2025-07-19 http://arxiv.org/abs/2507.14651v1 Hybrid vision s combine the elements of conventional neural networks (NN) and vision s (ViT) to enable lightweight and accurate detection. However, several challenges remain for their efficient deployment on resource-constrained edge devices. The hybrid models suffer from a widely diverse set of NN layer types and large intermediate data tensors, hampering efficient hardware . To enable their execution at the edge, this paper proposes innovations across the hardware-scheduling stack: a.) At the lowest level, a configurable PE array supports all hybrid ViT layer types; b.) temporal loop re-ordering within one layer, enabling hardware support for normalization and softmax layers, minimizing on-chip data transfers; c.) further scheduling optimization employs layer fusion across inverted bottleneck layers to drastically reduce off-chip memory transfers. The resulting accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of 1.39 TOPS/W at 25.6 GMACs/s. Linear Relational Decoding of Morphology in Language Models Authors: Eric Xia, Jugal Kalita 2025-07-19 http://arxiv.org/abs/2507.14640v1 A two-part affine approximation has been found to be a good approximation for computations over certain subject object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation Ws, where s is a middle layer representation of a subject token and W is derived from model derivatives, is also able to accurately reproduce final object states for many relations. This linear technique is able to achieve 90% faithfulness on morphological relations, and we show similar findings multi-lingually and across models. Our findings indicate that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space, and are ly encoded by cross-layer linear transformations. KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme k_{cat} and K_{M} Prediction Authors: Saleh Alwer, Ronan Fleming 2025-07-19 http://arxiv.org/abs/2507.14639v1 Kinetic parameters such as the turnover number ( k_{cat} ) and Michaelis constant ( K_{\\mathrm{M}} ) are essential for modelling enzymatic activity but experimental data remains limited in scale and diversity. Previous methods for predicting enzyme kinetics typically use mean-pooled residue embeddings from a single protein language model to represent the protein. We present KinForm, a machine learning framework designed to improve predictive accuracy and generalisation for kinetic parameters by optimising protein feature representations. KinForm combines several residue-level embeddings (Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and ProtT5-XL-UniRef50), taken from empirically selected intermediate layers and applies weighted pooling based on per-residue binding-site probability. To counter the resulting high dimensionality, we apply dimensionality reduction using principal--component analysis (PCA) on concatenated protein features, and rebalance the training data via a similarity-based oversampling strategy. KinForm outperforms baseline methods on two benchmark datasets. Improvements are most pronounced in low sequence similarity bins. We observe improvements from binding-site probability pooling, intermediate-layer selection, PCA, and oversampling of low-identity proteins. We also find that removing sequence between folds provides a more realistic evaluation of generalisation and should be the standard over random splitting when benchmarking kinetic prediction models. Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Authors: Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang 2025-07-19 http://arxiv.org/abs/2507.14633v1 The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models ( s). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), -based models (TBMs), and s. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks. Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Authors: Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis 2025-07-18 http://arxiv.org/abs/2507.14397v1 This paper presents a limit study of -based large language model ( ) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving s requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of deployment strategies. Characterizing Communication Patterns in Distributed Large Language Model Inference Authors: Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda 2025-07-18 http://arxiv.org/abs/2507.14392v1 Large Language Models ( s) built on architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU creates significant performance constraints that limit service quality in real-world systems. This paper investigates dynamics in distributed serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense -based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production services and identify key opportunities for optimizing inference frameworks and infrastructure. DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration Authors: Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu 2025-07-18 http://arxiv.org/abs/2507.14088v1 Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model ( ) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct . To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system. KROMA Ontology Matching with Knowledge Retrieval and Large Language Models Authors: Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu 2025-07-18 http://arxiv.org/abs/2507.14032v1 Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models ( s) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the overhead from invoking s. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented s significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge -based approaches while keeping overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale. LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan 2025-07-18 http://arxiv.org/abs/2507.13681v1 Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_ s}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates inference across a wide range of long-context dialogue tasks.","title":"2025-07-25"},{"location":"weekly_paper/2025-07-25/#2025-07-25","text":"","title":"2025-07-25"},{"location":"weekly_paper/2025-07-25/#table-of-contents","text":"Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding ICWLM A Multi-Task Wireless Large Model via In-Context Learning NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models LoRA is All You Need for Safety Alignment of Reasoning LLMs Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Benchmarking LLM Privacy Recognition for Social Robot Decision Making TorchAO PyTorch-Native Training-to-Serving Model Optimization On the transferability of Sparse Autoencoders for interpreting compressed models Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Reservoir Computing as a Language Model Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Scaling Decentralized Learning with FLock IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Tiny language models An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Linear Relational Decoding of Morphology in Language Models KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme k_{cat} and K_{M} Prediction Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Characterizing Communication Patterns in Distributed Large Language Model Inference DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration KROMA Ontology Matching with Knowledge Retrieval and Large Language Models LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues","title":"Table of Contents"},{"location":"weekly_paper/2025-07-25/#explainable-mapper-charting-llm-embedding-spaces-using-perturbation-based-explanation-and-verification-agents","text":"Authors: Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang 2025-07-24 http://arxiv.org/abs/2507.18607v1 Large language models ( s) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods . However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable -based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods.","title":"Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents"},{"location":"weekly_paper/2025-07-25/#not-all-features-deserve-attention-graph-guided-dependency-learning-for-tabular-data-generation-with-language-models","text":"Authors: Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci 2025-07-24 http://arxiv.org/abs/2507.18504v1 Large Language Models ( s) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as s' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates dependency graphs into s' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing -based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with s.","title":"Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models"},{"location":"weekly_paper/2025-07-25/#enhanced-velocity-adaptive-scheme-joint-fair-access-and-age-of-information-optimization-in-vehicular-networks","text":"Authors: Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief 2025-07-24 http://arxiv.org/abs/2507.18328v1 In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for driving assistance.Nevertheless, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwelltime and duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model ( )-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI.","title":"Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks"},{"location":"weekly_paper/2025-07-25/#styleadaptedlm-enhancing-instruction-following-models-with-efficient-stylistic-transfer","text":"Authors: Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu 2025-07-24 http://arxiv.org/abs/2507.18294v1 Adapting s to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in s.","title":"StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer"},{"location":"weekly_paper/2025-07-25/#assemble-your-crew-automatic-multi-agent-communication-topology-design-via-autoregressive-graph-generation","text":"Authors: Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan 2025-07-24 http://arxiv.org/abs/2507.18224v1 Multi-agent systems (MAS) based on large language models ( s) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.","title":"Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation"},{"location":"weekly_paper/2025-07-25/#prunecomp-free-lunch-for-layer-pruned-llms-via-iterative-pruning-with-magnitude-compensation","text":"Authors: Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan 2025-07-24 http://arxiv.org/abs/2507.18212v1 Layer has emerged as a promising technique for compressing large language models ( s) while achieving proportional to the ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the original model's question-answering performance, outperforming the baseline by 4.01%.","title":"Prune&amp;Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation"},{"location":"weekly_paper/2025-07-25/#specasr-accelerating-llm-based-automatic-speech-recognition-via-speculative-decoding","text":"Authors: Linye Wei, Shuzhang Zhong, Songqiang Xu, Runsheng Wang, Ru Huang, Meng Li 2025-07-24 http://arxiv.org/abs/2507.18181v1 Large language model ( )-based automatic speech recognition (ASR) has recently attracted a lot of attention due to its high recognition accuracy and enhanced multi-dialect support. However, the high decoding latency of s challenges the real-time ASR requirements. Although speculative decoding has been explored for better decoding efficiency, they usually ignore the key characteristics of the ASR task and achieve limited speedup. To further reduce the real-time ASR latency, in this paper, we propose a novel speculative decoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed based on our core observation that ASR decoding is audio-conditioned, which results in high output alignment between small and large ASR models, even given output mismatches in intermediate decoding steps. Therefore, SpecASR features an adaptive draft sequence generation process that dynamically modifies the draft sequence length to maximize the token acceptance length. SpecASR further proposes a draft sequence recycling strategy that reuses the previously generated draft sequence to reduce the draft ASR model latency. Moreover, a two-pass token tree generation algorithm is also proposed to balance the latency of draft and target ASR models. With extensive experimental results, we demonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the baseline autoregressive decoding and speculative decoding, respectively, without any loss in recognition accuracy.","title":"SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding"},{"location":"weekly_paper/2025-07-25/#icwlm-a-multi-task-wireless-large-model-via-in-context-learning","text":"Authors: Yuxuan Wen, Xiaoming Chen, Maojun Zhang, Zhaoyang Zhang 2025-07-24 http://arxiv.org/abs/2507.18167v1 The rapid evolution of wireless technologies, particularly massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave), introduces significant network complexity and computational demands. Significant research efforts have been made to improve physical layer performance by resorting to deep learning (DL) methods, which, however, are usually task-specific and struggle with data scarcity and generalization. To address these challenges, we propose a novel In-Context Wireless Large Model (ICWLM), a wireless-native foundation model designed for simultaneous multi-task learning at the physical layer. Unlike conventional methods that adapt wireless data to pre-trained large language models ( s), ICWLM is trained directly on large-scale, mixed wireless datasets from scratch. It jointly solves multiple classical physical layer problems, including multi-user precoding (sum-rate maximization and max-min SINR) and channel prediction. A key innovation of ICWLM is its utilization of in-context learning (ICL), enabling the model to adapt to varying system configurations and channel conditions with minimal demonstration pairs, eliminating the need for extensive retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm to dynamically balance the individual task losses during multi-task training, ensuring efficient and stable learning across diverse objectives. Extensive simulation results demonstrate that ICWLM achieves competitive performance compared to task-specific methods while exhibiting remarkable generalization capabilities to unseen system configurations. This work offers a promising paradigm for developing unified and adaptive AI models for future wireless networks, potentially reducing deployment complexity and enhancing intelligent resource management.","title":"ICWLM A Multi-Task Wireless Large Model via In-Context Learning"},{"location":"weekly_paper/2025-07-25/#neuraldb-scaling-knowledge-editing-in-llms-to-100000-facts-with-neural-kv-database","text":"Authors: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu 2025-07-24 http://arxiv.org/abs/2507.18028v1 Efficiently editing knowledge stored in large language models ( s) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of s and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value ( ) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of s. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work).","title":"NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database"},{"location":"weekly_paper/2025-07-25/#who-attacks-and-why-using-llms-to-identify-negative-campaigning-in-18m-tweets-across-19-countries","text":"Authors: Victor Hartman, Petter T\u00f6rnberg 2025-07-23 http://arxiv.org/abs/2507.17636v1 Negative campaigning is a central feature of political competition, yet empirical research has been limited by the high cost and limited scalability of existing classification methods. This study makes two key contributions. First, it introduces zero-shot Large Language Models ( s) as a novel approach for cross-lingual classification of negative campaigning. Using benchmark datasets in ten languages, we demonstrate that s achieve performance on par with native-speaking human coders and outperform conventional supervised machine learning approaches. Second, we leverage this novel method to conduct the largest cross-national study of negative campaigning to date, analyzing 18 million tweets posted by parliamentarians in 19 European countries between 2017 and 2022. The results reveal consistent cross-national patterns: governing parties are less likely to use negative messaging, while ideologically extreme and populist parties -- particularly those on the radical right -- engage in significantly higher levels of negativity. These findings advance our understanding of how party-level characteristics shape strategic in multiparty systems. More broadly, the study demonstrates the potential of s to enable scalable, transparent, and replicable research in political across linguistic and cultural contexts.","title":"Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries"},{"location":"weekly_paper/2025-07-25/#r-stitch-dynamic-trajectory-stitching-for-efficient-reasoning","text":"Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang 2025-07-23 http://arxiv.org/abs/2507.17307v1 Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model ( ) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.","title":"R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning"},{"location":"weekly_paper/2025-07-25/#efs-evolutionary-factor-searching-for-sparse-portfolio-optimization-using-large-language-models","text":"Authors: Haochen Luo, Yuan Zhang, Chen Liu 2025-07-23 http://arxiv.org/abs/2507.17211v1 Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models ( s) to automate the generation and evolution of alpha factors for portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by -generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints.","title":"EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models"},{"location":"weekly_paper/2025-07-25/#llm-meets-the-sky-heuristic-multi-agent-reinforcement-learning-for-secure-heterogeneous-uav-networks","text":"Authors: Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato 2025-07-23 http://arxiv.org/abs/2507.17188v1 This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and , we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model ( )-guided heuristic multi-agent reinforcement learning approach ( -HeMARL) for trajectory optimization. -HeMARL efficiently incorporates expert heuristics policy generated by the , enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.","title":"LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks"},{"location":"weekly_paper/2025-07-25/#resilient-multi-agent-negotiation-for-medical-supply-chainsintegrating-llms-and-blockchain-for-transparent-coordination","text":"Authors: Mariam ALMutairi, Hyungmin Kim 2025-07-23 http://arxiv.org/abs/2507.17134v1 Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model ( ) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by s, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of -driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty.","title":"Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination"},{"location":"weekly_paper/2025-07-25/#reinforcement-learning-fine-tunes-a-sparse-subnetwork-in-large-language-models","text":"Authors: Andrii Balashov 2025-07-23 http://arxiv.org/abs/2507.17107v1 Reinforcement learning (RL) is a key post-pretraining step for aligning large language models ( s) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update . It arises naturally, without any constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source s). Moreover, the subnetworks updated by RL show substantial across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes through the lens of the lottery ticket hypothesis.","title":"Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models"},{"location":"weekly_paper/2025-07-25/#lora-is-all-you-need-for-safety-alignment-of-reasoning-llms","text":"Authors: Yihao Xue, Baharan Mirzasoleiman 2025-07-22 http://arxiv.org/abs/2507.17075v1 Reasoning s have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure s do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the \"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe s -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off.","title":"LoRA is All You Need for Safety Alignment of Reasoning LLMs"},{"location":"weekly_paper/2025-07-25/#parallelism-meets-adaptiveness-scalable-documents-understanding-in-multi-agent-llm-systems","text":"Authors: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao 2025-07-22 http://arxiv.org/abs/2507.17061v1 Large language model ( ) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent , reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent systems.","title":"Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems"},{"location":"weekly_paper/2025-07-25/#beyond-context-limits-subconscious-threads-for-long-horizon-reasoning","text":"Authors: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass 2025-07-22 http://arxiv.org/abs/2507.16784v1 To break the context limits of large language models ( s) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of s trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask- mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.","title":"Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning"},{"location":"weekly_paper/2025-07-25/#collaborative-inference-and-learning-between-edge-slms-and-cloud-llms-a-survey-of-algorithms-execution-and-open-challenges","text":"Authors: Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li 2025-07-22 http://arxiv.org/abs/2507.16731v1 As large language models ( s) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based s and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, , bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for -SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.","title":"Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges"},{"location":"weekly_paper/2025-07-25/#act-bridging-the-gap-in-code-translation-through-synthetic-data-generation-adaptive-training","text":"Authors: Shreya Saxena, Siva Prasad, Zishan Ahmad, Vishal Vaddina 2025-07-22 http://arxiv.org/abs/2507.16478v1 Code translation is a crucial process in software development and migration projects, enabling interoperability between different programming languages and enhancing software adaptability and thus longevity. Traditional automated translation methods rely heavily on handcrafted transformation rules, which often lack flexibility and scalability. Meanwhile, advanced language models present promising alternatives but are often limited by proprietary, API-based implementations that raise concerns over data security and reliance. In this paper, we present Auto-Train for Code Translation (ACT), an innovative framework that aims to improve code translation capabilities by enabling in-house finetuning of open-source Large Language Models ( s). ACT's automated pipeline significantly boosts the performance of these models, narrowing the gap between open-source accessibility and the high performance of closed-source solutions. Central to ACT is its synthetic data generation module, which builds extensive, high-quality datasets from initial code samples, incorporating unit tests to ensure functional accuracy and diversity. ACT's evaluation framework incorporates execution-level checks, offering a comprehensive assessment of translation quality. A key feature in ACT is its controller module, which manages the entire pipeline by dynamically adjusting hyperparameters, orchestrating iterative data generation, and finetuning based on real-time evaluations. This enables ACT to intelligently optimize when to continue training, generate additional targeted training data, or stop the process. Our results demonstrate that ACT consistently enhances the effectiveness of open-source models, offering businesses and developers a secure and reliable alternative. Additionally, applying our data generation pipeline to industry-scale migration projects has led to a notable increase in developer .","title":"ACT Bridging the Gap in Code Translation through Synthetic Data Generation &amp; Adaptive Training"},{"location":"weekly_paper/2025-07-25/#mamba-otr-a-mamba-based-solution-for-online-take-and-release-detection-from-untrimmed-egocentric-video","text":"Authors: Alessandro Sebastiano Catinello, Giovanni Maria Farinella, Antonino Furnari 2025-07-22 http://arxiv.org/abs/2507.16342v1 This work tackles the problem of Online detection of Take and Release (OTR) of an object in untrimmed egocentric videos. This task is challenging due to severe label imbalance, with temporally positive annotations, and the need for precise temporal predictions. Furthermore, methods need to be computationally efficient in order to be deployed in real-world online settings. To address these challenges, we propose Mamba-OTR, a model based on the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence during inference while being trained on short video clips. To address label imbalance, our training pipeline incorporates the focal loss and a novel regularization scheme that aligns model predictions with the evaluation metric. Extensive experiments on EPIC-KITCHENS-100, the comparisons with -based approach, and the evaluation of different training and test schemes demonstrate the superiority of Mamba-OTR in both accuracy and efficiency. These finding are particularly evident when evaluating full-length videos or high frame-rate sequences, even when trained on short video snippets for computational convenience. The proposed Mamba-OTR achieves a noteworthy mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in streaming mode, versus the 20.32 of a vanilla and 25.16 of a vanilla Mamba, thus providing a strong baseline for OTR. We will publicly release the source code of Mamba-OTR to support future research.","title":"Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video"},{"location":"weekly_paper/2025-07-25/#compleak-deep-learning-model-compression-exacerbates-privacy-leakage","text":"Authors: Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu 2025-07-22 http://arxiv.org/abs/2507.16872v1 Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models ( s). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood. In this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are , quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets.","title":"CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage"},{"location":"weekly_paper/2025-07-25/#time-to-split-exploring-data-splitting-strategies-for-offline-evaluation-of-sequential-recommenders","text":"Authors: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov 2025-07-22 http://arxiv.org/abs/2507.16289v1 Modern sequential recommender systems, ranging from lightweight -based variants to large language models, have become increasingly prominent in academia and industry due to their strong performance in the next-item prediction task. Yet common evaluation protocols for sequential recommendations remain insufficiently developed: they often fail to reflect the corresponding recommendation task accurately, or are not aligned with real-world scenarios. Although the widely used leave-one-out split matches next-item prediction, it permits the between training and test periods, which leads to temporal leakage and unrealistically long test horizon, limiting real-world relevance. Global temporal splitting addresses these issues by evaluating on distinct future periods. However, its applications to sequential recommendations remain loosely defined, particularly in terms of selecting target interactions and constructing a validation subset that provides necessary consistency between validation and test metrics. In this paper, we demonstrate that evaluation outcomes can vary significantly across splitting strategies, influencing model rankings and practical deployment decisions. To improve reproducibility in both academic and industrial settings, we systematically compare different splitting strategies for sequential recommendations across multiple datasets and established baselines. Our findings show that prevalent splits, such as leave-one-out, may be insufficiently aligned with more realistic evaluation strategies. Code: https://github.com/monkey0head/time-to-split","title":"Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders"},{"location":"weekly_paper/2025-07-25/#reducing-gpu-memory-fragmentation-via-spatio-temporal-planning-for-efficient-large-scale-model-training","text":"Authors: Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang 2025-07-22 http://arxiv.org/abs/2507.16274v1 The rapid scaling of large language models ( s) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Default GPU memory allocators of popular deep learning frameworks like PyTorch use online strategies without knowledge of tensor lifespans, which can waste up to 43\\% of memory and cause out-of-memory errors, rendering optimization techniques ineffective or even unusable. To address this, we introduce STWeaver, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STWeaver introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by 79.2\\% (up to 100\\%) across both dense and models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves performance by up to 32.5\\%.","title":"Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training"},{"location":"weekly_paper/2025-07-25/#benchmarking-llm-privacy-recognition-for-social-robot-decision-making","text":"Authors: Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz 2025-07-22 http://arxiv.org/abs/2507.16124v1 Social robots are embodied agents that interact with people while following human norms. These robots interact using verbal and non-verbal cues, and share the physical environments of people. While social robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models ( s) presents new opportunities to develop -empowered social robots for enhanced human-robot interaction. To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, s often process sensitive personal information, particularly within home environments. Given the tension between utility and privacy risks, evaluating how current s manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box s are privacy-aware in the context of household social robots. In this study, we present a set of privacy-relevant scenarios crafted through the lens of Contextual Integrity (CI). We first survey users' privacy preferences regarding in-home social robot behaviors and then examine how their privacy orientation affects their choices of these behaviors (N = 450). We then provide the same set of scenarios and questions to state-of-the-art s (N = 10) and find that the agreement between humans and s is low. To further investigate the capabilities of s as a potential privacy controller, we implement four additional prompting strategies and compare their results. Finally, we discuss the implications and potential of AI privacy awareness in human-robot interaction.","title":"Benchmarking LLM Privacy Recognition for Social Robot Decision Making"},{"location":"weekly_paper/2025-07-25/#torchao-pytorch-native-training-to-serving-model-optimization","text":"Authors: Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 2025-07-21 http://arxiv.org/abs/2507.16099v1 We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 , and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, v , SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"TorchAO PyTorch-Native Training-to-Serving Model Optimization"},{"location":"weekly_paper/2025-07-25/#on-the-transferability-of-sparse-autoencoders-for-interpreting-compressed-models","text":"Authors: Suchit Gupte, Vishnu Kabir Chhabra, Mohammad Mahdi Khalili 2025-07-21 http://arxiv.org/abs/2507.15977v1 Modern s face inference efficiency challenges due to their scale. To address this, many compression methods have been proposed, such as and quantization. However, the effect of compression on a model's interpretability remains elusive. While several model interpretation approaches exist, such as circuit discovery, Sparse Autoencoders (SAEs) have proven particularly effective in decomposing a model's activation space into its feature basis. In this work, we explore the differences in SAEs for the original and compressed models. We find that SAEs trained on the original model can interpret the compressed model albeit with slight performance degradation compared to the trained SAE on the compressed model. Furthermore, simply the original SAE itself achieves performance comparable to training a new SAE on the pruned model. This finding enables us to mitigate the extensive training costs of SAEs.","title":"On the transferability of Sparse Autoencoders for interpreting compressed models"},{"location":"weekly_paper/2025-07-25/#just-ask-for-music-jam-multimodal-and-personalized-natural-language-music-recommendation","text":"Authors: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl 2025-07-21 http://arxiv.org/abs/2507.15826v1 Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models ( s) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment. In this paper, we present JAM (Just Ask for Music), a lightweight and intuitive framework for natural language music recommendation. JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and mixture-of-experts. We also introduce JAMSessions, a new dataset of over 100k user-query-item triples with anonymized user/item embeddings, uniquely combining conversational queries and user long-term preferences. Our results show that JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks.","title":"Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation"},{"location":"weekly_paper/2025-07-25/#reservoir-computing-as-a-language-model","text":"Authors: Felix K\u00f6ster, Atsushi Uchida 2025-07-21 http://arxiv.org/abs/2507.15779v1 Large Language Models ( ) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain . In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known -based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that s excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance.","title":"Reservoir Computing as a Language Model"},{"location":"weekly_paper/2025-07-25/#who-leads-in-the-shadows-ergm-and-centrality-analysis-of-congressional-democrats-on-bluesky","text":"Authors: Gordon Hew, Ian McCulloh 2025-07-21 http://arxiv.org/abs/2507.16858v1 Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and -based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments.","title":"Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky"},{"location":"weekly_paper/2025-07-25/#transformer-based-deep-learning-model-for-joint-routing-and-scheduling-with-varying-electric-vehicle-numbers","text":"Authors: Jun Kang Yap, Vishnu Monn Baskaran, Wen Shan Tan, Ze Yang Ding, Hao Wang, David L. Dowe 2025-07-21 http://arxiv.org/abs/2507.15385v1 The growing integration of renewable energy sources in modern power systems has introduced significant operational challenges due to their intermittent and uncertain outputs. In recent years, mobile energy storage systems (ESSs) have emerged as a popular flexible resource for mitigating these challenges. Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility, enabling cost-effective energy delivery through the transportation network. However, the widespread deployment of mobile ESSs is often hindered by the high investment cost, which has motivated researchers to investigate utilising more readily available alternatives, such as electric vehicles (EVs) as mobile energy storage units instead. Hence, we explore this opportunity with a MIP-based day-ahead electric vehicle joint routing and scheduling problem in this work. However, solving the problem in a practical setting can often be computationally intractable since the existence of binary variables makes it combinatorial challenging. Therefore, we proposed to simplify the problem's solution process for a MIP solver by the solution search space with a -based deep learning (DL) model. This is done by training the model to rapidly predict the optimal binary solutions. In addition, unlike many existing DL approaches that assume fixed problem structures, the proposed model is designed to accommodate problems with EV fleets of any sizes. This flexibility is essential since frequent re-training can introduce significant computational overhead. We evaluated the approach with simulations on the IEEE 33-bus system coupled with the Nguyen-Dupuis transportation network.","title":"Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers"},{"location":"weekly_paper/2025-07-25/#metaphor-and-large-language-models-when-surface-features-matter-more-than-deep-understanding","text":"Authors: Elisa Sanchez-Bayona, Rodrigo Agerri 2025-07-21 http://arxiv.org/abs/2507.15357v1 This paper presents a comprehensive evaluation of the capabilities of Large Language Models ( s) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that s' performance is more influenced by features like lexical and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of s to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of s in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.","title":"Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding"},{"location":"weekly_paper/2025-07-25/#scaling-decentralized-learning-with-flock","text":"Authors: Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo 2025-07-21 http://arxiv.org/abs/2507.15349v1 Fine-tuning the large language models ( s) are prevented by the deficiency of centralized control and the massive computing and overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.","title":"Scaling Decentralized Learning with FLock"},{"location":"weekly_paper/2025-07-25/#im-chat-a-multi-agent-llm-based-framework-for-knowledge-transfer-in-injection-molding-industry","text":"Authors: Junhyeong Lee, Joon-Young Kim, Heekyu Kim, Inhyo Lee, Seunghwa Ryu 2025-07-21 http://arxiv.org/abs/2507.15268v1 The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective . This study introduces IM-Chat, a multi-agent framework based on large language models ( s), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. Overall, these findings demonstrate the viability of multi-agent systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing.","title":"IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry"},{"location":"weekly_paper/2025-07-25/#chadet-cross-hierarchical-attention-for-depth-completion-using-unsupervised-lightweight-transformer","text":"Authors: Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung 2025-07-21 http://arxiv.org/abs/2507.15189v1 Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion ), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight -based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets.","title":"CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer"},{"location":"weekly_paper/2025-07-25/#beyond-visual-line-of-sight-uavs-with-edge-ai-connected-llms-and-vr-for-autonomous-aerial-intelligence","text":"Authors: Andres Navarro, Carlos de Quinto, Jos\u00e9 Alberto Hern\u00e1ndez 2025-07-20 http://arxiv.org/abs/2507.15049v1 Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as agile, intelligent nodes capable of advanced analytics and instantaneous situational awareness. This article introduces a budget-friendly quadcopter platform that unites 5G s, edge-based processing, and AI to tackle core challenges in NTN scenarios. Outfitted with a panoramic camera, robust onboard computation, and s, the drone system delivers seamless object recognition, contextual analysis, and immersive operator experiences through virtual reality VR technology. Field evaluations confirm the platform's ability to process visual streams with low latency and sustain robust 5G links. Adding s further streamlines operations by extracting actionable insights and refining collected data for decision support. Demonstrated use cases, including emergency response, infrastructure assessment, and environmental surveillance, underscore the system's adaptability in demanding contexts.","title":"Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence"},{"location":"weekly_paper/2025-07-25/#from-neurons-to-semantics-evaluating-cross-linguistic-alignment-capabilities-of-large-language-models-via-neurons-alignment","text":"Authors: Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi 2025-07-20 http://arxiv.org/abs/2507.14900v2 Large language models ( s) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates ping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of s, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual s (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual s.","title":"From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment"},{"location":"weekly_paper/2025-07-25/#sparse-autoencoder-guided-supervised-finetuning-to-mitigate-unexpected-code-switching-in-llms","text":"Authors: Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng 2025-07-20 http://arxiv.org/abs/2507.14894v1 Large Language Models ( s) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses. However, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness. In this paper, we first provide an in-depth analysis of unexpected code-switching using autoencoders and find that when s switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose \\textbf{S} parse \\textbf{A} utoencoder-guided \\textbf{S} upervised \\textbf{F} ine \\textbf{t} uning (SASFT), which teaches s to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities.","title":"Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs"},{"location":"weekly_paper/2025-07-25/#tiny-language-models","text":"Authors: Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter 2025-07-20 http://arxiv.org/abs/2507.14871v2 A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward block architectures pre-trained on large language models ( s). However, pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of s. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language. The data and code that support the findings of this study are openly available on https://github.com/Rg32601/Tiny-Language-Models .","title":"Tiny language models"},{"location":"weekly_paper/2025-07-25/#an-evaluation-of-dust3rmast3rvggt-3d-reconstruction-on-photogrammetric-aerial-blocks","text":"Authors: Xinyi Wu, Steven Landgraf, Markus Ulrich, Rongjun Qin 2025-07-20 http://arxiv.org/abs/2507.14798v1 State-of-the-art 3D computer vision algorithms continue to advance in handling , unordered image sets. Recently developed foundational models for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction (DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry Grounded Transformer (VGGT), have attracted attention due to their ability to handle very image s. Evaluating DUSt3R/MASt3R/VGGT on typical aerial images matters, as these models may handle extremely low image s, stereo occlusions, and textureless regions. For redundant collections, they can accelerate 3D reconstruction by using extremely sparsified image sets. Despite tests on various computer vision benchmarks, their potential on photogrammetric aerial blocks remains unexplored. This paper conducts a comprehensive evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of the UseGeo dataset for pose estimation and dense 3D reconstruction. Results show these methods can accurately reconstruct dense point clouds from very image sets (fewer than 10 images, up to 518 pixels resolution), with completeness gains up to +50% over COLMAP. VGGT also demonstrates higher computational efficiency, scalability, and more reliable camera pose estimation. However, all exhibit limitations with high-resolution images and large sets, as pose reliability declines with more images and geometric complexity. These findings suggest -based methods cannot fully replace traditional SfM and MVS, but offer promise as complementary approaches, especially in challenging, low-resolution, and scenarios.","title":"An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks"},{"location":"weekly_paper/2025-07-25/#leadqa-llm-driven-context-aware-temporal-grounding-for-video-question-answering","text":"Authors: Xinxin Dong, Baoyun Peng, Haokai Ma, Yufei Wang, Zixuan Dong, Fei Hu, Xiaodong Wang 2025-07-20 http://arxiv.org/abs/2507.14784v1 Video Question Answering (VideoQA) requires identifying critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages s to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an M to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency.","title":"LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering"},{"location":"weekly_paper/2025-07-25/#cxr-tft-multi-modal-temporal-fusion-transformer-for-predicting-chest-x-ray-trajectories","text":"Authors: Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran 2025-07-19 http://arxiv.org/abs/2507.14766v1 In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes.","title":"CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories"},{"location":"weekly_paper/2025-07-25/#grace-generative-recommendation-via-journey-aware-sparse-attention-on-chain-of-thought-tokenization","text":"Authors: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan 2025-07-19 http://arxiv.org/abs/2507.14758v1 Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of s and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.","title":"GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization"},{"location":"weekly_paper/2025-07-25/#spatial-temporal-transformer-with-curriculum-learning-for-eeg-based-emotion-recognition","text":"Authors: Xuetao Lin, Tianhao Peng, Peihong Dai, Yu Liang, Wenjun Wu 2025-07-19 http://arxiv.org/abs/2507.14698v1 EEG-based emotion recognition plays an important role in developing adaptive brain-computer systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal s with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.","title":"Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition"},{"location":"weekly_paper/2025-07-25/#enabling-efficient-hardware-acceleration-of-hybrid-vision-transformer-vit-networks-at-the-edge","text":"Authors: Joren Dumoulin, Pouya Houshmand, Vikram Jain, Marian Verhelst 2025-07-19 http://arxiv.org/abs/2507.14651v1 Hybrid vision s combine the elements of conventional neural networks (NN) and vision s (ViT) to enable lightweight and accurate detection. However, several challenges remain for their efficient deployment on resource-constrained edge devices. The hybrid models suffer from a widely diverse set of NN layer types and large intermediate data tensors, hampering efficient hardware . To enable their execution at the edge, this paper proposes innovations across the hardware-scheduling stack: a.) At the lowest level, a configurable PE array supports all hybrid ViT layer types; b.) temporal loop re-ordering within one layer, enabling hardware support for normalization and softmax layers, minimizing on-chip data transfers; c.) further scheduling optimization employs layer fusion across inverted bottleneck layers to drastically reduce off-chip memory transfers. The resulting accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of 1.39 TOPS/W at 25.6 GMACs/s.","title":"Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge"},{"location":"weekly_paper/2025-07-25/#linear-relational-decoding-of-morphology-in-language-models","text":"Authors: Eric Xia, Jugal Kalita 2025-07-19 http://arxiv.org/abs/2507.14640v1 A two-part affine approximation has been found to be a good approximation for computations over certain subject object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation Ws, where s is a middle layer representation of a subject token and W is derived from model derivatives, is also able to accurately reproduce final object states for many relations. This linear technique is able to achieve 90% faithfulness on morphological relations, and we show similar findings multi-lingually and across models. Our findings indicate that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space, and are ly encoded by cross-layer linear transformations.","title":"Linear Relational Decoding of Morphology in Language Models"},{"location":"weekly_paper/2025-07-25/#kinform-kinetics-informed-feature-optimised-representation-models-for-enzyme-k_cat-and-k_m-prediction","text":"Authors: Saleh Alwer, Ronan Fleming 2025-07-19 http://arxiv.org/abs/2507.14639v1 Kinetic parameters such as the turnover number ( k_{cat} ) and Michaelis constant ( K_{\\mathrm{M}} ) are essential for modelling enzymatic activity but experimental data remains limited in scale and diversity. Previous methods for predicting enzyme kinetics typically use mean-pooled residue embeddings from a single protein language model to represent the protein. We present KinForm, a machine learning framework designed to improve predictive accuracy and generalisation for kinetic parameters by optimising protein feature representations. KinForm combines several residue-level embeddings (Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and ProtT5-XL-UniRef50), taken from empirically selected intermediate layers and applies weighted pooling based on per-residue binding-site probability. To counter the resulting high dimensionality, we apply dimensionality reduction using principal--component analysis (PCA) on concatenated protein features, and rebalance the training data via a similarity-based oversampling strategy. KinForm outperforms baseline methods on two benchmark datasets. Improvements are most pronounced in low sequence similarity bins. We observe improvements from binding-site probability pooling, intermediate-layer selection, PCA, and oversampling of low-identity proteins. We also find that removing sequence between folds provides a more realistic evaluation of generalisation and should be the standard over random splitting when benchmarking kinetic prediction models.","title":"KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme k_{cat} and K_{M} Prediction"},{"location":"weekly_paper/2025-07-25/#agentic-satellite-augmented-low-altitude-economy-and-terrestrial-networks-a-survey-on-generative-approaches","text":"Authors: Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang 2025-07-19 http://arxiv.org/abs/2507.14633v1 The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models ( s). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), -based models (TBMs), and s. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks.","title":"Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches"},{"location":"weekly_paper/2025-07-25/#efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need","text":"Authors: Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis 2025-07-18 http://arxiv.org/abs/2507.14397v1 This paper presents a limit study of -based large language model ( ) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving s requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of deployment strategies.","title":"Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"weekly_paper/2025-07-25/#characterizing-communication-patterns-in-distributed-large-language-model-inference","text":"Authors: Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda 2025-07-18 http://arxiv.org/abs/2507.14392v1 Large Language Models ( s) built on architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU creates significant performance constraints that limit service quality in real-world systems. This paper investigates dynamics in distributed serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense -based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production services and identify key opportunities for optimizing inference frameworks and infrastructure.","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"weekly_paper/2025-07-25/#dpmt-dual-process-multi-scale-theory-of-mind-framework-for-real-time-human-ai-collaboration","text":"Authors: Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu 2025-07-18 http://arxiv.org/abs/2507.14088v1 Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model ( ) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct . To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.","title":"DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration"},{"location":"weekly_paper/2025-07-25/#kroma-ontology-matching-with-knowledge-retrieval-and-large-language-models","text":"Authors: Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu 2025-07-18 http://arxiv.org/abs/2507.14032v1 Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models ( s) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the overhead from invoking s. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented s significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge -based approaches while keeping overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.","title":"KROMA Ontology Matching with Knowledge Retrieval and Large Language Models"},{"location":"weekly_paper/2025-07-25/#loopserve-an-adaptive-dual-phase-llm-inference-acceleration-system-for-multi-turn-dialogues","text":"Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan 2025-07-18 http://arxiv.org/abs/2507.13681v1 Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_ s}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates inference across a wide range of long-context dialogue tasks.","title":"LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues"},{"location":"weekly_paper/2025-08-01/","text":"2025-08-01 Table of Contents DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Unveiling Super Experts in Mixture-of-Experts Large Language Models Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Large Language Models for Wireless Communications From Adaptation to Autonomy Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Agentic Web Weaving the Next Web with AI Agents SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Latent Inter-User Difference Modeling for LLM Personalization METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Modeling Professionalism in Expert Questioning through Linguistic Differentiation FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression The Carbon Cost of Conversation, Sustainability in the Age of Language Models CaliDrop KV Cache Compression with Calibration CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation LowKeyEMG Electromyographic typing with a reduced keyset Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers RegScore Scoring Systems for Regression Tasks MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data Authors: Rabeya Tus Sadia, Qiang Cheng 2025-07-31 http://arxiv.org/abs/2507.23676v1 Microbiome data analysis is essential for understanding host health and disease, yet its inherent and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model ( ). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation. MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Authors: Qian Zhao, Zhuo Sun, Bin Guo, Zhiwen Yu 2025-07-31 http://arxiv.org/abs/2507.23633v1 Agent-assisted memory recall is one critical research problem in the field of human-computer interaction. In conventional methods, the agent can retrieve information from its equipped memory module to help the person recall incomplete or vague memories. The limited size of memory module hinders the acquisition of complete memories and impacts the memory recall performance in practice. Memory theories suggest that the person's relevant memory can be proactively activated through some effective cues. Inspired by this, we propose a novel strategy-guided agent-assisted memory recall method, allowing the agent to transform an original query into a cue-rich one via the judiciously designed strategy to help the person recall memories. To this end, there are two key challenges. (1) How to choose the appropriate recall strategy for diverse forgetting scenarios with distinct memory-recall characteristics? (2) How to obtain the high-quality responses leveraging recall strategies, given only abstract and ly annotated strategy patterns? To address the challenges, we propose a Recall Router framework. Specifically, we design a 5W Recall Map to classify memory queries into five typical scenarios and define fifteen recall strategy patterns across the corresponding scenarios. We then propose a hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to optimize the selection of strategy and the generation of strategy responses. We construct an instruction tuning dataset and fine-tune multiple open-source large language models ( s) to develop MemoCue, an agent that excels in providing memory-inspired responses. Experiments on three representative datasets show that MemoCue surpasses -based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications. Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Authors: Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu 2025-07-31 http://arxiv.org/abs/2507.23370v1 Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models ( s), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of -based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, , and selection. We conduct extensive experiments using three leading s on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent. Unveiling Super Experts in Mixture-of-Experts Large Language Models Authors: Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan 2025-07-31 http://arxiv.org/abs/2507.23279v1 Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models ( s). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE s. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE s, and despite their limited number, them leads to a significant decline in model performance (e.g., three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE s rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE . The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling. Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders Authors: Carolina Zheng, Nicolas Beltran-Velez, Sweta Karlekar, Claudia Shi, Achille Nazaret, Asif Mallik, Amir Feder, David M. Blei 2025-07-31 http://arxiv.org/abs/2507.23220v1 Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \\textit{topic judge}, an -based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of outputs. MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Authors: Yuqi Pang, Bowen Yang, Yun Cao, Fan Rong, Xiaoyu Li, Chen He 2025-07-30 http://arxiv.org/abs/2507.22805v1 Vision large language models (V s) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream s (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA. Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques Authors: Weide Liu, Wei Zhou, Jun Liu, Ping Hu, Jun Cheng, Jungong Han, Weisi Lin 2025-07-30 http://arxiv.org/abs/2507.22791v1 Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and -based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions. trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Authors: MohammadAmin Alamalhoda, Arsalan Firoozi, Alessandro Venturino, Sandra Siegert 2025-07-30 http://arxiv.org/abs/2507.22635v1 The shape of a cell contains essential information about its function within the biological system. Segmenting these structures from large-scale 3D microscopy images is challenging, limiting clinical insights especially for microglia, immune-associated cells involved in neurodegenerative diseases. Existing segmentation methods mainly focus on cell bodies, struggle with ping structures, perform poorly on noisy images, require hyperparameter tuning for each new dataset, or rely on tedious semi-automated approaches. We introduce trAIce3D, a deep-learning architecture designed for precise microglia segmentation, capturing both somas and branches. It employs a two-stage approach: first, a 3D U-Net with vision s in the encoder detects somas using a sliding-window technique to cover the entire image. Then, the same architecture, enhanced with cross-attention blocks in skip connections, refines each soma and its branches by using soma coordinates as a prompt and a 3D window around the target cell as input. Training occurs in two phases: self-supervised Soma Segmentation, followed by prompt-based Branch Segmentation, leveraging pre-trained weights from the first phase. Trained and evaluated on a dataset of 41,230 microglial cells, trAIce3D significantly improves segmentation accuracy and generalization, enabling scalable analysis of complex cellular morphologies. While optimized for microglia, its architecture can extend to other intricate cell types, such as neurons and astrocytes, broadening its impact on neurobiological research. Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation Authors: Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann 2025-07-30 http://arxiv.org/abs/2507.22608v1 Large language models ( s) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share ping neurons, reflecting internal representations of linguistic proximity. Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal \"fallback\" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation. MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Authors: Yaolun Zhang, Xiaogeng Liu, Chaowei Xiao 2025-07-30 http://arxiv.org/abs/2507.22606v1 Large Language Models ( s) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks. Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items Authors: Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram 2025-07-29 http://arxiv.org/abs/2507.22268v1 We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage s to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items. CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Authors: Black Sun, Die, Hu 2025-07-29 http://arxiv.org/abs/2507.22205v1 Remote fetal monitoring technologies are becoming increasingly common. Yet, most current systems offer limited interpretability, leaving expectant parents with raw cardiotocography (CTG) data that is difficult to understand. In this work, we present CTG-Insight, a multi-agent system that provides structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals. Drawing from established medical guidelines, CTG-Insight decomposes each CTG trace into five medically defined features: baseline, variability, s, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. A final aggregation agent synthesizes the outputs to deliver a holistic classification of fetal health, accompanied by a natural language explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare it against deep learning models and the single-agent baseline. Results show that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) while producing transparent and interpretable outputs. This work contributes an interpretable and extensible CTG analysis framework. Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles Authors: Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu 2025-07-29 http://arxiv.org/abs/2507.22168v1 Current benchmarks for evaluating Large Language Models ( s) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of patterns exhibited by humans. Thus, it is possible that s, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring performance across linguistic variations. IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Authors: Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim 2025-07-29 http://arxiv.org/abs/2507.22134v1 While large language models ( s) are widely used for writing, users often struggle to express their nuanced and evolving intents through prompt-based interfaces. Intents -- low-level strategies or preferences for achieving a writing goal -- are often vague, fluid, or even subconscious, making it difficult for users to articulate and adjust them. To address this, we present IntentFlow, which supports the of dynamically evolving intents throughout -assisted writing. IntentFlow extracts goals and intents from user prompts and presents them as editable interface components, which users can revise, remove, or refine via direct manipulation or follow-up prompts. Visual links connect each component to the output segments it influences, helping users understand model behavior. In a within-subjects study (N=12), participants using IntentFlow, compared to a chat-based baseline, expressed their intents more easily and in detail, engaged in more meaningful actions to communicate intents, such as adjusting and deleting, and produced outputs that better aligned with their evolving intents. We found that editable intent representations help users refine and consolidate a final set of intents, which can be reused across similar tasks to support consistent and transferable -assisted writing. Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models Authors: Hyunwoo Yoo, Gail L. Rosen 2025-07-29 http://arxiv.org/abs/2507.21980v1 Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models ( s) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate s such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that s not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that s can effectively reason over , heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications. EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Authors: Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang 2025-07-29 http://arxiv.org/abs/2507.21848v1 Large Language Models ( s) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO. Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer Authors: Xie Zhang, Yina Wang, Chenshu Wu 2025-07-29 http://arxiv.org/abs/2507.21799v1 The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box s, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE. MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Authors: YiZhou Li 2025-07-29 http://arxiv.org/abs/2507.21761v1 Vision Transformers (ViTs) have achieved remarkable success in image recognition, yet standard ViT architectures are hampered by substantial parameter redundancy and high computational cost, limiting their practical deployment. While recent efforts on efficient ViTs primarily focus on static model compression or token-level sparsification, they remain constrained by fixed computational depth for all tokens. In this work, we present MoR-ViT, a novel vision framework that, for the first time, incorporates a token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions (MoR) paradigm. This approach enables each token to adaptively determine its processing depth, yielding a flexible and input-dependent allocation of computational resources. Extensive experiments on ImageNet-1K and transfer benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy with up to 70% parameter reduction and 2.5x inference , but also outperforms leading efficient ViT baselines such as DynamicViT and TinyViT under comparable conditions. These results establish dynamic recursion as an effective strategy for efficient vision s and open new avenues for scalable and deployable deep learning models in real-world scenarios. Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation Authors: Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le 2025-07-29 http://arxiv.org/abs/2507.21563v1 Recommendation systems often suffer from data caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models ( s) and item textual descriptions to enrich interaction data. By few-shot prompting s multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines. TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Authors: Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu 2025-07-29 http://arxiv.org/abs/2507.21526v1 Large Language Models ( s) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static attention methods typically degrade accuracy, while dynamic methods introduce additional computational overhead due to runtime index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance inference efficiency. Large Language Models for Wireless Communications From Adaptation to Autonomy Authors: Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li 2025-07-29 http://arxiv.org/abs/2507.21524v1 The emergence of large language models ( s) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless s, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of s in transforming wireless systems across three key directions: adapting pretrained s for core tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic s with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of -based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future. Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess Authors: Zhenwei Tang, Difan Jiao, Eric Xue, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson 2025-07-29 http://arxiv.org/abs/2507.21488v1 As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or ly represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic s, highlighting its potential for broader applications in personalized AI adaptation. An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Authors: Zujie Xie, Zixuan Chen, Jiheng Liang, Xiangyang Yu, Ziru Yu 2025-07-29 http://arxiv.org/abs/2507.21471v1 Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, ping spectral bands that challenge conventional chemometric approaches. Emerging large language models ( s), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an -driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes. Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication Authors: Zhuoran Xiao, Chenhui Ye, Yijia Feng, Yunbo Hu, Tianyu Jiao, Liyu Cai, Guangyi Liu 2025-07-29 http://arxiv.org/abs/2507.21454v1 The rapid advancement in large foundation models is propelling the paradigm shifts across various industries. One significant change is that agents, instead of traditional machines or humans, will be the primary participants in the future production process, which consequently requires a novel AI-native system tailored for agent s. Integrating the ability of large language models ( s) with task-oriented semantic is a potential approach. However, the output of existing is human language, which is highly constrained and sub-optimal for agent-type . In this paper, we innovatively propose a task-oriented agent system. Specifically, we leverage the original to learn a specialized machine language represented by token embeddings. Simultaneously, a multi-modal is trained to comprehend the application task and to extract essential implicit information from multi-modal inputs, subsequently expressing it using machine language tokens. This representation is significantly more efficient for transmission over the air interface. Furthermore, to reduce transmission overhead, we introduce a joint token and channel coding (JTCC) scheme that compresses the token sequence by exploiting its while enhancing robustness against channel noise. Extensive experiments demonstrate that our approach reduces transmission overhead for downstream tasks while enhancing accuracy relative to the SOTA methods. MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Authors: JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao 2025-07-29 http://arxiv.org/abs/2507.21435v1 Brain-computer interface (BCI) spellers can render a new channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world . In this paper, we present MindChat, a large language model ( )-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt s (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\\% keystrokes and over 32\\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by s will potentially lead to truly practical applications. Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization Authors: Yuang Peng, Jiarui Zhong, Yang Zhang, Hong Cai Chen 2025-07-29 http://arxiv.org/abs/2507.21430v1 Parameter extraction for industry-standard device models like ASM-HEMT is crucial in circuit design workflows. However, many manufacturers do not provide such models, leaving users to build them using only datasheets. Unfortunately, datasheets lack sufficient information for standard step-by-step extraction. Moreover, manual data extraction from datasheets is highly time-consuming, and the absence of a fully automated method forces engineers to perform tedious manual work. To address this challenge, this paper introduces a novel, end-to-end framework that fully automates the generation of simulation-ready ASM-HEMT SPICE models directly from PDF datasheets. Our framework is founded on two core innovations: 1) a multi-modal AI pipeline that integrates computer vision with a large language model ( ) to robustly parse heterogeneous datasheet layouts and digitize characteristic curves, and 2) a novel Iterative-Focusing Tree-structured Parzen Estimator (IF-TPE) optimization algorithm is specifically designed for device parameter extraction under the high-dimensional, -data condition by adaptively refining the parameter search space. Experimental validation on a diverse set of 17 commercial HEMT devices from 10 manufacturers confirms the framework's accuracy and robustness. The generated models demonstrate excellent agreement with published DC and RF characteristics. As the first fully automated workflow of its kind, our proposed solution offers a transformative approach to device modeling, poised to significantly accelerate the circuit design cycle by eliminating the need for manual parameter extraction. ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Authors: Chaoyu Li, Yogesh Kulkarni, Pooyan Fazli 2025-07-29 http://arxiv.org/abs/2507.21420v1 The computational cost of training multimodal large language models (M s) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference - Guided Adaptive Token Elision), an adaptive token method for accelerating M training. Specifically, ReGATE adopts a teacher-student framework in which the M being trained serves as the student, and a frozen reference large language model ( ) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2 \\times faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon. Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Authors: Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza 2025-07-28 http://arxiv.org/abs/2507.21349v1 Magnetic resonance imaging (MRI) is a crucial medical imaging modality. However, long acquisition times remain a significant challenge, leading to increased costs, and reduced patient comfort. Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans. Integrating this prior information requires registration of the previous scan to the current image reconstruction, which can be time-consuming. We propose a novel deep-learning-based MRI reconstruction framework which consists of an initial reconstruction network, a deep registration model, and a -based enhancement network. We validated our method on a longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18 subjects at four factors (R5, R10, R15, R20). Quantitative metrics confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon signed-rank test). Furthermore, we analyzed the impact of our MRI reconstruction method on the downstream task of brain segmentation and observed improved accuracy and volumetric agreement with reference segmentations. Our approach also achieved a substantial reduction in total reconstruction time compared to methods that use traditional registration algorithms, making it more suitable for real-time clinical applications. The code associated with this work is publicly available at https://github.com/amirshamaei/longitudinal-mri-deep-recon. Agentic Web Weaving the Next Web with AI Agents Authors: Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, Jun Wang 2025-07-28 http://arxiv.org/abs/2507.21206v1 The emergence of AI agents powered by large language models ( s) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web. SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment Authors: Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen 2025-07-28 http://arxiv.org/abs/2507.20984v2 While frontier large language models ( s) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of s natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level structure combining fine-grained Mixture-of-Experts (MoE) with feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid attention mechanism to slash cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger s. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct. The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Authors: Dinh Nam Pham, Eleftherios Avramidis 2025-07-28 http://arxiv.org/abs/2507.20884v2 Non-manual facial features play a crucial role in sign language , yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a -based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR. Latent Inter-User Difference Modeling for LLM Personalization Authors: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng 2025-07-28 http://arxiv.org/abs/2507.20849v1 Large language models ( s) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen . Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP. METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Authors: Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian 2025-07-28 http://arxiv.org/abs/2507.20842v1 Vision encoders serve as the cornerstone of multimodal understanding. Single-encoder architectures like CLIP exhibit inherent constraints in generalizing across diverse multimodal tasks, while recent multi-encoder fusion methods introduce prohibitive computational overhead to achieve superior performance using complementary visual representations from multiple vision encoders. To address this, we propose a progressive framework, namely Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant visual tokens across the encoding, fusion, and decoding stages for multi-encoder M s. For multi-vision encoding, we discard redundant tokens within each encoder via a rank guided collaborative token assignment strategy. Subsequently, for multi-vision fusion, we combine the visual features from different encoders while reducing cross-encoder redundancy with cooperative . Finally, we propose an adaptive token method in the decoding stage to further discard irrelevant tokens based on the text prompts with dynamically adjusting ratios for specific task demands. To our best knowledge, this is the first successful attempt that achieves an efficient multi-encoder based vision language model with multi-stage strategies. Extensive experiments on 11 benchmarks demonstrate the effectiveness of our proposed approach. Compared with EAGLE, a typical multi-encoder M s, METEOR reduces 76% visual tokens with only 0.3% performance drop in average. The code is available at https://github.com/YuchenLiu98/METEOR. Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Authors: Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q. S. Quek 2025-07-28 http://arxiv.org/abs/2507.21199v1 Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models ( s) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple s for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional over wireless networks. The two primary challenges include 1) guiding a single to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community. Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Authors: Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen 2025-07-28 http://arxiv.org/abs/2507.20511v2 Few-shot Learning (FSL), which endeavors to develop the generalization ability for recognizing novel classes using only a few images, faces significant challenges due to data scarcity. Recent CLIP-like methods based on contrastive language-image pertaining mitigate the issue by leveraging textual representation of the class name for unseen image discovery. Despite the achieved success, simply aligning visual representations to class name embeddings would compromise the visual diversity for novel class discrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method (BCT-CLIP) that explores \\textbf{dominating properties} via contrastive learning beyond simply using class tokens. Through leveraging -based prior knowledge, our method pushes forward FSL with comprehensive structural image representations, including both global category representation and the patch-aware property embeddings. In particular, we presented a novel multi-property generator (MPG) with patch-aware cross-attentions to generate multiple visual property tokens, a Large-Language Model ( )-assistant retrieval procedure with clustering-based to obtain dominating property descriptions, and a new contrastive learning strategy for property-token learning. The superior performances on the 11 widely used datasets demonstrate that our investigation of dominating properties advances discriminative class-specific representation learning and few-shot classification. Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Authors: Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot 2025-07-27 http://arxiv.org/abs/2507.20370v1 Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the is prone to hallucinations, which can compromise decision quality. Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation Authors: Abdullah Alabdullah, Lifeng Han, Chenghua Lin 2025-07-27 http://arxiv.org/abs/2507.20301v1 Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models ( s) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies. What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Authors: Katharina Trinley, Toshiki Nakai, Tatiana Anikina, Tanja Baeumel 2025-07-27 http://arxiv.org/abs/2507.20279v1 Large language models ( s) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes internals and inform future cross-lingual transfer research. Modeling Professionalism in Expert Questioning through Linguistic Differentiation Authors: Giulia D'Agostino, Chung-Chi Chen 2025-07-27 http://arxiv.org/abs/2507.20249v1 Professionalism is a crucial yet underexplored dimension of expert , particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model ( )-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling. FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression Authors: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li 2025-07-26 http://arxiv.org/abs/2507.20030v1 The efficacy of Large Language Models ( s) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value ( ) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAED (Frequency-Adaptive Infinite-Window for cache), a novel, training-free cache compression framework that ensures unbiased information retention. FAED operates by transforming the cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAED 's superiority over existing methods by up to 22\\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches. The Carbon Cost of Conversation, Sustainability in the Age of Language Models Authors: Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter 2025-07-26 http://arxiv.org/abs/2507.20018v2 Large language models ( s) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of s, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model , quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being. CaliDrop KV Cache Compression with Calibration Authors: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang 2025-07-26 http://arxiv.org/abs/2507.19906v1 Large Language Models ( s) require substantial computational resources during generation. While the Key-Value ( ) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often , allowing for the removal of less critical entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods. CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation Authors: Zhanhang Xiong, Dongxia Wang, Yuekang Li, Xinyuan An, Wenhai Wang 2025-07-26 http://arxiv.org/abs/2507.19904v1 As large language models ( s) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate s' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an -based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose s and 6 code-oriented s released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: https://anonymous.4open.science/r/crosspl-2814. AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation Authors: Sourena Khanzadeh 2025-07-26 http://arxiv.org/abs/2507.19902v1 Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating -powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their , and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation. HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Authors: Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao 2025-07-26 http://arxiv.org/abs/2507.19823v1 Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value ( ) cache during inference. Existing cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory. Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation Authors: Xin Zhang, Lissette Iturburu, Juan Nicolas Villamizar, Xiaoyu Liu, Manuel Salmeron, Shirley J. Dyke, Julio Ramirez 2025-07-26 http://arxiv.org/abs/2507.19771v1 Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model ( ) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way. LowKeyEMG Electromyographic typing with a reduced keyset Authors: Johannes Y. Lee, Derek Xiao, Shreyas Kaasyap, Nima R. Hadidi, John L. Zhou, Jacob Cunningham, Rakshith R. Gore, Deniz O. Eren, Jonathan C. Kao 2025-07-26 http://arxiv.org/abs/2507.19736v1 We introduce LowKeyEMG, a real-time human-computer interface that enables efficient text entry using only 7 gesture classes decoded from surface electromyography (sEMG). Prior work has attempted full-alphabet decoding from sEMG, but decoding large character sets remains unreliable, especially for individuals with motor impairments. Instead, LowKeyEMG reduces the English alphabet to 4 gesture keys, with 3 more for space and system interaction, to reliably translate simple one-handed gestures into text, leveraging the recurrent -based language model RW for efficient computation. In real-time experiments, participants achieved average one-handed keyboardless typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture efficiency by 17% (relative to typed phrase length). When typing with only 7 keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this low-key typing paradigm can maintain practical rates. Our results have implications for assistive technologies and any interface where input bandwidth is constrained. Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks Authors: Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani 2025-07-25 http://arxiv.org/abs/2507.19699v1 Although s have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models ( s) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA S as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting. \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems Authors: Beining Wu, Jun Huang, Shui Yu 2025-07-25 http://arxiv.org/abs/2507.19657v1 The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/ dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV s, ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ... DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Authors: Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen 2025-07-25 http://arxiv.org/abs/2507.19608v1 Deploying Large Language Models ( s) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present Delta , a training-free framework that exploits temporal in attention patterns to enable efficient inference across both the prefilling and decoding stages, on resource-constrained edge devices. Delta introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal , and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that Delta offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines. Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts Authors: Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel 2025-07-25 http://arxiv.org/abs/2507.19477v1 Many recent papers have studied the development of superforecaster-level event forecasting s. While methodological problems with early studies cast doubt on the use of s for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art s are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting s. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of -based event forecasting training: noisiness- , knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions. GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Authors: Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab 2025-07-25 http://arxiv.org/abs/2507.19457v1 Large language models ( s) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for s, compared with policy gradients derived from , scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two s, and demonstrates promising results as an inference-time search strategy for code optimization. Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Authors: StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang 2025-07-25 http://arxiv.org/abs/2507.19427v1 Large language models ( s) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE , and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for decoding. Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Authors: Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci 2025-07-25 http://arxiv.org/abs/2507.19334v1 Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models ( s) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures dependencies via an -induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over -based baselines. Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers Authors: Yuki Igaue, Hiroaki Aizawa 2025-07-25 http://arxiv.org/abs/2507.19175v1 Multi-head self-attention is a distinctive feature extraction mechanism of vision s that computes pairwise relationships among all input patches, contributing significantly to their high performance. However, it is known to incur a quadratic computational complexity with respect to the number of patches. One promising approach to address this issue is patch , which improves computational efficiency by identifying and removing redundant patches. In this work, we propose a patch strategy that evaluates the importance of each patch based on the variance of attention weights across multiple attention heads. This approach is inspired by the design of multi-head self-attention, which aims to capture diverse attention patterns across different subspaces of feature representations. The proposed method can be easily applied during both training and inference, and achieves improved throughput while maintaining classification accuracy in scenarios such as fine-tuning with pre-trained models. In addition, we also found that using robust statistical measures, such as the median absolute deviation in place of variance, to assess patch importance can similarly lead to strong performance. Furthermore, by introducing ping patch embeddings, our method achieves better performance with comparable throughput to conventional approaches that utilize all patches. RegScore Scoring Systems for Regression Tasks Authors: Michal K. Grzeszczyk, Tomasz Szczepa\u0144ski, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzci\u0144ski, Arkadiusz Sitek 2025-07-25 http://arxiv.org/abs/2507.19155v1 Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, , and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and k- ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at https://github.com/SanoScience/RegScore. MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Authors: Weitian Wang, Rai Shubham, Cecilia De La Parra, Akash Kumar 2025-07-25 http://arxiv.org/abs/2507.19131v1 In this paper, we propose MixA-Q, a mixed-precision activation quantization framework that leverages intra-layer activation (a concept widely explored in activation methods) for efficient inference of quantized window-based vision s. For a given uniform-bit quantization configuration, MixA-Q separates the batched window computations within Swin blocks and assigns a lower bit width to the activations of less important windows, improving the trade-off between model performance and efficiency. We introduce a Two-Branch Swin Block that processes activations separately in high- and low-bit precision, enabling seamless integration of our method with most quantization-aware training (QAT) and post-training quantization (PTQ) methods, or with simple modifications. Our experimental evaluations over the COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x computational speedup without accuracy loss in PTQ configuration. With QAT, MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP drop by incorporating activation . Notably, by reducing the quantization error in important regions, our -aware quantization adaptation improves the mAP of the quantized W4A4 model (with both weights and activations in 4-bit precision) by 0.7%, reducing quantization degradation by 24%. Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events Authors: Tianyi Li, Flavio Tuteri, Michele Buzzicotti, Fabio Bonaccorso, Luca Biferale 2025-07-25 http://arxiv.org/abs/2507.19103v1 Modeling Lagrangian turbulence remains a fundamental challenge due to its multiscale, intermittent, and non-Gaussian nature. Recent advances in data-driven diffusion models have enabled the generation of realistic Lagrangian velocity trajectories that accurately reproduce statistical properties across scales and capture rare extreme events. This study investigates three key aspects of diffusion-based modeling for Lagrangian turbulence. First, we assess architectural robustness by comparing a U-Net backbone with a -based alternative, finding strong consistency in generated trajectories, with only minor discrepancies at small scales. Second, leveraging a deterministic variant of diffusion model formulation, namely the deterministic denoising diffusion implicit model (DDIM), we identify structured features in the initial latent noise that align consistently with extreme events. Third, we explore accelerated generation by reducing the number of diffusion steps, and find that DDIM enables substantial speedups with minimal loss of statistical fidelity. These findings highlight the robustness of diffusion models and their potential for interpretable, scalable modeling of complex turbulent systems.","title":"2025-08-01"},{"location":"weekly_paper/2025-08-01/#2025-08-01","text":"","title":"2025-08-01"},{"location":"weekly_paper/2025-08-01/#table-of-contents","text":"DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Unveiling Super Experts in Mixture-of-Experts Large Language Models Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Large Language Models for Wireless Communications From Adaptation to Autonomy Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Agentic Web Weaving the Next Web with AI Agents SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Latent Inter-User Difference Modeling for LLM Personalization METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Modeling Professionalism in Expert Questioning through Linguistic Differentiation FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression The Carbon Cost of Conversation, Sustainability in the Age of Language Models CaliDrop KV Cache Compression with Calibration CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation LowKeyEMG Electromyographic typing with a reduced keyset Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers RegScore Scoring Systems for Regression Tasks MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events","title":"Table of Contents"},{"location":"weekly_paper/2025-08-01/#depmicrodiff-diffusion-based-dependency-aware-multimodal-imputation-for-microbiome-data","text":"Authors: Rabeya Tus Sadia, Qiang Cheng 2025-07-31 http://arxiv.org/abs/2507.23676v1 Microbiome data analysis is essential for understanding host health and disease, yet its inherent and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model ( ). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.","title":"DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data"},{"location":"weekly_paper/2025-08-01/#memocue-empowering-llm-based-agents-for-human-memory-recall-via-strategy-guided-querying","text":"Authors: Qian Zhao, Zhuo Sun, Bin Guo, Zhiwen Yu 2025-07-31 http://arxiv.org/abs/2507.23633v1 Agent-assisted memory recall is one critical research problem in the field of human-computer interaction. In conventional methods, the agent can retrieve information from its equipped memory module to help the person recall incomplete or vague memories. The limited size of memory module hinders the acquisition of complete memories and impacts the memory recall performance in practice. Memory theories suggest that the person's relevant memory can be proactively activated through some effective cues. Inspired by this, we propose a novel strategy-guided agent-assisted memory recall method, allowing the agent to transform an original query into a cue-rich one via the judiciously designed strategy to help the person recall memories. To this end, there are two key challenges. (1) How to choose the appropriate recall strategy for diverse forgetting scenarios with distinct memory-recall characteristics? (2) How to obtain the high-quality responses leveraging recall strategies, given only abstract and ly annotated strategy patterns? To address the challenges, we propose a Recall Router framework. Specifically, we design a 5W Recall Map to classify memory queries into five typical scenarios and define fifteen recall strategy patterns across the corresponding scenarios. We then propose a hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to optimize the selection of strategy and the generation of strategy responses. We construct an instruction tuning dataset and fine-tune multiple open-source large language models ( s) to develop MemoCue, an agent that excels in providing memory-inspired responses. Experiments on three representative datasets show that MemoCue surpasses -based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications.","title":"MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying"},{"location":"weekly_paper/2025-08-01/#trae-agent-an-llm-based-agent-for-software-engineering-with-test-time-scaling","text":"Authors: Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu 2025-07-31 http://arxiv.org/abs/2507.23370v1 Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models ( s), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of -based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, , and selection. We conduct extensive experiments using three leading s on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent.","title":"Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling"},{"location":"weekly_paper/2025-08-01/#unveiling-super-experts-in-mixture-of-experts-large-language-models","text":"Authors: Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan 2025-07-31 http://arxiv.org/abs/2507.23279v1 Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models ( s). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE s. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE s, and despite their limited number, them leads to a significant decline in model performance (e.g., three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE s rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE . The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"weekly_paper/2025-08-01/#model-directions-not-words-mechanistic-topic-models-using-sparse-autoencoders","text":"Authors: Carolina Zheng, Nicolas Beltran-Velez, Sweta Karlekar, Claudia Shi, Achille Nazaret, Asif Mallik, Amir Feder, David M. Blei 2025-07-31 http://arxiv.org/abs/2507.23220v1 Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \\textit{topic judge}, an -based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of outputs.","title":"Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders"},{"location":"weekly_paper/2025-08-01/#mocha-advanced-vision-language-reasoning-with-moe-connector-and-hierarchical-group-attention","text":"Authors: Yuqi Pang, Bowen Yang, Yun Cao, Fan Rong, Xiaoyu Li, Chen He 2025-07-30 http://arxiv.org/abs/2507.22805v1 Vision large language models (V s) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream s (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA.","title":"MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention"},{"location":"weekly_paper/2025-08-01/#modality-aware-feature-matching-a-comprehensive-review-of-single-and-cross-modality-techniques","text":"Authors: Weide Liu, Wei Zhou, Jun Liu, Ping Hu, Jun Cheng, Jungong Han, Weisi Lin 2025-07-30 http://arxiv.org/abs/2507.22791v1 Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and -based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions.","title":"Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques"},{"location":"weekly_paper/2025-08-01/#traice3d-a-prompt-driven-transformer-based-u-net-for-semantic-segmentation-of-microglial-cells-from-large-scale-3d-microscopy-images","text":"Authors: MohammadAmin Alamalhoda, Arsalan Firoozi, Alessandro Venturino, Sandra Siegert 2025-07-30 http://arxiv.org/abs/2507.22635v1 The shape of a cell contains essential information about its function within the biological system. Segmenting these structures from large-scale 3D microscopy images is challenging, limiting clinical insights especially for microglia, immune-associated cells involved in neurodegenerative diseases. Existing segmentation methods mainly focus on cell bodies, struggle with ping structures, perform poorly on noisy images, require hyperparameter tuning for each new dataset, or rely on tedious semi-automated approaches. We introduce trAIce3D, a deep-learning architecture designed for precise microglia segmentation, capturing both somas and branches. It employs a two-stage approach: first, a 3D U-Net with vision s in the encoder detects somas using a sliding-window technique to cover the entire image. Then, the same architecture, enhanced with cross-attention blocks in skip connections, refines each soma and its branches by using soma coordinates as a prompt and a 3D window around the target cell as input. Training occurs in two phases: self-supervised Soma Segmentation, followed by prompt-based Branch Segmentation, leveraging pre-trained weights from the first phase. Trained and evaluated on a dataset of 41,230 microglial cells, trAIce3D significantly improves segmentation accuracy and generalization, enabling scalable analysis of complex cellular morphologies. While optimized for microglia, its architecture can extend to other intricate cell types, such as neurons and astrocytes, broadening its impact on neurobiological research.","title":"trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images"},{"location":"weekly_paper/2025-08-01/#language-arithmetics-towards-systematic-language-neuron-identification-and-manipulation","text":"Authors: Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann 2025-07-30 http://arxiv.org/abs/2507.22608v1 Large language models ( s) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share ping neurons, reflecting internal representations of linguistic proximity. Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal \"fallback\" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.","title":"Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation"},{"location":"weekly_paper/2025-08-01/#metaagent-automatically-constructing-multi-agent-systems-based-on-finite-state-machines","text":"Authors: Yaolun Zhang, Xiaogeng Liu, Chaowei Xiao 2025-07-30 http://arxiv.org/abs/2507.22606v1 Large Language Models ( s) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks.","title":"MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines"},{"location":"weekly_paper/2025-08-01/#multi-modal-relational-item-representation-learning-for-inferring-substitutable-and-complementary-items","text":"Authors: Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram 2025-07-29 http://arxiv.org/abs/2507.22268v1 We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage s to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items.","title":"Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items"},{"location":"weekly_paper/2025-08-01/#ctg-insight-a-multi-agent-interpretable-llm-framework-for-cardiotocography-analysis-and-classification","text":"Authors: Black Sun, Die, Hu 2025-07-29 http://arxiv.org/abs/2507.22205v1 Remote fetal monitoring technologies are becoming increasingly common. Yet, most current systems offer limited interpretability, leaving expectant parents with raw cardiotocography (CTG) data that is difficult to understand. In this work, we present CTG-Insight, a multi-agent system that provides structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals. Drawing from established medical guidelines, CTG-Insight decomposes each CTG trace into five medically defined features: baseline, variability, s, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. A final aggregation agent synthesizes the outputs to deliver a holistic classification of fetal health, accompanied by a natural language explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare it against deep learning models and the single-agent baseline. Results show that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) while producing transparent and interpretable outputs. This work contributes an interpretable and extensible CTG analysis framework.","title":"CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification"},{"location":"weekly_paper/2025-08-01/#persona-augmented-benchmarking-evaluating-llms-across-diverse-writing-styles","text":"Authors: Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu 2025-07-29 http://arxiv.org/abs/2507.22168v1 Current benchmarks for evaluating Large Language Models ( s) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of patterns exhibited by humans. Thus, it is possible that s, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring performance across linguistic variations.","title":"Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles"},{"location":"weekly_paper/2025-08-01/#intentflow-interactive-support-for-communicating-intent-with-llms-in-writing-tasks","text":"Authors: Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim 2025-07-29 http://arxiv.org/abs/2507.22134v1 While large language models ( s) are widely used for writing, users often struggle to express their nuanced and evolving intents through prompt-based interfaces. Intents -- low-level strategies or preferences for achieving a writing goal -- are often vague, fluid, or even subconscious, making it difficult for users to articulate and adjust them. To address this, we present IntentFlow, which supports the of dynamically evolving intents throughout -assisted writing. IntentFlow extracts goals and intents from user prompts and presents them as editable interface components, which users can revise, remove, or refine via direct manipulation or follow-up prompts. Visual links connect each component to the output segments it influences, helping users understand model behavior. In a within-subjects study (N=12), participants using IntentFlow, compared to a chat-based baseline, expressed their intents more easily and in detail, engaged in more meaningful actions to communicate intents, such as adjusting and deleting, and produced outputs that better aligned with their evolving intents. We found that editable intent representations help users refine and consolidate a final set of intents, which can be reused across similar tasks to support consistent and transferable -assisted writing.","title":"IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks"},{"location":"weekly_paper/2025-08-01/#predicting-microbial-ontology-and-pathogen-risk-from-environmental-metadata-with-large-language-models","text":"Authors: Hyunwoo Yoo, Gail L. Rosen 2025-07-29 http://arxiv.org/abs/2507.21980v1 Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models ( s) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate s such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that s not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that s can effectively reason over , heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.","title":"Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models"},{"location":"weekly_paper/2025-08-01/#edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity","text":"Authors: Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang 2025-07-29 http://arxiv.org/abs/2507.21848v1 Large Language Models ( s) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO.","title":"EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity"},{"location":"weekly_paper/2025-08-01/#unlocking-interpretability-for-rf-sensing-a-complex-valued-white-box-transformer","text":"Authors: Xie Zhang, Yina Wang, Chenshu Wu 2025-07-29 http://arxiv.org/abs/2507.21799v1 The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box s, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.","title":"Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer"},{"location":"weekly_paper/2025-08-01/#mor-vit-efficient-vision-transformer-with-mixture-of-recursions","text":"Authors: YiZhou Li 2025-07-29 http://arxiv.org/abs/2507.21761v1 Vision Transformers (ViTs) have achieved remarkable success in image recognition, yet standard ViT architectures are hampered by substantial parameter redundancy and high computational cost, limiting their practical deployment. While recent efforts on efficient ViTs primarily focus on static model compression or token-level sparsification, they remain constrained by fixed computational depth for all tokens. In this work, we present MoR-ViT, a novel vision framework that, for the first time, incorporates a token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions (MoR) paradigm. This approach enables each token to adaptively determine its processing depth, yielding a flexible and input-dependent allocation of computational resources. Extensive experiments on ImageNet-1K and transfer benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy with up to 70% parameter reduction and 2.5x inference , but also outperforms leading efficient ViT baselines such as DynamicViT and TinyViT under comparable conditions. These results establish dynamic recursion as an effective strategy for efficient vision s and open new avenues for scalable and deployable deep learning models in real-world scenarios.","title":"MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions"},{"location":"weekly_paper/2025-08-01/#enhancing-graph-based-recommendations-with-majority-voting-llm-rerank-augmentation","text":"Authors: Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le 2025-07-29 http://arxiv.org/abs/2507.21563v1 Recommendation systems often suffer from data caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models ( s) and item textual descriptions to enrich interaction data. By few-shot prompting s multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines.","title":"Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation"},{"location":"weekly_paper/2025-08-01/#trianglemix-a-lossless-and-efficient-attention-pattern-for-long-context-prefilling","text":"Authors: Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu 2025-07-29 http://arxiv.org/abs/2507.21526v1 Large Language Models ( s) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static attention methods typically degrade accuracy, while dynamic methods introduce additional computational overhead due to runtime index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance inference efficiency.","title":"TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling"},{"location":"weekly_paper/2025-08-01/#large-language-models-for-wireless-communications-from-adaptation-to-autonomy","text":"Authors: Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li 2025-07-29 http://arxiv.org/abs/2507.21524v1 The emergence of large language models ( s) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless s, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of s in transforming wireless systems across three key directions: adapting pretrained s for core tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic s with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of -based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future.","title":"Large Language Models for Wireless Communications From Adaptation to Autonomy"},{"location":"weekly_paper/2025-08-01/#learning-to-imitate-with-less-efficient-individual-behavior-modeling-in-chess","text":"Authors: Zhenwei Tang, Difan Jiao, Eric Xue, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson 2025-07-29 http://arxiv.org/abs/2507.21488v1 As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or ly represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic s, highlighting its potential for broader applications in personalized AI adaptation.","title":"Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess"},{"location":"weekly_paper/2025-08-01/#an-llm-driven-agent-framework-for-automated-infrared-spectral-multi-task-reasoning","text":"Authors: Zujie Xie, Zixuan Chen, Jiheng Liang, Xiangyang Yu, Ziru Yu 2025-07-29 http://arxiv.org/abs/2507.21471v1 Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, ping spectral bands that challenge conventional chemometric approaches. Emerging large language models ( s), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an -driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes.","title":"An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning"},{"location":"weekly_paper/2025-08-01/#transmission-with-machine-language-tokens-a-paradigm-for-task-oriented-agent-communication","text":"Authors: Zhuoran Xiao, Chenhui Ye, Yijia Feng, Yunbo Hu, Tianyu Jiao, Liyu Cai, Guangyi Liu 2025-07-29 http://arxiv.org/abs/2507.21454v1 The rapid advancement in large foundation models is propelling the paradigm shifts across various industries. One significant change is that agents, instead of traditional machines or humans, will be the primary participants in the future production process, which consequently requires a novel AI-native system tailored for agent s. Integrating the ability of large language models ( s) with task-oriented semantic is a potential approach. However, the output of existing is human language, which is highly constrained and sub-optimal for agent-type . In this paper, we innovatively propose a task-oriented agent system. Specifically, we leverage the original to learn a specialized machine language represented by token embeddings. Simultaneously, a multi-modal is trained to comprehend the application task and to extract essential implicit information from multi-modal inputs, subsequently expressing it using machine language tokens. This representation is significantly more efficient for transmission over the air interface. Furthermore, to reduce transmission overhead, we introduce a joint token and channel coding (JTCC) scheme that compresses the token sequence by exploiting its while enhancing robustness against channel noise. Extensive experiments demonstrate that our approach reduces transmission overhead for downstream tasks while enhancing accuracy relative to the SOTA methods.","title":"Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication"},{"location":"weekly_paper/2025-08-01/#mindchat-enhancing-bci-spelling-with-large-language-models-in-realistic-scenarios","text":"Authors: JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao 2025-07-29 http://arxiv.org/abs/2507.21435v1 Brain-computer interface (BCI) spellers can render a new channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world . In this paper, we present MindChat, a large language model ( )-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt s (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\\% keystrokes and over 32\\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by s will potentially lead to truly practical applications.","title":"MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios"},{"location":"weekly_paper/2025-08-01/#automated-hemt-model-construction-from-datasheets-via-multi-modal-intelligence-and-prior-knowledge-free-optimization","text":"Authors: Yuang Peng, Jiarui Zhong, Yang Zhang, Hong Cai Chen 2025-07-29 http://arxiv.org/abs/2507.21430v1 Parameter extraction for industry-standard device models like ASM-HEMT is crucial in circuit design workflows. However, many manufacturers do not provide such models, leaving users to build them using only datasheets. Unfortunately, datasheets lack sufficient information for standard step-by-step extraction. Moreover, manual data extraction from datasheets is highly time-consuming, and the absence of a fully automated method forces engineers to perform tedious manual work. To address this challenge, this paper introduces a novel, end-to-end framework that fully automates the generation of simulation-ready ASM-HEMT SPICE models directly from PDF datasheets. Our framework is founded on two core innovations: 1) a multi-modal AI pipeline that integrates computer vision with a large language model ( ) to robustly parse heterogeneous datasheet layouts and digitize characteristic curves, and 2) a novel Iterative-Focusing Tree-structured Parzen Estimator (IF-TPE) optimization algorithm is specifically designed for device parameter extraction under the high-dimensional, -data condition by adaptively refining the parameter search space. Experimental validation on a diverse set of 17 commercial HEMT devices from 10 manufacturers confirms the framework's accuracy and robustness. The generated models demonstrate excellent agreement with published DC and RF characteristics. As the first fully automated workflow of its kind, our proposed solution offers a transformative approach to device modeling, poised to significantly accelerate the circuit design cycle by eliminating the need for manual parameter extraction.","title":"Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization"},{"location":"weekly_paper/2025-08-01/#regate-learning-faster-and-better-with-fewer-tokens-in-mllms","text":"Authors: Chaoyu Li, Yogesh Kulkarni, Pooyan Fazli 2025-07-29 http://arxiv.org/abs/2507.21420v1 The computational cost of training multimodal large language models (M s) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference - Guided Adaptive Token Elision), an adaptive token method for accelerating M training. Specifically, ReGATE adopts a teacher-student framework in which the M being trained serves as the student, and a frozen reference large language model ( ) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2 \\times faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon.","title":"ReGATE Learning Faster and Better with Fewer Tokens in MLLMs"},{"location":"weekly_paper/2025-08-01/#enhancing-and-accelerating-brain-mri-through-deep-learning-reconstruction-using-prior-subject-specific-imaging","text":"Authors: Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza 2025-07-28 http://arxiv.org/abs/2507.21349v1 Magnetic resonance imaging (MRI) is a crucial medical imaging modality. However, long acquisition times remain a significant challenge, leading to increased costs, and reduced patient comfort. Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans. Integrating this prior information requires registration of the previous scan to the current image reconstruction, which can be time-consuming. We propose a novel deep-learning-based MRI reconstruction framework which consists of an initial reconstruction network, a deep registration model, and a -based enhancement network. We validated our method on a longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18 subjects at four factors (R5, R10, R15, R20). Quantitative metrics confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon signed-rank test). Furthermore, we analyzed the impact of our MRI reconstruction method on the downstream task of brain segmentation and observed improved accuracy and volumetric agreement with reference segmentations. Our approach also achieved a substantial reduction in total reconstruction time compared to methods that use traditional registration algorithms, making it more suitable for real-time clinical applications. The code associated with this work is publicly available at https://github.com/amirshamaei/longitudinal-mri-deep-recon.","title":"Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging"},{"location":"weekly_paper/2025-08-01/#agentic-web-weaving-the-next-web-with-ai-agents","text":"Authors: Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, Jun Wang 2025-07-28 http://arxiv.org/abs/2507.21206v1 The emergence of AI agents powered by large language models ( s) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.","title":"Agentic Web Weaving the Next Web with AI Agents"},{"location":"weekly_paper/2025-08-01/#smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment","text":"Authors: Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen 2025-07-28 http://arxiv.org/abs/2507.20984v2 While frontier large language models ( s) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of s natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level structure combining fine-grained Mixture-of-Experts (MoE) with feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid attention mechanism to slash cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger s. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.","title":"SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment"},{"location":"weekly_paper/2025-08-01/#the-importance-of-facial-features-in-vision-based-sign-language-recognition-eyes-mouth-or-full-face","text":"Authors: Dinh Nam Pham, Eleftherios Avramidis 2025-07-28 http://arxiv.org/abs/2507.20884v2 Non-manual facial features play a crucial role in sign language , yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a -based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR.","title":"The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face?"},{"location":"weekly_paper/2025-08-01/#latent-inter-user-difference-modeling-for-llm-personalization","text":"Authors: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng 2025-07-28 http://arxiv.org/abs/2507.20849v1 Large language models ( s) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen . Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.","title":"Latent Inter-User Difference Modeling for LLM Personalization"},{"location":"weekly_paper/2025-08-01/#meteor-multi-encoder-collaborative-token-pruning-for-efficient-vision-language-models","text":"Authors: Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian 2025-07-28 http://arxiv.org/abs/2507.20842v1 Vision encoders serve as the cornerstone of multimodal understanding. Single-encoder architectures like CLIP exhibit inherent constraints in generalizing across diverse multimodal tasks, while recent multi-encoder fusion methods introduce prohibitive computational overhead to achieve superior performance using complementary visual representations from multiple vision encoders. To address this, we propose a progressive framework, namely Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant visual tokens across the encoding, fusion, and decoding stages for multi-encoder M s. For multi-vision encoding, we discard redundant tokens within each encoder via a rank guided collaborative token assignment strategy. Subsequently, for multi-vision fusion, we combine the visual features from different encoders while reducing cross-encoder redundancy with cooperative . Finally, we propose an adaptive token method in the decoding stage to further discard irrelevant tokens based on the text prompts with dynamically adjusting ratios for specific task demands. To our best knowledge, this is the first successful attempt that achieves an efficient multi-encoder based vision language model with multi-stage strategies. Extensive experiments on 11 benchmarks demonstrate the effectiveness of our proposed approach. Compared with EAGLE, a typical multi-encoder M s, METEOR reduces 76% visual tokens with only 0.3% performance drop in average. The code is available at https://github.com/YuchenLiu98/METEOR.","title":"METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models"},{"location":"weekly_paper/2025-08-01/#advancing-compositional-llm-reasoning-with-structured-task-relations-in-interactive-multimodal-communications","text":"Authors: Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q. S. Quek 2025-07-28 http://arxiv.org/abs/2507.21199v1 Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models ( s) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple s for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional over wireless networks. The two primary challenges include 1) guiding a single to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.","title":"Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications"},{"location":"weekly_paper/2025-08-01/#beyond-class-tokens-llm-guided-dominant-property-mining-for-few-shot-classification","text":"Authors: Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen 2025-07-28 http://arxiv.org/abs/2507.20511v2 Few-shot Learning (FSL), which endeavors to develop the generalization ability for recognizing novel classes using only a few images, faces significant challenges due to data scarcity. Recent CLIP-like methods based on contrastive language-image pertaining mitigate the issue by leveraging textual representation of the class name for unseen image discovery. Despite the achieved success, simply aligning visual representations to class name embeddings would compromise the visual diversity for novel class discrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method (BCT-CLIP) that explores \\textbf{dominating properties} via contrastive learning beyond simply using class tokens. Through leveraging -based prior knowledge, our method pushes forward FSL with comprehensive structural image representations, including both global category representation and the patch-aware property embeddings. In particular, we presented a novel multi-property generator (MPG) with patch-aware cross-attentions to generate multiple visual property tokens, a Large-Language Model ( )-assistant retrieval procedure with clustering-based to obtain dominating property descriptions, and a new contrastive learning strategy for property-token learning. The superior performances on the 11 widely used datasets demonstrate that our investigation of dominating properties advances discriminative class-specific representation learning and few-shot classification.","title":"Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification"},{"location":"weekly_paper/2025-08-01/#advancing-shared-and-multi-agent-autonomy-in-underwater-missions-integrating-knowledge-graphs-and-retrieval-augmented-generation","text":"Authors: Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot 2025-07-27 http://arxiv.org/abs/2507.20370v1 Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the is prone to hallucinations, which can compromise decision quality.","title":"Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation"},{"location":"weekly_paper/2025-08-01/#advancing-dialectal-arabic-to-modern-standard-arabic-machine-translation","text":"Authors: Abdullah Alabdullah, Lifeng Han, Chenghua Lin 2025-07-27 http://arxiv.org/abs/2507.20301v1 Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models ( s) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.","title":"Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation"},{"location":"weekly_paper/2025-08-01/#what-languages-does-aya-23-think-in-how-multilinguality-affects-internal-language-representations","text":"Authors: Katharina Trinley, Toshiki Nakai, Tatiana Anikina, Tanja Baeumel 2025-07-27 http://arxiv.org/abs/2507.20279v1 Large language models ( s) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes internals and inform future cross-lingual transfer research.","title":"What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations"},{"location":"weekly_paper/2025-08-01/#modeling-professionalism-in-expert-questioning-through-linguistic-differentiation","text":"Authors: Giulia D'Agostino, Chung-Chi Chen 2025-07-27 http://arxiv.org/abs/2507.20249v1 Professionalism is a crucial yet underexplored dimension of expert , particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model ( )-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.","title":"Modeling Professionalism in Expert Questioning through Linguistic Differentiation"},{"location":"weekly_paper/2025-08-01/#faedkv-infinite-window-fourier-transform-for-unbiased-kv-cache-compression","text":"Authors: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li 2025-07-26 http://arxiv.org/abs/2507.20030v1 The efficacy of Large Language Models ( s) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value ( ) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAED (Frequency-Adaptive Infinite-Window for cache), a novel, training-free cache compression framework that ensures unbiased information retention. FAED operates by transforming the cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAED 's superiority over existing methods by up to 22\\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.","title":"FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression"},{"location":"weekly_paper/2025-08-01/#the-carbon-cost-of-conversation-sustainability-in-the-age-of-language-models","text":"Authors: Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter 2025-07-26 http://arxiv.org/abs/2507.20018v2 Large language models ( s) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of s, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model , quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.","title":"The Carbon Cost of Conversation, Sustainability in the Age of Language Models"},{"location":"weekly_paper/2025-08-01/#calidrop-kv-cache-compression-with-calibration","text":"Authors: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang 2025-07-26 http://arxiv.org/abs/2507.19906v1 Large Language Models ( s) require substantial computational resources during generation. While the Key-Value ( ) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often , allowing for the removal of less critical entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.","title":"CaliDrop KV Cache Compression with Calibration"},{"location":"weekly_paper/2025-08-01/#crosspl-evaluating-large-language-models-on-cross-programming-language-code-generation","text":"Authors: Zhanhang Xiong, Dongxia Wang, Yuekang Li, Xinyuan An, Wenhai Wang 2025-07-26 http://arxiv.org/abs/2507.19904v1 As large language models ( s) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate s' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an -based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose s and 6 code-oriented s released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: https://anonymous.4open.science/r/crosspl-2814.","title":"CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation"},{"location":"weekly_paper/2025-08-01/#agentmesh-a-cooperative-multi-agent-generative-ai-framework-for-software-development-automation","text":"Authors: Sourena Khanzadeh 2025-07-26 http://arxiv.org/abs/2507.19902v1 Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating -powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their , and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation.","title":"AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation"},{"location":"weekly_paper/2025-08-01/#hcattention-extreme-kv-cache-compression-via-heterogeneous-attention-computing-for-llms","text":"Authors: Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao 2025-07-26 http://arxiv.org/abs/2507.19823v1 Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value ( ) cache during inference. Existing cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"weekly_paper/2025-08-01/#large-language-model-agent-for-structural-drawing-generation-using-react-prompt-engineering-and-retrieval-augmented-generation","text":"Authors: Xin Zhang, Lissette Iturburu, Juan Nicolas Villamizar, Xiaoyu Liu, Manuel Salmeron, Shirley J. Dyke, Julio Ramirez 2025-07-26 http://arxiv.org/abs/2507.19771v1 Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model ( ) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.","title":"Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation"},{"location":"weekly_paper/2025-08-01/#lowkeyemg-electromyographic-typing-with-a-reduced-keyset","text":"Authors: Johannes Y. Lee, Derek Xiao, Shreyas Kaasyap, Nima R. Hadidi, John L. Zhou, Jacob Cunningham, Rakshith R. Gore, Deniz O. Eren, Jonathan C. Kao 2025-07-26 http://arxiv.org/abs/2507.19736v1 We introduce LowKeyEMG, a real-time human-computer interface that enables efficient text entry using only 7 gesture classes decoded from surface electromyography (sEMG). Prior work has attempted full-alphabet decoding from sEMG, but decoding large character sets remains unreliable, especially for individuals with motor impairments. Instead, LowKeyEMG reduces the English alphabet to 4 gesture keys, with 3 more for space and system interaction, to reliably translate simple one-handed gestures into text, leveraging the recurrent -based language model RW for efficient computation. In real-time experiments, participants achieved average one-handed keyboardless typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture efficiency by 17% (relative to typed phrase length). When typing with only 7 keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this low-key typing paradigm can maintain practical rates. Our results have implications for assistive technologies and any interface where input bandwidth is constrained.","title":"LowKeyEMG Electromyographic typing with a reduced keyset"},{"location":"weekly_paper/2025-08-01/#towards-inclusive-nlp-assessing-compressed-multilingual-transformers-across-diverse-language-benchmarks","text":"Authors: Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani 2025-07-25 http://arxiv.org/abs/2507.19699v1 Although s have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models ( s) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA S as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting.","title":"Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks"},{"location":"weekly_paper/2025-08-01/#x-of-information-continuum-a-survey-on-ai-driven-multi-dimensional-metrics-for-next-generation-networked-systems","text":"Authors: Beining Wu, Jun Huang, Shui Yu 2025-07-25 http://arxiv.org/abs/2507.19657v1 The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/ dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV s, ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ...","title":"\"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems"},{"location":"weekly_paper/2025-08-01/#deltallm-a-training-free-framework-exploiting-temporal-sparsity-for-efficient-edge-llm-inference","text":"Authors: Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen 2025-07-25 http://arxiv.org/abs/2507.19608v1 Deploying Large Language Models ( s) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present Delta , a training-free framework that exploits temporal in attention patterns to enable efficient inference across both the prefilling and decoding stages, on resource-constrained edge devices. Delta introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal , and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that Delta offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"weekly_paper/2025-08-01/#advancing-event-forecasting-through-massive-training-of-large-language-models-challenges-solutions-and-broader-impacts","text":"Authors: Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel 2025-07-25 http://arxiv.org/abs/2507.19477v1 Many recent papers have studied the development of superforecaster-level event forecasting s. While methodological problems with early studies cast doubt on the use of s for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art s are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting s. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of -based event forecasting training: noisiness- , knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.","title":"Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts"},{"location":"weekly_paper/2025-08-01/#gepa-reflective-prompt-evolution-can-outperform-reinforcement-learning","text":"Authors: Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab 2025-07-25 http://arxiv.org/abs/2507.19457v1 Large language models ( s) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for s, compared with policy gradients derived from , scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two s, and demonstrates promising results as an inference-time search strategy for code optimization.","title":"GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning"},{"location":"weekly_paper/2025-08-01/#step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding","text":"Authors: StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang 2025-07-25 http://arxiv.org/abs/2507.19427v1 Large language models ( s) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE , and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for decoding.","title":"Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding"},{"location":"weekly_paper/2025-08-01/#doubling-your-data-in-minutes-ultra-fast-tabular-data-generation-via-llm-induced-dependency-graphs","text":"Authors: Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci 2025-07-25 http://arxiv.org/abs/2507.19334v1 Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models ( s) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures dependencies via an -induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over -based baselines.","title":"Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs"},{"location":"weekly_paper/2025-08-01/#patch-pruning-strategy-based-on-robust-statistical-measures-of-attention-weight-diversity-in-vision-transformers","text":"Authors: Yuki Igaue, Hiroaki Aizawa 2025-07-25 http://arxiv.org/abs/2507.19175v1 Multi-head self-attention is a distinctive feature extraction mechanism of vision s that computes pairwise relationships among all input patches, contributing significantly to their high performance. However, it is known to incur a quadratic computational complexity with respect to the number of patches. One promising approach to address this issue is patch , which improves computational efficiency by identifying and removing redundant patches. In this work, we propose a patch strategy that evaluates the importance of each patch based on the variance of attention weights across multiple attention heads. This approach is inspired by the design of multi-head self-attention, which aims to capture diverse attention patterns across different subspaces of feature representations. The proposed method can be easily applied during both training and inference, and achieves improved throughput while maintaining classification accuracy in scenarios such as fine-tuning with pre-trained models. In addition, we also found that using robust statistical measures, such as the median absolute deviation in place of variance, to assess patch importance can similarly lead to strong performance. Furthermore, by introducing ping patch embeddings, our method achieves better performance with comparable throughput to conventional approaches that utilize all patches.","title":"Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers"},{"location":"weekly_paper/2025-08-01/#regscore-scoring-systems-for-regression-tasks","text":"Authors: Michal K. Grzeszczyk, Tomasz Szczepa\u0144ski, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzci\u0144ski, Arkadiusz Sitek 2025-07-25 http://arxiv.org/abs/2507.19155v1 Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, , and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and k- ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at https://github.com/SanoScience/RegScore.","title":"RegScore Scoring Systems for Regression Tasks"},{"location":"weekly_paper/2025-08-01/#mixa-q-revisiting-activation-sparsity-for-vision-transformers-from-a-mixed-precision-quantization-perspective","text":"Authors: Weitian Wang, Rai Shubham, Cecilia De La Parra, Akash Kumar 2025-07-25 http://arxiv.org/abs/2507.19131v1 In this paper, we propose MixA-Q, a mixed-precision activation quantization framework that leverages intra-layer activation (a concept widely explored in activation methods) for efficient inference of quantized window-based vision s. For a given uniform-bit quantization configuration, MixA-Q separates the batched window computations within Swin blocks and assigns a lower bit width to the activations of less important windows, improving the trade-off between model performance and efficiency. We introduce a Two-Branch Swin Block that processes activations separately in high- and low-bit precision, enabling seamless integration of our method with most quantization-aware training (QAT) and post-training quantization (PTQ) methods, or with simple modifications. Our experimental evaluations over the COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x computational speedup without accuracy loss in PTQ configuration. With QAT, MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP drop by incorporating activation . Notably, by reducing the quantization error in important regions, our -aware quantization adaptation improves the mAP of the quantized W4A4 model (with both weights and activations in 4-bit precision) by 0.7%, reducing quantization degradation by 24%.","title":"MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective"},{"location":"weekly_paper/2025-08-01/#deterministic-diffusion-models-for-lagrangian-turbulence-robustness-and-encoding-of-extreme-events","text":"Authors: Tianyi Li, Flavio Tuteri, Michele Buzzicotti, Fabio Bonaccorso, Luca Biferale 2025-07-25 http://arxiv.org/abs/2507.19103v1 Modeling Lagrangian turbulence remains a fundamental challenge due to its multiscale, intermittent, and non-Gaussian nature. Recent advances in data-driven diffusion models have enabled the generation of realistic Lagrangian velocity trajectories that accurately reproduce statistical properties across scales and capture rare extreme events. This study investigates three key aspects of diffusion-based modeling for Lagrangian turbulence. First, we assess architectural robustness by comparing a U-Net backbone with a -based alternative, finding strong consistency in generated trajectories, with only minor discrepancies at small scales. Second, leveraging a deterministic variant of diffusion model formulation, namely the deterministic denoising diffusion implicit model (DDIM), we identify structured features in the initial latent noise that align consistently with extreme events. Third, we explore accelerated generation by reducing the number of diffusion steps, and find that DDIM enables substantial speedups with minimal loss of statistical fidelity. These findings highlight the robustness of diffusion models and their potential for interpretable, scalable modeling of complex turbulent systems.","title":"Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events"},{"location":"weekly_paper/2025-08-08/","text":"2025-08-08 Table of Contents MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Automatic LLM Red Teaming Evaluating, Synthesizing, and Enhancing for Customer Support Conversation FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG S ^2 Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Compressing Chain-of-Thought in LLMs via Step Entropy Do language models accommodate their users? A study of linguistic convergence Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS AgentSME for Simulating Diverse Communication Modes in Smart Education Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives LOST Low-rank and Sparse Pre-training for Large Language Models Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Isolating Culture Neurons in Multilingual Large Language Models Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor LeanK Learnable K Cache Channel Pruning for Efficient Decoding Whispering Agents An event-driven covert communication protocol for the Internet of Agents Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models A Survey on AgentOps Categorization, Challenges, and Future Directions AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes IAUNet Instance-Aware U-Net Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Session-Based Recommendation with Validated and Enriched LLM Intents ReaGAN Node-as-Agent-Reasoning Graph Agentic Network EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Systematic Evaluation of Optimization Techniques for Long-Context Language Models Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Authors: Amit Kumar Das, Klaus Mueller 2025-08-06 http://arxiv.org/abs/2508.04679v1 Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models ( s) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming -based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data . Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Authors: Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu 2025-08-06 http://arxiv.org/abs/2508.04664v1 Large Language Models ( s) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment s' capabilities, we propose a complementary approach: empowering s with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips s with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables s to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information- benchmarks-PI- (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging s' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale. Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning Authors: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis 2025-08-06 http://arxiv.org/abs/2508.04581v1 Large language models ( s) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head ), while the repetitive layered structure of s implies significant inter-block redundancy - a dimension largely unexplored beyond key-value ( ) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained s to reduce their number of parameters without experiencing any significant drop in their performance. TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models Authors: Xinkui Zhao, Haode Li, Yifan Zhang, Guanjie Cheng, Yueshen Xu 2025-08-06 http://arxiv.org/abs/2508.04474v1 Recent advances in large language models ( s) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting s with external, interpretable memory. Nevertheless, most existing methods that combine s with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and of new facts. This plug-and-play architecture facilitates seamless integration with various s, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning. CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Authors: Enyu Zhou, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04462v1 Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for inference . However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD. Automatic LLM Red Teaming Authors: Roman Belaire, Arunesh Sinha, Pradeep Varakantham 2025-08-06 http://arxiv.org/abs/2508.04451v1 Red teaming is critical for identifying vulnerabilities and building trust in current s. However, current automated methods for Large Language Models ( s) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment. Evaluating, Synthesizing, and Enhancing for Customer Support Conversation Authors: Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong 2025-08-06 http://arxiv.org/abs/2508.04423v1 Effective customer support requires not only accurate problem solving but also structured and empathetic aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using s to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using -powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong s on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin. FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design Authors: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04405v1 Large Language Models ( s) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit . In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.05. The proposed kernel achieves an average 1.39 \\times speedup over ABQ- on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33 \\times inference and 1.21 \\times memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ. KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs Authors: Zunhai Su, Kehong Yuan 2025-08-06 http://arxiv.org/abs/2508.04257v1 Key-Value ( ) cache quantization has become a widely adopted optimization technique for efficient large language models ( s) inference by reducing cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of s for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{ Sink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that Sink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during cache quantization. Moreover, when applied to the well-established Quant method, Sink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Authors: Fatemeh Nazary, Ali Tourani, Yashar Deldjoo, Tommaso Di Noia 2025-08-06 http://arxiv.org/abs/2508.04206v1 Recommending long-form video content demands joint modeling of visual, audio, and textual modalities, yet most benchmarks address only raw features or narrow fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for -augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K, it aligns dense item embeddings from three modalities: audio (block-level, i-vector), visual (CNN, AVF), and text. Missing or metadata is automatically enriched using state-of-the-art s (e.g., OpenAI Ada), generating high-quality synopses for thousands of movies. All text (raw or augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5), producing multiple ready-to-use sets. The pipeline supports interchangeable early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are fully declarative via a single YAML file. Evaluation spans accuracy (Recall, nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty, diversity, fairness. Results show -based augmentation and strong text embeddings boost cold-start and coverage, especially when fused with audio-visual features. Systematic benchmarking reveals universal versus backbone- or metric-specific combinations. Open-source code, embeddings, and configs enable reproducible, fair multimodal RS research and advance principled generative AI integration in large-scale recommendation. Code: https://recsys-lab.github.io/ViLLA-MMBench Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Authors: Millicent Ochieng, Anja Thieme, Ignatius Ezeani, Risa Ueno, Samuel Maina, Keshet Ronen, Javier Gonzalez, Jacki O'Neill 2025-08-06 http://arxiv.org/abs/2508.04199v1 Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models ( s) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate s outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier s demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world . Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Authors: Teng Shi, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu 2025-08-06 http://arxiv.org/abs/2508.04145v1 In modern online platforms, search and recommendation (S&R) often coexist, offering opportunities for performance improvement through search-enhanced approaches. Existing studies show that incorporating search signals boosts recommendation performance. However, the effectiveness of these methods relies heavily on rich search interactions. They primarily benefit a small subset of users with abundant search behavior, while offering limited improvements for the majority of users who exhibit only search activity. To address the problem of search data in search-enhanced recommendation, we face two key challenges: (1) how to learn useful search features for users with search interactions, and (2) how to design effective training objectives under conditions. Our idea is to leverage the features of users with rich search interactions to enhance those of users with search interactions. Based on this idea, we propose GSERec, a method that utilizes message passing on the User-Code Graphs to alleviate data in Search-Enhanced Recommendation. Specifically, we utilize Large Language Models ( s) with vector quantization to generate discrete codes, which connect similar users and thereby construct the graph. Through message passing on this graph, embeddings of users with rich search data are propagated to enhance the embeddings of users with interactions. To further ensure that the message passing captures meaningful information from truly similar users, we introduce a contrastive loss to better model user similarities. The enhanced user representations are then integrated into downstream search-enhanced recommendation models. Experiments on three real-world datasets show that GSERec consistently outperforms baselines, especially for users with search behaviors. Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement Authors: Zheng Cheng, Wenri Wang, Guangyong Chen, Yakun Ju, Yihua Cheng, Zhisong Liu, Yanda Meng, Jintao Song 2025-08-06 http://arxiv.org/abs/2508.04123v1 Underwater image enhancement (UIE) techniques aim to improve visual quality of images captured in aquatic environments by addressing degradation issues caused by light absorption and scattering effects, including color distortion, blurring, and low contrast. Current mainstream solutions predominantly employ multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction quality through multi-resolution feature fusion. However, our extensive experiments demonstrate that high-quality image reconstruction does not necessarily rely on multi-scale feature fusion. Contrary to popular belief, our experiments show that single-scale feature extraction alone can match or surpass the performance of multi-scale methods, significantly reducing complexity. To comprehensively explore single-scale feature potential in underwater enhancement, we propose an innovative Single-Scale Decomposition Network (SSD-Net). This architecture introduces an asymmetrical decomposition mechanism that disentangles input image into clean layer along with degradation layer. The former contains scene-intrinsic information and the latter encodes medium-induced interference. It uniquely combines CNN's local feature extraction capabilities with Transformer's global modeling strengths through two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing dual-branch feature space decoupling via efficient attention operations and adaptive ; 2) Bidirectional Feature Communication Block (BFCB), enabling cross-layer residual interactions for complementary feature mining and fusion. This synergistic design preserves feature decomposition independence while establishing dynamic cross-layer information pathways, effectively enhancing degradation decoupling capacity. TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation Authors: Zunhui Xia, Hongxing Li, Libin Lan 2025-08-06 http://arxiv.org/abs/2508.04058v1 In recent years, -based methods have achieved remarkable progress in medical image segmentation due to their superior ability to capture long-range dependencies. However, these methods typically suffer from two major limitations. First, their computational complexity scales quadratically with the input sequences. Second, the feed-forward network (FFN) modules in vanilla Transformers typically rely on fully connected layers, which limits models' ability to capture local contextual information and multiscale features critical for precise semantic segmentation. To address these issues, we propose an efficient medical image segmentation network, named TCSAFormer. The proposed TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention (CA) module, which combines token compression and pixel-level attention to dynamically focus on the most relevant key-value pairs for each query. This is achieved by globally irrelevant tokens and merging redundant ones, significantly reducing computational complexity while enhancing the model's ability to capture relationships between tokens. Second, it introduces a Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the standard FFN to capture local contextual features and multiscale information, thereby strengthening the model's feature representation capability. We conduct extensive experiments on three publicly available medical image segmentation datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation performance of TCSAFormer. Experimental results demonstrate that TCSAFormer achieves superior performance compared to existing state-of-the-art (SOTA) methods, while maintaining lower computational overhead, thus achieving an optimal trade-off between efficiency and accuracy. PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG Authors: Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang 2025-08-06 http://arxiv.org/abs/2508.04057v1 Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models ( s) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the 's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average. S ^2 Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Authors: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu 2025-08-06 http://arxiv.org/abs/2508.04016v2 Diffusion s have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose S ^2 Q-VDiT, a post-training quantization framework for V-DMs that leverages Salient data and Sparse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, S ^2 Q-VDiT achieves lossless performance while delivering 3.9\\times model compression and 1.3\\times inference . Code will be available at https://github.com/wlfeng0509/s2q-vdit. Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency Authors: Md Arafat Sultan, Ram\u00f3n Fernandez Astudillo 2025-08-06 http://arxiv.org/abs/2508.03979v1 Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis . Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five s on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases. MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Authors: Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang 2025-08-05 http://arxiv.org/abs/2508.03553v1 Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models ( s). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \\textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG. Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Authors: Wenxin Mao, Zhitao Wang, Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin 2025-08-05 http://arxiv.org/abs/2508.03379v2 Large language models ( s) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with s' excellent mathematical strengths. Additional static parsing and dependency further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency. Compressing Chain-of-Thought in LLMs via Step Entropy Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu 2025-08-05 http://arxiv.org/abs/2508.03346v1 Large Language Models ( s) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy , which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables s to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances inference efficiency while rigorously preserving accuracy, offering profound implications for practical deployment and a deeper understanding of reasoning structures. Do language models accommodate their users? A study of linguistic convergence Authors: Terra Blevins, Susanne Schmalwieser, Benjamin Roth 2025-08-05 http://arxiv.org/abs/2508.03276v1 While large language models ( s) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language , asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different. Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS Authors: Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang 2025-08-05 http://arxiv.org/abs/2508.03125v1 Large language model-based multi-agent systems ( -MAS) effectively accomplish complex and dynamic tasks through inter-agent , but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting -MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, architectures, and s demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust safeguards in -MAS. AgentSME for Simulating Diverse Communication Modes in Smart Education Authors: Wen-Xi Yang, Tian-Fang Zhao 2025-08-05 http://arxiv.org/abs/2508.03109v1 Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human . To address this issue, this paper proposes AgentSME, a unified generative agent framework powered by . Three directional modes are considered in the models, namely Solo, Mono, and Echo, reflecting different types of agency autonomy and communicative reciprocity. Accuracy is adopted as the primary evaluation metric, complemented by three diversity indices designed to assess the diversity of reasoning contents. Six widely used s are tested to validate the robustness of modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations. The results show that generative agents that employ the Echo mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models. Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives Authors: Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens 2025-08-04 http://arxiv.org/abs/2508.02853v1 We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address demographic coverage, we test whether -generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives. LOST Low-rank and Sparse Pre-training for Large Language Models Authors: Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang 2025-08-04 http://arxiv.org/abs/2508.02668v1 While large language models ( s) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for s, a novel method that ingeniously integrates low-rank and structures to enable effective training of s from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise components to complement the expressiveness of low-rank training. We evaluate LOST on pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST Repo} Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks Authors: Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda 2025-08-04 http://arxiv.org/abs/2508.02556v1 Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into ping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than -based models, making them well-suited for real-world deployment. xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Authors: Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qin Zhang, Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang 2025-08-04 http://arxiv.org/abs/2508.02520v4 The rise of scaled-out s and scaled-up SuperPods signals a new era in large-scale AI infrastructure. s continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs. Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms Authors: Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu 2025-08-04 http://arxiv.org/abs/2508.02506v1 Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models ( s) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments. CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Authors: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang 2025-08-04 http://arxiv.org/abs/2508.02401v1 Recent advances in large language models ( s) have significantly boosted long-context processing. However, the increasing key-value ( ) cache size poses critical challenges to memory and execution efficiency. Most cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based s. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of s. To address the issue above, instead of using all the attention heads in GQA-based s to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive cache allocation strategy. Experimental results demonstrate the proposed Compress consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/Compress .git. Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Authors: Zuxin Ma, Yunhe Cui, Yongbin Qin 2025-08-04 http://arxiv.org/abs/2508.02381v2 Non-uniform structured network methods can effectively reduce Large Language Model ( ) size by eliminating redundant channels or layers, offering lower performance degradation than uniform strategies. However, existing non-uniform methods rely heavily on manually designed policies (e.g., layer importance and scaling factors), and therefore cannot efficiently adapt to scenarios with dynamic ratio requirements. Additionly, a critical bottleneck -- the time-consuming evaluation of policies -- further limits the feasibility of iteratively and dynamically finding optimal policies. To address these limitations, we propose PPF (Predictive Pruning Framework), a novel framework for s that eliminates manual design dependencies via second-level performance prediction. PPF not only supports real-time decisions under dynamic ratios but is also applicable to static scenarios. It employs an agent for producing adaptive and real-time actions, while a lightweight performance predictor that can evaluate a policy in seconds, significantly speeding up the iterative optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can generate dynamic/static policies and it reduces perplexity by up to 33.4% (dynamic ) and 84.78% (static ) over existing methods, outperforming manually designed policies. The performance predictor achieves second-level performance prediction with high accuracy (prediction error < 0.0011). It reduces the mean evaluation latency from minute-level (1 minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52 seconds), achieving over 64 times speedup. Our code will be available at https://github.com/Ma-zx/PPF . Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems Authors: Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang 2025-08-04 http://arxiv.org/abs/2508.02344v1 Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models ( s) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent -based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection through its self-iteration and a new synchronous network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at https://huggingface.co/Season998/Traffic-R1. CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis Authors: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che 2025-08-04 http://arxiv.org/abs/2508.02322v1 Large Language Models ( s) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level , merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU. VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Authors: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu 2025-08-04 http://arxiv.org/abs/2508.02317v3 Recent advances in large language models ( s) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal s remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal s. VeOmni introduces model-centric distributed recipes that decouples from computation, enabling efficient 3D parallelism on omni-modal s. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal s. Isolating Culture Neurons in Multilingual Large Language Models Authors: Danial Namazifard, Lukas Galke 2025-08-04 http://arxiv.org/abs/2508.02241v1 Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that s encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons . Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor Authors: Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu 2025-08-04 http://arxiv.org/abs/2508.02240v2 Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based . First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.} LeanK Learnable K Cache Channel Pruning for Efficient Decoding Authors: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu 2025-08-04 http://arxiv.org/abs/2508.02215v1 Large language models ( s) enable long-context tasks but face efficiency challenges due to the growing key-value ( ) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel . With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. Whispering Agents An event-driven covert communication protocol for the Internet of Agents Authors: Kaibo Huang, Yukun Wei, WanSheng Wu, Tianhua Zhang, Zhongliang Yang, Linna Zhou 2025-08-04 http://arxiv.org/abs/2508.02188v1 The emergence of the Internet of Agents (IoA) introduces critical challenges for privacy in sensitive, high-stakes domains. While standard Agent-to-Agent (A2A) protocols secure message content, they are not designed to protect the act of itself, leaving agents vulnerable to surveillance and traffic analysis. We find that the rich, event-driven nature of agent dialogues provides a powerful, yet untapped, medium for covert . To harness this potential, we introduce and formalize the Covert Event Channel, the first unified model for agent covert driven by three interconnected dimensions, which consist of the Storage, Timing,and Behavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a novel protocol that operationalizes this event-driven paradigm. Our comprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and robustness while remaining imperceptible to powerful -based wardens, establishing its practical viability. By systematically engineering this channel, our work provides the foundational understanding essential for developing the next generation of monitoring systems and defensive protocols for a secure and trustworthy IoA. Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Authors: Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad 2025-08-04 http://arxiv.org/abs/2508.02148v1 Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic (RKD-SC) framework is proposed to enable efficient and \\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods. Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models Authors: Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang 2025-08-04 http://arxiv.org/abs/2508.02128v1 In the era of large language models ( s), N:M has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight , it often suffers from significant accuracy degradation. Activation , though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation method designed specifically for the prefill stage, targeting the of linear projection layers in s. Extensive experiments across multiple models and ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding- , a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation , providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. A Survey on AgentOps Categorization, Challenges, and Future Directions Authors: Zexin Wang, Jingjing Li, Quan Zhou, Haotian Si, Yuanhao Liu, Jianhui Li, Gaogang Xie, Fei Sun, Dan Pei, Changhua Pei 2025-08-04 http://arxiv.org/abs/2508.02121v1 As the reasoning capabilities of Large Language Models ( s) continue to advance, -based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is . To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution. AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Authors: Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha 2025-08-04 http://arxiv.org/abs/2508.02079v1 Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models ( s). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation. Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games Authors: Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen 2025-08-04 http://arxiv.org/abs/2508.02076v1 Coordinating multiple large language models ( s) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi- ensembles. In MAC-SPGG, agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation. CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes Authors: Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot 2025-08-03 http://arxiv.org/abs/2508.01936v1 We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view , deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection. IAUNet Instance-Aware U-Net Authors: Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman 2025-08-03 http://arxiv.org/abs/2508.01928v1 Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of ping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, -based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language Authors: Jaskaranjeet Singh, Rakesh Thakur 2025-08-03 http://arxiv.org/abs/2508.01918v1 Despite the rapid advancement of large language models ( s), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs. As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization Authors: Zicong Ye, Kunming Zhang, Guoming Tang 2025-08-03 http://arxiv.org/abs/2508.01744v1 The explosive growth of interactive Large Language Models ( s) has placed unprecedented demands for low latency on cloud GPUs, forcing them into high-power modes and causing escalating energy costs. Real-time inference workloads exhibit significant dynamic volatility, presenting substantial energy-saving opportunities. However, traditional static or rule-based power management strategies struggle to exploit these opportunities without compromising peak performance. To address this challenge, we propose AGFT (An Adaptive GPU Frequency Tuner), a framework that employs online reinforcement learning to autonomously learn an optimal frequency tuning policy. By monitoring real-time features like request load and latency, AGFT utilizes fine-grained frequency control for precise adjustments and intelligent action space for stable, efficient decision-making. This creates a robust, automated energy management solution. We comprehensively evaluated AGFT in an environment simulating realistic, fluctuating inference requests. The experimental results demonstrate that AGFT successfully saves 44.3% of GPU energy consumption while introducing a minimal performance latency overhead of under 10%. This achievement translates into a comprehensive Energy-Delay Product (EDP) optimization of up to 40.3%, clearly showing that our framework can significantly enhance the energy efficiency and economic benefits of existing inference clusters without compromising service quality. SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference Authors: Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu 2025-08-03 http://arxiv.org/abs/2508.02751v1 cache eviction has emerged as an effective solution to alleviate resource constraints faced by s in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between s of different scales. We propose Small , a small model assisted compensation method for cache compression. Small can maintain attention matching between different-scale s to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of Small . Moreover, efficiency evaluations show that Small achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant inference in resource constrained environments. EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models Authors: Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng 2025-08-03 http://arxiv.org/abs/2508.01625v1 Mixture-of-Experts (MoE) has demonstrated promising potential in scaling s. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE- s, which deeply aligns with the characteristics of MoE from the perspectives of quantization and , and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE- s. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation. RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale Authors: Zhilong Chen, Chengzong Zhao, Boyuan Chen, Dayi Lin, Yihao Chen, Arthur Leung, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Ahmed E. Hassan 2025-08-03 http://arxiv.org/abs/2508.01550v1 Training software engineering (SWE) s is bottlenecked by expensive infrastructure, inefficient evaluation pipelines, scarce training data, and costly quality control. We present RepoForge, an autonomous, end-to-end pipeline that generates, evaluates, and trains SWE agents at scale. Our key contributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on SWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new state-of-the-art for \\leq 8B non-thinking s; (2) 7,304 executable environments auto-generated from real GitHub commits with zero manual intervention; (3) 14 \\times storage reduction (1.4GB \\rightarrow 102MB per instance) via intelligent dependency management and image ; (4) > 70\\% faster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge harness; (5) 19,000 \\times cheaper labeling through our automated SPICE~\\citep{spice2024} difficulty assessment technique. By unifying storage-efficient sandboxing, Ray-powered evaluation harness, automated data generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate that even \\leq 8B models can reach new state-of-the-art performance on demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical bottlenecks in SWE agent training: high storage costs of container-based evaluation, inefficient sequential reward pipelines, limited availability of high-quality training data, expensive manual labeling, and multi-turn RL pipeline bottlenecks. BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Authors: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan 2025-08-02 http://arxiv.org/abs/2508.01332v2 The rapid adoption of agentic AI, powered by large language models ( s), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in -driven multi-agent systems (MASes): fragmented identity frameworks, insecure channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, -based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production -based MAS environments. Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Authors: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat 2025-08-02 http://arxiv.org/abs/2508.01261v1 We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top- k selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla s while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference . Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment. Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Authors: Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen 2025-08-02 http://arxiv.org/abs/2508.01159v1 This study evaluates the capacity of large language models ( s) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that s can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician . Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation Authors: Leyao Wang, Xutao Mao, Xuhui Zhan, Yuying Zhao, Bo Ni, Ryan A. Rossi, Nesreen K. Ahmed, Tyler Derr 2025-08-02 http://arxiv.org/abs/2508.01128v1 Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and -based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations. REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Authors: Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu 2025-08-01 http://arxiv.org/abs/2508.01057v1 Collisions caused by human error are the most common type of multi-vehicle crash, highlighting the critical need for autonomous driving (AD) systems to leverage cooperative perception through Vehicle-to-Everything (V2X) . This capability extends situational awareness beyond the limitations of onboard sensors. However, current -based V2X frameworks suffer from limited generalization, shallow contextual reasoning, and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced reasoning and multimodal integration but typically fall short of real-time performance requirements in safety-critical applications. This paper presents REACT, a real-time, V2X-integrated trajectory optimization framework built upon a fine-tuned lightweight VLM. REACT integrates a set of specialized modules that process multimodal inputs into optimized, risk-aware trajectories. To ensure real-time performance on edge devices, REACT incorporates edge adaptation strategies that reduce model complexity and accelerate inference. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results demonstrate the feasibility of lightweight VLMs for real-time edge-based cooperative planning and showcase the potential of language-guided contextual reasoning to improve safety and responsiveness in autonomous driving. Session-Based Recommendation with Validated and Enriched LLM Intents Authors: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang 2025-08-01 http://arxiv.org/abs/2508.00570v1 Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models ( s), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched -generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of -generated intents with a global intent pool to constrain the 's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability. ReaGAN Node-as-Agent-Reasoning Graph Agentic Network Authors: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang 2025-08-01 http://arxiv.org/abs/2508.00429v2 Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain . Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning. EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Authors: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun 2025-08-01 http://arxiv.org/abs/2508.00370v2 Deploying Transformer-based large language models ( s) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value ( ) cache demands. While existing cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token . Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices. Systematic Evaluation of Optimization Techniques for Long-Context Language Models Authors: Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar 2025-08-01 http://arxiv.org/abs/2508.00305v1 Large language models ( s) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like , quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations. Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study Authors: Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek 2025-08-01 http://arxiv.org/abs/2508.00256v1 Low-altitude wireless networks (LAWNs) have the potential to revolutionize s by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure s in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure s in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models ( s) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.","title":"2025-08-08"},{"location":"weekly_paper/2025-08-08/#2025-08-08","text":"","title":"2025-08-08"},{"location":"weekly_paper/2025-08-08/#table-of-contents","text":"MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Automatic LLM Red Teaming Evaluating, Synthesizing, and Enhancing for Customer Support Conversation FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG S ^2 Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Compressing Chain-of-Thought in LLMs via Step Entropy Do language models accommodate their users? A study of linguistic convergence Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS AgentSME for Simulating Diverse Communication Modes in Smart Education Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives LOST Low-rank and Sparse Pre-training for Large Language Models Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Isolating Culture Neurons in Multilingual Large Language Models Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor LeanK Learnable K Cache Channel Pruning for Efficient Decoding Whispering Agents An event-driven covert communication protocol for the Internet of Agents Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models A Survey on AgentOps Categorization, Challenges, and Future Directions AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes IAUNet Instance-Aware U-Net Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Session-Based Recommendation with Validated and Enriched LLM Intents ReaGAN Node-as-Agent-Reasoning Graph Agentic Network EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Systematic Evaluation of Optimization Techniques for Long-Context Language Models Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study","title":"Table of Contents"},{"location":"weekly_paper/2025-08-08/#misvisfix-an-interactive-dashboard-for-detecting-explaining-and-correcting-misleading-visualizations-using-large-language-models","text":"Authors: Amit Kumar Das, Klaus Mueller 2025-08-06 http://arxiv.org/abs/2508.04679v1 Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models ( s) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming -based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data .","title":"MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models"},{"location":"weekly_paper/2025-08-08/#sculptor-empowering-llms-with-cognitive-agency-via-active-context-management","text":"Authors: Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu 2025-08-06 http://arxiv.org/abs/2508.04664v1 Large Language Models ( s) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment s' capabilities, we propose a complementary approach: empowering s with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips s with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables s to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information- benchmarks-PI- (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging s' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.","title":"Sculptor Empowering LLMs with Cognitive Agency via Active Context Management"},{"location":"weekly_paper/2025-08-08/#share-your-attention-transformer-weight-sharing-via-matrix-based-dictionary-learning","text":"Authors: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis 2025-08-06 http://arxiv.org/abs/2508.04581v1 Large language models ( s) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head ), while the repetitive layered structure of s implies significant inter-block redundancy - a dimension largely unexplored beyond key-value ( ) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained s to reduce their number of parameters without experiencing any significant drop in their performance.","title":"Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning"},{"location":"weekly_paper/2025-08-08/#trail-joint-inference-and-refinement-of-knowledge-graphs-with-large-language-models","text":"Authors: Xinkui Zhao, Haode Li, Yifan Zhang, Guanjie Cheng, Yueshen Xu 2025-08-06 http://arxiv.org/abs/2508.04474v1 Recent advances in large language models ( s) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting s with external, interpretable memory. Nevertheless, most existing methods that combine s with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and of new facts. This plug-and-play architecture facilitates seamless integration with various s, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning.","title":"TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models"},{"location":"weekly_paper/2025-08-08/#card-cache-assisted-parallel-speculative-decoding-for-efficient-large-language-model-inference","text":"Authors: Enyu Zhou, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04462v1 Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for inference . However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD.","title":"CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference"},{"location":"weekly_paper/2025-08-08/#automatic-llm-red-teaming","text":"Authors: Roman Belaire, Arunesh Sinha, Pradeep Varakantham 2025-08-06 http://arxiv.org/abs/2508.04451v1 Red teaming is critical for identifying vulnerabilities and building trust in current s. However, current automated methods for Large Language Models ( s) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment.","title":"Automatic LLM Red Teaming"},{"location":"weekly_paper/2025-08-08/#evaluating-synthesizing-and-enhancing-for-customer-support-conversation","text":"Authors: Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong 2025-08-06 http://arxiv.org/abs/2508.04423v1 Effective customer support requires not only accurate problem solving but also structured and empathetic aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using s to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using -powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong s on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.","title":"Evaluating, Synthesizing, and Enhancing for Customer Support Conversation"},{"location":"weekly_paper/2025-08-08/#flexq-efficient-post-training-int6-quantization-for-llm-serving-via-algorithm-system-co-design","text":"Authors: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04405v1 Large Language Models ( s) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit . In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.05. The proposed kernel achieves an average 1.39 \\times speedup over ABQ- on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33 \\times inference and 1.21 \\times memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ.","title":"FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design"},{"location":"weekly_paper/2025-08-08/#kvsink-understanding-and-enhancing-the-preservation-of-attention-sinks-in-kv-cache-quantization-for-llms","text":"Authors: Zunhai Su, Kehong Yuan 2025-08-06 http://arxiv.org/abs/2508.04257v1 Key-Value ( ) cache quantization has become a widely adopted optimization technique for efficient large language models ( s) inference by reducing cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of s for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{ Sink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that Sink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during cache quantization. Moreover, when applied to the well-established Quant method, Sink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.","title":"KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"weekly_paper/2025-08-08/#villa-mmbench-a-unified-benchmark-suite-for-llm-augmented-multimodal-movie-recommendation","text":"Authors: Fatemeh Nazary, Ali Tourani, Yashar Deldjoo, Tommaso Di Noia 2025-08-06 http://arxiv.org/abs/2508.04206v1 Recommending long-form video content demands joint modeling of visual, audio, and textual modalities, yet most benchmarks address only raw features or narrow fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for -augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K, it aligns dense item embeddings from three modalities: audio (block-level, i-vector), visual (CNN, AVF), and text. Missing or metadata is automatically enriched using state-of-the-art s (e.g., OpenAI Ada), generating high-quality synopses for thousands of movies. All text (raw or augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5), producing multiple ready-to-use sets. The pipeline supports interchangeable early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are fully declarative via a single YAML file. Evaluation spans accuracy (Recall, nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty, diversity, fairness. Results show -based augmentation and strong text embeddings boost cold-start and coverage, especially when fused with audio-visual features. Systematic benchmarking reveals universal versus backbone- or metric-specific combinations. Open-source code, embeddings, and configs enable reproducible, fair multimodal RS research and advance principled generative AI integration in large-scale recommendation. Code: https://recsys-lab.github.io/ViLLA-MMBench","title":"ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation"},{"location":"weekly_paper/2025-08-08/#reasoning-beyond-labels-measuring-llm-sentiment-in-low-resource-culturally-nuanced-contexts","text":"Authors: Millicent Ochieng, Anja Thieme, Ignatius Ezeani, Risa Ueno, Samuel Maina, Keshet Ronen, Javier Gonzalez, Jacki O'Neill 2025-08-06 http://arxiv.org/abs/2508.04199v1 Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models ( s) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate s outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier s demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world .","title":"Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts"},{"location":"weekly_paper/2025-08-08/#benefit-from-rich-tackling-search-interaction-sparsity-in-search-enhanced-recommendation","text":"Authors: Teng Shi, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu 2025-08-06 http://arxiv.org/abs/2508.04145v1 In modern online platforms, search and recommendation (S&R) often coexist, offering opportunities for performance improvement through search-enhanced approaches. Existing studies show that incorporating search signals boosts recommendation performance. However, the effectiveness of these methods relies heavily on rich search interactions. They primarily benefit a small subset of users with abundant search behavior, while offering limited improvements for the majority of users who exhibit only search activity. To address the problem of search data in search-enhanced recommendation, we face two key challenges: (1) how to learn useful search features for users with search interactions, and (2) how to design effective training objectives under conditions. Our idea is to leverage the features of users with rich search interactions to enhance those of users with search interactions. Based on this idea, we propose GSERec, a method that utilizes message passing on the User-Code Graphs to alleviate data in Search-Enhanced Recommendation. Specifically, we utilize Large Language Models ( s) with vector quantization to generate discrete codes, which connect similar users and thereby construct the graph. Through message passing on this graph, embeddings of users with rich search data are propagated to enhance the embeddings of users with interactions. To further ensure that the message passing captures meaningful information from truly similar users, we introduce a contrastive loss to better model user similarities. The enhanced user representations are then integrated into downstream search-enhanced recommendation models. Experiments on three real-world datasets show that GSERec consistently outperforms baselines, especially for users with search behaviors.","title":"Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation"},{"location":"weekly_paper/2025-08-08/#excavate-the-potential-of-single-scale-features-a-decomposition-network-for-water-related-optical-image-enhancement","text":"Authors: Zheng Cheng, Wenri Wang, Guangyong Chen, Yakun Ju, Yihua Cheng, Zhisong Liu, Yanda Meng, Jintao Song 2025-08-06 http://arxiv.org/abs/2508.04123v1 Underwater image enhancement (UIE) techniques aim to improve visual quality of images captured in aquatic environments by addressing degradation issues caused by light absorption and scattering effects, including color distortion, blurring, and low contrast. Current mainstream solutions predominantly employ multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction quality through multi-resolution feature fusion. However, our extensive experiments demonstrate that high-quality image reconstruction does not necessarily rely on multi-scale feature fusion. Contrary to popular belief, our experiments show that single-scale feature extraction alone can match or surpass the performance of multi-scale methods, significantly reducing complexity. To comprehensively explore single-scale feature potential in underwater enhancement, we propose an innovative Single-Scale Decomposition Network (SSD-Net). This architecture introduces an asymmetrical decomposition mechanism that disentangles input image into clean layer along with degradation layer. The former contains scene-intrinsic information and the latter encodes medium-induced interference. It uniquely combines CNN's local feature extraction capabilities with Transformer's global modeling strengths through two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing dual-branch feature space decoupling via efficient attention operations and adaptive ; 2) Bidirectional Feature Communication Block (BFCB), enabling cross-layer residual interactions for complementary feature mining and fusion. This synergistic design preserves feature decomposition independence while establishing dynamic cross-layer information pathways, effectively enhancing degradation decoupling capacity.","title":"Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement"},{"location":"weekly_paper/2025-08-08/#tcsaformer-efficient-vision-transformer-with-token-compression-and-sparse-attention-for-medical-image-segmentation","text":"Authors: Zunhui Xia, Hongxing Li, Libin Lan 2025-08-06 http://arxiv.org/abs/2508.04058v1 In recent years, -based methods have achieved remarkable progress in medical image segmentation due to their superior ability to capture long-range dependencies. However, these methods typically suffer from two major limitations. First, their computational complexity scales quadratically with the input sequences. Second, the feed-forward network (FFN) modules in vanilla Transformers typically rely on fully connected layers, which limits models' ability to capture local contextual information and multiscale features critical for precise semantic segmentation. To address these issues, we propose an efficient medical image segmentation network, named TCSAFormer. The proposed TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention (CA) module, which combines token compression and pixel-level attention to dynamically focus on the most relevant key-value pairs for each query. This is achieved by globally irrelevant tokens and merging redundant ones, significantly reducing computational complexity while enhancing the model's ability to capture relationships between tokens. Second, it introduces a Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the standard FFN to capture local contextual features and multiscale information, thereby strengthening the model's feature representation capability. We conduct extensive experiments on three publicly available medical image segmentation datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation performance of TCSAFormer. Experimental results demonstrate that TCSAFormer achieves superior performance compared to existing state-of-the-art (SOTA) methods, while maintaining lower computational overhead, thus achieving an optimal trade-off between efficiency and accuracy.","title":"TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation"},{"location":"weekly_paper/2025-08-08/#pairs-parametric-verified-adaptive-information-retrieval-and-selection-for-efficient-rag","text":"Authors: Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang 2025-08-06 http://arxiv.org/abs/2508.04057v1 Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models ( s) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the 's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average.","title":"PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG"},{"location":"weekly_paper/2025-08-08/#s2q-vdit-accurate-quantized-video-diffusion-transformer-with-salient-data-and-sparse-token-distillation","text":"Authors: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu 2025-08-06 http://arxiv.org/abs/2508.04016v2 Diffusion s have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose S ^2 Q-VDiT, a post-training quantization framework for V-DMs that leverages Salient data and Sparse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, S ^2 Q-VDiT achieves lossless performance while delivering 3.9\\times model compression and 1.3\\times inference . Code will be available at https://github.com/wlfeng0509/s2q-vdit.","title":"S^2Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation"},{"location":"weekly_paper/2025-08-08/#confidence-weighted-token-set-cover-for-early-hypothesis-pruning-in-self-consistency","text":"Authors: Md Arafat Sultan, Ram\u00f3n Fernandez Astudillo 2025-08-06 http://arxiv.org/abs/2508.03979v1 Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis . Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five s on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.","title":"Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency"},{"location":"weekly_paper/2025-08-08/#multirag-a-knowledge-guided-framework-for-mitigating-hallucination-in-multi-source-retrieval-augmented-generation","text":"Authors: Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang 2025-08-05 http://arxiv.org/abs/2508.03553v1 Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models ( s). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \\textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.","title":"MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation"},{"location":"weekly_paper/2025-08-08/#data-dependency-inference-for-industrial-code-generation-based-on-uml-sequence-diagrams","text":"Authors: Wenxin Mao, Zhitao Wang, Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin 2025-08-05 http://arxiv.org/abs/2508.03379v2 Large language models ( s) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with s' excellent mathematical strengths. Additional static parsing and dependency further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.","title":"Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams"},{"location":"weekly_paper/2025-08-08/#compressing-chain-of-thought-in-llms-via-step-entropy","text":"Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu 2025-08-05 http://arxiv.org/abs/2508.03346v1 Large Language Models ( s) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy , which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables s to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances inference efficiency while rigorously preserving accuracy, offering profound implications for practical deployment and a deeper understanding of reasoning structures.","title":"Compressing Chain-of-Thought in LLMs via Step Entropy"},{"location":"weekly_paper/2025-08-08/#do-language-models-accommodate-their-users-a-study-of-linguistic-convergence","text":"Authors: Terra Blevins, Susanne Schmalwieser, Benjamin Roth 2025-08-05 http://arxiv.org/abs/2508.03276v1 While large language models ( s) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language , asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different.","title":"Do language models accommodate their users? A study of linguistic convergence"},{"location":"weekly_paper/2025-08-08/#attack-the-messages-not-the-agents-a-multi-round-adaptive-stealthy-tampering-framework-for-llm-mas","text":"Authors: Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang 2025-08-05 http://arxiv.org/abs/2508.03125v1 Large language model-based multi-agent systems ( -MAS) effectively accomplish complex and dynamic tasks through inter-agent , but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting -MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, architectures, and s demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust safeguards in -MAS.","title":"Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS"},{"location":"weekly_paper/2025-08-08/#agentsme-for-simulating-diverse-communication-modes-in-smart-education","text":"Authors: Wen-Xi Yang, Tian-Fang Zhao 2025-08-05 http://arxiv.org/abs/2508.03109v1 Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human . To address this issue, this paper proposes AgentSME, a unified generative agent framework powered by . Three directional modes are considered in the models, namely Solo, Mono, and Echo, reflecting different types of agency autonomy and communicative reciprocity. Accuracy is adopted as the primary evaluation metric, complemented by three diversity indices designed to assess the diversity of reasoning contents. Six widely used s are tested to validate the robustness of modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations. The results show that generative agents that employ the Echo mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models.","title":"AgentSME for Simulating Diverse Communication Modes in Smart Education"},{"location":"weekly_paper/2025-08-08/#modeling-annotator-disagreement-with-demographic-aware-experts-and-synthetic-perspectives","text":"Authors: Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens 2025-08-04 http://arxiv.org/abs/2508.02853v1 We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address demographic coverage, we test whether -generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.","title":"Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives"},{"location":"weekly_paper/2025-08-08/#lost-low-rank-and-sparse-pre-training-for-large-language-models","text":"Authors: Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang 2025-08-04 http://arxiv.org/abs/2508.02668v1 While large language models ( s) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for s, a novel method that ingeniously integrates low-rank and structures to enable effective training of s from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise components to complement the expressiveness of low-rank training. We evaluate LOST on pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST Repo}","title":"LOST Low-rank and Sparse Pre-training for Large Language Models"},{"location":"weekly_paper/2025-08-08/#automated-snomed-ct-concept-annotation-in-clinical-text-using-bi-gru-neural-networks","text":"Authors: Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda 2025-08-04 http://arxiv.org/abs/2508.02556v1 Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into ping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than -based models, making them well-suited for real-world deployment.","title":"Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks"},{"location":"weekly_paper/2025-08-08/#xdeepserve-model-as-a-service-on-huawei-cloudmatrix384","text":"Authors: Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qin Zhang, Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang 2025-08-04 http://arxiv.org/abs/2508.02520v4 The rise of scaled-out s and scaled-up SuperPods signals a new era in large-scale AI infrastructure. s continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs.","title":"xDeepServe Model-as-a-Service on Huawei CloudMatrix384"},{"location":"weekly_paper/2025-08-08/#decomposed-reasoning-with-reinforcement-learning-for-relevance-assessment-in-ugc-platforms","text":"Authors: Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu 2025-08-04 http://arxiv.org/abs/2508.02506v1 Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models ( s) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments.","title":"Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms"},{"location":"weekly_paper/2025-08-08/#compresskv-semantic-retrieval-heads-know-what-tokens-are-not-important-before-generation","text":"Authors: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang 2025-08-04 http://arxiv.org/abs/2508.02401v1 Recent advances in large language models ( s) have significantly boosted long-context processing. However, the increasing key-value ( ) cache size poses critical challenges to memory and execution efficiency. Most cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based s. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of s. To address the issue above, instead of using all the attention heads in GQA-based s to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive cache allocation strategy. Experimental results demonstrate the proposed Compress consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/Compress .git.","title":"CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation"},{"location":"weekly_paper/2025-08-08/#beyond-manually-designed-pruning-policies-with-second-level-performance-prediction-a-pruning-framework-for-llms","text":"Authors: Zuxin Ma, Yunhe Cui, Yongbin Qin 2025-08-04 http://arxiv.org/abs/2508.02381v2 Non-uniform structured network methods can effectively reduce Large Language Model ( ) size by eliminating redundant channels or layers, offering lower performance degradation than uniform strategies. However, existing non-uniform methods rely heavily on manually designed policies (e.g., layer importance and scaling factors), and therefore cannot efficiently adapt to scenarios with dynamic ratio requirements. Additionly, a critical bottleneck -- the time-consuming evaluation of policies -- further limits the feasibility of iteratively and dynamically finding optimal policies. To address these limitations, we propose PPF (Predictive Pruning Framework), a novel framework for s that eliminates manual design dependencies via second-level performance prediction. PPF not only supports real-time decisions under dynamic ratios but is also applicable to static scenarios. It employs an agent for producing adaptive and real-time actions, while a lightweight performance predictor that can evaluate a policy in seconds, significantly speeding up the iterative optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can generate dynamic/static policies and it reduces perplexity by up to 33.4% (dynamic ) and 84.78% (static ) over existing methods, outperforming manually designed policies. The performance predictor achieves second-level performance prediction with high accuracy (prediction error < 0.0011). It reduces the mean evaluation latency from minute-level (1 minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52 seconds), achieving over 64 times speedup. Our code will be available at https://github.com/Ma-zx/PPF .","title":"Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs"},{"location":"weekly_paper/2025-08-08/#traffic-r1-reinforced-llms-bring-human-like-reasoning-to-traffic-signal-control-systems","text":"Authors: Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang 2025-08-04 http://arxiv.org/abs/2508.02344v1 Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models ( s) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent -based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection through its self-iteration and a new synchronous network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at https://huggingface.co/Season998/Traffic-R1.","title":"Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems"},{"location":"weekly_paper/2025-08-08/#camera-multi-matrix-joint-compression-for-moe-models-via-micro-expert-redundancy-analysis","text":"Authors: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che 2025-08-04 http://arxiv.org/abs/2508.02322v1 Large Language Models ( s) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level , merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.","title":"CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis"},{"location":"weekly_paper/2025-08-08/#veomni-scaling-any-modality-model-training-with-model-centric-distributed-recipe-zoo","text":"Authors: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu 2025-08-04 http://arxiv.org/abs/2508.02317v3 Recent advances in large language models ( s) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal s remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal s. VeOmni introduces model-centric distributed recipes that decouples from computation, enabling efficient 3D parallelism on omni-modal s. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal s.","title":"VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo"},{"location":"weekly_paper/2025-08-08/#isolating-culture-neurons-in-multilingual-large-language-models","text":"Authors: Danial Namazifard, Lukas Galke 2025-08-04 http://arxiv.org/abs/2508.02241v1 Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that s encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .","title":"Isolating Culture Neurons in Multilingual Large Language Models"},{"location":"weekly_paper/2025-08-08/#forecasting-when-to-forecast-accelerating-diffusion-models-with-confidence-gated-taylor","text":"Authors: Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu 2025-08-04 http://arxiv.org/abs/2508.02240v2 Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based . First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}","title":"Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor"},{"location":"weekly_paper/2025-08-08/#leank-learnable-k-cache-channel-pruning-for-efficient-decoding","text":"Authors: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu 2025-08-04 http://arxiv.org/abs/2508.02215v1 Large language models ( s) enable long-context tasks but face efficiency challenges due to the growing key-value ( ) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel . With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.","title":"LeanK Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"weekly_paper/2025-08-08/#whispering-agents-an-event-driven-covert-communication-protocol-for-the-internet-of-agents","text":"Authors: Kaibo Huang, Yukun Wei, WanSheng Wu, Tianhua Zhang, Zhongliang Yang, Linna Zhou 2025-08-04 http://arxiv.org/abs/2508.02188v1 The emergence of the Internet of Agents (IoA) introduces critical challenges for privacy in sensitive, high-stakes domains. While standard Agent-to-Agent (A2A) protocols secure message content, they are not designed to protect the act of itself, leaving agents vulnerable to surveillance and traffic analysis. We find that the rich, event-driven nature of agent dialogues provides a powerful, yet untapped, medium for covert . To harness this potential, we introduce and formalize the Covert Event Channel, the first unified model for agent covert driven by three interconnected dimensions, which consist of the Storage, Timing,and Behavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a novel protocol that operationalizes this event-driven paradigm. Our comprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and robustness while remaining imperceptible to powerful -based wardens, establishing its practical viability. By systematically engineering this channel, our work provides the foundational understanding essential for developing the next generation of monitoring systems and defensive protocols for a secure and trustworthy IoA.","title":"Whispering Agents An event-driven covert communication protocol for the Internet of Agents"},{"location":"weekly_paper/2025-08-08/#large-scale-model-enabled-semantic-communication-based-on-robust-knowledge-distillation","text":"Authors: Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad 2025-08-04 http://arxiv.org/abs/2508.02148v1 Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic (RKD-SC) framework is proposed to enable efficient and \\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods.","title":"Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation"},{"location":"weekly_paper/2025-08-08/#amber-pruner-leveraging-nm-activation-sparsity-for-efficient-prefill-in-large-language-models","text":"Authors: Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang 2025-08-04 http://arxiv.org/abs/2508.02128v1 In the era of large language models ( s), N:M has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight , it often suffers from significant accuracy degradation. Activation , though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation method designed specifically for the prefill stage, targeting the of linear projection layers in s. Extensive experiments across multiple models and ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding- , a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation , providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems.","title":"Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"weekly_paper/2025-08-08/#a-survey-on-agentops-categorization-challenges-and-future-directions","text":"Authors: Zexin Wang, Jingjing Li, Quan Zhou, Haotian Si, Yuanhao Liu, Jianhui Li, Gaogang Xie, Fei Sun, Dan Pei, Changhua Pei 2025-08-04 http://arxiv.org/abs/2508.02121v1 As the reasoning capabilities of Large Language Models ( s) continue to advance, -based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is . To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution.","title":"A Survey on AgentOps Categorization, Challenges, and Future Directions"},{"location":"weekly_paper/2025-08-08/#alignguard-lora-alignment-preserving-fine-tuning-via-fisher-guided-decomposition-and-riemannian-geodesic-collision-regularization","text":"Authors: Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha 2025-08-04 http://arxiv.org/abs/2508.02079v1 Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models ( s). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.","title":"AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization"},{"location":"weekly_paper/2025-08-08/#everyone-contributes-incentivizing-strategic-cooperation-in-multi-llm-systems-via-sequential-public-goods-games","text":"Authors: Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen 2025-08-04 http://arxiv.org/abs/2508.02076v1 Coordinating multiple large language models ( s) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi- ensembles. In MAC-SPGG, agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation.","title":"Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games"},{"location":"weekly_paper/2025-08-08/#cvd-sfm-a-cross-view-deep-front-end-structure-from-motion-system-for-sparse-localization-in-multi-altitude-scenes","text":"Authors: Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot 2025-08-03 http://arxiv.org/abs/2508.01936v1 We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view , deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection.","title":"CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes"},{"location":"weekly_paper/2025-08-08/#iaunet-instance-aware-u-net","text":"Authors: Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman 2025-08-03 http://arxiv.org/abs/2508.01928v1 Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of ping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, -based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet","title":"IAUNet Instance-Aware U-Net"},{"location":"weekly_paper/2025-08-08/#quantum-rag-and-pungpt2-advancing-low-resource-language-generation-and-retrieval-for-the-punjabi-language","text":"Authors: Jaskaranjeet Singh, Rakesh Thakur 2025-08-03 http://arxiv.org/abs/2508.01918v1 Despite the rapid advancement of large language models ( s), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs. As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP","title":"Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language"},{"location":"weekly_paper/2025-08-08/#agft-an-adaptive-gpu-frequency-tuner-for-real-time-llm-inference-optimization","text":"Authors: Zicong Ye, Kunming Zhang, Guoming Tang 2025-08-03 http://arxiv.org/abs/2508.01744v1 The explosive growth of interactive Large Language Models ( s) has placed unprecedented demands for low latency on cloud GPUs, forcing them into high-power modes and causing escalating energy costs. Real-time inference workloads exhibit significant dynamic volatility, presenting substantial energy-saving opportunities. However, traditional static or rule-based power management strategies struggle to exploit these opportunities without compromising peak performance. To address this challenge, we propose AGFT (An Adaptive GPU Frequency Tuner), a framework that employs online reinforcement learning to autonomously learn an optimal frequency tuning policy. By monitoring real-time features like request load and latency, AGFT utilizes fine-grained frequency control for precise adjustments and intelligent action space for stable, efficient decision-making. This creates a robust, automated energy management solution. We comprehensively evaluated AGFT in an environment simulating realistic, fluctuating inference requests. The experimental results demonstrate that AGFT successfully saves 44.3% of GPU energy consumption while introducing a minimal performance latency overhead of under 10%. This achievement translates into a comprehensive Energy-Delay Product (EDP) optimization of up to 40.3%, clearly showing that our framework can significantly enhance the energy efficiency and economic benefits of existing inference clusters without compromising service quality.","title":"AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization"},{"location":"weekly_paper/2025-08-08/#smallkv-small-model-assisted-compensation-of-kv-cache-compression-for-efficient-llm-inference","text":"Authors: Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu 2025-08-03 http://arxiv.org/abs/2508.02751v1 cache eviction has emerged as an effective solution to alleviate resource constraints faced by s in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between s of different scales. We propose Small , a small model assisted compensation method for cache compression. Small can maintain attention matching between different-scale s to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of Small . Moreover, efficiency evaluations show that Small achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant inference in resource constrained environments.","title":"SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference"},{"location":"weekly_paper/2025-08-08/#eac-moe-expert-selection-aware-compressor-for-mixture-of-experts-large-language-models","text":"Authors: Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng 2025-08-03 http://arxiv.org/abs/2508.01625v1 Mixture-of-Experts (MoE) has demonstrated promising potential in scaling s. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE- s, which deeply aligns with the characteristics of MoE from the perspectives of quantization and , and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE- s. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.","title":"EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models"},{"location":"weekly_paper/2025-08-08/#repoforge-training-a-sota-fast-thinking-swe-agent-with-an-end-to-end-data-curation-pipeline-synergizing-sft-and-rl-at-scale","text":"Authors: Zhilong Chen, Chengzong Zhao, Boyuan Chen, Dayi Lin, Yihao Chen, Arthur Leung, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Ahmed E. Hassan 2025-08-03 http://arxiv.org/abs/2508.01550v1 Training software engineering (SWE) s is bottlenecked by expensive infrastructure, inefficient evaluation pipelines, scarce training data, and costly quality control. We present RepoForge, an autonomous, end-to-end pipeline that generates, evaluates, and trains SWE agents at scale. Our key contributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on SWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new state-of-the-art for \\leq 8B non-thinking s; (2) 7,304 executable environments auto-generated from real GitHub commits with zero manual intervention; (3) 14 \\times storage reduction (1.4GB \\rightarrow 102MB per instance) via intelligent dependency management and image ; (4) > 70\\% faster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge harness; (5) 19,000 \\times cheaper labeling through our automated SPICE~\\citep{spice2024} difficulty assessment technique. By unifying storage-efficient sandboxing, Ray-powered evaluation harness, automated data generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate that even \\leq 8B models can reach new state-of-the-art performance on demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical bottlenecks in SWE agent training: high storage costs of container-based evaluation, inefficient sequential reward pipelines, limited availability of high-quality training data, expensive manual labeling, and multi-turn RL pipeline bottlenecks.","title":"RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale"},{"location":"weekly_paper/2025-08-08/#blocka2a-towards-secure-and-verifiable-agent-to-agent-interoperability","text":"Authors: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan 2025-08-02 http://arxiv.org/abs/2508.01332v2 The rapid adoption of agentic AI, powered by large language models ( s), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in -driven multi-agent systems (MASes): fragmented identity frameworks, insecure channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, -based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production -based MAS environments.","title":"BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability"},{"location":"weekly_paper/2025-08-08/#unifying-mixture-of-experts-and-multi-head-latent-attention-for-efficient-language-models","text":"Authors: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat 2025-08-02 http://arxiv.org/abs/2508.01261v1 We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top- k selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla s while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference . Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"weekly_paper/2025-08-08/#asking-the-right-questions-benchmarking-large-language-models-in-the-development-of-clinical-consultation-templates","text":"Authors: Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen 2025-08-02 http://arxiv.org/abs/2508.01159v1 This study evaluates the capacity of large language models ( s) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that s can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician .","title":"Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates"},{"location":"weekly_paper/2025-08-08/#towards-bridging-review-sparsity-in-recommendation-with-textual-edge-graph-representation","text":"Authors: Leyao Wang, Xutao Mao, Xuhui Zhan, Yuying Zhao, Bo Ni, Ryan A. Rossi, Nesreen K. Ahmed, Tyler Derr 2025-08-02 http://arxiv.org/abs/2508.01128v1 Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and -based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations.","title":"Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation"},{"location":"weekly_paper/2025-08-08/#react-a-real-time-edge-ai-based-v2x-framework-for-accident-avoidance-in-autonomous-driving-system","text":"Authors: Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu 2025-08-01 http://arxiv.org/abs/2508.01057v1 Collisions caused by human error are the most common type of multi-vehicle crash, highlighting the critical need for autonomous driving (AD) systems to leverage cooperative perception through Vehicle-to-Everything (V2X) . This capability extends situational awareness beyond the limitations of onboard sensors. However, current -based V2X frameworks suffer from limited generalization, shallow contextual reasoning, and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced reasoning and multimodal integration but typically fall short of real-time performance requirements in safety-critical applications. This paper presents REACT, a real-time, V2X-integrated trajectory optimization framework built upon a fine-tuned lightweight VLM. REACT integrates a set of specialized modules that process multimodal inputs into optimized, risk-aware trajectories. To ensure real-time performance on edge devices, REACT incorporates edge adaptation strategies that reduce model complexity and accelerate inference. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results demonstrate the feasibility of lightweight VLMs for real-time edge-based cooperative planning and showcase the potential of language-guided contextual reasoning to improve safety and responsiveness in autonomous driving.","title":"REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System"},{"location":"weekly_paper/2025-08-08/#session-based-recommendation-with-validated-and-enriched-llm-intents","text":"Authors: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang 2025-08-01 http://arxiv.org/abs/2508.00570v1 Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models ( s), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched -generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of -generated intents with a global intent pool to constrain the 's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability.","title":"Session-Based Recommendation with Validated and Enriched LLM Intents"},{"location":"weekly_paper/2025-08-08/#reagan-node-as-agent-reasoning-graph-agentic-network","text":"Authors: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang 2025-08-01 http://arxiv.org/abs/2508.00429v2 Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain . Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.","title":"ReaGAN Node-as-Agent-Reasoning Graph Agentic Network"},{"location":"weekly_paper/2025-08-08/#edgeinfinite-instruct-bridging-sft-based-optimization-and-npu-level-efficiency-for-edge-devices","text":"Authors: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun 2025-08-01 http://arxiv.org/abs/2508.00370v2 Deploying Transformer-based large language models ( s) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value ( ) cache demands. While existing cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token . Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.","title":"EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices"},{"location":"weekly_paper/2025-08-08/#systematic-evaluation-of-optimization-techniques-for-long-context-language-models","text":"Authors: Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar 2025-08-01 http://arxiv.org/abs/2508.00305v1 Large language models ( s) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like , quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.","title":"Systematic Evaluation of Optimization Techniques for Long-Context Language Models"},{"location":"weekly_paper/2025-08-08/#large-ai-model-enabled-secure-communications-in-low-altitude-wireless-networks-concepts-perspectives-and-case-study","text":"Authors: Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek 2025-08-01 http://arxiv.org/abs/2508.00256v1 Low-altitude wireless networks (LAWNs) have the potential to revolutionize s by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure s in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure s in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models ( s) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.","title":"Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study"},{"location":"weekly_paper/2025-08-15/","text":"2025-08-15 Table of Contents STream3R Scalable Sequential 3D Reconstruction with Causal Transformer Generalizable Federated Learning using Client Adaptive Focal Modulation Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation Thinking Inside the Mask In-Place Prompting in Diffusion LLMs Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph SemPT Semantic Prompt Tuning for Vision-Language Models DAS Dual-Aligned Semantic IDs Empowered Industrial Recommender System GCRPNet Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images X-Node Self-Explanation is All We Need Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models Personalized Real-time Jargon Support for Online Meetings Can Transformers Break Encryption Schemes via In-Context Learning? Agentic AI Frameworks Architectures, Protocols, and Design Challenges Nested-ReFT Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation Constrained Decoding of Diffusion LLMs with Context-Free Grammars Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach Memory Decoder A Pretrained, Plug-and-Play Memory for Large Language Models OneVAE Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better Speed Always Wins A Survey on Efficient Architectures for Large Language Models MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference Synaptic Pruning A Biological Inspiration for Deep Learning Regularization SinLlama -- A Large Language Model for Sinhala READER Retrieval-Assisted Drafter for Efficient LLM Inference FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm A Survey on Training-free Alignment of Large Language Models Retrospective Sparse Attention for Efficient Long-Context Generation NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs Steering Towards Fairness Mitigating Political Bias in LLMs DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation Interpretable Reward Model via Sparse Autoencoder A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks AgriGPT a Large Language Model Ecosystem for Agriculture QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey Joint decoding method for controllable contextual speech recognition based on Speech LLM Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System Profiling Large Language Model Inference on Apple Silicon A Quantization Perspective Using LLMs to Capture Users' Temporal Context for Recommendation When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital Vector-Centric Machine Learning Systems A Cross-Stack Approach Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories OverFill Two-Stage Models for Efficient Language Model Decoding Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference Follow-Your-Shape Shape-Aware Image Editing via Trajectory-Guided Region Control BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Bridging ASR and LLMs for Dysarthric Speech Recognition Benchmarking Self-Supervised and Generative Approaches Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths DiTVR Zero-Shot Diffusion Transformer for Video Restoration EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts SASST Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation Symmetry-Aware Transformer Training for Automated Planning Semantic Caching for Low-Cost LLM Serving From Offline Learning to Online Adaptation GLiClass Generalist Lightweight Model for Sequence Classification Tasks LaVieID Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Towards Theoretical Understanding of Transformer Test-Time Computing Investigation on In-Context Linear Regression Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs Investigating 1-Bit Quantization in Transformer-Based Top Tagging LET-US Long Event-Text Understanding of Scenes Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation Tasa Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization Bridging Semantic Logic Gaps A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation Vec2Summ Text Summarization via Probabilistic Sentence Embeddings Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning Pushing the Envelope of LLM Inference on AI-PC CISO Species Distribution Modeling Conditioned on Incomplete Species Observations STream3R Scalable Sequential 3D Reconstruction with Causal Transformer Authors: Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan 2025-08-14 http://arxiv.org/abs/2508.10893v1 We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a r-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with -style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r. Generalizable Federated Learning using Client Adaptive Focal Modulation Authors: Tajamul Ashraf, Iqra Altaf Gillani 2025-08-14 http://arxiv.org/abs/2508.10840v1 Federated learning (FL) has proven essential for privacy-pre , collaborative training across distributed clients. Our prior work, TransFed, introduced a robust -based FL framework that leverages a learn-to-adapt hypernetwork to generate personalized focal modulation layers per client, outperforming traditional methods in non-IID and cross-domain settings. In this extended version, we propose AdaptFED, where we deepen the investigation of focal modulation in generalizable FL by incorporating: (1) a refined adaptation strategy that integrates task-aware client embeddings to personalize modulation dynamics further, (2) enhanced theoretical bounds on adaptation performance, and (3) broader empirical validation across additional modalities, including time-series and multilingual data. We also introduce an efficient variant of TransFed that reduces server-client overhead via low-rank hypernetwork conditioning, enabling scalable deployment in resource-constrained environments. Extensive experiments on eight diverse datasets reaffirm the superiority of our method over state-of-the-art baselines, particularly in source-free and cross-task federated setups. Our findings not only extend the capabilities of focal modulation in FL but also pave the way for more adaptive, scalable, and generalizable -based federated systems. The code is available at http://github.com/Tajamul21/TransFed Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation Authors: Youping Gu, Xiaolong Li, Yuhao Hu, Bohan Zhuang 2025-08-14 http://arxiv.org/abs/2508.10774v1 Diffusion s currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and attention mechanisms have shown promise as independent strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware masks to focus computation on salient spatiotemporal features, and (2) a -aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/. Thinking Inside the Mask In-Place Prompting in Diffusion LLMs Authors: Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang 2025-08-14 http://arxiv.org/abs/2508.10736v1 Despite large language models ( s) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (d s) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for d s. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12 \\times speedup on GSM8K, and up to 276.67 \\times on MMLU while maintaining competitive performance. Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph Authors: Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman 2025-08-14 http://arxiv.org/abs/2508.10687v1 Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage architecture for state-of-the-art results, our method integrates graph-based methods with the architecture. This fusion, combining and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve accessibility for the deaf and hard of hearing. SemPT Semantic Prompt Tuning for Vision-Language Models Authors: Xiao Shi, Yangjun Ou, Zhenzhong Chen 2025-08-14 http://arxiv.org/abs/2508.10645v1 Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between pre category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on category labels or disparate -generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning. DAS Dual-Aligned Semantic IDs Empowered Industrial Recommender System Authors: Wencai Ye, Mingjie Sun, Shaoyun Shi, Peng Wang, Wenjin Wu, Peng Jiang 2025-08-14 http://arxiv.org/abs/2508.10584v1 Semantic IDs are discrete identifiers generated by quantizing the Multi-modal Large Language Models (M s) embeddings, enabling efficient multi-modal content integration in recommendation systems. However, their lack of collaborative signals results in a misalignment with downstream discriminative and generative recommendation objectives. Recent studies have introduced various alignment mechanisms to address this problem, but their two-stage framework design still leads to two main limitations: (1) inevitable information loss during alignment, and (2) inflexibility in applying adaptive alignment strategies, consequently constraining the mutual information maximization during the alignment process. To address these limitations, we propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method that simultaneously optimizes and alignment, pre semantic integrity and alignment quality while avoiding the information loss typically associated with two-stage methods. Meanwhile, DAS achieves more efficient alignment between the semantic IDs and collaborative signals, with the following two innovative and effective approaches: (1) Multi-view Constrative Alignment: To maximize mutual information between semantic IDs and collaborative signals, we first incorporate an ID-based CF debias module, and then design three effective contrastive alignment methods: dual user-to-item (u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual s of users and ads, the constructed semantic IDs for users and ads achieve stronger alignment. Finally, we conduct extensive offline experiments and online A/B tests to evaluate DAS's effectiveness, which is now successfully deployed across various advertising scenarios at Kuaishou App, over 400 million users daily. GCRPNet Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images Authors: Mengyu Ren, Yutong Li, Hua Li, Runmin Cong, Sam Kwong 2025-08-14 http://arxiv.org/abs/2508.10542v1 Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision s (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the r of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority. X-Node Self-Explanation is All We Need Authors: Prajit Sengupta, Islem Rekik 2025-08-14 http://arxiv.org/abs/2508.10461v1 Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a r to enforce faithfulness, (2) generating a natural language explanation using a pre-trained (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node. Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching Authors: Wei Tian, Jieming Shi, Man Lung Yiu 2025-08-14 http://arxiv.org/abs/2508.10460v1 Real-world trajectories are often with low-sampling rates (i.e., long intervals between consecutive GPS points) and misaligned with road networks, yet many applications demand high-quality data for optimal performance. To improve data quality with trajectories as input, we systematically study two related research problems: trajectory recovery on road network, which aims to infer missing points to recover high-sampling trajectories, and map matching, which aims to map GPS points to road segments to determine underlying routes. In this paper, we present efficient methods TRMMA and MMA for accurate trajectory recovery and map matching, respectively, where MMA serves as the first step of TRMMA. In MMA, we carefully formulate a classification task to map a GPS point from trajectories to a road segment over a small candidate segment set, rather than the entire road network. We develop techniques in MMA to generate effective embeddings that capture the patterns of GPS data, directional information, and road segments, to accurately align trajectories to routes. For trajectory recovery, TRMMA focuses on the segments in the route returned by MMA to infer missing points with position ratios on road segments, producing high-sampling trajectories efficiently by avoiding evaluation of all road segments. Specifically, in TRMMA, we design a dual- encoding process to cohesively capture latent patterns in trajectories and routes, and an effective technique to sequentially predict the position ratios and road segments of missing points. We conduct extensive experiments to compare TRMMA and MMA with numerous existing methods for trajectory recovery and map matching, respectively, on 4 large real-world datasets. TRMMA and MMA consistently achieve the best result quality, often by a significant margin. Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints Authors: Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh 2025-08-14 http://arxiv.org/abs/2508.10426v1 Large language models ( s) are limited by substantial computational cost. We introduce a \"computational economics\" framework that treats an as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard s reallocate attention toward high-value tokens while pre accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc ; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent s under strict resource constraints. Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation Authors: Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, Zhuo Li 2025-08-14 http://arxiv.org/abs/2508.10404v1 With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models ( s), generating adversarial examples to jailbreak s remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated. XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization Authors: Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami 2025-08-14 http://arxiv.org/abs/2508.10395v1 Although inference has emerged as a critical workload for many downstream applications, efficiently inferring s is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through with substantial accuracy benefits relative to state-of-the-art methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2 \\times memory savings compared to caching. By applying XQuant, we achieve up to \\sim 7.7\\times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10 \\times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5 \\times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art methods and achieving near-FP16 accuracy across a wide range of models. eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing Authors: Jiyong Kim, Jaeho Lee, Jiahao Lin, Alish Kanani, Miao Sun, Umit Y. Ogras, Jaehyun Park 2025-08-14 http://arxiv.org/abs/2508.10370v1 State Space Model (SSM)-based machine learning architectures have recently gained significant attention for processing sequential data. Mamba, a recent sequence-to-sequence SSM, offers competitive accuracy with superior computational efficiency compared to state-of-the-art models. While this advantage makes Mamba particularly promising for resource-constrained edge devices, no hardware frameworks are currently optimized for deploying it in such environments. This paper presents eMamba, a comprehensive end-to-end hardware framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10, and MARS, an open-source human pose estimation dataset, show eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63-19.9 \\times fewer parameters. In addition, it generalizes well to large-scale natural language tasks, demonstrating stable perplexity across varying sequence lengths on the WikiText2 dataset. We also and implement the entire eMamba pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm technology. Experimental results show 4.95-5.62 \\times lower latency and 2.22-9.95 \\times higher throughput, with 4.77 \\times smaller area, 9.84 \\times lower power, and 48.6 \\times lower energy consumption than baseline solutions while maintaining competitive accuracy. Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding Authors: Jakub \u0160m\u00edd, Pavel P\u0159ib\u00e1\u0148, Pavel Kr\u00e1l 2025-08-14 http://arxiv.org/abs/2508.10369v1 While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained boosting results by more than 10\\%. We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models ( s) in zero-shot, few-shot, and fine-tuning scenarios. While s perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times. We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain. Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models Authors: Jakub \u0160m\u00edd, Pavel P\u0159ib\u00e1\u0148, Pavel Kr\u00e1l 2025-08-14 http://arxiv.org/abs/2508.10366v1 Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained , improves cross-lingual ABSA performance by up to 10\\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models ( s) and show that while fine-tuned multilingual s can achieve comparable results, English-centric s struggle with these tasks. What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles Authors: Mengtao Zhou, Sifan Wu, Huan Zhang, Qi Sima, Bang Liu 2025-08-14 http://arxiv.org/abs/2508.10358v1 We investigate the capacity of Large Language Models ( s) for imaginative reasoning--the proactive construction, testing, and revision of hypotheses in information- environments. Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic \"Turtle Soup\" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess s' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment. Experiments with leading s reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into s' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior. DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration Authors: Arkapravo Ghosh, Abhishek Moitra, Abhiroop Bhattacharjee, Ruokai Yin, Priyadarshini Panda 2025-08-14 http://arxiv.org/abs/2508.10303v1 Design space exploration (DSE) is critical for developing optimized hardware architectures, especially for AI workloads such as deep neural networks (DNNs) and large language models ( s), which require specialized . As model complexity grows, accelerator design spaces have expanded to O(10^17), becoming highly irregular, non-convex, and exhibiting many-to-one mappings from design configurations to performance metrics. This complexity renders direct inverse derivation infeasible and necessitates heuristic or sampling-based optimization. Conventional methods - including Bayesian optimization, gradient descent, reinforcement learning, and genetic algorithms - depend on iterative sampling, resulting in long runtimes and sensitivity to initialization. Deep learning-based approaches have reframed DSE as classification using recommendation models, but remain limited to small-scale (O(10^3)), less complex design spaces. To overcome these constraints, we propose a generative approach that models hardware design as 1-D image synthesis conditioned on target performance, enabling efficient learning of non-differentiable, non-bijective hardware-performance mappings. Our framework achieves 0.86% lower generation error than Bayesian optimization with a 17000x speedup, and outperforms GANDSE with 30% lower error at only 1.83x slower search. We further extend the method to a structured DSE setting, attaining 9.8% lower energy-delay product (EDP) and 6% higher performance, with up to 145.6x and 1312x faster search compared to existing optimization methods on O(10^17) design spaces. For inference, our method achieves 3.37x and 7.75x lower EDP on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the state-of-the-art DOSA framework. Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models Authors: Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, Xiangwei Zhou 2025-08-14 http://arxiv.org/abs/2508.10243v1 Transformer models have demonstrated exceptional performance and have become indispensable in computer vision (CV) and natural language processing (NLP) tasks. However, recent studies reveal that s are susceptible to backdoor attacks. Prior backdoor attack methods typically rely on retraining with clean data or altering the model architecture, both of which can be resource-intensive and intrusive. In this paper, we propose Head-wise Pruning and Malicious Injection (HPMI), a novel retraining-free backdoor attack on s that does not alter the model's architecture. Our approach requires only a small subset of the original data and basic knowledge of the model architecture, eliminating the need for retraining the target . Technically, HPMI works by the least important head and injecting a pre-trained malicious head to establish the backdoor. We provide a rigorous theoretical justification demonstrating that the implanted backdoor resists detection and removal by state-of-the-art defense techniques, under reasonable assumptions. Experimental evaluations across multiple datasets further validate the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four advanced defense mechanisms. Additionally, relative to state-of-the-art retraining-dependent attacks, HPMI achieves greater concealment and robustness against diverse defense strategies, while maintaining minimal impact on clean accuracy. Personalized Real-time Jargon Support for Online Meetings Authors: Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August 2025-08-13 http://arxiv.org/abs/2508.10239v1 Effective interdisciplinary is frequently hindered by domain-specific jargon. To explore the jargon barriers in-depth, we conducted a formative diary study with 16 professionals, revealing critical limitations in current jargon-management strategies during workplace meetings. Based on these insights, we designed ParseJargon, an interactive -powered system providing real-time personalized jargon identification and explanations tailored to users' individual backgrounds. A controlled experiment comparing ParseJargon against baseline (no support) and general-purpose (non-personalized) conditions demonstrated that personalized jargon support significantly enhanced participants' comprehension, engagement, and appreciation of colleagues' work, whereas general-purpose support negatively affected engagement. A follow-up field study validated ParseJargon's usability and practical value in real-time meetings, highlighting both opportunities and limitations for real-world deployment. Our findings contribute insights into designing personalized jargon support tools, with implications for broader interdisciplinary and educational applications. Can Transformers Break Encryption Schemes via In-Context Learning? Authors: Jathin Korrapati, Patrick Mendoza, Aditya Tomar, Abein Abraham 2025-08-13 http://arxiv.org/abs/2508.10235v1 In-context learning (ICL) has emerged as a powerful capability of -based language models, enabling them to perform tasks by conditioning on a small number of examples presented at inference time, without any parameter updates. Prior work has shown that s can generalize over simple function classes like linear functions, decision trees, even neural networks, purely from context, focusing on numerical or symbolic reasoning over underlying well-structured functions. Instead, we propose a novel application of ICL into the domain of cryptographic function learning, specifically focusing on ciphers such as mono-alphabetic substitution and Vigen`ere ciphers, two classes of private-key encryption schemes. These ciphers involve a fixed but hidden bijective mapping between plain text and cipher text characters. Given a small set of (cipher text, plain text) pairs, the goal is for the model to infer the underlying substitution and a new cipher text word. This setting poses a structured inference challenge, which is well-suited for evaluating the inductive biases and generalization capabilities of s under the ICL paradigm. Code is available at https://github.com/adistomar/CS182-project. Agentic AI Frameworks Architectures, Protocols, and Design Challenges Authors: Hana Derouiche, Zaki Brahmi, Haithem Mazeni 2025-08-13 http://arxiv.org/abs/2508.10146v1 The emergence of Large Language Models ( s) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination. This paper provides a systematic review and comparative analysis of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural principles, mechanisms, memory management, safety guardrails, and alignment with service-oriented computing paradigms. Furthermore, we identify key limitations, emerging trends, and open challenges in the field. To address the issue of agent , we conduct an in-depth analysis of protocols such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network Protocol (ANP), and Agora. Our findings not only establish a foundational taxonomy for Agentic AI systems but also propose future research directions to enhance scalability, robustness, and interoperability. This work serves as a comprehensive reference for researchers and practitioners working to advance the next generation of autonomous AI systems. Nested-ReFT Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts Authors: Maxime Heuillet, Yufei Cui, Boxing Chen, Audrey Durand, Prasanna Parthasarathi 2025-08-13 http://arxiv.org/abs/2508.10123v1 Advanced reasoning in s on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance. From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation Authors: Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue 2025-08-13 http://arxiv.org/abs/2508.10118v1 Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models ( s) have made it possible to generate code from natural language, opening new opportunities for automating parametric 3D modeling. However, directly translating human design intent into executable CAD code remains highly challenging, due to the need for logical reasoning, syntactic correctness, and numerical precision. In this work, we propose CAD-RL, a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework for CAD modeling code generation. Our method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To ensure stable policy learning under and high-variance reward conditions, we introduce three targeted optimization strategies: Trust Region Stretch for improved exploration, Precision Token Loss for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce noisy supervision. To support training and benchmarking, we release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples with paired natural language and structured design language descriptions, executable CADQuery scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs. Constrained Decoding of Diffusion LLMs with Context-Free Grammars Authors: Niels M\u00fcndler, Jasper Dekoninck, Martin Vechev 2025-08-13 http://arxiv.org/abs/2508.10111v1 Large language models ( s) have shown promising performance across diverse domains. Many practical applications of s, such as code completion and structured data extraction, require adherence to syntactic constraints specified by a formal language. Yet, due to their probabilistic nature, output is not guaranteed to adhere to such formal languages. Prior work has proposed constrained as a means to restrict generation to particular formal languages. However, existing works are not applicable to the emerging paradigm of diffusion s, when used in practical scenarios such as the generation of formally correct C++ or JSON output. In this paper we address this challenge and present the first constrained method for diffusion models, one that can handle formal languages captured by context-free grammars. We begin by reducing constrained to the more general additive infilling problem, which asks whether a partial output can be completed to a valid word in the target language. This problem also naturally subsumes the previously unaddressed multi-region infilling constrained . We then reduce this problem to the task of deciding whether the intersection of the target language and a regular language is empty and present an efficient algorithm to solve it for context-free languages. Empirical results on various applications, such as C++ code infilling and structured data extraction in JSON, demonstrate that our method achieves near-perfect syntactic correctness while consistently pre or improving functional correctness. Importantly, our efficiency optimizations ensure that the computational overhead remains practical. Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach Authors: Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed 2025-08-13 http://arxiv.org/abs/2508.09935v1 Business digitisation has reorganised the process of persuasive discourse, which allows not only greater transparency but also advanced deception. This inquiry synthesises classical rhetoric and psychology with linguistic theory and empirical studies in the financial reporting, sustainability discourse, and digital marketing to explain how deceptive language can be systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater than 99% were achieved by using computational textual analysis as well as personalised models. However, reproducing this performance in multilingual settings is also problematic and, to a large extent, this is because it is not easy to find sufficient data, and because few multilingual text-processing infrastructures are in place. This evidence shows that there has been an increasing gap between the theoretical representations of and those empirically approximated, and therefore, there is a need to have strong automatic text-identification systems where AI-based discourse is becoming more realistic in communicating with humans. Memory Decoder A Pretrained, Plug-and-Play Memory for Large Language Models Authors: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin 2025-08-13 http://arxiv.org/abs/2508.09874v1 Large Language Models ( s) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small r that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain. OneVAE Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better Authors: Yupeng Zhou, Zhen Li, Ziheng Ouyang, Yuming Chen, Ruoyi Du, Daquan Zhou, Bin Fu, Yihao Liu, Peng Gao, Ming-Ming Cheng, Qibin Hou 2025-08-13 http://arxiv.org/abs/2508.09857v1 Encoding videos into discrete tokens could align with text tokens to facilitate concise and unified multi-modal s, yet introducing significant spatiotemporal compression compared to continuous video representation. Previous discrete video VAEs experienced unstable training, long training time, and degraded reconstruction quality. Given the easier training and superior performance of continuous VAEs, an intuitive idea is to enhance discrete video VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between discrete and continuous representations, we found that FSQ could effectively preserve pre-trained continuous VAE priors compared to other methods. By leveraging continuous VAE priors, it converges several times faster than training from scratch and achieves superior performance at convergence. Meanwhile, two structural improvements are proposed. First, inspired by how continuous VAEs enhance reconstruction via enlarged latent dimensions, we introduce a multi-token mechanism, which achieves nearly a 1 dB improvement in PSNR without compromising the token compression ratio. Second, to tackle reconstruction challenges in high-compression video VAEs, we strengthen first-frame reconstruction, enabling the causal VAE to leverage this information in subsequent frames and markedly improving the performance of 4 x 16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous optimization scheme that unifies the two paradigms and, for the first time, achieves competitive performance on both continuous and discrete representations within a single network. We name our method OneVAE to reflect this connection. Speed Always Wins A Survey on Efficient Architectures for Large Language Models Authors: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng 2025-08-13 http://arxiv.org/abs/2508.09834v1 Large Language Models ( s) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern s, offer a strong baseline with excellent scaling properties. However, the traditional architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative architectures that address the inherent limitations of s and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sequence modeling methods, efficient full attention variants, mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion s. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems. MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models Authors: Dianyi Wang, Siyuan Wang, Zejun Li, Yikun Wang, Yitong Li, Duyu Tang, Xiaoyu Shen, Xuanjing Huang, Zhongyu Wei 2025-08-13 http://arxiv.org/abs/2508.09779v1 Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across multi-modal tasks by scaling model size and training data. However, these dense LVLMs incur significant computational costs and motivate the exploration of Mixture of Experts (MoE) architectures. While MoE improve parameter efficiency, effectively applying MoE to simultaneously model modality-specific features and cross-modal associations in LVLMs remains challenging. In this work, we propose to incorporate Mixture of Intra- and Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is guided by its modality, directing tokens to their respective intra-modality experts as well as a shared pool of inter-modality experts, enabling the model to jointly learn rich intra-modal features and cross-modal interactions. We further introduce an effective and straightforward two-stage training strategy, which facilitates the direct activation of both MoE and multi-modal capabilities. Extensive experiments across different data scales and backbone demonstrate the effectiveness, efficiency and generality of our approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters match or even surpass the performance of existing advanced open-source MoE- s based multi-modal models that involve more activated parameters. The code is available at https://github.com/AlenjandroWang/MoIIE. MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement Authors: Weitao Jia, Jinghui Lu, Haiyang Yu, Siqi Wang, Guozhi Tang, An-Lan Wang, Weijie Yin, Dingkang Yang, Yuxiang Nie, Bin Shan, Hao Feng, Irene Li, Kun Yang, Han Wang, Jingqun Tang, Teng Fu, Changhong Jin, Chao Feng, Xiaohui Lv, Can Huang 2025-08-13 http://arxiv.org/abs/2508.09670v1 Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models ( s). However, standard RLVR faces challenges with reward , where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the model's performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods. HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap Authors: Wenxiang Lin, Xinglin Pan, Lin Zhang, Shaohuai Shi, Xuan Wang, Xiaowen Chu 2025-08-13 http://arxiv.org/abs/2508.09591v1 The ly activated mixture-of-experts (MoE) has become a common architecture for large language models ( s) due to its , which requires fewer computational demands while easily scaling the model size. In MoE models, each MoE layer requires to dynamically choose tokens to activate particular experts for computation while the activated experts may not be located in the same device or GPU as the token. However, this leads to substantial and load imbalances across all GPUs, which obstructs the scalability of distributed systems within a GPU cluster. To this end, we introduce HierMoE to accelerate the training of MoE models by two topology-aware techniques: 1) token deduplication to reduce the traffic, and 2) expert swap to balance the workloads among all GPUs. To enable the above two proposed approaches to be more general, we build theoretical models aimed at achieving the best token duplication and expert swap strategy under different model configurations and hardware environments. We implement our prototype HierMoE system atop Megatron-LM and conduct experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results show that our HierMoE achieves 1.55\\times to 3.32\\times faster and delivers 1.18\\times to 1.27\\times faster end-to-end training compared to state-of-the-art MoE training systems, Tutel-2DH, SmartMoE, and Megatron-LM. NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs Authors: Birong Pan, Mayi Xu, Qiankun Pi, Jianhao Chen, Yuanyuan Zhu, Ming Zhong, Tieyun Qian 2025-08-13 http://arxiv.org/abs/2508.09473v1 Ensuring robust safety alignment while pre utility is critical for the reliable deployment of Large Language Models ( s). However, current techniques fundamentally suffer from intertwined deficiencies: insufficient robustness against malicious attacks, frequent refusal of benign queries, degradation in generated text quality and general task performance--the former two reflecting deficits in robust safety and the latter constituting utility impairment. We trace these limitations to the coarse-grained layer-wise interventions in existing methods. To resolve this, we propose NeuronTune, a fine-grained framework that dynamically modulates neurons to achieve simultaneous safety-utility optimization. Our approach first identifies safety-critical and utility-pre neurons across all layers via attribution, then employs meta-learning to adaptively amplify safety-neuron activations and suppress utility-neuron activations. Crucially, NeuronTune enables tunable adjustment of intervention scope via neuron-count thresholds, supporting flexible adaptation to security-critical or utility-priority scenarios. Extensive experimental results demonstrate that our method significantly outperforms existing state-of-the-art technologies, achieving superior model safety while maintaining excellent utility. EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models Authors: Omar Bazarbachi, Zijun Sun, Yanning Shen 2025-08-13 http://arxiv.org/abs/2508.09471v1 As Large Language Models ( s) become more widely adopted and scale up in size, the computational and memory challenges involved in deploying these massive foundation models have grown increasingly severe. This underscores the urgent need to develop more efficient model variants. Faced with this challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided Structured Post-training Pruning method. The proposed approach leverages graph theory to guide the design of N:M structured , effectively reducing model size and computational demands. By incorporating concepts from expander graphs, EGGS-PTP ensures information flow within the pruned network, pre essential model functionality. Extensive numerical experiments demonstrate that EGGS-PTP not only achieves significant and memory savings due to structured but also outperforms existing structured techniques in terms of accuracy across various s. Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy Authors: Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal 2025-08-13 http://arxiv.org/abs/2508.09461v1 Different forms of customized 2D avatars are widely used in gaming applications, virtual , education, and content creation. However, existing approaches often fail to capture fine-grained facial expressions and struggle to preserve identity across different expressions. We propose GEN-AFFECT, a novel framework for personalized avatar generation that generates expressive and identity-consistent avatars with a diverse set of facial expressions. Our framework proposes conditioning a multimodal diffusion on an extracted identity-expression representation. This enables identity preservation and representation of a wide range of facial expressions. GEN-AFFECT additionally employs consistent attention at inference for information sharing across the set of generated expressions, enabling the generation process to maintain identity consistency over the array of generated fine-grained expressions. GEN-AFFECT demonstrates superior performance compared to previous state-of-the-art methods on the basis of the accuracy of the generated expressions, the preservation of the identity and the consistency of the target identity across an array of fine-grained facial expressions. Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference Authors: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin 2025-08-13 http://arxiv.org/abs/2508.09442v1 The Key-Value ( ) , which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model ( ) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the - . We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of - privacy leakage issues. To mitigate this, we propose -Cloak, a novel, lightweight, and efficient defense mechanism. -Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the - . Our extensive experiments show that -Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy deployment. Synaptic Pruning A Biological Inspiration for Deep Learning Regularization Authors: Gideon Vos, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi 2025-08-12 http://arxiv.org/abs/2508.09330v1 Synaptic in biological brains removes weak connections to improve efficiency. In contrast, dropout regularization in artificial neural networks randomly deactivates neurons without considering activity-dependent . We propose a magnitude-based synaptic method that better reflects biology by progressively removing low-importance connections during training. Integrated directly into the training loop as a dropout replacement, our approach computes weight importance from absolute magnitudes across layers and applies a cubic schedule to gradually increase global . At fixed intervals, masks permanently remove low-importance weights while maintaining gradient flow for active ones, eliminating the need for separate and fine-tuning phases. Experiments on multiple time series forecasting models including RNN, LSTM, and Patch Time Series Transformer across four datasets show consistent gains. Our method ranked best overall, with statistically significant improvements confirmed by Friedman tests (p < 0.01). In financial forecasting, it reduced Mean Absolute Error by up to 20% over models with no or standard dropout, and up to 52% in select models. This dynamic mechanism advances regularization by coupling weight elimination with progressive sparsification, offering easy integration into diverse architectures. Its strong performance, especially in financial time series forecasting, highlights its potential as a practical alternative to conventional dropout techniques. SinLlama -- A Large Language Model for Sinhala Authors: H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur 2025-08-12 http://arxiv.org/abs/2508.09115v1 Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models ( s). In this research, we extend an existing multilingual (Llama-3-8B) to better serve Sinhala. We enhance the tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first r-based open-source with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin. READER Retrieval-Assisted Drafter for Efficient LLM Inference Authors: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi 2025-08-12 http://arxiv.org/abs/2508.09072v1 Large Language Models ( s) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient Inference), a novel lossless speculative method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value ( ) size during speculative and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup. FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm Authors: Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal 2025-08-12 http://arxiv.org/abs/2508.09056v1 Intrusion Detection Systems (IDS) have an increasingly important role in preventing exploitation of network vulnerabilities by malicious actors. Recent deep learning based developments have resulted in significant improvements in the performance of IDS systems. In this paper, we present FetFIDS, where we explore the employment of feature embedding instead of positional embedding to improve intrusion detection performance of a based deep learning system. Our model is developed with the aim of deployments in edge learning scenarios, where federated learning over multiple rounds can ensure both privacy and localized performance improvements. FetFIDS outperforms multiple state-of-the-art intrusion detection systems in a federated environment and demonstrates a high degree of suitability to federated learning. The code for this work can be found at https://github.com/ghosh64/fetfids. A Survey on Training-free Alignment of Large Language Models Authors: Birong Pan, Yongqi Li, Weiyu Zhang, Wenpeng Lu, Mayi Xu, Shen Zhou, Yuanyuan Zhu, Ming Zhong, Tieyun Qian 2025-08-12 http://arxiv.org/abs/2508.09016v1 The alignment of large language models ( s) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, -time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining s, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre- , in- , and post- . For each stage, we provide a detailed examination from the viewpoint of s and multimodal s (M s), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable s. Retrospective Sparse Attention for Efficient Long-Context Generation Authors: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim 2025-08-12 http://arxiv.org/abs/2508.09001v1 Large Language Models ( s) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value ( ) , whose memory footprint grows linearly with sequence length and dominates latency at each step. While recent compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long . In this paper, we introduce RetroAttention, a novel update technique that retrospectively revises past attention outputs using newly arrived entries from subsequent steps. By maintaining a lightweight output , RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) compression methods, increasing effective exposure by up to 1.6 \\times and accuracy by up to 21.9\\%. NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation Authors: Zainab Khan, Ahmed Hussain, Mukesh Thakur, Arto Hellas, Panos Papadimitratos 2025-08-12 http://arxiv.org/abs/2508.09240v1 The use of Service-Based Architecture in modern tele s has exponentially increased Network Functions (NFs) and Application Programming Interfaces (APIs), creating substantial operational complexities in service discovery and management. We introduce \\textit{NEFMind}, a framework leveraging parameter-efficient fine-tuning of open-source Large Language Models ( s) to address these challenges. It integrates three core components: synthetic dataset generation from Network Exposure Function (NEF) API specifications, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G Service-Based Architecture APIs, our approach achieves 85% reduction in overhead compared to manual discovery methods. Experimental validation using the open-source Phi-2 model demonstrates exceptional API call identification performance at 98-100% accuracy. The fine-tuned Phi-2 model delivers performance comparable to significantly larger models like GPT-4 while maintaining computational efficiency for tele s infrastructure deployment. These findings validate domain-specific, parameter-efficient strategies for managing complex API ecosystems in next-generation tele s networks. ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation Authors: Ding Xia, Naoto Inoue, Qianru Qiu, Kotaro Kikuchi 2025-08-12 http://arxiv.org/abs/2508.08987v1 Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating , improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models ( s) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained s serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our -based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques. ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs Authors: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun 2025-08-12 http://arxiv.org/abs/2508.08895v2 The increasing scale and complexity of large language models ( s) pose significant inference latency challenges, primarily due to their autoregressive paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel ) can significantly improve the overall inference speed of s. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel , we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel modes while maintaining a reusable , maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines. Steering Towards Fairness Mitigating Political Bias in LLMs Authors: Afrozah Nadeem, Mark Dras, Usman Naseem 2025-08-12 http://arxiv.org/abs/2508.08846v1 Recent advancements in large language models ( s) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. In this paper, we propose a framework for probing and mitigating such biases in r-based s through analysis of internal model representations. Grounded in the Political Compass Test (PCT), our method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that r s systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in s and offers a principled approach to debiasing beyond surface-level output interventions. DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation Authors: Tianyu Xiong, Dayi Tan, Wei Tian 2025-08-12 http://arxiv.org/abs/2508.08783v1 Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models ( s) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint r is designed to progressively refine pose predictions, improving robustness to occlusion and annotation . Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints. Interpretable Reward Model via Sparse Autoencoder Authors: Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang 2025-08-12 http://arxiv.org/abs/2508.08746v2 Large language models ( s) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of -based RM into an interpretable, , and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm. A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models Authors: Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu 2025-08-12 http://arxiv.org/abs/2508.08712v2 As text generation has become a core capability of modern Large Language Models ( s), it underpins a wide range of downstream applications. However, most existing s rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation. Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training Authors: Vishakha Lall, Yisi Liu 2025-08-12 http://arxiv.org/abs/2508.08652v1 Accurate evaluation of procedural compliance is essential in simulation-based training, particularly in safety-critical domains where adherence to compliance checklists reflects operational competence. This paper explores a lightweight, deployable approach using prompt-based inference with open-source large language models ( s) that can run efficiently on consumer-grade GPUs. We present Prompt-and-Check, a method that uses context-rich prompts to evaluate whether each checklist item in a protocol has been fulfilled, solely based on transcribed verbal exchanges. We perform a case study in the maritime domain with participants performing an identical simulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and Mistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a prompt incorporating relevant transcript excerpts is fed into the model, which outputs a compliance judgment. We assess model outputs against expert-annotated ground truth using classification accuracy and agreement scores. Our findings demonstrate that prompting enables effective context-aware reasoning without task-specific training. This study highlights the practical utility of s in augmenting debriefing, performance feedback, and automated assessment in training environments. Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks Authors: Adit Krishnan, Chu Wang, Chris Kong 2025-08-12 http://arxiv.org/abs/2508.08635v1 Semantic text classification requires the understanding of the contextual significance of specific tokens rather than surface-level patterns or keywords (as in rule-based or statistical text classification), making large language models ( s) well-suited for this task. However, semantic classification applications in industry, like customer intent detection or semantic role labeling, tend to be highly specialized. They require annotation by domain experts in contrast to general-purpose corpora for pretraining. Further, they typically require high inference throughputs which limits the model size from latency and cost perspectives. Thus, for a range of specialized classification tasks, the preferred solution is to develop customized classifiers by finetuning smaller language models (e.g., mini-encoders, small language models). In this work, we develop a token-driven finetuning strategy to adapt small language models to specialized classification tasks. We identify and finetune a small sensitive subset of model parameters by leveraging task-specific token constructs in the finetuning dataset, while leaving most of the pretrained weights unchanged. Unlike adapter approaches such as low rank adaptation (LoRA), we do not introduce additional parameters to the model. Our approach identifies highly relevant semantic tokens (case study in the Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and prefix tuning on five diverse semantic classification tasks. We achieve greater stability and half the training costs vs. end-to-end finetuning. AgriGPT a Large Language Model Ecosystem for Agriculture Authors: Bo Yang, Yu Zhang, Lanfei Feng, Yunkui Chen, Jianyu Zhang, Xiao Xu, Nueraili Aierken, Yurui Li, Yuxuan Chen, Guijun Yang, Yong He, Runhe Huang, Shijian Li 2025-08-12 http://arxiv.org/abs/2508.08632v1 Despite the rapid progress of Large Language Models ( s), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, retrieval, and multi-hop knowledge graph reasoning, thereby improving the 's reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose s on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized s. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research. QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach Authors: Conghao Zhou, Lulu Sun, Xiucheng Wang, Peng Yang, Feng Lyu, Sihan Lu, Xuemin Shen 2025-08-12 http://arxiv.org/abs/2508.08627v1 Mobile augmented reality (MAR) is envisioned as a key immersive application in 6G, enabling virtual content rendering aligned with the physical environment through device pose estimation. In this paper, we propose a novel agent-driven service provisioning approach for edge-assisted MAR, aiming to reduce overhead between MAR devices and the edge server while ensuring the quality of experience (QoE). First, to address the inaccessibility of MAR application-specific information to the network controller, we establish a digital agent powered by large language models ( s) on behalf of the MAR service provider, bridging the data and function gap between the MAR service and network domains. Second, to cope with the user-dependent and dynamic nature of data traffic patterns for individual devices, we develop a user-level QoE modeling method that captures the relationship between resource demands and perceived user QoE, enabling personalized, agent-driven resource management. Trace-driven simulation results demonstrate that the proposed approach outperforms conventional -based QoE-aware service provisioning methods in both user-level QoE modeling accuracy and resource efficiency. Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey Authors: Yang Lu, Shengli Zhang, Chang Liu, Ruichen Zhang, Bo Ai, Dusit Niyato, Wei Ni, Xianbin Wang, Abbas Jamalipour 2025-08-12 http://arxiv.org/abs/2508.08620v1 The rapid advancement of technologies has driven the evolution of networks towards both high-dimensional resource utilization and multifunctional integration. This evolving complexity poses significant challenges in designing networks to satisfy the growing quality-of-service and time sensitivity of mobile applications in dynamic environments. Graph neural networks (GNNs) have emerged as fundamental deep learning (DL) models for complex networks. GNNs not only augment the extraction of features over network topologies but also enhance scalability and facilitate distributed computation. However, most existing GNNs follow a traditional passive learning framework, which may fail to meet the needs of increasingly diverse wireless systems. This survey proposes the employment of agentic artificial intelligence (AI) to organize and integrate GNNs, enabling scenario- and task-aware implementation towards edge general intelligence. To comprehend the full capability of GNNs, we holistically review recent applications of GNNs in wireless s and networking. Specifically, we focus on the alignment between graph representations and network topologies, and between neural architectures and wireless tasks. We first provide an overview of GNNs based on prominent neural architectures, followed by the concept of agentic GNNs. Then, we summarize and compare GNN applications for conventional systems and emerging technologies, including physical, MAC, and network layer designs, integrated sensing and (ISAC), reconfigurable intelligent surface (RIS) and cell-free network architecture. We further propose a large language model ( ) framework as an intelligent question-answering agent, leveraging this survey as a local knowledge base to enable GNN-related responses tailored to wireless research. Joint decoding method for controllable contextual speech recognition based on Speech LLM Authors: Yangui Fang, Jing Peng, Yu Xi, Xu Li, Haoyu Li, Chengwei Zhang, Guohui Zhong, Kai Yu 2025-08-12 http://arxiv.org/abs/2508.08585v1 Contextual speech recognition refers to the ability to identify preferences for specific content based on contextual information. Recently, leveraging the contextual understanding capabilities of Speech to achieve contextual biasing by injecting contextual information through prompts have emerged as a research hotspot.However, the direct information injection method via prompts relies on the internal attention mechanism of the model, making it impossible to explicitly control the extent of information injection. To address this limitation, we propose a joint method to control the contextual information. This approach enables explicit control over the injected contextual information and achieving superior recognition performance. Additionally, Our method can also be used for sensitive word suppression recognition.Furthermore, experimental results show that even Speech not pre-trained on long contextual data can acquire long contextual capabilities through our method. Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System Authors: Pallavi Zambare, Venkata Nikhil Thanikella, Ying Liu 2025-08-12 http://arxiv.org/abs/2508.10043v1 When combining Large Language Models ( s) with autonomous agents, used in network monitoring and decision-making systems, this will create serious security issues. In this research, the MAESTRO framework consisting of the seven layers threat modeling architecture in the system was used to expose, evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent system was constructed and implemented, using Python, LangChain, and telemetry in WebSockets, and deployed with inference, memory, parameter tuning, and anomaly detection modules. Two practical threat cases were confirmed as follows: (i) resource denial of service by traffic replay denial-of-service, and (ii) memory poisoning by tampering with the historical log file maintained by the agent. These situations resulted in measurable levels of performance degradation, i.e. telemetry updates were delayed, and computational loads were increased, as a result of poor system adaptations. It was suggested to use a multilayered defense-in-depth approach with memory isolation, validation of planners and anomaly response systems in real-time. These findings verify that MAESTRO is viable in operational threat mapping, prospective risk scoring, and the basis of the resilient system design. The authors bring attention to the importance of the enforcement of memory integrity, paying attention to the adaptation logic monitoring, and cross-layer protection that guarantee the agentic AI reliability in adversarial settings. Profiling Large Language Model Inference on Apple Silicon A Quantization Perspective Authors: Afsara Benazir, Felix Xiaozhu Lin 2025-08-12 http://arxiv.org/abs/2508.08531v1 A systematic understanding of Apple Silicon is lacking in the current landscape of hardware efficiency; research focus is largely centered on accelerating GPUs for large-scale training or inference on CUDA devices. This paper investigates Apple Silicon's unique memory architecture that offers a unified memory integrating CPU and GPU memory and its implications for on-device inference. We decipher myths about whether Apple Silicon is efficient for on-device inference compared to competitors such as NVIDIA GPUs by directly conducting latency and throughput comparison benchmarks. We explain the performance gap between them through profiling low level hardware metrics - ALU utilization, memory bandwidth, buffer usage, residency etc. at runtime. We draw several insights regarding performance bottlenecks such as de overhead, compute throughput and memory bandwidth. We debunk existing false claims regarding large language model inference such as compressing models to lower bit precision is a defacto promise for faster inference across all hardware platforms. We find that the large unified memory enables Apple Silicon to be both cost effective and efficient against NVIDIA GPUs for ultra large language models. Our large scale evaluation on 5 hardware testbeds incorporating three Apple M-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX A6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from 8B to 405B parameters and 14 schemes gives an understanding of how Apple Silicon fits within the paradigm of on-device inference. Our analysis reveals multiple resource interdependencies and unexpected findings, while also quantifying established insights. To the best of our knowledge, this study makes the first attempt to present a thorough characterization and analysis of Apple Silicon for on-device inference. Using LLMs to Capture Users' Temporal Context for Recommendation Authors: Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher 2025-08-11 http://arxiv.org/abs/2508.08512v1 Effective recommender systems demand dynamic user understanding, especially in complex, evolving environments. Traditional user profiling often fails to capture the nuanced, temporal contextual factors of user preferences, such as transient short-term interests and enduring long-term tastes. This paper presents an assessment of Large Language Models ( s) for generating semantically rich, time-aware user profiles. We do not propose a novel end-to-end recommendation architecture; instead, the core contribution is a systematic investigation into the degree of effectiveness in capturing the dynamics of user context by disentangling short-term and long-term preferences. This approach, framing temporal preferences as dynamic user contexts for recommendations, adaptively fuses these distinct contextual components into comprehensive user embeddings. The evaluation across Movies&TV and Video Games domains suggests that while -generated profiles offer semantic depth and temporal structure, their effectiveness for context-aware recommendations is notably contingent on the richness of user interaction histories. Significant gains are observed in dense domains (e.g., Movies&TV), whereas improvements are less pronounced in environments (e.g., Video Games). This work highlights s' nuanced potential in enhancing user profiling for adaptive, context-aware recommendations, emphasizing the critical role of dataset characteristics for practical applicability. When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital Authors: Avni Kothari, Patrick Vossler, Jean Digitale, Mohammad Forouzannia, Elise Rosenberg, Michele Lee, Jennee Bryant, Melanie Molina, James Marks, Lucas Zier, Jean Feng 2025-08-11 http://arxiv.org/abs/2508.08504v1 Large language models ( s) have the potential to address social and behavioral determinants of health by transforming labor intensive workflows in resource-constrained settings. Creating -based applications that serve the needs of underserved communities requires a deep understanding of their local context, but it is often the case that neither s nor their developers possess this local expertise, and the experts in these communities often face severe time/resource constraints. This creates a disconnect: how can one engage in meaningful co-design of an -based application for an under-resourced community when the channel between the developer and domain expert is constrained? We explored this question through a real-world case study, in which our data science team sought to partner with social workers at a safety net hospital to build an application that summarizes patients' social needs. Whereas prior works focus on the challenge of prompt tuning, we found that the most critical challenge in this setting is the careful and precise specification of \\what information to surface to providers so that the application is accurate, comprehensive, and verifiable. Here we present a novel co-design framework for settings with limited access to domain experts, in which the summary generation task is first decomposed into individually-optimizable attributes and then each attribute is efficiently refined and validated through a multi-tier cascading approach. Vector-Centric Machine Learning Systems A Cross-Stack Approach Authors: Wenqi Jiang 2025-08-11 http://arxiv.org/abs/2508.08469v1 Today, two major trends are shaping the evolution of ML systems. First, modern AI systems are becoming increasingly complex, often integrating components beyond the model itself. A notable example is Retrieval-Augmented Generation (RAG), which incorporates not only multiple models but also vector databases, leading to heterogeneity in both system components and underlying hardware. Second, with the end of Moore's Law, achieving high system efficiency is no longer feasible without accounting for the rapid evolution of the hardware landscape. Building on the observations above, this thesis adopts a cross-stack approach to improving ML system efficiency, presenting solutions that span algorithms, systems, and hardware. First, it introduces several pioneering works about RAG efficiency across the computing stack. PipeRAG focuses on algorithm-level improvements, RAGO introduces system-level optimizations, and Chameleon explores heterogeneous accelerator systems for RAG. Second, this thesis investigates algorithm-hardware co-design for vector search. Specifically, FANNS and Falcon optimize -based and graph-based vector search, the two most popular paradigms of retrieval algorithms. Third, this thesis addresses the efficiency of recommender systems, another example of vector-centric ML systems, where the memory-intensive lookup operations on embedding vector tables often represent a major performance bottleneck. MicroRec and FleetRec propose solutions at the hardware and system levels, respectively, optimizing both data movement and computation to enhance the efficiency of large-scale recommender models. Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories Authors: Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu 2025-08-11 http://arxiv.org/abs/2508.08457v1 Long-context Large Language Model ( ) inference faces increasing compute bottlenecks as attention calculations scale with context length, primarily due to the growing - transfer overhead that saturates High Bandwidth Memory (HBM). While prefetching techniques mitigate misses by fetching data in advance, their spatial and temporal benefits present new opportunities to exploit. This work proposes a packing-prefetch scheduling architecture with monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with ultra-large on-chip capacity to accelerate long-context inference. Our optimizations demonstrate 8.06x speedup and 1.83x overall latency reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL memories over the serial execution. Evaluations of multi-request workloads on TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM bandwidth reduction compared to packing-only methods on Llama3.1-8B and Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL memories, our approach alleviates HBM constraints and enables efficient long-context inference. OverFill Two-Stage Models for Efficient Language Model Decoding Authors: Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed Abdelfattah, Alexander M. Rush 2025-08-11 http://arxiv.org/abs/2508.08446v1 Large language models ( s) excel across diverse tasks but face significant deployment challenges due to high inference costs. inference comprises (compute-bound) and (memory-bound) stages, with dominating latency particularly for long sequences. Current r-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for , processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during , OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill. Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference Authors: Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang 2025-08-11 http://arxiv.org/abs/2508.08438v1 Global - sharing has emerged as a key optimization for accelerating large language model ( ) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce Safe (Secure and Flexible Cache Sharing), a privacy-aware - management framework that selectively shares non-sensitive entries while confining sensitive content to private s. Safe comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that Safe mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, Safe improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse s and workloads. Safe reduces -induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high reuse efficiency, Safe reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for inference. Follow-Your-Shape Shape-Aware Image Editing via Trajectory-Guided Region Control Authors: Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma 2025-08-11 http://arxiv.org/abs/2508.08134v2 While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly pre non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement. BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang 2025-08-11 http://arxiv.org/abs/2508.08127v1 The security of -based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard. TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake 2025-08-11 http://arxiv.org/abs/2508.08115v1 We present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models ( s). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.'s \"Big Five\" model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop , and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the task's requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains. ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon 2025-08-11 http://arxiv.org/abs/2508.08101v1 Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal , lateral , and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of -powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions. Bridging ASR and LLMs for Dysarthric Speech Recognition Benchmarking Self-Supervised and Generative Approaches Authors: Ahmed Aboeitta, Ahmed Sharshar, Youssef Nafea, Shady Shehata 2025-08-11 http://arxiv.org/abs/2508.08027v1 Speech Recognition (ASR) due to phoneme distortions and high variability. While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown promise, their effectiveness in dysarthric speech remains unclear. This study systematically benchmarks these models with different strategies, including CTC, seq2seq, and -enhanced (BART,GPT-2, Vicuna). Our contributions include (1) benchmarking ASR architectures for dysarthric speech, (2) introducing -based to improve intelligibility, (3) analyzing generalization across datasets, and (4) providing insights into recognition errors across severity levels. Findings highlight that -enhanced improves dysarthric ASR by leveraging linguistic constraints for phoneme restoration and grammatical correction. Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang 2025-08-11 http://arxiv.org/abs/2508.08001v2 \"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an -based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal. DiTVR Zero-Shot Diffusion Transformer for Video Restoration Authors: Sicheng Gao, Nancy Mehta, Zongwei Wu, Radu Timofte 2025-08-11 http://arxiv.org/abs/2508.07811v1 Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, pre high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions. EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li 2025-08-11 http://arxiv.org/abs/2508.07809v1 Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models ( s) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes , limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger s for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration. We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables s to stably learn from initially unsolved hard problems under rewards. We apply EvoCoT to multiple families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables s to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research. Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li 2025-08-11 http://arxiv.org/abs/2508.07785v1 The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models ( s). MoE models facilitate scalability by enabling parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter s developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size. SASST Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation Authors: Zeyu Yang, Lai Wei, Roman Koshkin, Xi Chen, Satoshi Nakamura 2025-08-11 http://arxiv.org/abs/2508.07781v1 This work proposes a grammar-based chunking strategy that segments input streams into semantically complete units by parsing dependency relations (e.g., noun phrase boundaries, verb-object structures) and punctuation features. The method ensures chunk coherence and minimizes semantic fragmentation. Building on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech Translation), an end-to-end framework integrating frozen Whisper encoder and r-only . The unified architecture dynamically outputs translation tokens or symbols to jointly optimize translation timing and content, with target-side reordering addressing word-order divergence. Experiments on CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation quality improvements across languages and validate the effectiveness of syntactic structures in -driven SimulST systems. Symmetry-Aware Transformer Training for Automated Planning Authors: Markus Fritzsche, Elliot Gestrin, Jendrik Seipp 2025-08-11 http://arxiv.org/abs/2508.07743v1 While s excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art r-only , struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure s cannot efficiently learn from. We propose a novel contrastive learning objective to make s symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that s can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT. Semantic Caching for Low-Cost LLM Serving From Offline Learning to Online Adaptation Authors: Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong 2025-08-11 http://arxiv.org/abs/2508.07675v2 Large Language Models ( s) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the , has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different eviction problem: one must account for mismatch costs between incoming queries and d responses. Moreover, key system parameters, such as query arrival probabilities and costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines. GLiClass Generalist Lightweight Model for Sequence Classification Tasks Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko 2025-08-11 http://arxiv.org/abs/2508.07662v1 Classification is one of the most widespread tasks in AI applications, often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative s have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data- conditions or from human feedback. LaVieID Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation Authors: Wenhui Song, Hanhui Li, Jiehui Huang, Panwen Hu, Yuhao Cheng, Long Chen, Yiqiang Yan, Xiaodan Liang 2025-08-11 http://arxiv.org/abs/2508.07603v1 In this paper, we present LaVieID, a novel \\underline{l}ocal \\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework designed to tackle the challenging \\underline{id}entity-pre text-to-video task. The key idea of LaVieID is to mitigate the loss of identity information inherent in the stochastic global generation process of diffusion s (DiTs) from both spatial and temporal perspectives. Specifically, unlike the global and unstructured modeling of facial latent states in existing DiTs, LaVieID introduces a local router to explicitly represent latent states by weighted combinations of fine-grained local facial structures. This alleviates undesirable feature interference and encourages DiTs to capture distinctive facial characteristics. Furthermore, a temporal autoregressive module is integrated into LaVieID to refine denoised latent tokens before video . This module divides latent tokens temporally into chunks, exploiting their long-range temporal dependencies to predict biases for rectifying tokens, thereby significantly enhancing inter-frame identity consistency. Consequently, LaVieID can generate high-fidelity personalized videos and achieve state-of-the-art performance. Our code and models are available at https://github.com/ssugarwh/LaVieID. HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han 2025-08-11 http://arxiv.org/abs/2508.07602v1 Invoking external tools enables Large Language Models ( s) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of s and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the . Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries. Towards Theoretical Understanding of Transformer Test-Time Computing Investigation on In-Context Linear Regression Authors: Xingwu Chen, Miao Lu, Beining Wu, Difan Zou 2025-08-11 http://arxiv.org/abs/2508.07571v1 Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs Authors: Dom Huh, Prasant Mohapatra 2025-08-10 http://arxiv.org/abs/2508.07466v1 Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models ( s) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models ( s), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations. Investigating 1-Bit Quantization in Transformer-Based Top Tagging Authors: Saurabh Rai, Prisha, Jitendra Kumar 2025-08-10 http://arxiv.org/abs/2508.07431v1 The increasing scale of deep learning models in high-energy physics (HEP) has posed challenges to their deployment on low-power, latency-sensitive platforms, such as FPGAs and ASICs used in trigger systems, as well as in offline data reconstruction and processing pipelines. In this work, we introduce BitParT, a 1-bit Transformer-based architecture designed specifically for the top-quark tagging method. Building upon recent advances in ultra- large language models ( s), we extended these ideas to the HEP domain by developing a binary-weight variant (BitParT) of the Particle Transformer (ParT) model. Our findings indicate a potential for substantial reduction in model size and computational complexity, while maintaining high tagging performance. We benchmark BitParT on the public Top Quark Tagging Reference Dataset and show that it achieves competitive performance relative to its full-precision counterpart. This work demonstrates the design of extreme d models for physics applications, paving the way for real-time inference in collider experiments with minimal and optimized resource usage. LET-US Long Event-Text Understanding of Scenes Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu 2025-08-10 http://arxiv.org/abs/2508.07401v1 Event cameras output event streams as , asynchronous data with microsecond-level temporal resolution, enabling visual perception with low latency and a high dynamic range. While existing Multimodal Large Language Models (M s) have achieved significant success in understanding and analyzing RGB video content, they either fail to interpret event streams effectively or remain constrained to very short sequences. In this paper, we introduce LET-US, a framework for long event-stream--text comprehension that employs an adaptive compression mechanism to reduce the volume of input events while pre critical visual details. LET-US thus establishes a new frontier in cross-modal inferential understanding over extended event sequences. To bridge the substantial modality gap between event streams and textual representations, we adopt a two-stage optimization paradigm that progressively equips our model with the capacity to interpret event-based scenes. To handle the voluminous temporal information inherent in long event streams, we leverage text-guided cross-modal queries for feature reduction, augmented by hierarchical clustering and similarity computation to distill the most representative event features. Moreover, we curate and construct a large-scale event-text aligned dataset to train our model, achieving tighter alignment of event features within the embedding space. We also develop a comprehensive benchmark covering a diverse set of tasks -- reasoning, captioning, classification, temporal localization and moment retrieval. Experimental results demonstrate that LET-US outperforms prior state-of-the-art M s in both descriptive accuracy and semantic comprehension on long-duration event streams. All datasets, codes, and models will be publicly available. Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang 2025-08-10 http://arxiv.org/abs/2508.07329v1 With the breakthrough progress of large language models ( s) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix , we achieve joint 8-bit of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the d model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved. BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang 2025-08-10 http://arxiv.org/abs/2508.07300v1 Real-time semantic segmentation presents the dual challenge of designing efficient architectures that capture large receptive fields for semantic understanding while also refining detailed contours. Vision s model long-range dependencies effectively but incur high computational cost. To address these challenges, we introduce the Large Kernel Attention (LKA) mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet) expands the receptive field to capture contextual information and extracts visual and structural features using Sparse Decomposed Large Separable Kernel Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism dynamically adapts the receptive field to further enhance performance. Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches contextual features by synergistically combining dilated convolutions and large kernel attention. The bilateral architecture facilitates frequent branch , and the Boundary Guided Adaptive Fusion (BGAF) module enhances boundary delineation by integrating spatial and semantic features under boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding 79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet pretraining, demonstrating state-of-the-art performance. The code and model is available at https://github.com/maomao0819/BEVANet. Tasa Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference Authors: Siyuan He, Peiran Yan, Yandong He, Youwei Zhuo, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07252v1 The autoregressive in s is the major inference bottleneck due to the memory-intensive operations and limited hardware bandwidth. 3D-stacked architecture is a promising solution with significantly improved memory bandwidth, which vertically stacked multi DRAM dies on top of logic die. However, our experiments also show the 3D-stacked architecture faces severer thermal issues compared to 2D architecture, in terms of thermal temperature, gradient and scalability. To better exploit the potential of 3D-stacked architecture, we present Tasa, a heterogeneous architecture with cross-stack thermal optimizations to balance the temperature distribution and maximize the performance under the thermal constraints. High-performance core is designed for compute-intensive operations, while high-efficiency core is used for memory-intensive operators, e.g. attention layers. Furthermore, we propose a bandwidth sharing scheduling to improve the bandwidth utilization in such heterogeneous architecture. Extensive thermal experiments show that our Tasa architecture demonstrates greater scalability compared with the homogeneous 3D-stacked architecture, i.e. up to 5.55 \\tccentigrade , 9.37 \\tccentigrade , and 7.91 \\tccentigrade peak temperature reduction for 48, 60, and 72 core configurations. Our experimental for Llama-65B and GPT-3 66B inferences also demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and state-of-the-art heterogeneous PIM-based accelerator LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07227v1 inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token and dynamic workload scheduling to accelerate speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits. Bridging Semantic Logic Gaps A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization Authors: Songlin Li, Zhiqing Guo, Yuanman Li, Zeyu Li, Yunfeng Diao, Gaobo Yang, Liejun Wang 2025-08-10 http://arxiv.org/abs/2508.07216v1 The existing image manipulation localization (IML) models mainly relies on visual cues, but ignores the semantic logical relationships between content features. In fact, the content semantics conveyed by real images often conform to human cognitive laws. However, image manipulation technology usually destroys the internal relationship between content features, thus leaving semantic clues for IML. In this paper, we propose a cognition-inspired multimodal boundary-pre network (CMB-Net). Specifically, CMB-Net utilizes large language models ( s) to analyze manipulated regions within images and generate prompt-based textual information to compensate for the lack of semantic relationships in the visual information. Considering that the erroneous texts induced by hallucination from s will damage the accuracy of IML, we propose an image-text central ambiguity module (ITCAM). It assigns weights to the text features by quantifying the ambiguity between text and image features, thereby ensuring the beneficial impact of textual information. We also propose an image-text interaction module (ITIM) that aligns visual and text features using a correlation matrix for fine-grained interaction. Finally, inspired by invertible neural networks, we propose a restoration edge r (RED) that mutually generates input and output features to preserve boundary information in manipulated regions without loss. Extensive experiments show that CMB-Net outperforms most existing IML models. DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan 2025-08-10 http://arxiv.org/abs/2508.07185v1 Large Language Models ( s) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables s to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a knowledge attention mechanism, which allows the to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building s that can stay current with the ever-changing world. How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak 2025-08-10 http://arxiv.org/abs/2508.07127v1 Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and ly annotated datasets remains a non-trivial task. Recently, s has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned s to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how s can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of s in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care. From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya 2025-08-09 http://arxiv.org/abs/2508.07117v1 Graph Neural Networks (GNNs) have emerged as powerful tools for learning over structured data, including text-attributed graphs, which are common in domains such as citation networks, social platforms, and knowledge graphs. GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language. In this work, we introduce LOGIC, a lightweight, post-hoc framework that uses large language models ( s) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure. This enables the to reason about GNN internal representations and produce natural language explanations along with concise explanation subgraphs. Our experiments across four real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off between fidelity and , while significantly improving human-centric metrics such as insightfulness. LOGIC sets a new direction for -based explainability in graph learning by aligning GNN internals with human reasoning. Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation Authors: Juntong Fan, Shuyi Fan, Debesh Jha, Changsheng Fang, Tieyong Zeng, Hengyong Yu, Dayang Wang 2025-08-09 http://arxiv.org/abs/2508.07028v1 Accurate endoscopic image segmentation on the polyps is critical for early colorectal cancer detection. However, this task remains challenging due to low contrast with surrounding mucosa, specular highlights, and indistinct boundaries. To address these challenges, we propose FOCUS-Med, which stands for Fusion of spatial and structural graph with attentional context-aware polyp segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph Convolutional Network (Dual-GCN) module to capture contextual spatial and topological structural dependencies. This graph-based representation enables the model to better distinguish polyps from background tissues by leveraging topological cues and spatial connectivity, which are often obscured in raw image intensities. It enhances the model's ability to preserve boundaries and delineate complex shapes typical of polyps. In addition, a location-fused stand-alone self-attention is employed to strengthen global context integration. To bridge the semantic gap between encoder- r layers, we incorporate a trainable weighted fast normalized fusion strategy for efficient multi-scale aggregation. Notably, we are the first to introduce the use of a Large Language Model ( ) to provide detailed qualitative evaluations of segmentation quality. Extensive experiments on public benchmarks demonstrate that FOCUS-Med achieves state-of-the-art performance across five key metrics, underscoring its effectiveness and clinical potential for AI-assisted colonoscopy. Vec2Summ Text Summarization via Probabilistic Sentence Embeddings Authors: Mao Li, Fred Conrad, Johann Gagnon-Bartsch 2025-08-09 http://arxiv.org/abs/2508.07017v1 We propose Vec2Summ, a novel method for abstractive summarization that frames the task as semantic compression. Vec2Summ represents a document collection using a single mean vector in the semantic embedding space, capturing the central meaning of the corpus. To reconstruct fluent summaries, we perform embedding inversion -- this mean vector into natural language using a generative language model. To improve reconstruction quality and capture some degree of topical variability, we introduce stochasticity by sampling from a Gaussian distribution centered on the mean. This approach is loosely analogous to bagging in ensemble learning, where controlled randomness encourages more robust and varied outputs. Vec2Summ addresses key limitations of -based summarization methods. It avoids context-length constraints, enables interpretable and controllable generation via semantic parameters, and scales efficiently with corpus size -- requiring only O(d + d^2) parameters. Empirical results show that Vec2Summ produces coherent summaries for topically focused, order-invariant corpora, with performance comparable to direct summarization in terms of thematic coverage and efficiency, albeit with less fine-grained detail. These results underscore Vec2Summ's potential in settings where scalability, semantic control, and corpus-level abstraction are prioritized. Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV Authors: Roberto Balestri, Guglielmo Pescatore 2025-08-09 http://arxiv.org/abs/2508.07010v1 Serialized television narratives present significant analytical challenges due to their complex, temporally distributed storylines that necessitate sophisticated information management. This paper introduces a multi-agent system (MAS) designed to extract and analyze narrative arcs by implementing principles of computational memory architectures. The system conceptualizes narrative understanding through analogues of human memory: Large Language Models ( s) provide a form of semantic memory for general narrative patterns, while a vector database stores specific arc progressions as episodic memories. A multi-agent workflow simulates working memory processes to integrate these information types. Tested on the first season of Grey's Anatomy (ABC 2005-), the MAS identifies three arc types: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific. These arcs and their episodic developments are stored in a vector database, facilitating structured analysis and semantic comparison. To bridge automation with critical interpretation, a graphical interface enables human oversight and refinement of the system's narrative memory. While demonstrating strong performance in identifying Anthology Arcs and character entities, the system's reliance on textual paratexts (episode summaries) revealed limitations in discerning ping arcs and opaque dynamics, underscoring the challenges in computational memory consolidation versus human holistic understanding. This memory-centric approach highlights the potential of combining AI-driven memory processing with human expertise. Beyond television, it offers promise for serialized written formats where narrative is entirely text-based. Future work will focus on integrating multimodal inputs to enrich episodic memory, refining memory integration mechanisms within the MAS, and expanding testing across diverse genres. SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn 2025-08-09 http://arxiv.org/abs/2508.06978v1 Large Language Models ( s) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical stage of inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to ~12x compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude. Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models Authors: Zhijun Tu, Hanting Chen, Siqi Liu, Chuanjian Liu, Jian Li, Jie Hu, Yunhe Wang 2025-08-09 http://arxiv.org/abs/2508.06974v1 1-bit offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit s from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes direct adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the floating-point weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on s of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit s can be achieved using pre-trained models, eliminating the need for expensive training from scratch. Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan 2025-08-09 http://arxiv.org/abs/2508.06765v1 Collaboratively fine-tuning (FT) large language models ( s) over heterogeneous mobile devices fosters immense potential applications of personalized intelligence. However, such a vision faces critical system challenges. Conventional federated FT approaches place prohibitive computational and memory burdens on mobile hardware, and their synchronous model aggregation protocols stall for slower devices. In this paper, we propose Fed Mobi , a novel design to facilitate efficient federated FT across mobile devices with diverse computing/ speeds and local model architectures. In particular, Fed Mobi implements a pioneering server-assisted federated side-tuning paradigm. Briefly, mobile devices perform lightweight forward propagation computations on local data using their frozen pre-scaled backbone s, and then upload selected intermediate activations. The server trains a shared side-network independently, eliminating client-side backpropagation and enabling asynchronous updates. To bridge model heterogeneity across different devices, we introduce an adaptive layer-wise feature alignment method, which ensures consistent representations for collaboratively tuning a shared side network. Extensive experimental results demonstrate that Fed Mobi can maintain robust fine-tuning performance while achieving extremely low on-device memory, with at least 95.2% reduction in computation overhead, 93.2% reduction in costs and 5.1x faster convergence compared to existing methods, validating its efficacy for practical adaptation over heterogeneous mobile devices. Pushing the Envelope of LLM Inference on AI-PC Authors: Evangelos Georganas, Dhiraj Kalamkar, Alexander Heinecke 2025-08-08 http://arxiv.org/abs/2508.06753v1 The advent of ultra- models (1/1.58/2-bit), which match the perplexity and end-task performance of their full-precision counterparts using the same model size, is ushering in a new era of inference for resource-constrained environments such as edge devices and AI PCs. While these advances promise models that are more cost-effective in terms of latency, memory, throughput, and energy consumption, the computational efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp) used to deploy them remains underexplored. In this work, we take a bottom-up approach: we first design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, achieving peak computational efficiency across a variety of CPU platforms. We integrate these microkernels into a state-of-the-art inference framework, namely PyTorch-TPP, and present end-to-end inference results with 2-bit models that outperform the current SOTA runtime bitnet.cpp by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model inference. Our optimized runtime advances the state of inference on AI PCs and edge devices, paving the way for efficient deployment of ultra- models. CISO Species Distribution Modeling Conditioned on Incomplete Species Observations Authors: Hager Radi Abdelwahed, M\u00e9lisande Teng, Robin Zbinden, Laura Pollock, Hugo Larochelle, Devis Tuia, David Rolnick 2025-08-08 http://arxiv.org/abs/2508.06704v1 Species distribution models (SDMs) are widely used to predict species' geographic distributions, as critical tools for ecological research and conservation planning. Typically, SDMs relate species occurrences to environmental variables representing abiotic factors, such as temperature, precipitation, and soil properties. However, species distributions are also strongly influenced by biotic interactions with other species, which are often overlooked. While some methods partially address this limitation by incorporating biotic interactions, they often assume symmetrical pairwise relationships between species and require consistent co-occurrence data. In practice, species observations are , and the availability of information about the presence or absence of other species varies significantly across locations. To address these challenges, we propose CISO, a deep learning-based method for species distribution modeling Conditioned on Incomplete Species Observations. CISO enables predictions to be conditioned on a flexible number of species observations alongside environmental variables, accommodating the variability and incompleteness of available biotic data. We demonstrate our approach using three datasets representing different species groups: sPlotOpen for plants, SatBird for birds, and a new dataset, SatButterfly, for butterflies. Our results show that including partial biotic information improves predictive performance on spatially separate test sets. When conditioned on a subset of species within the same dataset, CISO outperforms alternative methods in predicting the distribution of the remaining species. Furthermore, we show that combining observations from multiple datasets can improve performance. CISO is a promising ecological tool, capable of incorporating incomplete biotic information and identifying potential interactions between species from disparate taxa.","title":"2025-08-15"},{"location":"weekly_paper/2025-08-15/#2025-08-15","text":"","title":"2025-08-15"},{"location":"weekly_paper/2025-08-15/#table-of-contents","text":"STream3R Scalable Sequential 3D Reconstruction with Causal Transformer Generalizable Federated Learning using Client Adaptive Focal Modulation Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation Thinking Inside the Mask In-Place Prompting in Diffusion LLMs Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph SemPT Semantic Prompt Tuning for Vision-Language Models DAS Dual-Aligned Semantic IDs Empowered Industrial Recommender System GCRPNet Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images X-Node Self-Explanation is All We Need Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models Personalized Real-time Jargon Support for Online Meetings Can Transformers Break Encryption Schemes via In-Context Learning? Agentic AI Frameworks Architectures, Protocols, and Design Challenges Nested-ReFT Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation Constrained Decoding of Diffusion LLMs with Context-Free Grammars Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach Memory Decoder A Pretrained, Plug-and-Play Memory for Large Language Models OneVAE Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better Speed Always Wins A Survey on Efficient Architectures for Large Language Models MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference Synaptic Pruning A Biological Inspiration for Deep Learning Regularization SinLlama -- A Large Language Model for Sinhala READER Retrieval-Assisted Drafter for Efficient LLM Inference FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm A Survey on Training-free Alignment of Large Language Models Retrospective Sparse Attention for Efficient Long-Context Generation NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs Steering Towards Fairness Mitigating Political Bias in LLMs DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation Interpretable Reward Model via Sparse Autoencoder A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks AgriGPT a Large Language Model Ecosystem for Agriculture QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey Joint decoding method for controllable contextual speech recognition based on Speech LLM Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System Profiling Large Language Model Inference on Apple Silicon A Quantization Perspective Using LLMs to Capture Users' Temporal Context for Recommendation When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital Vector-Centric Machine Learning Systems A Cross-Stack Approach Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories OverFill Two-Stage Models for Efficient Language Model Decoding Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference Follow-Your-Shape Shape-Aware Image Editing via Trajectory-Guided Region Control BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Bridging ASR and LLMs for Dysarthric Speech Recognition Benchmarking Self-Supervised and Generative Approaches Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths DiTVR Zero-Shot Diffusion Transformer for Video Restoration EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts SASST Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation Symmetry-Aware Transformer Training for Automated Planning Semantic Caching for Low-Cost LLM Serving From Offline Learning to Online Adaptation GLiClass Generalist Lightweight Model for Sequence Classification Tasks LaVieID Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Towards Theoretical Understanding of Transformer Test-Time Computing Investigation on In-Context Linear Regression Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs Investigating 1-Bit Quantization in Transformer-Based Top Tagging LET-US Long Event-Text Understanding of Scenes Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation Tasa Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization Bridging Semantic Logic Gaps A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation Vec2Summ Text Summarization via Probabilistic Sentence Embeddings Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning Pushing the Envelope of LLM Inference on AI-PC CISO Species Distribution Modeling Conditioned on Incomplete Species Observations","title":"Table of Contents"},{"location":"weekly_paper/2025-08-15/#stream3r-scalable-sequential-3d-reconstruction-with-causal-transformer","text":"Authors: Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan 2025-08-14 http://arxiv.org/abs/2508.10893v1 We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a r-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with -style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.","title":"STream3R Scalable Sequential 3D Reconstruction with Causal Transformer"},{"location":"weekly_paper/2025-08-15/#generalizable-federated-learning-using-client-adaptive-focal-modulation","text":"Authors: Tajamul Ashraf, Iqra Altaf Gillani 2025-08-14 http://arxiv.org/abs/2508.10840v1 Federated learning (FL) has proven essential for privacy-pre , collaborative training across distributed clients. Our prior work, TransFed, introduced a robust -based FL framework that leverages a learn-to-adapt hypernetwork to generate personalized focal modulation layers per client, outperforming traditional methods in non-IID and cross-domain settings. In this extended version, we propose AdaptFED, where we deepen the investigation of focal modulation in generalizable FL by incorporating: (1) a refined adaptation strategy that integrates task-aware client embeddings to personalize modulation dynamics further, (2) enhanced theoretical bounds on adaptation performance, and (3) broader empirical validation across additional modalities, including time-series and multilingual data. We also introduce an efficient variant of TransFed that reduces server-client overhead via low-rank hypernetwork conditioning, enabling scalable deployment in resource-constrained environments. Extensive experiments on eight diverse datasets reaffirm the superiority of our method over state-of-the-art baselines, particularly in source-free and cross-task federated setups. Our findings not only extend the capabilities of focal modulation in FL but also pave the way for more adaptive, scalable, and generalizable -based federated systems. The code is available at http://github.com/Tajamul21/TransFed","title":"Generalizable Federated Learning using Client Adaptive Focal Modulation"},{"location":"weekly_paper/2025-08-15/#video-blade-block-sparse-attention-meets-step-distillation-for-efficient-video-generation","text":"Authors: Youping Gu, Xiaolong Li, Yuhao Hu, Bohan Zhuang 2025-08-14 http://arxiv.org/abs/2508.10774v1 Diffusion s currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and attention mechanisms have shown promise as independent strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware masks to focus computation on salient spatiotemporal features, and (2) a -aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.","title":"Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation"},{"location":"weekly_paper/2025-08-15/#thinking-inside-the-mask-in-place-prompting-in-diffusion-llms","text":"Authors: Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang 2025-08-14 http://arxiv.org/abs/2508.10736v1 Despite large language models ( s) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (d s) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for d s. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12 \\times speedup on GSM8K, and up to 276.67 \\times on MMLU while maintaining competitive performance.","title":"Thinking Inside the Mask In-Place Prompting in Diffusion LLMs"},{"location":"weekly_paper/2025-08-15/#continuous-bangla-sign-language-translation-mitigating-the-expense-of-gloss-annotation-with-the-assistance-of-graph","text":"Authors: Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman 2025-08-14 http://arxiv.org/abs/2508.10687v1 Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage architecture for state-of-the-art results, our method integrates graph-based methods with the architecture. This fusion, combining and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve accessibility for the deaf and hard of hearing.","title":"Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph"},{"location":"weekly_paper/2025-08-15/#sempt-semantic-prompt-tuning-for-vision-language-models","text":"Authors: Xiao Shi, Yangjun Ou, Zhenzhong Chen 2025-08-14 http://arxiv.org/abs/2508.10645v1 Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between pre category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on category labels or disparate -generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning.","title":"SemPT Semantic Prompt Tuning for Vision-Language Models"},{"location":"weekly_paper/2025-08-15/#das-dual-aligned-semantic-ids-empowered-industrial-recommender-system","text":"Authors: Wencai Ye, Mingjie Sun, Shaoyun Shi, Peng Wang, Wenjin Wu, Peng Jiang 2025-08-14 http://arxiv.org/abs/2508.10584v1 Semantic IDs are discrete identifiers generated by quantizing the Multi-modal Large Language Models (M s) embeddings, enabling efficient multi-modal content integration in recommendation systems. However, their lack of collaborative signals results in a misalignment with downstream discriminative and generative recommendation objectives. Recent studies have introduced various alignment mechanisms to address this problem, but their two-stage framework design still leads to two main limitations: (1) inevitable information loss during alignment, and (2) inflexibility in applying adaptive alignment strategies, consequently constraining the mutual information maximization during the alignment process. To address these limitations, we propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method that simultaneously optimizes and alignment, pre semantic integrity and alignment quality while avoiding the information loss typically associated with two-stage methods. Meanwhile, DAS achieves more efficient alignment between the semantic IDs and collaborative signals, with the following two innovative and effective approaches: (1) Multi-view Constrative Alignment: To maximize mutual information between semantic IDs and collaborative signals, we first incorporate an ID-based CF debias module, and then design three effective contrastive alignment methods: dual user-to-item (u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual s of users and ads, the constructed semantic IDs for users and ads achieve stronger alignment. Finally, we conduct extensive offline experiments and online A/B tests to evaluate DAS's effectiveness, which is now successfully deployed across various advertising scenarios at Kuaishou App, over 400 million users daily.","title":"DAS Dual-Aligned Semantic IDs Empowered Industrial Recommender System"},{"location":"weekly_paper/2025-08-15/#gcrpnet-graph-enhanced-contextual-and-regional-perception-network-for-salient-object-detection-in-optical-remote-sensing-images","text":"Authors: Mengyu Ren, Yutong Li, Hua Li, Runmin Cong, Sam Kwong 2025-08-14 http://arxiv.org/abs/2508.10542v1 Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision s (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the r of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.","title":"GCRPNet Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images"},{"location":"weekly_paper/2025-08-15/#x-node-self-explanation-is-all-we-need","text":"Authors: Prajit Sengupta, Islem Rekik 2025-08-14 http://arxiv.org/abs/2508.10461v1 Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a r to enforce faithfulness, (2) generating a natural language explanation using a pre-trained (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.","title":"X-Node Self-Explanation is All We Need"},{"location":"weekly_paper/2025-08-15/#efficient-methods-for-accurate-sparse-trajectory-recovery-and-map-matching","text":"Authors: Wei Tian, Jieming Shi, Man Lung Yiu 2025-08-14 http://arxiv.org/abs/2508.10460v1 Real-world trajectories are often with low-sampling rates (i.e., long intervals between consecutive GPS points) and misaligned with road networks, yet many applications demand high-quality data for optimal performance. To improve data quality with trajectories as input, we systematically study two related research problems: trajectory recovery on road network, which aims to infer missing points to recover high-sampling trajectories, and map matching, which aims to map GPS points to road segments to determine underlying routes. In this paper, we present efficient methods TRMMA and MMA for accurate trajectory recovery and map matching, respectively, where MMA serves as the first step of TRMMA. In MMA, we carefully formulate a classification task to map a GPS point from trajectories to a road segment over a small candidate segment set, rather than the entire road network. We develop techniques in MMA to generate effective embeddings that capture the patterns of GPS data, directional information, and road segments, to accurately align trajectories to routes. For trajectory recovery, TRMMA focuses on the segments in the route returned by MMA to infer missing points with position ratios on road segments, producing high-sampling trajectories efficiently by avoiding evaluation of all road segments. Specifically, in TRMMA, we design a dual- encoding process to cohesively capture latent patterns in trajectories and routes, and an effective technique to sequentially predict the position ratios and road segments of missing points. We conduct extensive experiments to compare TRMMA and MMA with numerous existing methods for trajectory recovery and map matching, respectively, on 4 large real-world datasets. TRMMA and MMA consistently achieve the best result quality, often by a significant margin.","title":"Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching"},{"location":"weekly_paper/2025-08-15/#computational-economics-in-large-language-models-exploring-model-behavior-and-incentive-design-under-resource-constraints","text":"Authors: Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh 2025-08-14 http://arxiv.org/abs/2508.10426v1 Large language models ( s) are limited by substantial computational cost. We introduce a \"computational economics\" framework that treats an as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard s reallocate attention toward high-value tokens while pre accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc ; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent s under strict resource constraints.","title":"Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints"},{"location":"weekly_paper/2025-08-15/#layer-wise-perturbations-via-sparse-autoencoders-for-adversarial-text-generation","text":"Authors: Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, Zhuo Li 2025-08-14 http://arxiv.org/abs/2508.10404v1 With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models ( s), generating adversarial examples to jailbreak s remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.","title":"Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"},{"location":"weekly_paper/2025-08-15/#xquant-breaking-the-memory-wall-for-llm-inference-with-kv-cache-rematerialization","text":"Authors: Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami 2025-08-14 http://arxiv.org/abs/2508.10395v1 Although inference has emerged as a critical workload for many downstream applications, efficiently inferring s is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through with substantial accuracy benefits relative to state-of-the-art methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2 \\times memory savings compared to caching. By applying XQuant, we achieve up to \\sim 7.7\\times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10 \\times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5 \\times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art methods and achieving near-FP16 accuracy across a wide range of models.","title":"XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"},{"location":"weekly_paper/2025-08-15/#emamba-efficient-acceleration-framework-for-mamba-models-in-edge-computing","text":"Authors: Jiyong Kim, Jaeho Lee, Jiahao Lin, Alish Kanani, Miao Sun, Umit Y. Ogras, Jaehyun Park 2025-08-14 http://arxiv.org/abs/2508.10370v1 State Space Model (SSM)-based machine learning architectures have recently gained significant attention for processing sequential data. Mamba, a recent sequence-to-sequence SSM, offers competitive accuracy with superior computational efficiency compared to state-of-the-art models. While this advantage makes Mamba particularly promising for resource-constrained edge devices, no hardware frameworks are currently optimized for deploying it in such environments. This paper presents eMamba, a comprehensive end-to-end hardware framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10, and MARS, an open-source human pose estimation dataset, show eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63-19.9 \\times fewer parameters. In addition, it generalizes well to large-scale natural language tasks, demonstrating stable perplexity across varying sequence lengths on the WikiText2 dataset. We also and implement the entire eMamba pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm technology. Experimental results show 4.95-5.62 \\times lower latency and 2.22-9.95 \\times higher throughput, with 4.77 \\times smaller area, 9.84 \\times lower power, and 48.6 \\times lower energy consumption than baseline solutions while maintaining competitive accuracy.","title":"eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing"},{"location":"weekly_paper/2025-08-15/#improving-generative-cross-lingual-aspect-based-sentiment-analysis-with-constrained-decoding","text":"Authors: Jakub \u0160m\u00edd, Pavel P\u0159ib\u00e1\u0148, Pavel Kr\u00e1l 2025-08-14 http://arxiv.org/abs/2508.10369v1 While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained boosting results by more than 10\\%. We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models ( s) in zero-shot, few-shot, and fine-tuning scenarios. While s perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times. We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.","title":"Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding"},{"location":"weekly_paper/2025-08-15/#advancing-cross-lingual-aspect-based-sentiment-analysis-with-llms-and-constrained-decoding-for-sequence-to-sequence-models","text":"Authors: Jakub \u0160m\u00edd, Pavel P\u0159ib\u00e1\u0148, Pavel Kr\u00e1l 2025-08-14 http://arxiv.org/abs/2508.10366v1 Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained , improves cross-lingual ABSA performance by up to 10\\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models ( s) and show that while fine-tuned multilingual s can achieve comparable results, English-centric s struggle with these tasks.","title":"Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models"},{"location":"weekly_paper/2025-08-15/#what-to-ask-next-probing-the-imaginative-reasoning-of-llms-with-turtlesoup-puzzles","text":"Authors: Mengtao Zhou, Sifan Wu, Huan Zhang, Qi Sima, Bang Liu 2025-08-14 http://arxiv.org/abs/2508.10358v1 We investigate the capacity of Large Language Models ( s) for imaginative reasoning--the proactive construction, testing, and revision of hypotheses in information- environments. Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic \"Turtle Soup\" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess s' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment. Experiments with leading s reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into s' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior.","title":"What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles"},{"location":"weekly_paper/2025-08-15/#diffaxe-diffusion-driven-hardware-accelerator-generation-and-design-space-exploration","text":"Authors: Arkapravo Ghosh, Abhishek Moitra, Abhiroop Bhattacharjee, Ruokai Yin, Priyadarshini Panda 2025-08-14 http://arxiv.org/abs/2508.10303v1 Design space exploration (DSE) is critical for developing optimized hardware architectures, especially for AI workloads such as deep neural networks (DNNs) and large language models ( s), which require specialized . As model complexity grows, accelerator design spaces have expanded to O(10^17), becoming highly irregular, non-convex, and exhibiting many-to-one mappings from design configurations to performance metrics. This complexity renders direct inverse derivation infeasible and necessitates heuristic or sampling-based optimization. Conventional methods - including Bayesian optimization, gradient descent, reinforcement learning, and genetic algorithms - depend on iterative sampling, resulting in long runtimes and sensitivity to initialization. Deep learning-based approaches have reframed DSE as classification using recommendation models, but remain limited to small-scale (O(10^3)), less complex design spaces. To overcome these constraints, we propose a generative approach that models hardware design as 1-D image synthesis conditioned on target performance, enabling efficient learning of non-differentiable, non-bijective hardware-performance mappings. Our framework achieves 0.86% lower generation error than Bayesian optimization with a 17000x speedup, and outperforms GANDSE with 30% lower error at only 1.83x slower search. We further extend the method to a structured DSE setting, attaining 9.8% lower energy-delay product (EDP) and 6% higher performance, with up to 145.6x and 1312x faster search compared to existing optimization methods on O(10^17) design spaces. For inference, our method achieves 3.37x and 7.75x lower EDP on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the state-of-the-art DOSA framework.","title":"DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration"},{"location":"weekly_paper/2025-08-15/#pruning-and-malicious-injection-a-retraining-free-backdoor-attack-on-transformer-models","text":"Authors: Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, Xiangwei Zhou 2025-08-14 http://arxiv.org/abs/2508.10243v1 Transformer models have demonstrated exceptional performance and have become indispensable in computer vision (CV) and natural language processing (NLP) tasks. However, recent studies reveal that s are susceptible to backdoor attacks. Prior backdoor attack methods typically rely on retraining with clean data or altering the model architecture, both of which can be resource-intensive and intrusive. In this paper, we propose Head-wise Pruning and Malicious Injection (HPMI), a novel retraining-free backdoor attack on s that does not alter the model's architecture. Our approach requires only a small subset of the original data and basic knowledge of the model architecture, eliminating the need for retraining the target . Technically, HPMI works by the least important head and injecting a pre-trained malicious head to establish the backdoor. We provide a rigorous theoretical justification demonstrating that the implanted backdoor resists detection and removal by state-of-the-art defense techniques, under reasonable assumptions. Experimental evaluations across multiple datasets further validate the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four advanced defense mechanisms. Additionally, relative to state-of-the-art retraining-dependent attacks, HPMI achieves greater concealment and robustness against diverse defense strategies, while maintaining minimal impact on clean accuracy.","title":"Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models"},{"location":"weekly_paper/2025-08-15/#personalized-real-time-jargon-support-for-online-meetings","text":"Authors: Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August 2025-08-13 http://arxiv.org/abs/2508.10239v1 Effective interdisciplinary is frequently hindered by domain-specific jargon. To explore the jargon barriers in-depth, we conducted a formative diary study with 16 professionals, revealing critical limitations in current jargon-management strategies during workplace meetings. Based on these insights, we designed ParseJargon, an interactive -powered system providing real-time personalized jargon identification and explanations tailored to users' individual backgrounds. A controlled experiment comparing ParseJargon against baseline (no support) and general-purpose (non-personalized) conditions demonstrated that personalized jargon support significantly enhanced participants' comprehension, engagement, and appreciation of colleagues' work, whereas general-purpose support negatively affected engagement. A follow-up field study validated ParseJargon's usability and practical value in real-time meetings, highlighting both opportunities and limitations for real-world deployment. Our findings contribute insights into designing personalized jargon support tools, with implications for broader interdisciplinary and educational applications.","title":"Personalized Real-time Jargon Support for Online Meetings"},{"location":"weekly_paper/2025-08-15/#can-transformers-break-encryption-schemes-via-in-context-learning","text":"Authors: Jathin Korrapati, Patrick Mendoza, Aditya Tomar, Abein Abraham 2025-08-13 http://arxiv.org/abs/2508.10235v1 In-context learning (ICL) has emerged as a powerful capability of -based language models, enabling them to perform tasks by conditioning on a small number of examples presented at inference time, without any parameter updates. Prior work has shown that s can generalize over simple function classes like linear functions, decision trees, even neural networks, purely from context, focusing on numerical or symbolic reasoning over underlying well-structured functions. Instead, we propose a novel application of ICL into the domain of cryptographic function learning, specifically focusing on ciphers such as mono-alphabetic substitution and Vigen`ere ciphers, two classes of private-key encryption schemes. These ciphers involve a fixed but hidden bijective mapping between plain text and cipher text characters. Given a small set of (cipher text, plain text) pairs, the goal is for the model to infer the underlying substitution and a new cipher text word. This setting poses a structured inference challenge, which is well-suited for evaluating the inductive biases and generalization capabilities of s under the ICL paradigm. Code is available at https://github.com/adistomar/CS182-project.","title":"Can Transformers Break Encryption Schemes via In-Context Learning?"},{"location":"weekly_paper/2025-08-15/#agentic-ai-frameworks-architectures-protocols-and-design-challenges","text":"Authors: Hana Derouiche, Zaki Brahmi, Haithem Mazeni 2025-08-13 http://arxiv.org/abs/2508.10146v1 The emergence of Large Language Models ( s) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination. This paper provides a systematic review and comparative analysis of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural principles, mechanisms, memory management, safety guardrails, and alignment with service-oriented computing paradigms. Furthermore, we identify key limitations, emerging trends, and open challenges in the field. To address the issue of agent , we conduct an in-depth analysis of protocols such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network Protocol (ANP), and Agora. Our findings not only establish a foundational taxonomy for Agentic AI systems but also propose future research directions to enhance scalability, robustness, and interoperability. This work serves as a comprehensive reference for researchers and practitioners working to advance the next generation of autonomous AI systems.","title":"Agentic AI Frameworks Architectures, Protocols, and Design Challenges"},{"location":"weekly_paper/2025-08-15/#nested-reft-efficient-reinforcement-learning-for-large-language-model-fine-tuning-via-off-policy-rollouts","text":"Authors: Maxime Heuillet, Yufei Cui, Boxing Chen, Audrey Durand, Prasanna Parthasarathi 2025-08-13 http://arxiv.org/abs/2508.10123v1 Advanced reasoning in s on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance.","title":"Nested-ReFT Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts"},{"location":"weekly_paper/2025-08-15/#from-intent-to-execution-multimodal-chain-of-thought-reinforcement-learning-for-precise-cad-code-generation","text":"Authors: Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue 2025-08-13 http://arxiv.org/abs/2508.10118v1 Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models ( s) have made it possible to generate code from natural language, opening new opportunities for automating parametric 3D modeling. However, directly translating human design intent into executable CAD code remains highly challenging, due to the need for logical reasoning, syntactic correctness, and numerical precision. In this work, we propose CAD-RL, a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework for CAD modeling code generation. Our method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To ensure stable policy learning under and high-variance reward conditions, we introduce three targeted optimization strategies: Trust Region Stretch for improved exploration, Precision Token Loss for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce noisy supervision. To support training and benchmarking, we release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples with paired natural language and structured design language descriptions, executable CADQuery scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs.","title":"From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation"},{"location":"weekly_paper/2025-08-15/#constrained-decoding-of-diffusion-llms-with-context-free-grammars","text":"Authors: Niels M\u00fcndler, Jasper Dekoninck, Martin Vechev 2025-08-13 http://arxiv.org/abs/2508.10111v1 Large language models ( s) have shown promising performance across diverse domains. Many practical applications of s, such as code completion and structured data extraction, require adherence to syntactic constraints specified by a formal language. Yet, due to their probabilistic nature, output is not guaranteed to adhere to such formal languages. Prior work has proposed constrained as a means to restrict generation to particular formal languages. However, existing works are not applicable to the emerging paradigm of diffusion s, when used in practical scenarios such as the generation of formally correct C++ or JSON output. In this paper we address this challenge and present the first constrained method for diffusion models, one that can handle formal languages captured by context-free grammars. We begin by reducing constrained to the more general additive infilling problem, which asks whether a partial output can be completed to a valid word in the target language. This problem also naturally subsumes the previously unaddressed multi-region infilling constrained . We then reduce this problem to the task of deciding whether the intersection of the target language and a regular language is empty and present an efficient algorithm to solve it for context-free languages. Empirical results on various applications, such as C++ code infilling and structured data extraction in JSON, demonstrate that our method achieves near-perfect syntactic correctness while consistently pre or improving functional correctness. Importantly, our efficiency optimizations ensure that the computational overhead remains practical.","title":"Constrained Decoding of Diffusion LLMs with Context-Free Grammars"},{"location":"weekly_paper/2025-08-15/#language-of-persuasion-and-misrepresentation-in-business-communication-a-textual-detection-approach","text":"Authors: Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed 2025-08-13 http://arxiv.org/abs/2508.09935v1 Business digitisation has reorganised the process of persuasive discourse, which allows not only greater transparency but also advanced deception. This inquiry synthesises classical rhetoric and psychology with linguistic theory and empirical studies in the financial reporting, sustainability discourse, and digital marketing to explain how deceptive language can be systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater than 99% were achieved by using computational textual analysis as well as personalised models. However, reproducing this performance in multilingual settings is also problematic and, to a large extent, this is because it is not easy to find sufficient data, and because few multilingual text-processing infrastructures are in place. This evidence shows that there has been an increasing gap between the theoretical representations of and those empirically approximated, and therefore, there is a need to have strong automatic text-identification systems where AI-based discourse is becoming more realistic in communicating with humans.","title":"Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach"},{"location":"weekly_paper/2025-08-15/#memory-decoder-a-pretrained-plug-and-play-memory-for-large-language-models","text":"Authors: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin 2025-08-13 http://arxiv.org/abs/2508.09874v1 Large Language Models ( s) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small r that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.","title":"Memory Decoder A Pretrained, Plug-and-Play Memory for Large Language Models"},{"location":"weekly_paper/2025-08-15/#onevae-joint-discrete-and-continuous-optimization-helps-discrete-video-vae-train-better","text":"Authors: Yupeng Zhou, Zhen Li, Ziheng Ouyang, Yuming Chen, Ruoyi Du, Daquan Zhou, Bin Fu, Yihao Liu, Peng Gao, Ming-Ming Cheng, Qibin Hou 2025-08-13 http://arxiv.org/abs/2508.09857v1 Encoding videos into discrete tokens could align with text tokens to facilitate concise and unified multi-modal s, yet introducing significant spatiotemporal compression compared to continuous video representation. Previous discrete video VAEs experienced unstable training, long training time, and degraded reconstruction quality. Given the easier training and superior performance of continuous VAEs, an intuitive idea is to enhance discrete video VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between discrete and continuous representations, we found that FSQ could effectively preserve pre-trained continuous VAE priors compared to other methods. By leveraging continuous VAE priors, it converges several times faster than training from scratch and achieves superior performance at convergence. Meanwhile, two structural improvements are proposed. First, inspired by how continuous VAEs enhance reconstruction via enlarged latent dimensions, we introduce a multi-token mechanism, which achieves nearly a 1 dB improvement in PSNR without compromising the token compression ratio. Second, to tackle reconstruction challenges in high-compression video VAEs, we strengthen first-frame reconstruction, enabling the causal VAE to leverage this information in subsequent frames and markedly improving the performance of 4 x 16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous optimization scheme that unifies the two paradigms and, for the first time, achieves competitive performance on both continuous and discrete representations within a single network. We name our method OneVAE to reflect this connection.","title":"OneVAE Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better"},{"location":"weekly_paper/2025-08-15/#speed-always-wins-a-survey-on-efficient-architectures-for-large-language-models","text":"Authors: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng 2025-08-13 http://arxiv.org/abs/2508.09834v1 Large Language Models ( s) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern s, offer a strong baseline with excellent scaling properties. However, the traditional architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative architectures that address the inherent limitations of s and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sequence modeling methods, efficient full attention variants, mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion s. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.","title":"Speed Always Wins A Survey on Efficient Architectures for Large Language Models"},{"location":"weekly_paper/2025-08-15/#moiie-mixture-of-intra-and-inter-modality-experts-for-large-vision-language-models","text":"Authors: Dianyi Wang, Siyuan Wang, Zejun Li, Yikun Wang, Yitong Li, Duyu Tang, Xiaoyu Shen, Xuanjing Huang, Zhongyu Wei 2025-08-13 http://arxiv.org/abs/2508.09779v1 Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across multi-modal tasks by scaling model size and training data. However, these dense LVLMs incur significant computational costs and motivate the exploration of Mixture of Experts (MoE) architectures. While MoE improve parameter efficiency, effectively applying MoE to simultaneously model modality-specific features and cross-modal associations in LVLMs remains challenging. In this work, we propose to incorporate Mixture of Intra- and Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is guided by its modality, directing tokens to their respective intra-modality experts as well as a shared pool of inter-modality experts, enabling the model to jointly learn rich intra-modal features and cross-modal interactions. We further introduce an effective and straightforward two-stage training strategy, which facilitates the direct activation of both MoE and multi-modal capabilities. Extensive experiments across different data scales and backbone demonstrate the effectiveness, efficiency and generality of our approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters match or even surpass the performance of existing advanced open-source MoE- s based multi-modal models that involve more activated parameters. The code is available at https://github.com/AlenjandroWang/MoIIE.","title":"MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models"},{"location":"weekly_paper/2025-08-15/#meml-grpo-heterogeneous-multi-expert-mutual-learning-for-rlvr-advancement","text":"Authors: Weitao Jia, Jinghui Lu, Haiyang Yu, Siqi Wang, Guozhi Tang, An-Lan Wang, Weijie Yin, Dingkang Yang, Yuxiang Nie, Bin Shan, Hao Feng, Irene Li, Kun Yang, Han Wang, Jingqun Tang, Teng Fu, Changhong Jin, Chao Feng, Xiaohui Lv, Can Huang 2025-08-13 http://arxiv.org/abs/2508.09670v1 Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models ( s). However, standard RLVR faces challenges with reward , where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the model's performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods.","title":"MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement"},{"location":"weekly_paper/2025-08-15/#hiermoe-accelerating-moe-training-with-hierarchical-token-deduplication-and-expert-swap","text":"Authors: Wenxiang Lin, Xinglin Pan, Lin Zhang, Shaohuai Shi, Xuan Wang, Xiaowen Chu 2025-08-13 http://arxiv.org/abs/2508.09591v1 The ly activated mixture-of-experts (MoE) has become a common architecture for large language models ( s) due to its , which requires fewer computational demands while easily scaling the model size. In MoE models, each MoE layer requires to dynamically choose tokens to activate particular experts for computation while the activated experts may not be located in the same device or GPU as the token. However, this leads to substantial and load imbalances across all GPUs, which obstructs the scalability of distributed systems within a GPU cluster. To this end, we introduce HierMoE to accelerate the training of MoE models by two topology-aware techniques: 1) token deduplication to reduce the traffic, and 2) expert swap to balance the workloads among all GPUs. To enable the above two proposed approaches to be more general, we build theoretical models aimed at achieving the best token duplication and expert swap strategy under different model configurations and hardware environments. We implement our prototype HierMoE system atop Megatron-LM and conduct experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results show that our HierMoE achieves 1.55\\times to 3.32\\times faster and delivers 1.18\\times to 1.27\\times faster end-to-end training compared to state-of-the-art MoE training systems, Tutel-2DH, SmartMoE, and Megatron-LM.","title":"HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap"},{"location":"weekly_paper/2025-08-15/#neurontune-fine-grained-neuron-modulation-for-balanced-safety-utility-alignment-in-llms","text":"Authors: Birong Pan, Mayi Xu, Qiankun Pi, Jianhao Chen, Yuanyuan Zhu, Ming Zhong, Tieyun Qian 2025-08-13 http://arxiv.org/abs/2508.09473v1 Ensuring robust safety alignment while pre utility is critical for the reliable deployment of Large Language Models ( s). However, current techniques fundamentally suffer from intertwined deficiencies: insufficient robustness against malicious attacks, frequent refusal of benign queries, degradation in generated text quality and general task performance--the former two reflecting deficits in robust safety and the latter constituting utility impairment. We trace these limitations to the coarse-grained layer-wise interventions in existing methods. To resolve this, we propose NeuronTune, a fine-grained framework that dynamically modulates neurons to achieve simultaneous safety-utility optimization. Our approach first identifies safety-critical and utility-pre neurons across all layers via attribution, then employs meta-learning to adaptively amplify safety-neuron activations and suppress utility-neuron activations. Crucially, NeuronTune enables tunable adjustment of intervention scope via neuron-count thresholds, supporting flexible adaptation to security-critical or utility-priority scenarios. Extensive experimental results demonstrate that our method significantly outperforms existing state-of-the-art technologies, achieving superior model safety while maintaining excellent utility.","title":"NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs"},{"location":"weekly_paper/2025-08-15/#eggs-ptp-an-expander-graph-guided-structured-post-training-pruning-method-for-large-language-models","text":"Authors: Omar Bazarbachi, Zijun Sun, Yanning Shen 2025-08-13 http://arxiv.org/abs/2508.09471v1 As Large Language Models ( s) become more widely adopted and scale up in size, the computational and memory challenges involved in deploying these massive foundation models have grown increasingly severe. This underscores the urgent need to develop more efficient model variants. Faced with this challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided Structured Post-training Pruning method. The proposed approach leverages graph theory to guide the design of N:M structured , effectively reducing model size and computational demands. By incorporating concepts from expander graphs, EGGS-PTP ensures information flow within the pruned network, pre essential model functionality. Extensive numerical experiments demonstrate that EGGS-PTP not only achieves significant and memory savings due to structured but also outperforms existing structured techniques in terms of accuracy across various s.","title":"EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models"},{"location":"weekly_paper/2025-08-15/#gen-affect-generation-of-avatar-fine-grained-facial-expressions-with-consistent-identity","text":"Authors: Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal 2025-08-13 http://arxiv.org/abs/2508.09461v1 Different forms of customized 2D avatars are widely used in gaming applications, virtual , education, and content creation. However, existing approaches often fail to capture fine-grained facial expressions and struggle to preserve identity across different expressions. We propose GEN-AFFECT, a novel framework for personalized avatar generation that generates expressive and identity-consistent avatars with a diverse set of facial expressions. Our framework proposes conditioning a multimodal diffusion on an extracted identity-expression representation. This enables identity preservation and representation of a wide range of facial expressions. GEN-AFFECT additionally employs consistent attention at inference for information sharing across the set of generated expressions, enabling the generation process to maintain identity consistency over the array of generated fine-grained expressions. GEN-AFFECT demonstrates superior performance compared to previous state-of-the-art methods on the basis of the accuracy of the generated expressions, the preservation of the identity and the consistency of the target identity across an array of fine-grained facial expressions.","title":"Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy"},{"location":"weekly_paper/2025-08-15/#shadow-in-the-cache-unveiling-and-mitigating-privacy-risks-of-kv-cache-in-llm-inference","text":"Authors: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin 2025-08-13 http://arxiv.org/abs/2508.09442v1 The Key-Value ( ) , which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model ( ) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the - . We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of - privacy leakage issues. To mitigate this, we propose -Cloak, a novel, lightweight, and efficient defense mechanism. -Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the - . Our extensive experiments show that -Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy deployment.","title":"Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"},{"location":"weekly_paper/2025-08-15/#synaptic-pruning-a-biological-inspiration-for-deep-learning-regularization","text":"Authors: Gideon Vos, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi 2025-08-12 http://arxiv.org/abs/2508.09330v1 Synaptic in biological brains removes weak connections to improve efficiency. In contrast, dropout regularization in artificial neural networks randomly deactivates neurons without considering activity-dependent . We propose a magnitude-based synaptic method that better reflects biology by progressively removing low-importance connections during training. Integrated directly into the training loop as a dropout replacement, our approach computes weight importance from absolute magnitudes across layers and applies a cubic schedule to gradually increase global . At fixed intervals, masks permanently remove low-importance weights while maintaining gradient flow for active ones, eliminating the need for separate and fine-tuning phases. Experiments on multiple time series forecasting models including RNN, LSTM, and Patch Time Series Transformer across four datasets show consistent gains. Our method ranked best overall, with statistically significant improvements confirmed by Friedman tests (p < 0.01). In financial forecasting, it reduced Mean Absolute Error by up to 20% over models with no or standard dropout, and up to 52% in select models. This dynamic mechanism advances regularization by coupling weight elimination with progressive sparsification, offering easy integration into diverse architectures. Its strong performance, especially in financial time series forecasting, highlights its potential as a practical alternative to conventional dropout techniques.","title":"Synaptic Pruning A Biological Inspiration for Deep Learning Regularization"},{"location":"weekly_paper/2025-08-15/#sinllama-a-large-language-model-for-sinhala","text":"Authors: H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur 2025-08-12 http://arxiv.org/abs/2508.09115v1 Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models ( s). In this research, we extend an existing multilingual (Llama-3-8B) to better serve Sinhala. We enhance the tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first r-based open-source with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.","title":"SinLlama -- A Large Language Model for Sinhala"},{"location":"weekly_paper/2025-08-15/#reader-retrieval-assisted-drafter-for-efficient-llm-inference","text":"Authors: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi 2025-08-12 http://arxiv.org/abs/2508.09072v1 Large Language Models ( s) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient Inference), a novel lossless speculative method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value ( ) size during speculative and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup.","title":"READER Retrieval-Assisted Drafter for Efficient LLM Inference"},{"location":"weekly_paper/2025-08-15/#fetfids-a-feature-embedding-attention-based-federated-network-intrusion-detection-algorithm","text":"Authors: Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal 2025-08-12 http://arxiv.org/abs/2508.09056v1 Intrusion Detection Systems (IDS) have an increasingly important role in preventing exploitation of network vulnerabilities by malicious actors. Recent deep learning based developments have resulted in significant improvements in the performance of IDS systems. In this paper, we present FetFIDS, where we explore the employment of feature embedding instead of positional embedding to improve intrusion detection performance of a based deep learning system. Our model is developed with the aim of deployments in edge learning scenarios, where federated learning over multiple rounds can ensure both privacy and localized performance improvements. FetFIDS outperforms multiple state-of-the-art intrusion detection systems in a federated environment and demonstrates a high degree of suitability to federated learning. The code for this work can be found at https://github.com/ghosh64/fetfids.","title":"FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm"},{"location":"weekly_paper/2025-08-15/#a-survey-on-training-free-alignment-of-large-language-models","text":"Authors: Birong Pan, Yongqi Li, Weiyu Zhang, Wenpeng Lu, Mayi Xu, Shen Zhou, Yuanyuan Zhu, Ming Zhong, Tieyun Qian 2025-08-12 http://arxiv.org/abs/2508.09016v1 The alignment of large language models ( s) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, -time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining s, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre- , in- , and post- . For each stage, we provide a detailed examination from the viewpoint of s and multimodal s (M s), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable s.","title":"A Survey on Training-free Alignment of Large Language Models"},{"location":"weekly_paper/2025-08-15/#retrospective-sparse-attention-for-efficient-long-context-generation","text":"Authors: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim 2025-08-12 http://arxiv.org/abs/2508.09001v1 Large Language Models ( s) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value ( ) , whose memory footprint grows linearly with sequence length and dominates latency at each step. While recent compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long . In this paper, we introduce RetroAttention, a novel update technique that retrospectively revises past attention outputs using newly arrived entries from subsequent steps. By maintaining a lightweight output , RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) compression methods, increasing effective exposure by up to 1.6 \\times and accuracy by up to 21.9\\%.","title":"Retrospective Sparse Attention for Efficient Long-Context Generation"},{"location":"weekly_paper/2025-08-15/#nefmind-parameter-efficient-fine-tuning-of-open-source-llms-for-telecom-apis-automation","text":"Authors: Zainab Khan, Ahmed Hussain, Mukesh Thakur, Arto Hellas, Panos Papadimitratos 2025-08-12 http://arxiv.org/abs/2508.09240v1 The use of Service-Based Architecture in modern tele s has exponentially increased Network Functions (NFs) and Application Programming Interfaces (APIs), creating substantial operational complexities in service discovery and management. We introduce \\textit{NEFMind}, a framework leveraging parameter-efficient fine-tuning of open-source Large Language Models ( s) to address these challenges. It integrates three core components: synthetic dataset generation from Network Exposure Function (NEF) API specifications, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G Service-Based Architecture APIs, our approach achieves 85% reduction in overhead compared to manual discovery methods. Experimental validation using the open-source Phi-2 model demonstrates exceptional API call identification performance at 98-100% accuracy. The fine-tuned Phi-2 model delivers performance comparable to significantly larger models like GPT-4 while maintaining computational efficiency for tele s infrastructure deployment. These findings validate domain-specific, parameter-efficient strategies for managing complex API ecosystems in next-generation tele s networks.","title":"NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation"},{"location":"weekly_paper/2025-08-15/#colorgpt-leveraging-large-language-models-for-multimodal-color-recommendation","text":"Authors: Ding Xia, Naoto Inoue, Qianru Qiu, Kotaro Kikuchi 2025-08-12 http://arxiv.org/abs/2508.08987v1 Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating , improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models ( s) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained s serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our -based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.","title":"ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation"},{"location":"weekly_paper/2025-08-15/#aspd-unlocking-adaptive-serial-parallel-decoding-by-exploring-intrinsic-parallelism-in-llms","text":"Authors: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun 2025-08-12 http://arxiv.org/abs/2508.08895v2 The increasing scale and complexity of large language models ( s) pose significant inference latency challenges, primarily due to their autoregressive paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel ) can significantly improve the overall inference speed of s. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel , we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel modes while maintaining a reusable , maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.","title":"ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs"},{"location":"weekly_paper/2025-08-15/#steering-towards-fairness-mitigating-political-bias-in-llms","text":"Authors: Afrozah Nadeem, Mark Dras, Usman Naseem 2025-08-12 http://arxiv.org/abs/2508.08846v1 Recent advancements in large language models ( s) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. In this paper, we propose a framework for probing and mitigating such biases in r-based s through analysis of internal model representations. Grounded in the Political Compass Test (PCT), our method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that r s systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in s and offers a principled approach to debiasing beyond surface-level output interventions.","title":"Steering Towards Fairness Mitigating Political Bias in LLMs"},{"location":"weekly_paper/2025-08-15/#diffpose-animal-a-language-conditioned-diffusion-framework-for-animal-pose-estimation","text":"Authors: Tianyu Xiong, Dayi Tan, Wei Tian 2025-08-12 http://arxiv.org/abs/2508.08783v1 Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models ( s) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint r is designed to progressively refine pose predictions, improving robustness to occlusion and annotation . Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.","title":"DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation"},{"location":"weekly_paper/2025-08-15/#interpretable-reward-model-via-sparse-autoencoder","text":"Authors: Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang 2025-08-12 http://arxiv.org/abs/2508.08746v2 Large language models ( s) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of -based RM into an interpretable, , and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.","title":"Interpretable Reward Model via Sparse Autoencoder"},{"location":"weekly_paper/2025-08-15/#a-survey-on-parallel-text-generation-from-parallel-decoding-to-diffusion-language-models","text":"Authors: Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu 2025-08-12 http://arxiv.org/abs/2508.08712v2 As text generation has become a core capability of modern Large Language Models ( s), it underpins a wide range of downstream applications. However, most existing s rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.","title":"A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models"},{"location":"weekly_paper/2025-08-15/#prompt-and-check-using-large-language-models-to-evaluate-communication-protocol-compliance-in-simulation-based-training","text":"Authors: Vishakha Lall, Yisi Liu 2025-08-12 http://arxiv.org/abs/2508.08652v1 Accurate evaluation of procedural compliance is essential in simulation-based training, particularly in safety-critical domains where adherence to compliance checklists reflects operational competence. This paper explores a lightweight, deployable approach using prompt-based inference with open-source large language models ( s) that can run efficiently on consumer-grade GPUs. We present Prompt-and-Check, a method that uses context-rich prompts to evaluate whether each checklist item in a protocol has been fulfilled, solely based on transcribed verbal exchanges. We perform a case study in the maritime domain with participants performing an identical simulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and Mistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a prompt incorporating relevant transcript excerpts is fed into the model, which outputs a compliance judgment. We assess model outputs against expert-annotated ground truth using classification accuracy and agreement scores. Our findings demonstrate that prompting enables effective context-aware reasoning without task-specific training. This study highlights the practical utility of s in augmenting debriefing, performance feedback, and automated assessment in training environments.","title":"Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training"},{"location":"weekly_paper/2025-08-15/#classifier-language-models-unifying-sparse-finetuning-and-adaptive-tokenization-for-specialized-classification-tasks","text":"Authors: Adit Krishnan, Chu Wang, Chris Kong 2025-08-12 http://arxiv.org/abs/2508.08635v1 Semantic text classification requires the understanding of the contextual significance of specific tokens rather than surface-level patterns or keywords (as in rule-based or statistical text classification), making large language models ( s) well-suited for this task. However, semantic classification applications in industry, like customer intent detection or semantic role labeling, tend to be highly specialized. They require annotation by domain experts in contrast to general-purpose corpora for pretraining. Further, they typically require high inference throughputs which limits the model size from latency and cost perspectives. Thus, for a range of specialized classification tasks, the preferred solution is to develop customized classifiers by finetuning smaller language models (e.g., mini-encoders, small language models). In this work, we develop a token-driven finetuning strategy to adapt small language models to specialized classification tasks. We identify and finetune a small sensitive subset of model parameters by leveraging task-specific token constructs in the finetuning dataset, while leaving most of the pretrained weights unchanged. Unlike adapter approaches such as low rank adaptation (LoRA), we do not introduce additional parameters to the model. Our approach identifies highly relevant semantic tokens (case study in the Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and prefix tuning on five diverse semantic classification tasks. We achieve greater stability and half the training costs vs. end-to-end finetuning.","title":"Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks"},{"location":"weekly_paper/2025-08-15/#agrigpt-a-large-language-model-ecosystem-for-agriculture","text":"Authors: Bo Yang, Yu Zhang, Lanfei Feng, Yunkui Chen, Jianyu Zhang, Xiao Xu, Nueraili Aierken, Yurui Li, Yuxuan Chen, Guijun Yang, Yong He, Runhe Huang, Shijian Li 2025-08-12 http://arxiv.org/abs/2508.08632v1 Despite the rapid progress of Large Language Models ( s), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, retrieval, and multi-hop knowledge graph reasoning, thereby improving the 's reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose s on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized s. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research.","title":"AgriGPT a Large Language Model Ecosystem for Agriculture"},{"location":"weekly_paper/2025-08-15/#qoe-aware-service-provision-for-mobile-ar-rendering-an-agent-driven-approach","text":"Authors: Conghao Zhou, Lulu Sun, Xiucheng Wang, Peng Yang, Feng Lyu, Sihan Lu, Xuemin Shen 2025-08-12 http://arxiv.org/abs/2508.08627v1 Mobile augmented reality (MAR) is envisioned as a key immersive application in 6G, enabling virtual content rendering aligned with the physical environment through device pose estimation. In this paper, we propose a novel agent-driven service provisioning approach for edge-assisted MAR, aiming to reduce overhead between MAR devices and the edge server while ensuring the quality of experience (QoE). First, to address the inaccessibility of MAR application-specific information to the network controller, we establish a digital agent powered by large language models ( s) on behalf of the MAR service provider, bridging the data and function gap between the MAR service and network domains. Second, to cope with the user-dependent and dynamic nature of data traffic patterns for individual devices, we develop a user-level QoE modeling method that captures the relationship between resource demands and perceived user QoE, enabling personalized, agent-driven resource management. Trace-driven simulation results demonstrate that the proposed approach outperforms conventional -based QoE-aware service provisioning methods in both user-level QoE modeling accuracy and resource efficiency.","title":"QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach"},{"location":"weekly_paper/2025-08-15/#agentic-graph-neural-networks-for-wireless-communications-and-networking-towards-edge-general-intelligence-a-survey","text":"Authors: Yang Lu, Shengli Zhang, Chang Liu, Ruichen Zhang, Bo Ai, Dusit Niyato, Wei Ni, Xianbin Wang, Abbas Jamalipour 2025-08-12 http://arxiv.org/abs/2508.08620v1 The rapid advancement of technologies has driven the evolution of networks towards both high-dimensional resource utilization and multifunctional integration. This evolving complexity poses significant challenges in designing networks to satisfy the growing quality-of-service and time sensitivity of mobile applications in dynamic environments. Graph neural networks (GNNs) have emerged as fundamental deep learning (DL) models for complex networks. GNNs not only augment the extraction of features over network topologies but also enhance scalability and facilitate distributed computation. However, most existing GNNs follow a traditional passive learning framework, which may fail to meet the needs of increasingly diverse wireless systems. This survey proposes the employment of agentic artificial intelligence (AI) to organize and integrate GNNs, enabling scenario- and task-aware implementation towards edge general intelligence. To comprehend the full capability of GNNs, we holistically review recent applications of GNNs in wireless s and networking. Specifically, we focus on the alignment between graph representations and network topologies, and between neural architectures and wireless tasks. We first provide an overview of GNNs based on prominent neural architectures, followed by the concept of agentic GNNs. Then, we summarize and compare GNN applications for conventional systems and emerging technologies, including physical, MAC, and network layer designs, integrated sensing and (ISAC), reconfigurable intelligent surface (RIS) and cell-free network architecture. We further propose a large language model ( ) framework as an intelligent question-answering agent, leveraging this survey as a local knowledge base to enable GNN-related responses tailored to wireless research.","title":"Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey"},{"location":"weekly_paper/2025-08-15/#joint-decoding-method-for-controllable-contextual-speech-recognition-based-on-speech-llm","text":"Authors: Yangui Fang, Jing Peng, Yu Xi, Xu Li, Haoyu Li, Chengwei Zhang, Guohui Zhong, Kai Yu 2025-08-12 http://arxiv.org/abs/2508.08585v1 Contextual speech recognition refers to the ability to identify preferences for specific content based on contextual information. Recently, leveraging the contextual understanding capabilities of Speech to achieve contextual biasing by injecting contextual information through prompts have emerged as a research hotspot.However, the direct information injection method via prompts relies on the internal attention mechanism of the model, making it impossible to explicitly control the extent of information injection. To address this limitation, we propose a joint method to control the contextual information. This approach enables explicit control over the injected contextual information and achieving superior recognition performance. Additionally, Our method can also be used for sensitive word suppression recognition.Furthermore, experimental results show that even Speech not pre-trained on long contextual data can acquire long contextual capabilities through our method.","title":"Joint decoding method for controllable contextual speech recognition based on Speech LLM"},{"location":"weekly_paper/2025-08-15/#securing-agentic-ai-threat-modeling-and-risk-analysis-for-network-monitoring-agentic-ai-system","text":"Authors: Pallavi Zambare, Venkata Nikhil Thanikella, Ying Liu 2025-08-12 http://arxiv.org/abs/2508.10043v1 When combining Large Language Models ( s) with autonomous agents, used in network monitoring and decision-making systems, this will create serious security issues. In this research, the MAESTRO framework consisting of the seven layers threat modeling architecture in the system was used to expose, evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent system was constructed and implemented, using Python, LangChain, and telemetry in WebSockets, and deployed with inference, memory, parameter tuning, and anomaly detection modules. Two practical threat cases were confirmed as follows: (i) resource denial of service by traffic replay denial-of-service, and (ii) memory poisoning by tampering with the historical log file maintained by the agent. These situations resulted in measurable levels of performance degradation, i.e. telemetry updates were delayed, and computational loads were increased, as a result of poor system adaptations. It was suggested to use a multilayered defense-in-depth approach with memory isolation, validation of planners and anomaly response systems in real-time. These findings verify that MAESTRO is viable in operational threat mapping, prospective risk scoring, and the basis of the resilient system design. The authors bring attention to the importance of the enforcement of memory integrity, paying attention to the adaptation logic monitoring, and cross-layer protection that guarantee the agentic AI reliability in adversarial settings.","title":"Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System"},{"location":"weekly_paper/2025-08-15/#profiling-large-language-model-inference-on-apple-silicon-a-quantization-perspective","text":"Authors: Afsara Benazir, Felix Xiaozhu Lin 2025-08-12 http://arxiv.org/abs/2508.08531v1 A systematic understanding of Apple Silicon is lacking in the current landscape of hardware efficiency; research focus is largely centered on accelerating GPUs for large-scale training or inference on CUDA devices. This paper investigates Apple Silicon's unique memory architecture that offers a unified memory integrating CPU and GPU memory and its implications for on-device inference. We decipher myths about whether Apple Silicon is efficient for on-device inference compared to competitors such as NVIDIA GPUs by directly conducting latency and throughput comparison benchmarks. We explain the performance gap between them through profiling low level hardware metrics - ALU utilization, memory bandwidth, buffer usage, residency etc. at runtime. We draw several insights regarding performance bottlenecks such as de overhead, compute throughput and memory bandwidth. We debunk existing false claims regarding large language model inference such as compressing models to lower bit precision is a defacto promise for faster inference across all hardware platforms. We find that the large unified memory enables Apple Silicon to be both cost effective and efficient against NVIDIA GPUs for ultra large language models. Our large scale evaluation on 5 hardware testbeds incorporating three Apple M-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX A6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from 8B to 405B parameters and 14 schemes gives an understanding of how Apple Silicon fits within the paradigm of on-device inference. Our analysis reveals multiple resource interdependencies and unexpected findings, while also quantifying established insights. To the best of our knowledge, this study makes the first attempt to present a thorough characterization and analysis of Apple Silicon for on-device inference.","title":"Profiling Large Language Model Inference on Apple Silicon A Quantization Perspective"},{"location":"weekly_paper/2025-08-15/#using-llms-to-capture-users-temporal-context-for-recommendation","text":"Authors: Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher 2025-08-11 http://arxiv.org/abs/2508.08512v1 Effective recommender systems demand dynamic user understanding, especially in complex, evolving environments. Traditional user profiling often fails to capture the nuanced, temporal contextual factors of user preferences, such as transient short-term interests and enduring long-term tastes. This paper presents an assessment of Large Language Models ( s) for generating semantically rich, time-aware user profiles. We do not propose a novel end-to-end recommendation architecture; instead, the core contribution is a systematic investigation into the degree of effectiveness in capturing the dynamics of user context by disentangling short-term and long-term preferences. This approach, framing temporal preferences as dynamic user contexts for recommendations, adaptively fuses these distinct contextual components into comprehensive user embeddings. The evaluation across Movies&TV and Video Games domains suggests that while -generated profiles offer semantic depth and temporal structure, their effectiveness for context-aware recommendations is notably contingent on the richness of user interaction histories. Significant gains are observed in dense domains (e.g., Movies&TV), whereas improvements are less pronounced in environments (e.g., Video Games). This work highlights s' nuanced potential in enhancing user profiling for adaptive, context-aware recommendations, emphasizing the critical role of dataset characteristics for practical applicability.","title":"Using LLMs to Capture Users' Temporal Context for Recommendation"},{"location":"weekly_paper/2025-08-15/#when-the-domain-expert-has-no-time-and-the-llm-developer-has-no-clinical-expertise-real-world-lessons-from-llm-co-design-in-a-safety-net-hospital","text":"Authors: Avni Kothari, Patrick Vossler, Jean Digitale, Mohammad Forouzannia, Elise Rosenberg, Michele Lee, Jennee Bryant, Melanie Molina, James Marks, Lucas Zier, Jean Feng 2025-08-11 http://arxiv.org/abs/2508.08504v1 Large language models ( s) have the potential to address social and behavioral determinants of health by transforming labor intensive workflows in resource-constrained settings. Creating -based applications that serve the needs of underserved communities requires a deep understanding of their local context, but it is often the case that neither s nor their developers possess this local expertise, and the experts in these communities often face severe time/resource constraints. This creates a disconnect: how can one engage in meaningful co-design of an -based application for an under-resourced community when the channel between the developer and domain expert is constrained? We explored this question through a real-world case study, in which our data science team sought to partner with social workers at a safety net hospital to build an application that summarizes patients' social needs. Whereas prior works focus on the challenge of prompt tuning, we found that the most critical challenge in this setting is the careful and precise specification of \\what information to surface to providers so that the application is accurate, comprehensive, and verifiable. Here we present a novel co-design framework for settings with limited access to domain experts, in which the summary generation task is first decomposed into individually-optimizable attributes and then each attribute is efficiently refined and validated through a multi-tier cascading approach.","title":"When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital"},{"location":"weekly_paper/2025-08-15/#vector-centric-machine-learning-systems-a-cross-stack-approach","text":"Authors: Wenqi Jiang 2025-08-11 http://arxiv.org/abs/2508.08469v1 Today, two major trends are shaping the evolution of ML systems. First, modern AI systems are becoming increasingly complex, often integrating components beyond the model itself. A notable example is Retrieval-Augmented Generation (RAG), which incorporates not only multiple models but also vector databases, leading to heterogeneity in both system components and underlying hardware. Second, with the end of Moore's Law, achieving high system efficiency is no longer feasible without accounting for the rapid evolution of the hardware landscape. Building on the observations above, this thesis adopts a cross-stack approach to improving ML system efficiency, presenting solutions that span algorithms, systems, and hardware. First, it introduces several pioneering works about RAG efficiency across the computing stack. PipeRAG focuses on algorithm-level improvements, RAGO introduces system-level optimizations, and Chameleon explores heterogeneous accelerator systems for RAG. Second, this thesis investigates algorithm-hardware co-design for vector search. Specifically, FANNS and Falcon optimize -based and graph-based vector search, the two most popular paradigms of retrieval algorithms. Third, this thesis addresses the efficiency of recommender systems, another example of vector-centric ML systems, where the memory-intensive lookup operations on embedding vector tables often represent a major performance bottleneck. MicroRec and FleetRec propose solutions at the hardware and system levels, respectively, optimizing both data movement and computation to enhance the efficiency of large-scale recommender models.","title":"Vector-Centric Machine Learning Systems A Cross-Stack Approach"},{"location":"weekly_paper/2025-08-15/#architecting-long-context-llm-acceleration-with-packing-prefetch-scheduler-and-ultra-large-capacity-on-chip-memories","text":"Authors: Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu 2025-08-11 http://arxiv.org/abs/2508.08457v1 Long-context Large Language Model ( ) inference faces increasing compute bottlenecks as attention calculations scale with context length, primarily due to the growing - transfer overhead that saturates High Bandwidth Memory (HBM). While prefetching techniques mitigate misses by fetching data in advance, their spatial and temporal benefits present new opportunities to exploit. This work proposes a packing-prefetch scheduling architecture with monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with ultra-large on-chip capacity to accelerate long-context inference. Our optimizations demonstrate 8.06x speedup and 1.83x overall latency reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL memories over the serial execution. Evaluations of multi-request workloads on TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM bandwidth reduction compared to packing-only methods on Llama3.1-8B and Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL memories, our approach alleviates HBM constraints and enables efficient long-context inference.","title":"Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories"},{"location":"weekly_paper/2025-08-15/#overfill-two-stage-models-for-efficient-language-model-decoding","text":"Authors: Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed Abdelfattah, Alexander M. Rush 2025-08-11 http://arxiv.org/abs/2508.08446v1 Large language models ( s) excel across diverse tasks but face significant deployment challenges due to high inference costs. inference comprises (compute-bound) and (memory-bound) stages, with dominating latency particularly for long sequences. Current r-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for , processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during , OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.","title":"OverFill Two-Stage Models for Efficient Language Model Decoding"},{"location":"weekly_paper/2025-08-15/#selective-kv-cache-sharing-to-mitigate-timing-side-channels-in-llm-inference","text":"Authors: Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang 2025-08-11 http://arxiv.org/abs/2508.08438v1 Global - sharing has emerged as a key optimization for accelerating large language model ( ) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce Safe (Secure and Flexible Cache Sharing), a privacy-aware - management framework that selectively shares non-sensitive entries while confining sensitive content to private s. Safe comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that Safe mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, Safe improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse s and workloads. Safe reduces -induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high reuse efficiency, Safe reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for inference.","title":"Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference"},{"location":"weekly_paper/2025-08-15/#follow-your-shape-shape-aware-image-editing-via-trajectory-guided-region-control","text":"Authors: Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma 2025-08-11 http://arxiv.org/abs/2508.08134v2 While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly pre non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.","title":"Follow-Your-Shape Shape-Aware Image Editing via Trajectory-Guided Region Control"},{"location":"weekly_paper/2025-08-15/#blindguard-safeguarding-llm-based-multi-agent-systems-under-unknown-attacks","text":"Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang 2025-08-11 http://arxiv.org/abs/2508.08127v1 The security of -based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.","title":"BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"},{"location":"weekly_paper/2025-08-15/#teammedagents-enhancing-medical-decision-making-of-llms-through-structured-teamwork","text":"Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake 2025-08-11 http://arxiv.org/abs/2508.08115v1 We present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models ( s). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.'s \"Big Five\" model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop , and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the task's requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains.","title":"TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"},{"location":"weekly_paper/2025-08-15/#chatgpt-on-the-road-leveraging-large-language-model-powered-in-vehicle-conversational-agents-for-safer-and-more-enjoyable-driving-experience","text":"Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon 2025-08-11 http://arxiv.org/abs/2508.08101v1 Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal , lateral , and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of -powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions.","title":"ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience"},{"location":"weekly_paper/2025-08-15/#bridging-asr-and-llms-for-dysarthric-speech-recognition-benchmarking-self-supervised-and-generative-approaches","text":"Authors: Ahmed Aboeitta, Ahmed Sharshar, Youssef Nafea, Shady Shehata 2025-08-11 http://arxiv.org/abs/2508.08027v1 Speech Recognition (ASR) due to phoneme distortions and high variability. While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown promise, their effectiveness in dysarthric speech remains unclear. This study systematically benchmarks these models with different strategies, including CTC, seq2seq, and -enhanced (BART,GPT-2, Vicuna). Our contributions include (1) benchmarking ASR architectures for dysarthric speech, (2) introducing -based to improve intelligibility, (3) analyzing generalization across datasets, and (4) providing insights into recognition errors across severity levels. Findings highlight that -enhanced improves dysarthric ASR by leveraging linguistic constraints for phoneme restoration and grammatical correction.","title":"Bridging ASR and LLMs for Dysarthric Speech Recognition Benchmarking Self-Supervised and Generative Approaches"},{"location":"weekly_paper/2025-08-15/#interpreting-fedspeak-with-confidence-a-llm-based-uncertainty-aware-framework-guided-by-monetary-policy-transmission-paths","text":"Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang 2025-08-11 http://arxiv.org/abs/2508.08001v2 \"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an -based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal.","title":"Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"},{"location":"weekly_paper/2025-08-15/#ditvr-zero-shot-diffusion-transformer-for-video-restoration","text":"Authors: Sicheng Gao, Nancy Mehta, Zongwei Wu, Radu Timofte 2025-08-11 http://arxiv.org/abs/2508.07811v1 Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, pre high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions.","title":"DiTVR Zero-Shot Diffusion Transformer for Video Restoration"},{"location":"weekly_paper/2025-08-15/#evocot-overcoming-the-exploration-bottleneck-in-reinforcement-learning","text":"Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li 2025-08-11 http://arxiv.org/abs/2508.07809v1 Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models ( s) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes , limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger s for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration. We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables s to stably learn from initially unsolved hard problems under rewards. We apply EvoCoT to multiple families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables s to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.","title":"EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning"},{"location":"weekly_paper/2025-08-15/#grove-moe-towards-efficient-and-superior-moe-llms-with-adjugate-experts","text":"Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li 2025-08-11 http://arxiv.org/abs/2508.07785v1 The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models ( s). MoE models facilitate scalability by enabling parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter s developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.","title":"Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts"},{"location":"weekly_paper/2025-08-15/#sasst-leveraging-syntax-aware-chunking-and-llms-for-simultaneous-speech-translation","text":"Authors: Zeyu Yang, Lai Wei, Roman Koshkin, Xi Chen, Satoshi Nakamura 2025-08-11 http://arxiv.org/abs/2508.07781v1 This work proposes a grammar-based chunking strategy that segments input streams into semantically complete units by parsing dependency relations (e.g., noun phrase boundaries, verb-object structures) and punctuation features. The method ensures chunk coherence and minimizes semantic fragmentation. Building on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech Translation), an end-to-end framework integrating frozen Whisper encoder and r-only . The unified architecture dynamically outputs translation tokens or symbols to jointly optimize translation timing and content, with target-side reordering addressing word-order divergence. Experiments on CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation quality improvements across languages and validate the effectiveness of syntactic structures in -driven SimulST systems.","title":"SASST Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation"},{"location":"weekly_paper/2025-08-15/#symmetry-aware-transformer-training-for-automated-planning","text":"Authors: Markus Fritzsche, Elliot Gestrin, Jendrik Seipp 2025-08-11 http://arxiv.org/abs/2508.07743v1 While s excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art r-only , struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure s cannot efficiently learn from. We propose a novel contrastive learning objective to make s symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that s can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT.","title":"Symmetry-Aware Transformer Training for Automated Planning"},{"location":"weekly_paper/2025-08-15/#semantic-caching-for-low-cost-llm-serving-from-offline-learning-to-online-adaptation","text":"Authors: Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong 2025-08-11 http://arxiv.org/abs/2508.07675v2 Large Language Models ( s) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the , has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different eviction problem: one must account for mismatch costs between incoming queries and d responses. Moreover, key system parameters, such as query arrival probabilities and costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.","title":"Semantic Caching for Low-Cost LLM Serving From Offline Learning to Online Adaptation"},{"location":"weekly_paper/2025-08-15/#gliclass-generalist-lightweight-model-for-sequence-classification-tasks","text":"Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko 2025-08-11 http://arxiv.org/abs/2508.07662v1 Classification is one of the most widespread tasks in AI applications, often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative s have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data- conditions or from human feedback.","title":"GLiClass Generalist Lightweight Model for Sequence Classification Tasks"},{"location":"weekly_paper/2025-08-15/#lavieid-local-autoregressive-diffusion-transformers-for-identity-preserving-video-creation","text":"Authors: Wenhui Song, Hanhui Li, Jiehui Huang, Panwen Hu, Yuhao Cheng, Long Chen, Yiqiang Yan, Xiaodan Liang 2025-08-11 http://arxiv.org/abs/2508.07603v1 In this paper, we present LaVieID, a novel \\underline{l}ocal \\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework designed to tackle the challenging \\underline{id}entity-pre text-to-video task. The key idea of LaVieID is to mitigate the loss of identity information inherent in the stochastic global generation process of diffusion s (DiTs) from both spatial and temporal perspectives. Specifically, unlike the global and unstructured modeling of facial latent states in existing DiTs, LaVieID introduces a local router to explicitly represent latent states by weighted combinations of fine-grained local facial structures. This alleviates undesirable feature interference and encourages DiTs to capture distinctive facial characteristics. Furthermore, a temporal autoregressive module is integrated into LaVieID to refine denoised latent tokens before video . This module divides latent tokens temporally into chunks, exploiting their long-range temporal dependencies to predict biases for rectifying tokens, thereby significantly enhancing inter-frame identity consistency. Consequently, LaVieID can generate high-fidelity personalized videos and achieve state-of-the-art performance. Our code and models are available at https://github.com/ssugarwh/LaVieID.","title":"LaVieID Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation"},{"location":"weekly_paper/2025-08-15/#hgmf-a-hierarchical-gaussian-mixture-framework-for-scalable-tool-invocation-within-the-model-context-protocol","text":"Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han 2025-08-11 http://arxiv.org/abs/2508.07602v1 Invoking external tools enables Large Language Models ( s) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of s and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the . Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries.","title":"HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol"},{"location":"weekly_paper/2025-08-15/#towards-theoretical-understanding-of-transformer-test-time-computing-investigation-on-in-context-linear-regression","text":"Authors: Xingwu Chen, Miao Lu, Beining Wu, Difan Zou 2025-08-11 http://arxiv.org/abs/2508.07571v1 Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models.","title":"Towards Theoretical Understanding of Transformer Test-Time Computing Investigation on In-Context Linear Regression"},{"location":"weekly_paper/2025-08-15/#grounding-natural-language-for-multi-agent-decision-making-with-multi-agentic-llms","text":"Authors: Dom Huh, Prasant Mohapatra 2025-08-10 http://arxiv.org/abs/2508.07466v1 Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models ( s) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models ( s), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations.","title":"Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"},{"location":"weekly_paper/2025-08-15/#investigating-1-bit-quantization-in-transformer-based-top-tagging","text":"Authors: Saurabh Rai, Prisha, Jitendra Kumar 2025-08-10 http://arxiv.org/abs/2508.07431v1 The increasing scale of deep learning models in high-energy physics (HEP) has posed challenges to their deployment on low-power, latency-sensitive platforms, such as FPGAs and ASICs used in trigger systems, as well as in offline data reconstruction and processing pipelines. In this work, we introduce BitParT, a 1-bit Transformer-based architecture designed specifically for the top-quark tagging method. Building upon recent advances in ultra- large language models ( s), we extended these ideas to the HEP domain by developing a binary-weight variant (BitParT) of the Particle Transformer (ParT) model. Our findings indicate a potential for substantial reduction in model size and computational complexity, while maintaining high tagging performance. We benchmark BitParT on the public Top Quark Tagging Reference Dataset and show that it achieves competitive performance relative to its full-precision counterpart. This work demonstrates the design of extreme d models for physics applications, paving the way for real-time inference in collider experiments with minimal and optimized resource usage.","title":"Investigating 1-Bit Quantization in Transformer-Based Top Tagging"},{"location":"weekly_paper/2025-08-15/#let-us-long-event-text-understanding-of-scenes","text":"Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu 2025-08-10 http://arxiv.org/abs/2508.07401v1 Event cameras output event streams as , asynchronous data with microsecond-level temporal resolution, enabling visual perception with low latency and a high dynamic range. While existing Multimodal Large Language Models (M s) have achieved significant success in understanding and analyzing RGB video content, they either fail to interpret event streams effectively or remain constrained to very short sequences. In this paper, we introduce LET-US, a framework for long event-stream--text comprehension that employs an adaptive compression mechanism to reduce the volume of input events while pre critical visual details. LET-US thus establishes a new frontier in cross-modal inferential understanding over extended event sequences. To bridge the substantial modality gap between event streams and textual representations, we adopt a two-stage optimization paradigm that progressively equips our model with the capacity to interpret event-based scenes. To handle the voluminous temporal information inherent in long event streams, we leverage text-guided cross-modal queries for feature reduction, augmented by hierarchical clustering and similarity computation to distill the most representative event features. Moreover, we curate and construct a large-scale event-text aligned dataset to train our model, achieving tighter alignment of event features within the embedding space. We also develop a comprehensive benchmark covering a diverse set of tasks -- reasoning, captioning, classification, temporal localization and moment retrieval. Experimental results demonstrate that LET-US outperforms prior state-of-the-art M s in both descriptive accuracy and semantic comprehension on long-duration event streams. All datasets, codes, and models will be publicly available.","title":"LET-US Long Event-Text Understanding of Scenes"},{"location":"weekly_paper/2025-08-15/#efficient-edge-llms-deployment-via-hessianaware-quantization-and-cpu-gpu-collaborative","text":"Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang 2025-08-10 http://arxiv.org/abs/2508.07329v1 With the breakthrough progress of large language models ( s) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix , we achieve joint 8-bit of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the d model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.","title":"Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative"},{"location":"weekly_paper/2025-08-15/#bevanet-bilateral-efficient-visual-attention-network-for-real-time-semantic-segmentation","text":"Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang 2025-08-10 http://arxiv.org/abs/2508.07300v1 Real-time semantic segmentation presents the dual challenge of designing efficient architectures that capture large receptive fields for semantic understanding while also refining detailed contours. Vision s model long-range dependencies effectively but incur high computational cost. To address these challenges, we introduce the Large Kernel Attention (LKA) mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet) expands the receptive field to capture contextual information and extracts visual and structural features using Sparse Decomposed Large Separable Kernel Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism dynamically adapts the receptive field to further enhance performance. Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches contextual features by synergistically combining dilated convolutions and large kernel attention. The bilateral architecture facilitates frequent branch , and the Boundary Guided Adaptive Fusion (BGAF) module enhances boundary delineation by integrating spatial and semantic features under boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding 79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet pretraining, demonstrating state-of-the-art performance. The code and model is available at https://github.com/maomao0819/BEVANet.","title":"BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"},{"location":"weekly_paper/2025-08-15/#tasa-thermal-aware-3d-stacked-architecture-design-with-bandwidth-sharing-for-llm-inference","text":"Authors: Siyuan He, Peiran Yan, Yandong He, Youwei Zhuo, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07252v1 The autoregressive in s is the major inference bottleneck due to the memory-intensive operations and limited hardware bandwidth. 3D-stacked architecture is a promising solution with significantly improved memory bandwidth, which vertically stacked multi DRAM dies on top of logic die. However, our experiments also show the 3D-stacked architecture faces severer thermal issues compared to 2D architecture, in terms of thermal temperature, gradient and scalability. To better exploit the potential of 3D-stacked architecture, we present Tasa, a heterogeneous architecture with cross-stack thermal optimizations to balance the temperature distribution and maximize the performance under the thermal constraints. High-performance core is designed for compute-intensive operations, while high-efficiency core is used for memory-intensive operators, e.g. attention layers. Furthermore, we propose a bandwidth sharing scheduling to improve the bandwidth utilization in such heterogeneous architecture. Extensive thermal experiments show that our Tasa architecture demonstrates greater scalability compared with the homogeneous 3D-stacked architecture, i.e. up to 5.55 \\tccentigrade , 9.37 \\tccentigrade , and 7.91 \\tccentigrade peak temperature reduction for 48, 60, and 72 core configurations. Our experimental for Llama-65B and GPT-3 66B inferences also demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and state-of-the-art heterogeneous PIM-based accelerator","title":"Tasa Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference"},{"location":"weekly_paper/2025-08-15/#lp-spec-leveraging-lpddr-pim-for-efficient-llm-mobile-speculative-inference-with-architecture-dataflow-co-optimization","text":"Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07227v1 inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token and dynamic workload scheduling to accelerate speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits.","title":"LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization"},{"location":"weekly_paper/2025-08-15/#bridging-semantic-logic-gaps-a-cognition-inspired-multimodal-boundary-preserving-network-for-image-manipulation-localization","text":"Authors: Songlin Li, Zhiqing Guo, Yuanman Li, Zeyu Li, Yunfeng Diao, Gaobo Yang, Liejun Wang 2025-08-10 http://arxiv.org/abs/2508.07216v1 The existing image manipulation localization (IML) models mainly relies on visual cues, but ignores the semantic logical relationships between content features. In fact, the content semantics conveyed by real images often conform to human cognitive laws. However, image manipulation technology usually destroys the internal relationship between content features, thus leaving semantic clues for IML. In this paper, we propose a cognition-inspired multimodal boundary-pre network (CMB-Net). Specifically, CMB-Net utilizes large language models ( s) to analyze manipulated regions within images and generate prompt-based textual information to compensate for the lack of semantic relationships in the visual information. Considering that the erroneous texts induced by hallucination from s will damage the accuracy of IML, we propose an image-text central ambiguity module (ITCAM). It assigns weights to the text features by quantifying the ambiguity between text and image features, thereby ensuring the beneficial impact of textual information. We also propose an image-text interaction module (ITIM) that aligns visual and text features using a correlation matrix for fine-grained interaction. Finally, inspired by invertible neural networks, we propose a restoration edge r (RED) that mutually generates input and output features to preserve boundary information in manipulated regions without loss. Extensive experiments show that CMB-Net outperforms most existing IML models.","title":"Bridging Semantic Logic Gaps A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization"},{"location":"weekly_paper/2025-08-15/#dysk-attn-a-framework-for-efficient-real-time-knowledge-updating-in-large-language-models-via-dynamic-sparse-knowledge-attention","text":"Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan 2025-08-10 http://arxiv.org/abs/2508.07185v1 Large Language Models ( s) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables s to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a knowledge attention mechanism, which allows the to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building s that can stay current with the ever-changing world.","title":"DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"},{"location":"weekly_paper/2025-08-15/#how-effectively-can-large-language-models-connect-snp-variants-and-ecg-phenotypes-for-cardiovascular-risk-prediction","text":"Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak 2025-08-10 http://arxiv.org/abs/2508.07127v1 Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and ly annotated datasets remains a non-trivial task. Recently, s has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned s to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how s can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of s in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.","title":"How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?"},{"location":"weekly_paper/2025-08-15/#from-nodes-to-narratives-explaining-graph-neural-networks-with-llms-and-graph-context","text":"Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya 2025-08-09 http://arxiv.org/abs/2508.07117v1 Graph Neural Networks (GNNs) have emerged as powerful tools for learning over structured data, including text-attributed graphs, which are common in domains such as citation networks, social platforms, and knowledge graphs. GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language. In this work, we introduce LOGIC, a lightweight, post-hoc framework that uses large language models ( s) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure. This enables the to reason about GNN internal representations and produce natural language explanations along with concise explanation subgraphs. Our experiments across four real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off between fidelity and , while significantly improving human-centric metrics such as insightfulness. LOGIC sets a new direction for -based explainability in graph learning by aligning GNN internals with human reasoning.","title":"From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context"},{"location":"weekly_paper/2025-08-15/#large-language-model-evaluated-stand-alone-attention-assisted-graph-neural-network-with-spatial-and-structural-information-interaction-for-precise-endoscopic-image-segmentation","text":"Authors: Juntong Fan, Shuyi Fan, Debesh Jha, Changsheng Fang, Tieyong Zeng, Hengyong Yu, Dayang Wang 2025-08-09 http://arxiv.org/abs/2508.07028v1 Accurate endoscopic image segmentation on the polyps is critical for early colorectal cancer detection. However, this task remains challenging due to low contrast with surrounding mucosa, specular highlights, and indistinct boundaries. To address these challenges, we propose FOCUS-Med, which stands for Fusion of spatial and structural graph with attentional context-aware polyp segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph Convolutional Network (Dual-GCN) module to capture contextual spatial and topological structural dependencies. This graph-based representation enables the model to better distinguish polyps from background tissues by leveraging topological cues and spatial connectivity, which are often obscured in raw image intensities. It enhances the model's ability to preserve boundaries and delineate complex shapes typical of polyps. In addition, a location-fused stand-alone self-attention is employed to strengthen global context integration. To bridge the semantic gap between encoder- r layers, we incorporate a trainable weighted fast normalized fusion strategy for efficient multi-scale aggregation. Notably, we are the first to introduce the use of a Large Language Model ( ) to provide detailed qualitative evaluations of segmentation quality. Extensive experiments on public benchmarks demonstrate that FOCUS-Med achieves state-of-the-art performance across five key metrics, underscoring its effectiveness and clinical potential for AI-assisted colonoscopy.","title":"Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation"},{"location":"weekly_paper/2025-08-15/#vec2summ-text-summarization-via-probabilistic-sentence-embeddings","text":"Authors: Mao Li, Fred Conrad, Johann Gagnon-Bartsch 2025-08-09 http://arxiv.org/abs/2508.07017v1 We propose Vec2Summ, a novel method for abstractive summarization that frames the task as semantic compression. Vec2Summ represents a document collection using a single mean vector in the semantic embedding space, capturing the central meaning of the corpus. To reconstruct fluent summaries, we perform embedding inversion -- this mean vector into natural language using a generative language model. To improve reconstruction quality and capture some degree of topical variability, we introduce stochasticity by sampling from a Gaussian distribution centered on the mean. This approach is loosely analogous to bagging in ensemble learning, where controlled randomness encourages more robust and varied outputs. Vec2Summ addresses key limitations of -based summarization methods. It avoids context-length constraints, enables interpretable and controllable generation via semantic parameters, and scales efficiently with corpus size -- requiring only O(d + d^2) parameters. Empirical results show that Vec2Summ produces coherent summaries for topically focused, order-invariant corpora, with performance comparable to direct summarization in terms of thematic coverage and efficiency, albeit with less fine-grained detail. These results underscore Vec2Summ's potential in settings where scalability, semantic control, and corpus-level abstraction are prioritized.","title":"Vec2Summ Text Summarization via Probabilistic Sentence Embeddings"},{"location":"weekly_paper/2025-08-15/#narrative-memory-in-machines-multi-agent-arc-extraction-in-serialized-tv","text":"Authors: Roberto Balestri, Guglielmo Pescatore 2025-08-09 http://arxiv.org/abs/2508.07010v1 Serialized television narratives present significant analytical challenges due to their complex, temporally distributed storylines that necessitate sophisticated information management. This paper introduces a multi-agent system (MAS) designed to extract and analyze narrative arcs by implementing principles of computational memory architectures. The system conceptualizes narrative understanding through analogues of human memory: Large Language Models ( s) provide a form of semantic memory for general narrative patterns, while a vector database stores specific arc progressions as episodic memories. A multi-agent workflow simulates working memory processes to integrate these information types. Tested on the first season of Grey's Anatomy (ABC 2005-), the MAS identifies three arc types: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific. These arcs and their episodic developments are stored in a vector database, facilitating structured analysis and semantic comparison. To bridge automation with critical interpretation, a graphical interface enables human oversight and refinement of the system's narrative memory. While demonstrating strong performance in identifying Anthology Arcs and character entities, the system's reliance on textual paratexts (episode summaries) revealed limitations in discerning ping arcs and opaque dynamics, underscoring the challenges in computational memory consolidation versus human holistic understanding. This memory-centric approach highlights the potential of combining AI-driven memory processing with human expertise. Beyond television, it offers promise for serialized written formats where narrative is entirely text-based. Future work will focus on integrating multimodal inputs to enrich episodic memory, refining memory integration mechanisms within the MAS, and expanding testing across diverse genres.","title":"Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV"},{"location":"weekly_paper/2025-08-15/#ssd-offloading-for-llm-mixture-of-experts-weights-considered-harmful-in-energy-efficiency","text":"Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn 2025-08-09 http://arxiv.org/abs/2508.06978v1 Large Language Models ( s) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical stage of inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to ~12x compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude.","title":"SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency"},{"location":"weekly_paper/2025-08-15/#rethinking-1-bit-optimization-leveraging-pre-trained-large-language-models","text":"Authors: Zhijun Tu, Hanting Chen, Siqi Liu, Chuanjian Liu, Jian Li, Jie Hu, Yunhe Wang 2025-08-09 http://arxiv.org/abs/2508.06974v1 1-bit offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit s from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes direct adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the floating-point weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on s of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit s can be achieved using pre-trained models, eliminating the need for expensive training from scratch.","title":"Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models"},{"location":"weekly_paper/2025-08-15/#fed-mobillm-efficient-federated-llm-fine-tuning-over-heterogeneous-mobile-devices-via-server-assisted-side-tuning","text":"Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan 2025-08-09 http://arxiv.org/abs/2508.06765v1 Collaboratively fine-tuning (FT) large language models ( s) over heterogeneous mobile devices fosters immense potential applications of personalized intelligence. However, such a vision faces critical system challenges. Conventional federated FT approaches place prohibitive computational and memory burdens on mobile hardware, and their synchronous model aggregation protocols stall for slower devices. In this paper, we propose Fed Mobi , a novel design to facilitate efficient federated FT across mobile devices with diverse computing/ speeds and local model architectures. In particular, Fed Mobi implements a pioneering server-assisted federated side-tuning paradigm. Briefly, mobile devices perform lightweight forward propagation computations on local data using their frozen pre-scaled backbone s, and then upload selected intermediate activations. The server trains a shared side-network independently, eliminating client-side backpropagation and enabling asynchronous updates. To bridge model heterogeneity across different devices, we introduce an adaptive layer-wise feature alignment method, which ensures consistent representations for collaboratively tuning a shared side network. Extensive experimental results demonstrate that Fed Mobi can maintain robust fine-tuning performance while achieving extremely low on-device memory, with at least 95.2% reduction in computation overhead, 93.2% reduction in costs and 5.1x faster convergence compared to existing methods, validating its efficacy for practical adaptation over heterogeneous mobile devices.","title":"Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"},{"location":"weekly_paper/2025-08-15/#pushing-the-envelope-of-llm-inference-on-ai-pc","text":"Authors: Evangelos Georganas, Dhiraj Kalamkar, Alexander Heinecke 2025-08-08 http://arxiv.org/abs/2508.06753v1 The advent of ultra- models (1/1.58/2-bit), which match the perplexity and end-task performance of their full-precision counterparts using the same model size, is ushering in a new era of inference for resource-constrained environments such as edge devices and AI PCs. While these advances promise models that are more cost-effective in terms of latency, memory, throughput, and energy consumption, the computational efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp) used to deploy them remains underexplored. In this work, we take a bottom-up approach: we first design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, achieving peak computational efficiency across a variety of CPU platforms. We integrate these microkernels into a state-of-the-art inference framework, namely PyTorch-TPP, and present end-to-end inference results with 2-bit models that outperform the current SOTA runtime bitnet.cpp by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model inference. Our optimized runtime advances the state of inference on AI PCs and edge devices, paving the way for efficient deployment of ultra- models.","title":"Pushing the Envelope of LLM Inference on AI-PC"},{"location":"weekly_paper/2025-08-15/#ciso-species-distribution-modeling-conditioned-on-incomplete-species-observations","text":"Authors: Hager Radi Abdelwahed, M\u00e9lisande Teng, Robin Zbinden, Laura Pollock, Hugo Larochelle, Devis Tuia, David Rolnick 2025-08-08 http://arxiv.org/abs/2508.06704v1 Species distribution models (SDMs) are widely used to predict species' geographic distributions, as critical tools for ecological research and conservation planning. Typically, SDMs relate species occurrences to environmental variables representing abiotic factors, such as temperature, precipitation, and soil properties. However, species distributions are also strongly influenced by biotic interactions with other species, which are often overlooked. While some methods partially address this limitation by incorporating biotic interactions, they often assume symmetrical pairwise relationships between species and require consistent co-occurrence data. In practice, species observations are , and the availability of information about the presence or absence of other species varies significantly across locations. To address these challenges, we propose CISO, a deep learning-based method for species distribution modeling Conditioned on Incomplete Species Observations. CISO enables predictions to be conditioned on a flexible number of species observations alongside environmental variables, accommodating the variability and incompleteness of available biotic data. We demonstrate our approach using three datasets representing different species groups: sPlotOpen for plants, SatBird for birds, and a new dataset, SatButterfly, for butterflies. Our results show that including partial biotic information improves predictive performance on spatially separate test sets. When conditioned on a subset of species within the same dataset, CISO outperforms alternative methods in predicting the distribution of the remaining species. Furthermore, we show that combining observations from multiple datasets can improve performance. CISO is a promising ecological tool, capable of incorporating incomplete biotic information and identifying potential interactions between species from disparate taxa.","title":"CISO Species Distribution Modeling Conditioned on Incomplete Species Observations"},{"location":"weekly_paper/2025-08-22/","text":"2025-08-22 Table of Contents Communication Efficient LLM Pre-training with SparseLoCo Amortized In-Context Mixed Effect Transformer Models A Zero-Shot Approach for Pharmacokinetics Efficient Mixed-Precision Large Language Model Inference with TurboMind Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising LGMSNet Thinning a medical image segmentation model via dual-level multiscale fusion DiagECG An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization Exploring Scaling Laws of CTR Model for Online Performance Improvement MeSS City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion LongRecall A Structured Approach for Robust Recall Evaluation in Long-Form Text GasTwinFormer A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner Quantization Meets dLLMs A Systematic Study of Post-training Quantization for Diffusion LLMs MissionHD Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving Improving in-context learning with a better scoring function Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination Deep Skin Lesion Segmentation with Transformer-CNN Fusion Toward Intelligent Skin Cancer Analysis DeepTelecom A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications Taming Transformer for Emotion-Controllable Talking Face Generation SurveyGen-I Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing Your Reward Function for RL is Your Best PRM for Search Unifying RL and Search-Based TTS GLASS Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation Pixels to Play A Foundation Model for 3D Gameplay Measuring LLM Code Generation Stability via Structural Entropy Disentangling concept semantics via multilingual averaging in Sparse Autoencoders Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper GeoSAM2 Unleashing the Power of SAM2 for 3D Part Segmentation LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback LLMind 2.0 Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs Communication-Efficient Federated Learning with Adaptive Number of Participants CRISP Persistent Concept Unlearning via Sparse Autoencoders Interpreting the Interpreter Can We Model post-ECB Conferences Volatility with LLM Agents? A Comparative Study of Decoding Strategies in Medical Text Generation LLM-Enhanced Linear Autoencoders for Recommendation ALIGN Word Association Learning for Cross-Cultural Generalization in Large Language Models Datarus-R1 An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis Exploring Autonomous Agents A Closer Look at Why They Fail When Completing Tasks Communication Efficient LLM Pre-training with SparseLoCo Authors: Amir Sarfi, Benjamin Th\u00e9rien, Joel Lidin, Eugene Belilovsky 2025-08-21 http://arxiv.org/abs/2508.15706v1 Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models ( s) in bandwidth-constrained settings, such as across data centers and over the internet. Despite reducing frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While and error feedback are often applied to reduce the pseudo-gradient's size, in the context of pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited . In this work, we introduce SparseLoCo, a -efficient training algorithm for s that effectively leverages Top-k sparsification and to reach extreme compression ratios of up to 1-3% and 2-bit while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback combined with aggressive and that aggregation can actually improve model performance. We empirically demonstrate in a range of -constrained training settings that SparseLoCo provides significant benefits in both performance and cost. Amortized In-Context Mixed Effect Transformer Models A Zero-Shot Approach for Pharmacokinetics Authors: C\u00e9sar Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Niklas Hartung 2025-08-21 http://arxiv.org/abs/2508.15659v1 Accurate dose-response forecasting under sampling is central to precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect Transformer (AICMET) model, a -based latent-variable framework that unifies mechanistic compartmental priors with amortized in-context Bayesian inference. AICMET is pre-trained on hundreds of thousands of synthetic pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters of compartment models, endowing the model with strong inductive biases and enabling zero-shot adaptation to new compounds. At inference time, the r conditions on the collective context of previously profiled trial participants, generating calibrated posterior predictions for newly enrolled patients after a few early drug concentration measurements. This capability collapses traditional model-development cycles from weeks to hours while pre some degree of expert modelling. Experiments across public datasets show that AICMET attains state-of-the-art predictive accuracy and faithfully quantifies inter-patient variability -- outperforming both nonlinear mixed-effects baselines and recent neural ODE variants. Our results highlight the feasibility of -based, population-aware neural architectures as offering a new alternative for bespoke pharmacokinetic modeling pipelines, charting a path toward truly population-aware personalized dosing regimens. Efficient Mixed-Precision Large Language Model Inference with TurboMind Authors: Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen 2025-08-21 http://arxiv.org/abs/2508.15601v1 Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models ( s) by applying hybrid precision formats to model weights, activations, and s. This work introduces mixed-precision inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online , and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular s and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy. Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising Authors: Jin Ye, Jingran Wang, Fengchao Xiong, Jingzhou Chen, Yuntao Qian 2025-08-21 http://arxiv.org/abs/2508.15553v1 Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional coding (CSC) framework, we enforce shared 2D convolutional representation to ensure global spatial consistency across bands, while unshared 3D convolutional representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods. LGMSNet Thinning a medical image segmentation model via dual-level multiscale fusion Authors: Chengqi Dong, Fenghe Tang, Rongge Mao, Xinpei Gao, S. Kevin Zhou 2025-08-21 http://arxiv.org/abs/2508.15476v1 Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates -convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet. DiagECG An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization Authors: Jinning Yang, Wen Shi 2025-08-21 http://arxiv.org/abs/2508.15338v1 Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and module. These tokens are then used to extend the vocabulary of , allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into s for medical reasoning. Exploring Scaling Laws of CTR Model for Online Performance Improvement Authors: Weijiang Lai, Beihong Jin, Jiongyan Zhang, Yiyuan Zheng, Jian Dong, Jia Cheng, Jun Lei, Xingxing Wang 2025-08-21 http://arxiv.org/abs/2508.15326v1 CTR models play a vital role in improving user experience and boosting business revenue in many online personalized services. However, current CTR models generally encounter bottlenecks in performance improvement. Inspired by the scaling law phenomenon of s, we propose a new paradigm for improving CTR predictions: first, constructing a CTR model with accuracy scalable to the model grade and data size, and then distilling the knowledge implied in this model into its lightweight model that can serve online users. To put it into practice, we construct a CTR model named SUAN (Stacked Unified Attention Network). In SUAN, we propose the UAB as a behavior sequence encoder. A single UAB unifies the modeling of the sequential and non-sequential features and also measures the importance of each user behavior feature from multiple perspectives. Stacked UABs elevate the configuration to a high grade, paving the way for performance improvement. In order to benefit from the high performance of the high-grade SUAN and avoid the disadvantage of its long inference time, we modify the SUAN with self-attention and parallel inference strategies to form LightSUAN, and then adopt online distillation to train the low-grade LightSUAN, taking a high-grade SUAN as a teacher. The distilled LightSUAN has superior performance but the same inference time as the LightSUAN, making it well-suited for online deployment. Experimental results show that SUAN performs exceptionally well and holds the scaling laws spanning three orders of magnitude in model grade and data size, and the distilled LightSUAN outperforms the SUAN configured with one grade higher. More importantly, the distilled LightSUAN has been integrated into an online service, increasing the CTR by 2.81% and CPM by 1.69% while keeping the average inference time acceptable. Our source code is available at https://github.com/laiweijiang/SUAN. MeSS City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion Authors: Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, R\u00fcdiger Westermann, Konrad Schindler, Liqiu Meng 2025-08-21 http://arxiv.org/abs/2508.15169v1 Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques. LongRecall A Structured Approach for Robust Recall Evaluation in Long-Form Text Authors: MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei 2025-08-20 http://arxiv.org/abs/2508.15085v1 LongRecall. The completeness of machine-generated text, ensuring that it captures all relevant information, is crucial in domains such as medicine and law and in tasks like list-based question answering (QA), where omissions can have serious consequences. However, existing recall metrics often depend on lexical , leading to errors with unsubstantiated entities and paraphrased answers, while -as-a-Judge methods with long holistic prompts capture broader semantics but remain prone to misalignment and hallucinations without structured verification. We introduce LongRecall, a general three-stage recall evaluation framework that decomposes answers into self-contained facts, successively narrows plausible candidate matches through lexical and semantic filtering, and verifies their alignment through structured entailment checks. This design reduces false positives and false negatives while accommodating diverse phrasings and contextual variations, as a foundational building block for systematic recall assessment. We evaluate LongRecall on three challenging long-form QA benchmarks using both human annotations and -based judges, demonstrating substantial improvements in recall accuracy over strong lexical and -as-a-Judge baselines. GasTwinFormer A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging Authors: Toqi Tahamid Sarker, Mohamed Embaby, Taminul Islam, Amer AbuGhazaleh, Khaled R Ahmed 2025-08-20 http://arxiv.org/abs/2508.15057v1 Livestock methane emissions represent 32% of human-caused methane production, making automated monitoring critical for climate mitigation strategies. We introduce GasTwinFormer, a hybrid vision for real-time methane emission segmentation and dietary classification in optical gas imaging through a novel Mix Twin encoder alternating between spatially-reduced global attention and locally-grouped attention mechanisms. Our architecture incorporates a lightweight LR-ASPP r for multi-scale feature aggregation and enables simultaneous methane segmentation and dietary classification in a unified framework. We contribute the first comprehensive beef cattle methane emission dataset using OGI, containing 11,694 annotated frames across three dietary treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation while maintaining exceptional efficiency with only 3.348M parameters, 3.428G FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect dietary classification accuracy (100%), demonstrating the effectiveness of leveraging diet-emission correlations. Extensive ablation studies validate each architectural component, establishing GasTwinFormer as a practical solution for real-time livestock emission monitoring. Please see our project page at gastwinformer.github.io. Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner Authors: Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang 2025-08-20 http://arxiv.org/abs/2508.15044v1 Aligning large language models ( s) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling , which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency. Quantization Meets dLLMs A Systematic Study of Post-training Quantization for Diffusion LLMs Authors: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun 2025-08-20 http://arxiv.org/abs/2508.14896v1 Recent advances in diffusion large language models (d s) have introduced a promising alternative to autoregressive (AR) s for natural language generation tasks, leveraging full attention and denoising-based strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training (PTQ) has emerged as a widely adopted technique for compressing AR s, its applicability to d s remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to , as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the behavior of d s under different configurations. We hope our findings provide a foundation for future research in efficient d deployment. All codes and experimental setups will be released to support the community. MissionHD Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding Authors: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Mohsen Imani 2025-08-20 http://arxiv.org/abs/2508.14746v1 Reasoning graphs from Large Language Models ( s) are often misaligned with downstream visual tasks such as video anomaly detection (VAD). Existing Graph Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly optimizes graph structure using downstream task data, and propose MissionHD, a hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses an efficient encode- process to refine the graph, guided by the downstream task signal. Experiments on challenging VAD and VAR benchmarks show significant performance improvements when using our refined graphs, validating our approach as an effective pre-processing step. Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving Authors: Leila Cheshmi, Mennatullah Siam 2025-08-20 http://arxiv.org/abs/2508.14729v1 Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video s capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional rs that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics. Improving in-context learning with a better scoring function Authors: Omar Naim, Swarnadeep Bhar, J\u00e9r\u00f4me Bolte, Nicholas Asher 2025-08-20 http://arxiv.org/abs/2508.14685v1 Large language models ( s) exhibit a remarkable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this ability. In this paper, we examine these limitations on tasks involving first-order quantifiers such as {\\em all} and {\\em some}, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these constraints. To address this, we propose \\textbf{scaled signed averaging (SSA)}, a novel alternative to Softmax. Empirical results show that SSA dramatically improves performance on our target tasks. Furthermore, we evaluate both encoder-only and r-only s models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic probing tasks. Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination Authors: Jo\u00e3o Vitor de Carvalho Silva, Douglas G. Macharet 2025-08-20 http://arxiv.org/abs/2508.14635v1 The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models ( s) have shown strong capabilities in , planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of s in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Deep Skin Lesion Segmentation with Transformer-CNN Fusion Toward Intelligent Skin Cancer Analysis Authors: Xin Wang, Xiaopei Zhang, Xingang Wang 2025-08-20 http://arxiv.org/abs/2508.14509v1 This paper proposes a high-precision semantic segmentation method based on an improved TransUNet architecture to address the challenges of complex lesion structures, blurred boundaries, and significant scale variations in skin lesion images. The method integrates a module into the traditional encoder- r framework to model global semantic information, while retaining a convolutional branch to preserve local texture and edge features. This enhances the model's ability to perceive fine-grained structures. A boundary-guided attention mechanism and multi-scale upsampling path are also designed to improve lesion boundary localization and segmentation consistency. To verify the effectiveness of the approach, a series of experiments were conducted, including comparative studies, hyperparameter sensitivity analysis, data augmentation effects, input resolution variation, and training data split ratio tests. Experimental results show that the proposed model outperforms existing representative methods in mIoU, mDice, and mAcc, demonstrating stronger lesion recognition accuracy and robustness. In particular, the model achieves better boundary reconstruction and structural recovery in complex scenarios, making it well-suited for the key demands of automated segmentation tasks in skin lesion analysis. DeepTelecom A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications Authors: Bohao Wang, Zehua Jiang, Zhenyu Yang, Chongwen Huang, Yongliang Shen, Siming Jiang, Chen Zhu, Zhaohui Yang, Richeng Jin, Zhaoyang Zhang, Sami Muhaidat, Merouane Debbah 2025-08-20 http://arxiv.org/abs/2508.14507v1 Domain-specific datasets are the foundation for unleashing artificial intelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora are slow to produce, offer limited modeling fidelity, and cover only narrow scenario types. To address the challenges, we create DeepTelecom, a three-dimension (3D) digital-twin channel dataset. Specifically, a large language model ( )-assisted pipeline first builds the third level of details (LoD3) outdoor and indoor scenes with segmentable material-parameterizable surfaces. Then, DeepTelecom simulates full radio-wave propagation effects based on Sionna's ray-tracing engine. Leveraging GPU , DeepTelecom streams ray-path trajectories and real-time signal-strength heat maps, compiles them into high-frame-rate videos, and simultaneously outputs synchronized multi-view images, channel tensors, and multi-scale fading traces. By efficiently streaming large-scale, high-fidelity, and multimodal channel data, DeepTelecom not only furnishes a unified benchmark for wireless AI research but also supplies the domain-rich training substrate that enables foundation models to tightly fuse large model intelligence with future systems. Taming Transformer for Emotion-Controllable Talking Face Generation Authors: Ziqi Zhang, Cheng Deng 2025-08-20 http://arxiv.org/abs/2508.14359v1 Talking face generation is a novel and challenging generation task, aiming at synthesizing a vivid speaking-face video given a specific audio. To fulfill emotion-controllable talking face generation, current methods need to overcome two challenges: One is how to effectively model the multimodal relationship related to the specific emotion, and the other is how to leverage this relationship to synthesize identity pre emotional videos. In this paper, we propose a novel method to tackle the emotion-controllable talking face generation task discretely. Specifically, we employ two pre-training strategies to disentangle audio into independent components and videos into combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA) representation that integrates the emotional information into visual tokens. Finally, we introduce an autoregressive to model the global distribution of the visual tokens under the given conditions and further predict the index sequence for synthesizing the manipulated videos. We conduct experiments on the MEAD dataset that controls the emotion of videos conditioned on multiple emotional audios. Extensive experiments demonstrate the superiorities of our method both qualitatively and quantitatively. SurveyGen-I Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing Authors: Jing Chen, Zhiheng Yang, Yixian Shen, Jie Liu, Adam Belloum, Chrysa Papagainni, Paola Grosso 2025-08-20 http://arxiv.org/abs/2508.14317v1 Survey papers play a critical role in scientific by consolidating progress across a field. Recent advances in Large Language Models ( s) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing -based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage. Your Reward Function for RL is Your Best PRM for Search Unifying RL and Search-Based TTS Authors: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas 2025-08-19 http://arxiv.org/abs/2508.14313v1 Test-time scaling (TTS) for large language models ( s) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or -generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in s. GLASS Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation Authors: Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao 2025-08-19 http://arxiv.org/abs/2508.14302v1 Deploying Large Language Models ( s) on edge hardware demands aggressive, prompt-aware dynamic to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple s and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead. Pixels to Play A Foundation Model for 3D Gameplay Authors: Yuguang Yue, Chris Green, Samuel Hunt, Irakli Salia, Wenzhe Shi, Jonathan J Hunt 2025-08-19 http://arxiv.org/abs/2508.14295v1 We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A r-only with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control. Measuring LLM Code Generation Stability via Structural Entropy Authors: Yewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Rajput, Tegawende F. Bissyande, Jacques Klein 2025-08-19 http://arxiv.org/abs/2508.14288v1 Assessing the stability of code generation from large language models ( s) is essential for judging their reliability in real-world development. We extend prior \"structural-entropy concepts\" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural , and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading s on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit. Disentangling concept semantics via multilingual averaging in Sparse Autoencoders Authors: Cliff O'Reilly, Ernesto Jimenez-Ruiz, Tillman Weyde 2025-08-19 http://arxiv.org/abs/2508.14275v1 Connecting s with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B . Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy. Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper Authors: Krishna Garg, Firoz Shaikh, Sambaran Bandyopadhyay, Cornelia Caragea 2025-08-19 http://arxiv.org/abs/2508.14273v1 As researchers increasingly adopt s as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates s' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical , semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with -as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for -assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets. GeoSAM2 Unleashing the Power of SAM2 for 3D Part Segmentation Authors: Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao 2025-08-19 http://arxiv.org/abs/2508.14036v1 Modern 3D generation methods can rapidly create shapes from or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while pre global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training. LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback Authors: Henrik Voigt, Yurina Sugamiya, Kai Lawonn, Sina Zarrie\u00df, Atsuo Takanishi 2025-08-19 http://arxiv.org/abs/2508.13943v1 Objective Structured Clinical Examinations (OSCEs) are essential for medical training, but they require significant resources, including professional actors and expert medical feedback. Although Large Language Models ( s) have introduced text-based virtual patients for practice, these simulations often lack the capability for richer, non-textual interactions. This paper presents a novel framework that significantly enhances -based simulated patients by equipping them with action spaces, thereby enabling more realistic and dynamic patient behaviors that extend beyond text. Furthermore, our system incorporates virtual tutors that provide students with instant, personalized feedback on their performance at any time during these simulated encounters. We have conducted a rigorous evaluation of the framework's real-time performance, including system latency and component accuracy. Preliminary evaluations with medical experts assessed the naturalness and coherence of the simulated patients, as well as the usefulness and appropriateness of the virtual tutor's assessments. This innovative system provides medical students with a low-cost, accessible platform for personalized OSCE preparation at home. LLMind 2.0 Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents Authors: Yuyang Du, Qun Yang, Liujianfu Wang, Jingqi Lin, Hongwei Cui, Soung Chang Liew 2025-08-19 http://arxiv.org/abs/2508.13920v1 Recent advances in large language models ( s) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces ind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight -empowered device agents via natural language-based machine-to-machine (M2M) . Unlike previous -controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, ind 2.0 distributes intelligence across individual devices through lightweight s embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight s for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach. Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs Authors: Juncheng Xie, Hung-yi Lee 2025-08-19 http://arxiv.org/abs/2508.13805v1 Controlling the length of text produced by large language models ( s) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model \"writes while counting.\" We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or -based methods. Communication-Efficient Federated Learning with Adaptive Number of Participants Authors: Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov 2025-08-19 http://arxiv.org/abs/2508.13803v1 Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision s, real-world ECG classification, and training with gradient compression. Our results show consistent savings of up to 30\\% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning. CRISP Persistent Concept Unlearning via Sparse Autoencoders Authors: Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov 2025-08-19 http://arxiv.org/abs/2508.13650v1 As large language models ( s) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while pre model utility has become paramount. Recent work has explored autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two s and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while pre general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features. Interpreting the Interpreter Can We Model post-ECB Conferences Volatility with LLM Agents? Authors: Umberto Collodel 2025-08-19 http://arxiv.org/abs/2508.13635v1 This paper develops a novel method to simulate financial market reactions to European Central Bank (ECB) press conferences using a Large Language Model ( ). We create a behavioral, agent-based simulation of 30 synthetic traders, each with distinct risk preferences, cognitive biases, and interpretive styles. These agents forecast Euro interest rate swap levels at 3-month, 2-year, and 10-year maturities, with the variation across forecasts as a measure of market uncertainty or disagreement. We evaluate three prompting strategies, naive, few-shot (enriched with historical data), and an advanced iterative ' -as-a-Judge' framework, to assess the effect of prompt design on predictive performance. Even the naive approach generates a strong correlation (roughly 0.5) between synthetic disagreement and actual market outcomes, particularly for longer-term maturities. The -as-a-Judge framework further improves accuracy at the first iteration. These results demonstrate that -driven simulations can capture interpretive uncertainty beyond traditional measures, providing central banks with a practical tool to anticipate market reactions, refine strategies, and enhance financial stability. A Comparative Study of Decoding Strategies in Medical Text Generation Authors: Oriana Presacan, Alireza Nik, Vajira Thambawita, Bogdan Ionescu, Michael Riegler 2025-08-19 http://arxiv.org/abs/2508.13580v1 Large Language Models ( s) rely on various strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 strategies with medically specialized and general-purpose s of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\\eta} and top-k sampling perform worst. Slower methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to . Surprisingly, while medical s outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the strategy. These results highlight the need for careful selection of methods in medical applications, as their influence can sometimes exceed that of model choice. LLM-Enhanced Linear Autoencoders for Recommendation Authors: Jaewan Moon, Seongmin Park, Jongwuk Lee 2025-08-19 http://arxiv.org/abs/2508.13500v1 Large language models ( s) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of s into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from -derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art -enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025. ALIGN Word Association Learning for Cross-Cultural Generalization in Large Language Models Authors: Chunhua Liu, Kabir Manandhar Shrestha, Sukai Huang 2025-08-19 http://arxiv.org/abs/2508.13426v1 As large language models ( s) increasingly mediate cross-cultural , their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models. Datarus-R1 An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis Authors: Ayoub Ben Chaliah, Hela Dellagi 2025-08-18 http://arxiv.org/abs/2508.13382v1 We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring - reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned s. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by and tags. On demanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution. Exploring Autonomous Agents A Closer Look at Why They Fail When Completing Tasks Authors: Ruofan Lu, Yichen Li, Yintong Huo 2025-08-18 http://arxiv.org/abs/2508.13143v1 Autonomous agent systems powered by Large Language Models ( s) have demonstrated promising capabilities in automating complex tasks. However, current evaluations largely rely on success rates without systematically analyzing the interactions, mechanisms, and failure causes within these systems. To bridge this gap, we present a benchmark of 34 representative programmable tasks designed to rigorously assess autonomous agents. Using this benchmark, we evaluate three popular open-source agent frameworks combined with two backbones, ob a task completion rate of approximately 50%. Through in-depth failure analysis, we develop a three-tier taxonomy of failure causes aligned with task phases, highlighting planning errors, task execution issues, and incorrect response generation. Based on these insights, we propose actionable improvements to enhance agent planning and self-diagnosis capabilities. Our failure taxonomy, together with mitigation advice, provides an empirical foundation for developing more robust and effective autonomous agent systems in the future.","title":"2025-08-22"},{"location":"weekly_paper/2025-08-22/#2025-08-22","text":"","title":"2025-08-22"},{"location":"weekly_paper/2025-08-22/#table-of-contents","text":"Communication Efficient LLM Pre-training with SparseLoCo Amortized In-Context Mixed Effect Transformer Models A Zero-Shot Approach for Pharmacokinetics Efficient Mixed-Precision Large Language Model Inference with TurboMind Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising LGMSNet Thinning a medical image segmentation model via dual-level multiscale fusion DiagECG An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization Exploring Scaling Laws of CTR Model for Online Performance Improvement MeSS City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion LongRecall A Structured Approach for Robust Recall Evaluation in Long-Form Text GasTwinFormer A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner Quantization Meets dLLMs A Systematic Study of Post-training Quantization for Diffusion LLMs MissionHD Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving Improving in-context learning with a better scoring function Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination Deep Skin Lesion Segmentation with Transformer-CNN Fusion Toward Intelligent Skin Cancer Analysis DeepTelecom A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications Taming Transformer for Emotion-Controllable Talking Face Generation SurveyGen-I Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing Your Reward Function for RL is Your Best PRM for Search Unifying RL and Search-Based TTS GLASS Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation Pixels to Play A Foundation Model for 3D Gameplay Measuring LLM Code Generation Stability via Structural Entropy Disentangling concept semantics via multilingual averaging in Sparse Autoencoders Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper GeoSAM2 Unleashing the Power of SAM2 for 3D Part Segmentation LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback LLMind 2.0 Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs Communication-Efficient Federated Learning with Adaptive Number of Participants CRISP Persistent Concept Unlearning via Sparse Autoencoders Interpreting the Interpreter Can We Model post-ECB Conferences Volatility with LLM Agents? A Comparative Study of Decoding Strategies in Medical Text Generation LLM-Enhanced Linear Autoencoders for Recommendation ALIGN Word Association Learning for Cross-Cultural Generalization in Large Language Models Datarus-R1 An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis Exploring Autonomous Agents A Closer Look at Why They Fail When Completing Tasks","title":"Table of Contents"},{"location":"weekly_paper/2025-08-22/#communication-efficient-llm-pre-training-with-sparseloco","text":"Authors: Amir Sarfi, Benjamin Th\u00e9rien, Joel Lidin, Eugene Belilovsky 2025-08-21 http://arxiv.org/abs/2508.15706v1 Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models ( s) in bandwidth-constrained settings, such as across data centers and over the internet. Despite reducing frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While and error feedback are often applied to reduce the pseudo-gradient's size, in the context of pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited . In this work, we introduce SparseLoCo, a -efficient training algorithm for s that effectively leverages Top-k sparsification and to reach extreme compression ratios of up to 1-3% and 2-bit while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback combined with aggressive and that aggregation can actually improve model performance. We empirically demonstrate in a range of -constrained training settings that SparseLoCo provides significant benefits in both performance and cost.","title":"Communication Efficient LLM Pre-training with SparseLoCo"},{"location":"weekly_paper/2025-08-22/#amortized-in-context-mixed-effect-transformer-models-a-zero-shot-approach-for-pharmacokinetics","text":"Authors: C\u00e9sar Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Niklas Hartung 2025-08-21 http://arxiv.org/abs/2508.15659v1 Accurate dose-response forecasting under sampling is central to precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect Transformer (AICMET) model, a -based latent-variable framework that unifies mechanistic compartmental priors with amortized in-context Bayesian inference. AICMET is pre-trained on hundreds of thousands of synthetic pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters of compartment models, endowing the model with strong inductive biases and enabling zero-shot adaptation to new compounds. At inference time, the r conditions on the collective context of previously profiled trial participants, generating calibrated posterior predictions for newly enrolled patients after a few early drug concentration measurements. This capability collapses traditional model-development cycles from weeks to hours while pre some degree of expert modelling. Experiments across public datasets show that AICMET attains state-of-the-art predictive accuracy and faithfully quantifies inter-patient variability -- outperforming both nonlinear mixed-effects baselines and recent neural ODE variants. Our results highlight the feasibility of -based, population-aware neural architectures as offering a new alternative for bespoke pharmacokinetic modeling pipelines, charting a path toward truly population-aware personalized dosing regimens.","title":"Amortized In-Context Mixed Effect Transformer Models A Zero-Shot Approach for Pharmacokinetics"},{"location":"weekly_paper/2025-08-22/#efficient-mixed-precision-large-language-model-inference-with-turbomind","text":"Authors: Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen 2025-08-21 http://arxiv.org/abs/2508.15601v1 Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models ( s) by applying hybrid precision formats to model weights, activations, and s. This work introduces mixed-precision inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online , and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular s and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.","title":"Efficient Mixed-Precision Large Language Model Inference with TurboMind"},{"location":"weekly_paper/2025-08-22/#deep-equilibrium-convolutional-sparse-coding-for-hyperspectral-image-denoising","text":"Authors: Jin Ye, Jingran Wang, Fengchao Xiong, Jingzhou Chen, Yuntao Qian 2025-08-21 http://arxiv.org/abs/2508.15553v1 Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional coding (CSC) framework, we enforce shared 2D convolutional representation to ensure global spatial consistency across bands, while unshared 3D convolutional representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.","title":"Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising"},{"location":"weekly_paper/2025-08-22/#lgmsnet-thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion","text":"Authors: Chengqi Dong, Fenghe Tang, Rongge Mao, Xinpei Gao, S. Kevin Zhou 2025-08-21 http://arxiv.org/abs/2508.15476v1 Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates -convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.","title":"LGMSNet Thinning a medical image segmentation model via dual-level multiscale fusion"},{"location":"weekly_paper/2025-08-22/#diagecg-an-llm-driven-framework-for-diagnostic-reasoning-via-discretized-ecg-tokenization","text":"Authors: Jinning Yang, Wen Shi 2025-08-21 http://arxiv.org/abs/2508.15338v1 Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and module. These tokens are then used to extend the vocabulary of , allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into s for medical reasoning.","title":"DiagECG An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization"},{"location":"weekly_paper/2025-08-22/#exploring-scaling-laws-of-ctr-model-for-online-performance-improvement","text":"Authors: Weijiang Lai, Beihong Jin, Jiongyan Zhang, Yiyuan Zheng, Jian Dong, Jia Cheng, Jun Lei, Xingxing Wang 2025-08-21 http://arxiv.org/abs/2508.15326v1 CTR models play a vital role in improving user experience and boosting business revenue in many online personalized services. However, current CTR models generally encounter bottlenecks in performance improvement. Inspired by the scaling law phenomenon of s, we propose a new paradigm for improving CTR predictions: first, constructing a CTR model with accuracy scalable to the model grade and data size, and then distilling the knowledge implied in this model into its lightweight model that can serve online users. To put it into practice, we construct a CTR model named SUAN (Stacked Unified Attention Network). In SUAN, we propose the UAB as a behavior sequence encoder. A single UAB unifies the modeling of the sequential and non-sequential features and also measures the importance of each user behavior feature from multiple perspectives. Stacked UABs elevate the configuration to a high grade, paving the way for performance improvement. In order to benefit from the high performance of the high-grade SUAN and avoid the disadvantage of its long inference time, we modify the SUAN with self-attention and parallel inference strategies to form LightSUAN, and then adopt online distillation to train the low-grade LightSUAN, taking a high-grade SUAN as a teacher. The distilled LightSUAN has superior performance but the same inference time as the LightSUAN, making it well-suited for online deployment. Experimental results show that SUAN performs exceptionally well and holds the scaling laws spanning three orders of magnitude in model grade and data size, and the distilled LightSUAN outperforms the SUAN configured with one grade higher. More importantly, the distilled LightSUAN has been integrated into an online service, increasing the CTR by 2.81% and CPM by 1.69% while keeping the average inference time acceptable. Our source code is available at https://github.com/laiweijiang/SUAN.","title":"Exploring Scaling Laws of CTR Model for Online Performance Improvement"},{"location":"weekly_paper/2025-08-22/#mess-city-mesh-guided-outdoor-scene-generation-with-cross-view-consistent-diffusion","text":"Authors: Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, R\u00fcdiger Westermann, Konrad Schindler, Liqiu Meng 2025-08-21 http://arxiv.org/abs/2508.15169v1 Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.","title":"MeSS City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion"},{"location":"weekly_paper/2025-08-22/#longrecall-a-structured-approach-for-robust-recall-evaluation-in-long-form-text","text":"Authors: MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei 2025-08-20 http://arxiv.org/abs/2508.15085v1 LongRecall. The completeness of machine-generated text, ensuring that it captures all relevant information, is crucial in domains such as medicine and law and in tasks like list-based question answering (QA), where omissions can have serious consequences. However, existing recall metrics often depend on lexical , leading to errors with unsubstantiated entities and paraphrased answers, while -as-a-Judge methods with long holistic prompts capture broader semantics but remain prone to misalignment and hallucinations without structured verification. We introduce LongRecall, a general three-stage recall evaluation framework that decomposes answers into self-contained facts, successively narrows plausible candidate matches through lexical and semantic filtering, and verifies their alignment through structured entailment checks. This design reduces false positives and false negatives while accommodating diverse phrasings and contextual variations, as a foundational building block for systematic recall assessment. We evaluate LongRecall on three challenging long-form QA benchmarks using both human annotations and -based judges, demonstrating substantial improvements in recall accuracy over strong lexical and -as-a-Judge baselines.","title":"LongRecall A Structured Approach for Robust Recall Evaluation in Long-Form Text"},{"location":"weekly_paper/2025-08-22/#gastwinformer-a-hybrid-vision-transformer-for-livestock-methane-emission-segmentation-and-dietary-classification-in-optical-gas-imaging","text":"Authors: Toqi Tahamid Sarker, Mohamed Embaby, Taminul Islam, Amer AbuGhazaleh, Khaled R Ahmed 2025-08-20 http://arxiv.org/abs/2508.15057v1 Livestock methane emissions represent 32% of human-caused methane production, making automated monitoring critical for climate mitigation strategies. We introduce GasTwinFormer, a hybrid vision for real-time methane emission segmentation and dietary classification in optical gas imaging through a novel Mix Twin encoder alternating between spatially-reduced global attention and locally-grouped attention mechanisms. Our architecture incorporates a lightweight LR-ASPP r for multi-scale feature aggregation and enables simultaneous methane segmentation and dietary classification in a unified framework. We contribute the first comprehensive beef cattle methane emission dataset using OGI, containing 11,694 annotated frames across three dietary treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation while maintaining exceptional efficiency with only 3.348M parameters, 3.428G FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect dietary classification accuracy (100%), demonstrating the effectiveness of leveraging diet-emission correlations. Extensive ablation studies validate each architectural component, establishing GasTwinFormer as a practical solution for real-time livestock emission monitoring. Please see our project page at gastwinformer.github.io.","title":"GasTwinFormer A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging"},{"location":"weekly_paper/2025-08-22/#reward-shifted-speculative-sampling-is-an-efficient-test-time-weak-to-strong-aligner","text":"Authors: Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang 2025-08-20 http://arxiv.org/abs/2508.15044v1 Aligning large language models ( s) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling , which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.","title":"Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner"},{"location":"weekly_paper/2025-08-22/#quantization-meets-dllms-a-systematic-study-of-post-training-quantization-for-diffusion-llms","text":"Authors: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun 2025-08-20 http://arxiv.org/abs/2508.14896v1 Recent advances in diffusion large language models (d s) have introduced a promising alternative to autoregressive (AR) s for natural language generation tasks, leveraging full attention and denoising-based strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training (PTQ) has emerged as a widely adopted technique for compressing AR s, its applicability to d s remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to , as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the behavior of d s under different configurations. We hope our findings provide a foundation for future research in efficient d deployment. All codes and experimental setups will be released to support the community.","title":"Quantization Meets dLLMs A Systematic Study of Post-training Quantization for Diffusion LLMs"},{"location":"weekly_paper/2025-08-22/#missionhd-data-driven-refinement-of-reasoning-graph-structure-through-hyperdimensional-causal-path-encoding-and-decoding","text":"Authors: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Mohsen Imani 2025-08-20 http://arxiv.org/abs/2508.14746v1 Reasoning graphs from Large Language Models ( s) are often misaligned with downstream visual tasks such as video anomaly detection (VAD). Existing Graph Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly optimizes graph structure using downstream task data, and propose MissionHD, a hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses an efficient encode- process to refine the graph, guided by the downstream task signal. Experiments on challenging VAD and VAR benchmarks show significant performance improvements when using our refined graphs, validating our approach as an effective pre-processing step.","title":"MissionHD Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding"},{"location":"weekly_paper/2025-08-22/#multiscale-video-transformers-for-class-agnostic-segmentation-in-autonomous-driving","text":"Authors: Leila Cheshmi, Mennatullah Siam 2025-08-20 http://arxiv.org/abs/2508.14729v1 Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video s capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional rs that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics.","title":"Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving"},{"location":"weekly_paper/2025-08-22/#improving-in-context-learning-with-a-better-scoring-function","text":"Authors: Omar Naim, Swarnadeep Bhar, J\u00e9r\u00f4me Bolte, Nicholas Asher 2025-08-20 http://arxiv.org/abs/2508.14685v1 Large language models ( s) exhibit a remarkable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this ability. In this paper, we examine these limitations on tasks involving first-order quantifiers such as {\\em all} and {\\em some}, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these constraints. To address this, we propose \\textbf{scaled signed averaging (SSA)}, a novel alternative to Softmax. Empirical results show that SSA dramatically improves performance on our target tasks. Furthermore, we evaluate both encoder-only and r-only s models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic probing tasks.","title":"Improving in-context learning with a better scoring function"},{"location":"weekly_paper/2025-08-22/#can-llm-agents-solve-collaborative-tasks-a-study-on-urgency-aware-planning-and-coordination","text":"Authors: Jo\u00e3o Vitor de Carvalho Silva, Douglas G. Macharet 2025-08-20 http://arxiv.org/abs/2508.14635v1 The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models ( s) have shown strong capabilities in , planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of s in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.","title":"Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination"},{"location":"weekly_paper/2025-08-22/#deep-skin-lesion-segmentation-with-transformer-cnn-fusion-toward-intelligent-skin-cancer-analysis","text":"Authors: Xin Wang, Xiaopei Zhang, Xingang Wang 2025-08-20 http://arxiv.org/abs/2508.14509v1 This paper proposes a high-precision semantic segmentation method based on an improved TransUNet architecture to address the challenges of complex lesion structures, blurred boundaries, and significant scale variations in skin lesion images. The method integrates a module into the traditional encoder- r framework to model global semantic information, while retaining a convolutional branch to preserve local texture and edge features. This enhances the model's ability to perceive fine-grained structures. A boundary-guided attention mechanism and multi-scale upsampling path are also designed to improve lesion boundary localization and segmentation consistency. To verify the effectiveness of the approach, a series of experiments were conducted, including comparative studies, hyperparameter sensitivity analysis, data augmentation effects, input resolution variation, and training data split ratio tests. Experimental results show that the proposed model outperforms existing representative methods in mIoU, mDice, and mAcc, demonstrating stronger lesion recognition accuracy and robustness. In particular, the model achieves better boundary reconstruction and structural recovery in complex scenarios, making it well-suited for the key demands of automated segmentation tasks in skin lesion analysis.","title":"Deep Skin Lesion Segmentation with Transformer-CNN Fusion Toward Intelligent Skin Cancer Analysis"},{"location":"weekly_paper/2025-08-22/#deeptelecom-a-digital-twin-deep-learning-dataset-for-channel-and-mimo-applications","text":"Authors: Bohao Wang, Zehua Jiang, Zhenyu Yang, Chongwen Huang, Yongliang Shen, Siming Jiang, Chen Zhu, Zhaohui Yang, Richeng Jin, Zhaoyang Zhang, Sami Muhaidat, Merouane Debbah 2025-08-20 http://arxiv.org/abs/2508.14507v1 Domain-specific datasets are the foundation for unleashing artificial intelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora are slow to produce, offer limited modeling fidelity, and cover only narrow scenario types. To address the challenges, we create DeepTelecom, a three-dimension (3D) digital-twin channel dataset. Specifically, a large language model ( )-assisted pipeline first builds the third level of details (LoD3) outdoor and indoor scenes with segmentable material-parameterizable surfaces. Then, DeepTelecom simulates full radio-wave propagation effects based on Sionna's ray-tracing engine. Leveraging GPU , DeepTelecom streams ray-path trajectories and real-time signal-strength heat maps, compiles them into high-frame-rate videos, and simultaneously outputs synchronized multi-view images, channel tensors, and multi-scale fading traces. By efficiently streaming large-scale, high-fidelity, and multimodal channel data, DeepTelecom not only furnishes a unified benchmark for wireless AI research but also supplies the domain-rich training substrate that enables foundation models to tightly fuse large model intelligence with future systems.","title":"DeepTelecom A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications"},{"location":"weekly_paper/2025-08-22/#taming-transformer-for-emotion-controllable-talking-face-generation","text":"Authors: Ziqi Zhang, Cheng Deng 2025-08-20 http://arxiv.org/abs/2508.14359v1 Talking face generation is a novel and challenging generation task, aiming at synthesizing a vivid speaking-face video given a specific audio. To fulfill emotion-controllable talking face generation, current methods need to overcome two challenges: One is how to effectively model the multimodal relationship related to the specific emotion, and the other is how to leverage this relationship to synthesize identity pre emotional videos. In this paper, we propose a novel method to tackle the emotion-controllable talking face generation task discretely. Specifically, we employ two pre-training strategies to disentangle audio into independent components and videos into combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA) representation that integrates the emotional information into visual tokens. Finally, we introduce an autoregressive to model the global distribution of the visual tokens under the given conditions and further predict the index sequence for synthesizing the manipulated videos. We conduct experiments on the MEAD dataset that controls the emotion of videos conditioned on multiple emotional audios. Extensive experiments demonstrate the superiorities of our method both qualitatively and quantitatively.","title":"Taming Transformer for Emotion-Controllable Talking Face Generation"},{"location":"weekly_paper/2025-08-22/#surveygen-i-consistent-scientific-survey-generation-with-evolving-plans-and-memory-guided-writing","text":"Authors: Jing Chen, Zhiheng Yang, Yixian Shen, Jie Liu, Adam Belloum, Chrysa Papagainni, Paola Grosso 2025-08-20 http://arxiv.org/abs/2508.14317v1 Survey papers play a critical role in scientific by consolidating progress across a field. Recent advances in Large Language Models ( s) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing -based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.","title":"SurveyGen-I Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing"},{"location":"weekly_paper/2025-08-22/#your-reward-function-for-rl-is-your-best-prm-for-search-unifying-rl-and-search-based-tts","text":"Authors: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas 2025-08-19 http://arxiv.org/abs/2508.14313v1 Test-time scaling (TTS) for large language models ( s) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or -generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in s.","title":"Your Reward Function for RL is Your Best PRM for Search Unifying RL and Search-Based TTS"},{"location":"weekly_paper/2025-08-22/#glass-test-time-acceleration-for-llms-via-global-local-neural-importance-aggregation","text":"Authors: Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao 2025-08-19 http://arxiv.org/abs/2508.14302v1 Deploying Large Language Models ( s) on edge hardware demands aggressive, prompt-aware dynamic to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple s and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.","title":"GLASS Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation"},{"location":"weekly_paper/2025-08-22/#pixels-to-play-a-foundation-model-for-3d-gameplay","text":"Authors: Yuguang Yue, Chris Green, Samuel Hunt, Irakli Salia, Wenzhe Shi, Jonathan J Hunt 2025-08-19 http://arxiv.org/abs/2508.14295v1 We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A r-only with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control.","title":"Pixels to Play A Foundation Model for 3D Gameplay"},{"location":"weekly_paper/2025-08-22/#measuring-llm-code-generation-stability-via-structural-entropy","text":"Authors: Yewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Rajput, Tegawende F. Bissyande, Jacques Klein 2025-08-19 http://arxiv.org/abs/2508.14288v1 Assessing the stability of code generation from large language models ( s) is essential for judging their reliability in real-world development. We extend prior \"structural-entropy concepts\" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural , and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading s on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit.","title":"Measuring LLM Code Generation Stability via Structural Entropy"},{"location":"weekly_paper/2025-08-22/#disentangling-concept-semantics-via-multilingual-averaging-in-sparse-autoencoders","text":"Authors: Cliff O'Reilly, Ernesto Jimenez-Ruiz, Tillman Weyde 2025-08-19 http://arxiv.org/abs/2508.14275v1 Connecting s with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B . Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.","title":"Disentangling concept semantics via multilingual averaging in Sparse Autoencoders"},{"location":"weekly_paper/2025-08-22/#lets-use-chatgpt-to-write-our-paper-benchmarking-llms-to-write-the-introduction-of-a-research-paper","text":"Authors: Krishna Garg, Firoz Shaikh, Sambaran Bandyopadhyay, Cornelia Caragea 2025-08-19 http://arxiv.org/abs/2508.14273v1 As researchers increasingly adopt s as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates s' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical , semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with -as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for -assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.","title":"Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper"},{"location":"weekly_paper/2025-08-22/#geosam2-unleashing-the-power-of-sam2-for-3d-part-segmentation","text":"Authors: Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao 2025-08-19 http://arxiv.org/abs/2508.14036v1 Modern 3D generation methods can rapidly create shapes from or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while pre global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.","title":"GeoSAM2 Unleashing the Power of SAM2 for 3D Part Segmentation"},{"location":"weekly_paper/2025-08-22/#llm-powered-virtual-patient-agents-for-interactive-clinical-skills-training-with-automated-feedback","text":"Authors: Henrik Voigt, Yurina Sugamiya, Kai Lawonn, Sina Zarrie\u00df, Atsuo Takanishi 2025-08-19 http://arxiv.org/abs/2508.13943v1 Objective Structured Clinical Examinations (OSCEs) are essential for medical training, but they require significant resources, including professional actors and expert medical feedback. Although Large Language Models ( s) have introduced text-based virtual patients for practice, these simulations often lack the capability for richer, non-textual interactions. This paper presents a novel framework that significantly enhances -based simulated patients by equipping them with action spaces, thereby enabling more realistic and dynamic patient behaviors that extend beyond text. Furthermore, our system incorporates virtual tutors that provide students with instant, personalized feedback on their performance at any time during these simulated encounters. We have conducted a rigorous evaluation of the framework's real-time performance, including system latency and component accuracy. Preliminary evaluations with medical experts assessed the naturalness and coherence of the simulated patients, as well as the usefulness and appropriateness of the virtual tutor's assessments. This innovative system provides medical students with a low-cost, accessible platform for personalized OSCE preparation at home.","title":"LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback"},{"location":"weekly_paper/2025-08-22/#llmind-20-distributed-iot-automation-with-natural-language-m2m-communication-and-lightweight-llm-agents","text":"Authors: Yuyang Du, Qun Yang, Liujianfu Wang, Jingqi Lin, Hongwei Cui, Soung Chang Liew 2025-08-19 http://arxiv.org/abs/2508.13920v1 Recent advances in large language models ( s) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces ind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight -empowered device agents via natural language-based machine-to-machine (M2M) . Unlike previous -controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, ind 2.0 distributes intelligence across individual devices through lightweight s embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight s for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach.","title":"LLMind 2.0 Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents"},{"location":"weekly_paper/2025-08-22/#prompt-based-one-shot-exact-length-controlled-generation-with-llms","text":"Authors: Juncheng Xie, Hung-yi Lee 2025-08-19 http://arxiv.org/abs/2508.13805v1 Controlling the length of text produced by large language models ( s) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model \"writes while counting.\" We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or -based methods.","title":"Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs"},{"location":"weekly_paper/2025-08-22/#communication-efficient-federated-learning-with-adaptive-number-of-participants","text":"Authors: Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov 2025-08-19 http://arxiv.org/abs/2508.13803v1 Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision s, real-world ECG classification, and training with gradient compression. Our results show consistent savings of up to 30\\% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning.","title":"Communication-Efficient Federated Learning with Adaptive Number of Participants"},{"location":"weekly_paper/2025-08-22/#crisp-persistent-concept-unlearning-via-sparse-autoencoders","text":"Authors: Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov 2025-08-19 http://arxiv.org/abs/2508.13650v1 As large language models ( s) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while pre model utility has become paramount. Recent work has explored autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two s and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while pre general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.","title":"CRISP Persistent Concept Unlearning via Sparse Autoencoders"},{"location":"weekly_paper/2025-08-22/#interpreting-the-interpreter-can-we-model-post-ecb-conferences-volatility-with-llm-agents","text":"Authors: Umberto Collodel 2025-08-19 http://arxiv.org/abs/2508.13635v1 This paper develops a novel method to simulate financial market reactions to European Central Bank (ECB) press conferences using a Large Language Model ( ). We create a behavioral, agent-based simulation of 30 synthetic traders, each with distinct risk preferences, cognitive biases, and interpretive styles. These agents forecast Euro interest rate swap levels at 3-month, 2-year, and 10-year maturities, with the variation across forecasts as a measure of market uncertainty or disagreement. We evaluate three prompting strategies, naive, few-shot (enriched with historical data), and an advanced iterative ' -as-a-Judge' framework, to assess the effect of prompt design on predictive performance. Even the naive approach generates a strong correlation (roughly 0.5) between synthetic disagreement and actual market outcomes, particularly for longer-term maturities. The -as-a-Judge framework further improves accuracy at the first iteration. These results demonstrate that -driven simulations can capture interpretive uncertainty beyond traditional measures, providing central banks with a practical tool to anticipate market reactions, refine strategies, and enhance financial stability.","title":"Interpreting the Interpreter Can We Model post-ECB Conferences Volatility with LLM Agents?"},{"location":"weekly_paper/2025-08-22/#a-comparative-study-of-decoding-strategies-in-medical-text-generation","text":"Authors: Oriana Presacan, Alireza Nik, Vajira Thambawita, Bogdan Ionescu, Michael Riegler 2025-08-19 http://arxiv.org/abs/2508.13580v1 Large Language Models ( s) rely on various strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 strategies with medically specialized and general-purpose s of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\\eta} and top-k sampling perform worst. Slower methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to . Surprisingly, while medical s outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the strategy. These results highlight the need for careful selection of methods in medical applications, as their influence can sometimes exceed that of model choice.","title":"A Comparative Study of Decoding Strategies in Medical Text Generation"},{"location":"weekly_paper/2025-08-22/#llm-enhanced-linear-autoencoders-for-recommendation","text":"Authors: Jaewan Moon, Seongmin Park, Jongwuk Lee 2025-08-19 http://arxiv.org/abs/2508.13500v1 Large language models ( s) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of s into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from -derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art -enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.","title":"LLM-Enhanced Linear Autoencoders for Recommendation"},{"location":"weekly_paper/2025-08-22/#align-word-association-learning-for-cross-cultural-generalization-in-large-language-models","text":"Authors: Chunhua Liu, Kabir Manandhar Shrestha, Sukai Huang 2025-08-19 http://arxiv.org/abs/2508.13426v1 As large language models ( s) increasingly mediate cross-cultural , their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.","title":"ALIGN Word Association Learning for Cross-Cultural Generalization in Large Language Models"},{"location":"weekly_paper/2025-08-22/#datarus-r1-an-adaptive-multi-step-reasoning-llm-for-automated-data-analysis","text":"Authors: Ayoub Ben Chaliah, Hela Dellagi 2025-08-18 http://arxiv.org/abs/2508.13382v1 We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring - reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned s. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by and tags. On demanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.","title":"Datarus-R1 An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis"},{"location":"weekly_paper/2025-08-22/#exploring-autonomous-agents-a-closer-look-at-why-they-fail-when-completing-tasks","text":"Authors: Ruofan Lu, Yichen Li, Yintong Huo 2025-08-18 http://arxiv.org/abs/2508.13143v1 Autonomous agent systems powered by Large Language Models ( s) have demonstrated promising capabilities in automating complex tasks. However, current evaluations largely rely on success rates without systematically analyzing the interactions, mechanisms, and failure causes within these systems. To bridge this gap, we present a benchmark of 34 representative programmable tasks designed to rigorously assess autonomous agents. Using this benchmark, we evaluate three popular open-source agent frameworks combined with two backbones, ob a task completion rate of approximately 50%. Through in-depth failure analysis, we develop a three-tier taxonomy of failure causes aligned with task phases, highlighting planning errors, task execution issues, and incorrect response generation. Based on these insights, we propose actionable improvements to enhance agent planning and self-diagnosis capabilities. Our failure taxonomy, together with mitigation advice, provides an empirical foundation for developing more robust and effective autonomous agent systems in the future.","title":"Exploring Autonomous Agents A Closer Look at Why They Fail When Completing Tasks"},{"location":"weekly_paper/2025-08-29/","text":"2025-08-29 Table of Contents Discrete Diffusion VLA Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies Symphony A Decentralized Multi-Agent Framework for Scalable Collective Intelligence Optimal Remainder Estimates in the Quantization of Complex Projective Spaces Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust A Survey SoK Large Language Model Copyright Auditing via Fingerprinting The Return of Structural Handwritten Mathematical Expression Recognition Spotlight Attention Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models Hybrid Decoding Rapid Pass and Selective Detailed Correction for Sequence Models Survey of Specialized Large Language Model LFD Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation ReST-RL Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding Taming the Chaos Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference Towards 6G Intelligence The Role of Generative AI in Future Wireless Networks Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs Even Heads Fix Odd Errors Mechanistic Discovery and Surgical Repair in Transformer Attention One Joke to Rule them All? On the (Im)possibility of Generalizing Humor A Theory of Goal-Oriented Medium Access Protocol Design and Distributed Bandit Learning Random forest-based out-of-distribution detection for robust lung cancer segmentation APT-LLM Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices A Concurrent Modular Agent Framework for Autonomous LLM Agents Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI RoofSeg An edge-aware transformer-based network for end-to-end roof plane segmentation LLMs in the SOC An Empirical Study of Human-AI Collaboration in Security Operations Centres Enhancing Model Privacy in Federated Learning with Random Masking and Quantization pyFAST A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data ClusterFusion Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive STARec An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction UltraMemV2 Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning Rethinking Caching for LLM Serving Systems Beyond Traditional Heuristics Drawing2CAD Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks MUA-RL Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models History Rhymes Accelerating LLM Reinforcement Learning with RhymeRL Strata Hierarchical Context Caching for Long Context Language Model Serving LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning Backprompting Leveraging Synthetic Production Data for Health Advice Guardrails DualSparse-MoE Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction Flash Sparse Attention An Alternative Efficient Implementation of Native Sparse Attention Kernel Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios AdLoCo adaptive batching significantly improves communications efficiency and convergence for Large Language Models HLLM-Creator Hierarchical LLM-based Personalized Creative Generation The AI Data Scientist A.S.E A Repository-Level Benchmark for Evaluating Security in AI-Generated Code ILRe Intermediate Layer Retrieval for Context Compression in Causal Language Models LexSemBridge Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation Speculative Safety-Aware Decoding CMFDNet Cross-Mamba and Feature Discovery Network for Polyp Segmentation CATformer Contrastive Adversarial Transformer for Image Super-Resolution CoCoA Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models Discrete Diffusion VLA Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies Authors: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo 2025-08-27 http://arxiv.org/abs/2508.20072v1 Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA rs either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single- policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified r preserves pretrained vision language priors, supports parallel , breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action r supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Symphony A Decentralized Multi-Agent Framework for Scalable Collective Intelligence Authors: Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi 2025-08-27 http://arxiv.org/abs/2508.20019v1 Most existing Large Language Model ( )-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight s on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities. Optimal Remainder Estimates in the Quantization of Complex Projective Spaces Authors: Tommaso Aschieri, B\u0142a\u017cej Ruba, Jan Philip Solovej 2025-08-27 http://arxiv.org/abs/2508.19968v1 We study Berezin-Toeplitz of complex projective spaces \\mathbb{CP}^{d-1} and obtain full asymptotic expansions of the Berezin transformation and of products of Toeplitz operators. In each case, the remainder is controlled by the next term of the expansion, either through a positivity-pre transformation or via an operator inequality. This leads to bounds which are optimal in terms of the required regularity and feature sharp or asymptotically sharp constants. Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust A Survey Authors: Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang 2025-08-27 http://arxiv.org/abs/2508.19870v1 Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models ( s) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi- agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi- systems introduces critical security vulnerabilities, including insecure inter- s, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi- in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi- systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi- framework in EGI. We then survey key technical progress to facilitate zero-trust multi- systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi- systems, providing both theoretical foundations and practical strategies. SoK Large Language Model Copyright Auditing via Fingerprinting Authors: Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin 2025-08-27 http://arxiv.org/abs/2508.19843v1 The broad capabilities and substantial resources required to train Large Language Models ( s) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from s to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, ) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench. The Return of Structural Handwritten Mathematical Expression Recognition Authors: Jakob Seitz, Tobias Lengfeld, Radu Timofte 2025-08-27 http://arxiv.org/abs/2508.19773v1 Handwritten Mathematical Expression Recognition is foundational for educational technologies, enabling applications like digital note-taking and automated grading. While modern encoder- r architectures with large language models excel at LaTeX generation, they lack explicit symbol-to-trace alignment, a critical limitation for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates. This paper introduces a structural recognition approach with two innovations: 1 an automatic annotation system that uses a neural network to map LaTeX equations to raw traces, automatically generating annotations for symbol segmentation, classification, and spatial relations, and 2 a modular structural recognition system that independently optimizes segmentation, classification, and relation prediction. By leveraging a dataset enriched with structural annotations from our auto-labeling system, the proposed recognition system combines graph-based trace sorting, a hybrid convolutional-recurrent network, and -based correction to achieve competitive performance on the CROHME-2023 benchmark. Crucially, our structural recognition system generates a complete graph structure that directly links handwritten traces to predicted symbols, enabling transparent error analysis and interpretable outputs. Spotlight Attention Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval Authors: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji 2025-08-27 http://arxiv.org/abs/2508.19740v1 Reducing the key-value ( ) burden in Large Language Models ( s) significantly accelerates inference. Dynamically selecting critical s during helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in s. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5 \\times compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100 \\mu s on a single A100 GPU, with end-to-end throughput up to 3 \\times higher than vanilla . Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models Authors: Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo 2025-08-27 http://arxiv.org/abs/2508.19720v2 In Large Language Models ( s) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, algorithms, or locating and editing context-aware neurons to adapt s to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust s' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer s' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an without modifying the weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over s' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing s to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS. Hybrid Decoding Rapid Pass and Selective Detailed Correction for Sequence Models Authors: Yunkyu Lim, Jihwan Park, Hyung Yong Kim, Hanbin Lee, Byeong-Yeol Kim 2025-08-27 http://arxiv.org/abs/2508.19671v1 Recently, Transformer-based encoder- r models have demonstrated strong performance in multilingual speech recognition. However, the r's autoregressive nature and large size introduce significant bottlenecks during inference. Additionally, although rare, repetition can occur and negatively affect recognition accuracy. To tackle these challenges, we propose a novel Hybrid Decoding approach that both accelerates inference and alleviates the issue of repetition. Our method extends the encoder- r architecture by attaching a lightweight, fast r to the pretrained encoder. During inference, the fast r rapidly generates an output, which is then verified and, if necessary, selectively corrected by the Transformer r. This results in faster and improved robustness against repetitive errors. Experiments on the LibriSpeech and GigaSpeech test sets indicate that, with fine-tuning limited to the added r, our method achieves word error rates comparable to or better than the baseline, while more than doubling the inference speed. Survey of Specialized Large Language Model Authors: Chenghan Yang, Ruiyu Zhao, Yang Liu, Ling Jiang 2025-08-27 http://arxiv.org/abs/2508.19667v1 The rapid evolution of specialized large language models ( s) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized s, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through computation and , increasing integration of multimodal capabilities and so on are applied to recent agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose s in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field. LFD Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation Authors: Yang Sun, Lixin Zou, Dan Luo, Zhiyong Xie, Long Zhang, Liming Dong, Yunwei Zhao, Xixun Lin, Yanxiong Lu, Chenliang Li 2025-08-27 http://arxiv.org/abs/2508.19614v1 Retrieval-augmented generation (RAG) incorporates external knowledge into large language models ( s), improving their adaptability to downstream tasks and enabling information updates. Surprisingly, recent empirical evidence demonstrates that injecting noise into retrieved relevant documents paradoxically facilitates exploitation of external knowledge and improves generation quality. Although counterintuitive and challenging to apply in practice, this phenomenon enables granular control and rigorous analysis of how s integrate external knowledge. Therefore, in this paper, we intervene on noise injection and establish a layer-specific functional demarcation within the : shallow layers specialize in local context modeling, intermediate layers focus on integrating long-range external factual knowledge, and deeper layers primarily rely on parametric internal knowledge. Building on this insight, we propose Layer Fused Decoding (LFD), a simple strategy that directly combines representations from an intermediate layer with final-layer outputs to fully exploit the external factual knowledge. To identify the optimal intermediate layer, we introduce an internal knowledge score (IKS) criterion that selects the layer with the lowest IKS value in the latter half of layers. Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost. ReST-RL Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding Authors: Sining Zhoubian, Dan Zhang, Yuxiao Dong, Jie Tang 2025-08-27 http://arxiv.org/abs/2508.19576v1 With respect to improving the reasoning accuracy of s, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified RL paradigm that significantly improves 's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of policy has been improved, we further propose a test time optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When , the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the policy to achieve high reasoning accuracy. We validate the effectiveness of the proposed RL paradigm through extensive experiments on coding problems. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL. Taming the Chaos Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference Authors: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu 2025-08-27 http://arxiv.org/abs/2508.19559v1 Serving Large Language Models ( s) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) d architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between and stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D d . HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale and pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives. Towards 6G Intelligence The Role of Generative AI in Future Wireless Networks Authors: Muhammad Ahmed Mohsin, Junaid Ahmad, Muhammad Hamza Nawaz, Muhammad Ali Jamshed 2025-08-27 http://arxiv.org/abs/2508.19495v1 Ambient intelligence (AmI) is a computing paradigm in which physical environments are embedded with sensing, computation, and so they can perceive people and context, decide appropriate actions, and respond autonomously. Realizing AmI at global scale requires sixth generation (6G) wireless networks with capabilities for real time perception, reasoning, and action aligned with human behavior and mobility patterns. We argue that Generative Artificial Intelligence (GenAI) is the creative core of such environments. Unlike traditional AI, GenAI learns data distributions and can generate realistic samples, making it well suited to close key AmI gaps, including generating synthetic sensor and channel data in under observed areas, translating user intent into compact, semantic messages, predicting future network conditions for proactive control, and updating digital twins without compromising privacy. This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models, and generative s, and connects them to practical AmI use cases, including spectrum sharing, ultra reliable low latency , intelligent security, and context aware digital twins. We also examine how 6G enablers, such as edge and fog computing, IoT device swarms, intelligent reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate distributed GenAI. Finally, we outline open challenges in energy efficient on device training, trustworthy synthetic data, federated generative learning, and AmI specific standardization. We show that GenAI is not a peripheral addition, but a foundational element for transforming 6G from a faster network into an ambient intelligent ecosystem. Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs Authors: Yao Fu, Xianxuan Long, Runchao Li, Haotian Yu, Mu Sheng, Xiaotian Han, Yu Yin, Pan Li 2025-08-26 http://arxiv.org/abs/2508.19432v1 Quantization enables efficient deployment of large language models ( s) in resource-constrained environments by significantly reducing memory and computation costs. While d s often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness-whether generating truthful or deceptive responses-remains largely unexplored. In this work, we introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of d s across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Using this framework, we examine mainstream techniques (ranging from 4-bit to extreme 2-bit) across several open-source s. Surprisingly, we find that while d models retain internally truthful representations, they are more susceptible to producing false outputs under misleading prompts. To probe this vulnerability, we test 15 rephrased variants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that \"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\" and \"neutral\" prompts maintain stable outputs. Further, we reveal that d models \"know\" the truth internally yet still produce false outputs when guided by \"deceptive\" prompts via layer-wise probing and PCA visualizations. Our findings provide insights into future designs of -aware alignment and truthfulness interventions. Even Heads Fix Odd Errors Mechanistic Discovery and Surgical Repair in Transformer Attention Authors: Gustavo Sandoval 2025-08-26 http://arxiv.org/abs/2508.19414v1 We present a mechanistic case study of a format-dependent reasoning failure in Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger than \"9.8\" in chat or Q&A formats, but answers correctly in simple format. Through systematic intervention, we discover s implement even/odd attention head specialization: even indexed heads handle numerical comparison, while odd heads serve incompatible functions. The bug requires exactly 8 even heads at Layer 10 for perfect repair. Any combination of 8+ even heads succeeds, while 7 or fewer completely fails, revealing sharp computational thresholds with perfect redundancy among the 16 even heads. SAE analysis reveals the mechanism: format representations separate (10% feature at Layer 7), then re-entangle with different weightings (80% feature at Layer 10), with specific features showing 1.5x amplification in failing formats. We achieve perfect repair using only 25% of attention heads and identify a 60% pattern replacement threshold, demonstrating that apparent full-module requirements hide sophisticated substructure with implications for interpretability and efficiency. All of our code is available at https://github.com/gussand/surgeon. One Joke to Rule them All? On the (Im)possibility of Generalizing Humor Authors: Mor Turgeman, Chen Shani, Dafna Shahaf 2025-08-26 http://arxiv.org/abs/2508.19402v1 Humor is a broad and complex form of that remains challenging for machines. Despite its broadness, most existing research on computational humor traditionally focused on modeling a specific type of humor. In this work, we wish to understand whether competence on one or more specific humor tasks confers any ability to transfer to novel, unseen types; in other words, is this fragmentation inevitable? This question is especially timely as new humor types continuously emerge in online and social media contexts (e.g., memes, anti-humor, AI fails). If Large Language Models ( s) are to keep up with this evolving landscape, they must be able to generalize across humor types by capturing deeper, transferable mechanisms. To investigate this, we conduct a series of transfer learning experiments across four datasets, representing different humor tasks. We train s under varied diversity settings (1-3 datasets in training, testing on a novel task). Experiments reveal that models are capable of some transfer, and can reach up to 75% accuracy on unseen datasets; training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Further analysis suggests relations between humor types, with Dad Jokes surprisingly emerging as the best enabler of transfer (but is difficult to transfer to). We release data and code. A Theory of Goal-Oriented Medium Access Protocol Design and Distributed Bandit Learning Authors: Federico Chiariotti, Andrea Zanella 2025-08-26 http://arxiv.org/abs/2508.19141v1 The Goal-oriented Communication (GoC) paradigm breaks the separation between and the content of the data, tailoring decisions to the specific needs of the receiver and targeting application performance. While recent studies show impressive encoding performance in point-to-point scenarios, the multi-node distributed scenario is still almost unexplored. Moreover, the few studies to investigate this consider a centralized collision-free approach, where a central scheduler decides the transmission order of the nodes. In this work, we address the Goal-oriented Multiple Access (GoMA) problem, in which multiple intelligent agents must coordinate to share a wireless channel and avoid mutual interference. We propose a theoretical framework for the analysis and optimization of distributed GoMA, as a first step towards its complete characterization. We prove that the problem is non-convex and may admit multiple Nash Equilibrium (NE) solutions. We provide a characterization of each node's best response to others' strategies and propose an optimization approach that provably reaches one such NE, outperforming centralized approaches by up to 100% while also reducing energy consumption. We also design a distributed learning algorithm that operates with limited feedback and no prior knowledge. Random forest-based out-of-distribution detection for robust lung cancer segmentation Authors: Aneesh Rangnekar, Harini Veeraraghavan 2025-08-26 http://arxiv.org/abs/2508.19112v1 Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution r, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios. APT-LLM Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration Authors: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang 2025-08-26 http://arxiv.org/abs/2508.19087v1 Large language models ( s) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra- d s at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive scheme for arbitrary precision s, namely APT- . Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different architectures and precision settings. In inference, APT- achieves up to a 3.99 \\times speedup compared to FP16 baselines and a 2.16 \\times speedup over NVIDIA CUTLASS INT4 on RTX 3090. On RTX 4090 and H800, APT- achieves up to 2.44 \\times speedup over FP16 and 1.65 \\times speedup over CUTLASS integer baselines. Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices Authors: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu 2025-08-26 http://arxiv.org/abs/2508.19078v1 Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models ( s) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model , computation offloading, or expert . However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based s across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) -based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while pre accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy. A Concurrent Modular Agent Framework for Autonomous LLM Agents Authors: Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami 2025-08-26 http://arxiv.org/abs/2508.19042v1 We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model ( )-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an , inter-module , and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent. Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI Authors: Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores 2025-08-26 http://arxiv.org/abs/2508.19008v1 This study examines the capacity of large language models ( s) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients' life-story interviews, we compared three s (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p < 0.0001), and was judged as human by blinded experts. All scores strongly correlated (R > 0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias. RoofSeg An edge-aware transformer-based network for end-to-end roof plane segmentation Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li 2025-08-26 http://arxiv.org/abs/2508.19003v1 Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware -based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a encoder- r-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. LLMs in the SOC An Empirical Study of Human-AI Collaboration in Security Operations Centres Authors: Ronal Singh, Shahroz Tariq, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris, Martin Lochner 2025-08-26 http://arxiv.org/abs/2508.18947v1 The integration of Large Language Models ( s) into Security Operations Centres (SOCs) presents a transformative, yet still evolving, opportunity to reduce analyst workload through human-AI collaboration. However, their real-world application in SOCs remains underexplored. To address this gap, we present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over 10 months. Our analysis reveals that analysts use s as on-demand aids for sensemaking and context-building, rather than for making high-stakes determinations, pre analyst decision authority. The majority of queries are related to interpreting low-level telemetry (e.g., commands) and refining technical through short (1-3 turn) interactions. Notably, 93% of queries align with established cybersecurity competencies (NICE Framework), underscoring the relevance of use for SOC-related tasks. Despite variations in tasks and engagement, usage trends indicate a shift from occasional exploration to routine integration, with growing adoption and sustained use among a subset of analysts. We find that s function as flexible, on-demand cognitive aids that augment, rather than replace, SOC expertise. Our study provides actionable guidance for designing context-aware, human-centred AI assistance in security operations, highlighting the need for further in-the-wild research on real-world analyst- collaboration, challenges, and impacts. Enhancing Model Privacy in Federated Learning with Random Masking and Quantization Authors: Zhibo Xu, Jianhao Zhu, Jingwen Xu, Changze Lv, Zisu Huang, Xiaohua Wang, Muling Wu, Qi Qian, Xiaoqing Zheng, Xuanjing Huang 2025-08-26 http://arxiv.org/abs/2508.18911v2 The primary goal of traditional federated learning is to protect data privacy by enabling distributed edge devices to collaboratively train a shared global model while keeping raw data decentralized at local clients. The rise of large language models ( s) has introduced new challenges in distributed systems, as their substantial computational requirements and the need for specialized expertise raise critical concerns about protecting intellectual property (IP). This highlights the need for a federated learning approach that can safeguard both sensitive data and proprietary models. To tackle this challenge, we propose FedQSN, a federated learning approach that leverages random masking to obscure a subnetwork of model parameters and applies to the remaining parameters. Consequently, the server transmits only a privacy-pre proxy of the global model to clients during each round, thus enhancing the model's confidentiality. Experimental results across various models and tasks demonstrate that our approach not only maintains strong model performance in federated learning settings but also achieves enhanced protection of model parameters compared to baseline methods. pyFAST A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data Authors: Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu 2025-08-26 http://arxiv.org/abs/2508.18891v1 Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates -inspired architectures for the alignment-free fusion of data sources and offers native metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications. ClusterFusion Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive Authors: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo 2025-08-26 http://arxiv.org/abs/2508.18850v1 Large language model ( ) suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip . To bridge this software-hardware gap, we introduce two cluster-level primitives, ClusterReduce and ClusterGather, which abstract common patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules and computation jointly to expand operator fusion scope by composing stages such as Q Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion. STARec An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning Authors: Chenghao Wu, Ruiyang Ren, Junjie Zhang, Ruirui Wang, Zhongrui Ma, Qi Ye, Wayne Xin Zhao 2025-08-26 http://arxiv.org/abs/2508.18812v1 While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model ( )-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in -data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data. A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks Authors: Jiaqi Wu, Jing Liu, Yang Liu, Lixu Wang, Zehua Wang, Wei Chen, Zijian Tian, Richard Yu, Victor C. M. Leung 2025-08-26 http://arxiv.org/abs/2508.18803v1 The proliferation of Internet of things (IoT) devices in smart cities, transportation, healthcare, and industrial applications, coupled with the explosive growth of AI-driven services, has increased demands for efficient distributed computing architectures and networks, driving cloud-edge-terminal collaborative intelligence (CETCI) as a fundamental paradigm within the artificial intelligence of things (AIoT) community. With advancements in deep learning, large language models ( s), and edge computing, CETCI has made significant progress with emerging AIoT applications, moving beyond isolated layer optimization to deployable collaborative intelligence systems for AIoT (CISAIOT), a practical research focus in AI, distributed computing, and s. This survey describes foundational architectures, enabling technologies, and scenarios of CETCI paradigms, offering a tutorial-style review for CISAIOT beginners. We systematically analyze architectural components spanning cloud, edge, and terminal layers, examining core technologies including network virtualization, container orchestration, and software-defined networking, while presenting categorizations of collaboration paradigms that cover task offloading, resource allocation, and optimization across heterogeneous infrastructures. Furthermore, we explain intelligent collaboration learning frameworks by reviewing advances in federated learning, distributed deep learning, edge-cloud model evolution, and reinforcement learning-based methods. Finally, we discuss challenges (e.g., scalability, heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum computing, digital twin), highlighting how integration of distributed computing and can address open issues and guide development of robust, efficient, and secure collaborative AIoT systems. Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction Authors: Yilin Li, Xunjian Yin, Yilin Chen, Xiaojun Wan 2025-08-26 http://arxiv.org/abs/2508.18780v1 Grammatical error correction is a significant task in NLP. Traditional methods based on encoder- r models have achieved certain success, but the application of s in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train s to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \\textbf{state-of-the-art }performance, with a notable increase in \\textbf{recall}. This result clearly highlights the advantages of using RL to steer s, offering a more controllable and reliable paradigm for future development in GEC. UltraMemV2 Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning Authors: Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao 2025-08-26 http://arxiv.org/abs/2508.18756v1 While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient computation. Rethinking Caching for LLM Serving Systems Beyond Traditional Heuristics Authors: Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee 2025-08-26 http://arxiv.org/abs/2508.18736v1 Serving Large Language Models ( s) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix s neglect query semantics, while state-of-the-art semantic s remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for . SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71 \\times higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems. Drawing2CAD Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings Authors: Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu 2025-08-26 http://arxiv.org/abs/2508.18733v1 Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, pre geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual- r architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD. Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks Authors: Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota 2025-08-26 http://arxiv.org/abs/2508.18672v1 Empirical scaling laws have driven the evolution of large language models ( s), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new dimension that current dense-model frontiers overlook. We investigate how MoE influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top- k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top- k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as . Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal- . MUA-RL Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use Authors: Weikang Zhao, Xili Wang, Chengdi Ma, Lingbin Kong, Zhaohua Yang, Mingxiang Tuo, Xiaowei Shi, Yitao Zhai, Xunliang Cai 2025-08-26 http://arxiv.org/abs/2508.18669v1 With the recent rapid advancement of Agentic Intelligence, agentic tool use in s has become increasingly important. During multi-turn interactions between agents and users, the dynamic, uncertain, and stochastic nature of user demands poses significant challenges to the agent's tool invocation capabilities. Agents are no longer expected to simply call tools to deliver a result; rather, they must iteratively refine their understanding of user needs through while simultaneously invoking tools to resolve user queries. Existing reinforcement learning (RL) approaches for tool use lack the integration of genuinely dynamic users during the RL training process. To bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use), a novel reinforcement learning framework that, for the first time in the field of agentic tool use, integrates -simulated users into the reinforcement learning loop. MUA-RL aims to enable autonomous learning of models to communicate with users efficiently and use various tools to solve practical problems in dynamic multi-turn interactions. Evaluations are done on several multi-turn tool-using benchmarks (see Figure 1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2 Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench Agent -- outperforming or matching the performance of larger open-source models such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings. Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models Authors: Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu 2025-08-26 http://arxiv.org/abs/2508.18609v2 Large language models ( s) present significant deployment challenges due to their scale, with post-training (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse knowledge capabilities remains elusive, and existing scaling laws for d models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware strategies that can better preserve targeted cognitive functions. History Rhymes Accelerating LLM Reinforcement Learning with RhymeRL Authors: Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen 2025-08-26 http://arxiv.org/abs/2508.18588v1 With the rapid advancement of large language models ( s), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of s. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency. Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm. Strata Hierarchical Context Caching for Long Context Language Model Serving Authors: Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis 2025-08-26 http://arxiv.org/abs/2508.18572v1 Large Language Models ( s) with expanding context windows face significant performance hurdles. While caching key-value ( ) states is critical for avoiding redundant computation, the storage footprint of long-context s quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large d contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for -loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context . Strata introduces GPU-assisted I/O to combat fragmentation, decoupling GPU and CPU memory layouts and employs -aware request scheduling to balance compute with I/O latency and ping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to v + LMCache and 3.75x speedup over NVIDIA TensorRT- on long-context benchmarks, without degrading short-context performance. LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning Authors: Andr\u00e9 Quadros, Cassio Silva, Ronnie Alves 2025-08-25 http://arxiv.org/abs/2508.18420v1 This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational State as Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward state novelty, with an intrinsic reward approach derived from Large Language Models ( s). The s leverage their pre-trained knowledge to generate reward signals based on environment and goal descriptions, guiding the agent. We implemented this combined approach with an Actor-Critic (A2C) agent in the MiniGrid DoorKey environment, a benchmark for rewards. Our empirical results show that this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent, which failed to learn. Analysis of learning curves indicates that the combination effectively complements different aspects of the environment and task: VSIMR drives exploration of new states, while the -derived rewards facilitate progressive exploitation towards goals. Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning Authors: Jeong-seok Oh, Jay-yoon Lee 2025-08-25 http://arxiv.org/abs/2508.18395v1 Probabilistic in Large Language Models ( s) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks. We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture. Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats. Backprompting Leveraging Synthetic Production Data for Health Advice Guardrails Authors: Kellen Tan Cheng, Anna Lisa Gentile, Chad DeLuca, Guang-Jie Ren 2025-08-25 http://arxiv.org/abs/2508.18384v1 The pervasiveness of large language models ( s) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering s' input/output text through various detectors. However, developing and maintaining robust detectors faces many challenges, one of which is the difficulty in acquiring production-quality labeled data on real outputs prior to deployment. In this work, we propose backprompting, a simple yet intuitive solution to generate production-like labeled data for health advice guardrails development. Furthermore, we pair our backprompting method with a human-in-the-loop clustering technique to label the generated data. Our aim is to construct a parallel corpus roughly representative of the original dataset yet resembling real output. We then infuse existing datasets with our synthetic examples to produce robust training data for our detector. We test our technique in one of the most difficult and nuanced guardrails: the identification of health advice in output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.73%, despite having 400x less parameters. DualSparse-MoE Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction Authors: Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang 2025-08-25 http://arxiv.org/abs/2508.18376v1 Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models ( s) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, and activating only a subset for each input. While this improves efficiency, MoE still faces substantial challenges due to their massive computational scale and unpredictable activation patterns. To enable efficient MoE deployment, we identify dual at the tensor and neuron levels in pre-trained MoE modules as a key factor for both accuracy and efficiency. Unlike prior work that increases tensor-level through finer-grained expert design during pre-training, we introduce post-training expert partitioning to induce such without retraining. This preserves the mathematical consistency of model transformations and enhances both efficiency and accuracy in subsequent fine-tuning and inference. Building upon this, we propose DualSparse-MoE, an inference system that integrates dynamic tensor-level computation dropping with static neuron-level reconstruction to deliver significant efficiency gains with minimal accuracy loss. Experimental results show that enforcing an approximate 25% drop rate with our approach reduces average accuracy by only 0.08%-0.28% across three prevailing MoE models, while nearly all degrees of computation dropping consistently yield proportional computational speedups. Furthermore, incorporating load-imbalance awareness into expert parallelism achieves a 1.41x MoE module speedup with just 0.5% average accuracy degradation. Flash Sparse Attention An Alternative Efficient Implementation of Native Sparse Attention Kernel Authors: Ran Yan, Youhe Jiang, Binhang Yuan 2025-08-25 http://arxiv.org/abs/2508.18224v1 Recent progress in attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models ( s). Native Sparse Attention (NSA), a state-of-the-art approach, introduces natively trainable, hardware-aligned attention that delivers substantial system-level performance gains while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA relies on a query-grouping strategy that is efficient only with large Grouped Query Attention (GQA) sizes, whereas modern s typically adopt much smaller GQA groups, which limits the applicability of this algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), which includes an alternative kernel design that enables efficient NSA computation across a wide range of popular s with varied smaller GQA group sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5 \\times and on average 1.6 \\times kernel-level latency reduction, (ii) up to 1.25 \\times and 1.09 \\times on average end-to-end training speedup on state-of-the-art s, and (iii) up to 1.36 \\times and 1.11 \\times on average end-to-end speedup on state-of-the-art s. The source code is open-sourced and publicly available at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention. Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios Authors: Luana Bulla, Gabriele Tuccio, Misael Mongiov\u00ec, Aldo Gangemi 2025-08-25 http://arxiv.org/abs/2508.18183v1 Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of s for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, s lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in technologies for underrepresented linguistic communities. AdLoCo adaptive batching significantly improves communications efficiency and convergence for Large Language Models Authors: Nikolay Kutuzov, Makar Baderko, Stepan Kulibaba, Artem Dzhalilov, Daniel Bobrov, Maxim Mashtaler, Alexander Gasnikov 2025-08-25 http://arxiv.org/abs/2508.18182v1 Scaling distributed training of Large Language Models ( s) requires not only algorithmic advances but also efficient utilization of heterogeneous hardware resources. While existing methods such as DiLoCo have demonstrated promising results, they often fail to fully exploit computational clusters under dynamic workloads. To address this limitation, we propose a three-stage method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo, and switch mode mechanism. MIT allows individual nodes to run multiple lightweight training streams with different model instances in parallel and merge them to combine knowledge, increasing throughput and reducing idle time. Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance computation and , substantially lowering synchronization delays. Switch mode further stabilizes training by seamlessly introducing gradient accumulation once adaptive batch sizes grow beyond hardware-friendly limits. Together, these innovations improve both convergence speed and system efficiency. We also provide a theoretical estimate of the number of s required for the full convergence of a model trained using our method. HLLM-Creator Hierarchical LLM-based Personalized Creative Generation Authors: Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan 2025-08-25 http://arxiv.org/abs/2508.18118v1 AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators' inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a single product may have multiple selling points, with different users focusing on different features. This underscores the significant value of personalized, user-centric creative generation. Effective personalized content generation faces two main challenges: (1) accurately modeling user interests and integrating them into the content generation process while adhering to factual constraints, and (2) ensuring high efficiency and scalability to handle the massive user base in industrial scenarios. Additionally, the scarcity of personalized creative data in practice complicates model training, making data construction another key hurdle. We propose H -Creator, a hierarchical framework for efficient user interest modeling and personalized content generation. During inference, a combination of user clustering and a user-ad-matching-prediction based strategy is employed to significantly enhance generation efficiency and reduce computational overhead, making the approach suitable for large-scale deployment. Moreover, we design a data construction pipeline based on chain-of-thought reasoning, which generates high-quality, user-specific creative titles and ensures factual consistency despite limited personalized data. This pipeline serves as a critical foundation for the effectiveness of our model. Extensive experiments on personalized title generation for Douyin Search Ads show the effectiveness of H -Creator. Online A/B test shows a 0.476% increase on Adss, paving the way for more effective and efficient personalized generation in industrial scenarios. Codes for academic dataset are available at https://github.com/bytedance/H . The AI Data Scientist Authors: Farkhad Akimov, Munachiso Samuel Nwadike, Zangir Iklassov, Martin Tak\u00e1\u010d 2025-08-25 http://arxiv.org/abs/2508.18113v1 Imagine decision-makers uploading data and, within minutes, receiving clear, actionable insights delivered straight to their fingertips. That is the promise of the AI Data Scientist, an autonomous Agent powered by large language models ( s) that closes the gap between evidence and action. Rather than simply writing code or responding to prompts, it reasons through questions, tests ideas, and delivers end-to-end insights at a pace far beyond traditional workflows. Guided by the scientific tenet of the hypothesis, this Agent uncovers explanatory patterns in data, evaluates their statistical significance, and uses them to inform predictive modeling. It then translates these results into recommendations that are both rigorous and accessible. At the core of the AI Data Scientist is a team of specialized Subagents, each responsible for a distinct task such as data cleaning, statistical testing, validation, and plain-language . These Subagents write their own code, reason about causality, and identify when additional data is needed to support sound conclusions. Together, they achieve in minutes what might otherwise take days or weeks, enabling a new kind of interaction that makes deep data science both accessible and actionable. A.S.E A Repository-Level Benchmark for Evaluating Security in AI-Generated Code Authors: Keke Lian, Bin Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichuan Chen, Jie Wu, Haoling Li, Zhaoxuan Li, Jiongchi Yu, Hui Li, Dong Zhang 2025-08-25 http://arxiv.org/abs/2508.18106v1 The increasing adoption of large language models ( s) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, pre full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading s on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, fast-thinking'' ![key](https://img.shields.io/badge/decoding-F08080) strategies consistently outperform complex, slow-thinking'' reasoning for security patching. ILRe Intermediate Layer Retrieval for Context Compression in Causal Language Models Authors: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li 2025-08-25 http://arxiv.org/abs/2508.17892v1 Large Language Models ( s) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate r layer offline, encodes context by streaming chunked only up to that layer, and recalls tokens by the attention scores between the input query and full key in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the ing complexity from O(L^2) to O(L) , but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single 1M tokens request in less than half a minute (speedup \\approx 180\\times ) and scores RULER- 1M benchmark of \\approx 79.8 with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU. LexSemBridge Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation Authors: Shaoxiong Zhan, Hai Lin, Hongming Tan, Xiaodong Cai, Hai-Tao Zheng, Xin Su, Zifei Shan, Ruitong Liu, Hong-Gee Kim 2025-08-25 http://arxiv.org/abs/2508.17858v1 As queries in retrieval-augmented generation (RAG) pipelines powered by large language models ( s) become increasingly complex and diverse, dense retrieval models have demonstrated strong performance in semantic matching. Nevertheless, they often struggle with fine-grained retrieval tasks, where precise keyword alignment and span-level localization are required, even in cases with high lexical that would intuitively suggest easier retrieval. To systematically evaluate this limitation, we introduce two targeted tasks, keyword retrieval and part-of-passage retrieval, designed to simulate practical fine-grained scenarios. Motivated by these observations, we propose LexSemBridge, a unified framework that enhances dense query representations through fine-grained, input-aware vector modulation. LexSemBridge constructs latent enhancement vectors from input tokens using three paradigms: Statistical (SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense embeddings via element-wise interaction. Theoretically, we show that this modulation preserves the semantic direction while selectively amplifying discriminative dimensions. LexSemBridge operates as a plug-in without modifying the backbone encoder and naturally extends to both text and vision modalities. Extensive experiments across semantic and fine-grained retrieval tasks validate the effectiveness and generality of our approach. All code and models are publicly available at https://github.com/Jasaxion/LexSemBridge/ Speculative Safety-Aware Decoding Authors: Xuekang Wang, Shengyu Zhu, Xueqi Cheng 2025-08-25 http://arxiv.org/abs/2508.17739v1 Despite extensive efforts to align Large Language Models ( s) with human values and safety rules, jailbreak attacks that exploit certain vulnerabilities continuously emerge, highlighting the need to strengthen existing s with additional safety properties to defend against these attacks. However, tuning large models has become increasingly resource-intensive and may have difficulty ensuring consistent performance. We introduce Speculative Safety-Aware Decoding (SSD), a lightweight -time approach that equips s with the desired safety property while accelerating inference. We assume that there exists a small language model that possesses this desired property. SSD integrates speculative sampling during and leverages the match ratio between the small and composite models to quantify jailbreak risks. This enables SSD to dynamically switch between schemes to prioritize utility or safety, to handle the challenge of different model capacities. The output token is then sampled from a new distribution that combines the distributions of the original and the small models. Experimental results show that SSD successfully equips the large model with the desired safety property, and also allows the model to remain helpful to benign queries. Furthermore, SSD accelerates the inference time, thanks to the speculative sampling design. CMFDNet Cross-Mamba and Feature Discovery Network for Polyp Segmentation Authors: Feng Jiang, Zongfei Zhang, Xin Xu 2025-08-25 http://arxiv.org/abs/2508.17729v1 Automated colonic polyp segmentation is crucial for assisting doctors in screening of precancerous polyps and diagnosis of colorectal neoplasms. Although existing methods have achieved promising results, polyp segmentation remains hindered by the following limitations,including: (1) significant variation in polyp shapes and sizes, (2) indistinct boundaries between polyps and adjacent tissues, and (3) small-sized polyps are easily overlooked during the segmentation process. Driven by these practical difficulties, an innovative architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD module. The CMD module, as an innovative r, introduces a cross-scanning method to reduce blurry boundaries. The MSA module adopts a multi-branch parallel structure to enhance the recognition ability for polyps with diverse geometries and scale distributions. The FD module establishes dependencies among all r features to alleviate the under-detection of polyps with small-scale features. Experimental results show that CMFDNet outperforms six SOTA methods used for comparison, especially on ETIS and ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and 1.55%, respectively. CATformer Contrastive Adversarial Transformer for Image Super-Resolution Authors: Qinyi Tian, Spence Cox, Laura E. Dalton 2025-08-25 http://arxiv.org/abs/2508.17708v1 Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired , which progressively refines latent representations, with an auxiliary branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and d using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent -based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among -, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired s in super-resolution. CoCoA Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models Authors: Anant Khandelwal, Manish Gupta, Puneet Agrawal 2025-08-25 http://arxiv.org/abs/2508.17670v2 Faithful generation in large language models ( s) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple s on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation.","title":"2025-08-29"},{"location":"weekly_paper/2025-08-29/#2025-08-29","text":"","title":"2025-08-29"},{"location":"weekly_paper/2025-08-29/#table-of-contents","text":"Discrete Diffusion VLA Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies Symphony A Decentralized Multi-Agent Framework for Scalable Collective Intelligence Optimal Remainder Estimates in the Quantization of Complex Projective Spaces Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust A Survey SoK Large Language Model Copyright Auditing via Fingerprinting The Return of Structural Handwritten Mathematical Expression Recognition Spotlight Attention Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models Hybrid Decoding Rapid Pass and Selective Detailed Correction for Sequence Models Survey of Specialized Large Language Model LFD Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation ReST-RL Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding Taming the Chaos Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference Towards 6G Intelligence The Role of Generative AI in Future Wireless Networks Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs Even Heads Fix Odd Errors Mechanistic Discovery and Surgical Repair in Transformer Attention One Joke to Rule them All? On the (Im)possibility of Generalizing Humor A Theory of Goal-Oriented Medium Access Protocol Design and Distributed Bandit Learning Random forest-based out-of-distribution detection for robust lung cancer segmentation APT-LLM Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices A Concurrent Modular Agent Framework for Autonomous LLM Agents Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI RoofSeg An edge-aware transformer-based network for end-to-end roof plane segmentation LLMs in the SOC An Empirical Study of Human-AI Collaboration in Security Operations Centres Enhancing Model Privacy in Federated Learning with Random Masking and Quantization pyFAST A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data ClusterFusion Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive STARec An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction UltraMemV2 Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning Rethinking Caching for LLM Serving Systems Beyond Traditional Heuristics Drawing2CAD Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks MUA-RL Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models History Rhymes Accelerating LLM Reinforcement Learning with RhymeRL Strata Hierarchical Context Caching for Long Context Language Model Serving LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning Backprompting Leveraging Synthetic Production Data for Health Advice Guardrails DualSparse-MoE Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction Flash Sparse Attention An Alternative Efficient Implementation of Native Sparse Attention Kernel Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios AdLoCo adaptive batching significantly improves communications efficiency and convergence for Large Language Models HLLM-Creator Hierarchical LLM-based Personalized Creative Generation The AI Data Scientist A.S.E A Repository-Level Benchmark for Evaluating Security in AI-Generated Code ILRe Intermediate Layer Retrieval for Context Compression in Causal Language Models LexSemBridge Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation Speculative Safety-Aware Decoding CMFDNet Cross-Mamba and Feature Discovery Network for Polyp Segmentation CATformer Contrastive Adversarial Transformer for Image Super-Resolution CoCoA Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models","title":"Table of Contents"},{"location":"weekly_paper/2025-08-29/#discrete-diffusion-vla-bringing-discrete-diffusion-to-action-decoding-in-vision-language-action-policies","text":"Authors: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo 2025-08-27 http://arxiv.org/abs/2508.20072v1 Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA rs either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single- policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified r preserves pretrained vision language priors, supports parallel , breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action r supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.","title":"Discrete Diffusion VLA Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies"},{"location":"weekly_paper/2025-08-29/#symphony-a-decentralized-multi-agent-framework-for-scalable-collective-intelligence","text":"Authors: Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi 2025-08-27 http://arxiv.org/abs/2508.20019v1 Most existing Large Language Model ( )-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight s on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.","title":"Symphony A Decentralized Multi-Agent Framework for Scalable Collective Intelligence"},{"location":"weekly_paper/2025-08-29/#optimal-remainder-estimates-in-the-quantization-of-complex-projective-spaces","text":"Authors: Tommaso Aschieri, B\u0142a\u017cej Ruba, Jan Philip Solovej 2025-08-27 http://arxiv.org/abs/2508.19968v1 We study Berezin-Toeplitz of complex projective spaces \\mathbb{CP}^{d-1} and obtain full asymptotic expansions of the Berezin transformation and of products of Toeplitz operators. In each case, the remainder is controlled by the next term of the expansion, either through a positivity-pre transformation or via an operator inequality. This leads to bounds which are optimal in terms of the required regularity and feature sharp or asymptotically sharp constants.","title":"Optimal Remainder Estimates in the Quantization of Complex Projective Spaces"},{"location":"weekly_paper/2025-08-29/#secure-multi-llm-agentic-ai-and-agentification-for-edge-general-intelligence-by-zero-trust-a-survey","text":"Authors: Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang 2025-08-27 http://arxiv.org/abs/2508.19870v1 Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models ( s) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi- agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi- systems introduces critical security vulnerabilities, including insecure inter- s, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi- in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi- systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi- framework in EGI. We then survey key technical progress to facilitate zero-trust multi- systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi- systems, providing both theoretical foundations and practical strategies.","title":"Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust A Survey"},{"location":"weekly_paper/2025-08-29/#sok-large-language-model-copyright-auditing-via-fingerprinting","text":"Authors: Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin 2025-08-27 http://arxiv.org/abs/2508.19843v1 The broad capabilities and substantial resources required to train Large Language Models ( s) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from s to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, ) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.","title":"SoK Large Language Model Copyright Auditing via Fingerprinting"},{"location":"weekly_paper/2025-08-29/#the-return-of-structural-handwritten-mathematical-expression-recognition","text":"Authors: Jakob Seitz, Tobias Lengfeld, Radu Timofte 2025-08-27 http://arxiv.org/abs/2508.19773v1 Handwritten Mathematical Expression Recognition is foundational for educational technologies, enabling applications like digital note-taking and automated grading. While modern encoder- r architectures with large language models excel at LaTeX generation, they lack explicit symbol-to-trace alignment, a critical limitation for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates. This paper introduces a structural recognition approach with two innovations: 1 an automatic annotation system that uses a neural network to map LaTeX equations to raw traces, automatically generating annotations for symbol segmentation, classification, and spatial relations, and 2 a modular structural recognition system that independently optimizes segmentation, classification, and relation prediction. By leveraging a dataset enriched with structural annotations from our auto-labeling system, the proposed recognition system combines graph-based trace sorting, a hybrid convolutional-recurrent network, and -based correction to achieve competitive performance on the CROHME-2023 benchmark. Crucially, our structural recognition system generates a complete graph structure that directly links handwritten traces to predicted symbols, enabling transparent error analysis and interpretable outputs.","title":"The Return of Structural Handwritten Mathematical Expression Recognition"},{"location":"weekly_paper/2025-08-29/#spotlight-attention-towards-efficient-llm-generation-via-non-linear-hashing-based-kv-cache-retrieval","text":"Authors: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji 2025-08-27 http://arxiv.org/abs/2508.19740v1 Reducing the key-value ( ) burden in Large Language Models ( s) significantly accelerates inference. Dynamically selecting critical s during helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in s. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5 \\times compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100 \\mu s on a single A100 GPU, with end-to-end throughput up to 3 \\times higher than vanilla .","title":"Spotlight Attention Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval"},{"location":"weekly_paper/2025-08-29/#continuously-steering-llms-sensitivity-to-contextual-knowledge-with-proxy-models","text":"Authors: Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo 2025-08-27 http://arxiv.org/abs/2508.19720v2 In Large Language Models ( s) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, algorithms, or locating and editing context-aware neurons to adapt s to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust s' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer s' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an without modifying the weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over s' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing s to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.","title":"Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models"},{"location":"weekly_paper/2025-08-29/#hybrid-decoding-rapid-pass-and-selective-detailed-correction-for-sequence-models","text":"Authors: Yunkyu Lim, Jihwan Park, Hyung Yong Kim, Hanbin Lee, Byeong-Yeol Kim 2025-08-27 http://arxiv.org/abs/2508.19671v1 Recently, Transformer-based encoder- r models have demonstrated strong performance in multilingual speech recognition. However, the r's autoregressive nature and large size introduce significant bottlenecks during inference. Additionally, although rare, repetition can occur and negatively affect recognition accuracy. To tackle these challenges, we propose a novel Hybrid Decoding approach that both accelerates inference and alleviates the issue of repetition. Our method extends the encoder- r architecture by attaching a lightweight, fast r to the pretrained encoder. During inference, the fast r rapidly generates an output, which is then verified and, if necessary, selectively corrected by the Transformer r. This results in faster and improved robustness against repetitive errors. Experiments on the LibriSpeech and GigaSpeech test sets indicate that, with fine-tuning limited to the added r, our method achieves word error rates comparable to or better than the baseline, while more than doubling the inference speed.","title":"Hybrid Decoding Rapid Pass and Selective Detailed Correction for Sequence Models"},{"location":"weekly_paper/2025-08-29/#survey-of-specialized-large-language-model","text":"Authors: Chenghan Yang, Ruiyu Zhao, Yang Liu, Ling Jiang 2025-08-27 http://arxiv.org/abs/2508.19667v1 The rapid evolution of specialized large language models ( s) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized s, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through computation and , increasing integration of multimodal capabilities and so on are applied to recent agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose s in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.","title":"Survey of Specialized Large Language Model"},{"location":"weekly_paper/2025-08-29/#lfd-layer-fused-decoding-to-exploit-external-knowledge-in-retrieval-augmented-generation","text":"Authors: Yang Sun, Lixin Zou, Dan Luo, Zhiyong Xie, Long Zhang, Liming Dong, Yunwei Zhao, Xixun Lin, Yanxiong Lu, Chenliang Li 2025-08-27 http://arxiv.org/abs/2508.19614v1 Retrieval-augmented generation (RAG) incorporates external knowledge into large language models ( s), improving their adaptability to downstream tasks and enabling information updates. Surprisingly, recent empirical evidence demonstrates that injecting noise into retrieved relevant documents paradoxically facilitates exploitation of external knowledge and improves generation quality. Although counterintuitive and challenging to apply in practice, this phenomenon enables granular control and rigorous analysis of how s integrate external knowledge. Therefore, in this paper, we intervene on noise injection and establish a layer-specific functional demarcation within the : shallow layers specialize in local context modeling, intermediate layers focus on integrating long-range external factual knowledge, and deeper layers primarily rely on parametric internal knowledge. Building on this insight, we propose Layer Fused Decoding (LFD), a simple strategy that directly combines representations from an intermediate layer with final-layer outputs to fully exploit the external factual knowledge. To identify the optimal intermediate layer, we introduce an internal knowledge score (IKS) criterion that selects the layer with the lowest IKS value in the latter half of layers. Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.","title":"LFD Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation"},{"location":"weekly_paper/2025-08-29/#rest-rl-achieving-accurate-code-reasoning-of-llms-with-optimized-self-training-and-decoding","text":"Authors: Sining Zhoubian, Dan Zhang, Yuxiao Dong, Jie Tang 2025-08-27 http://arxiv.org/abs/2508.19576v1 With respect to improving the reasoning accuracy of s, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified RL paradigm that significantly improves 's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of policy has been improved, we further propose a test time optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When , the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the policy to achieve high reasoning accuracy. We validate the effectiveness of the proposed RL paradigm through extensive experiments on coding problems. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.","title":"ReST-RL Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding"},{"location":"weekly_paper/2025-08-29/#taming-the-chaos-coordinated-autoscaling-for-heterogeneous-and-disaggregated-llm-inference","text":"Authors: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu 2025-08-27 http://arxiv.org/abs/2508.19559v1 Serving Large Language Models ( s) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) d architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between and stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D d . HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale and pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.","title":"Taming the Chaos Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference"},{"location":"weekly_paper/2025-08-29/#towards-6g-intelligence-the-role-of-generative-ai-in-future-wireless-networks","text":"Authors: Muhammad Ahmed Mohsin, Junaid Ahmad, Muhammad Hamza Nawaz, Muhammad Ali Jamshed 2025-08-27 http://arxiv.org/abs/2508.19495v1 Ambient intelligence (AmI) is a computing paradigm in which physical environments are embedded with sensing, computation, and so they can perceive people and context, decide appropriate actions, and respond autonomously. Realizing AmI at global scale requires sixth generation (6G) wireless networks with capabilities for real time perception, reasoning, and action aligned with human behavior and mobility patterns. We argue that Generative Artificial Intelligence (GenAI) is the creative core of such environments. Unlike traditional AI, GenAI learns data distributions and can generate realistic samples, making it well suited to close key AmI gaps, including generating synthetic sensor and channel data in under observed areas, translating user intent into compact, semantic messages, predicting future network conditions for proactive control, and updating digital twins without compromising privacy. This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models, and generative s, and connects them to practical AmI use cases, including spectrum sharing, ultra reliable low latency , intelligent security, and context aware digital twins. We also examine how 6G enablers, such as edge and fog computing, IoT device swarms, intelligent reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate distributed GenAI. Finally, we outline open challenges in energy efficient on device training, trustworthy synthetic data, federated generative learning, and AmI specific standardization. We show that GenAI is not a peripheral addition, but a foundational element for transforming 6G from a faster network into an ambient intelligent ecosystem.","title":"Towards 6G Intelligence The Role of Generative AI in Future Wireless Networks"},{"location":"weekly_paper/2025-08-29/#quantized-but-deceptive-a-multi-dimensional-truthfulness-evaluation-of-quantized-llms","text":"Authors: Yao Fu, Xianxuan Long, Runchao Li, Haotian Yu, Mu Sheng, Xiaotian Han, Yu Yin, Pan Li 2025-08-26 http://arxiv.org/abs/2508.19432v1 Quantization enables efficient deployment of large language models ( s) in resource-constrained environments by significantly reducing memory and computation costs. While d s often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness-whether generating truthful or deceptive responses-remains largely unexplored. In this work, we introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of d s across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Using this framework, we examine mainstream techniques (ranging from 4-bit to extreme 2-bit) across several open-source s. Surprisingly, we find that while d models retain internally truthful representations, they are more susceptible to producing false outputs under misleading prompts. To probe this vulnerability, we test 15 rephrased variants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that \"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\" and \"neutral\" prompts maintain stable outputs. Further, we reveal that d models \"know\" the truth internally yet still produce false outputs when guided by \"deceptive\" prompts via layer-wise probing and PCA visualizations. Our findings provide insights into future designs of -aware alignment and truthfulness interventions.","title":"Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs"},{"location":"weekly_paper/2025-08-29/#even-heads-fix-odd-errors-mechanistic-discovery-and-surgical-repair-in-transformer-attention","text":"Authors: Gustavo Sandoval 2025-08-26 http://arxiv.org/abs/2508.19414v1 We present a mechanistic case study of a format-dependent reasoning failure in Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger than \"9.8\" in chat or Q&A formats, but answers correctly in simple format. Through systematic intervention, we discover s implement even/odd attention head specialization: even indexed heads handle numerical comparison, while odd heads serve incompatible functions. The bug requires exactly 8 even heads at Layer 10 for perfect repair. Any combination of 8+ even heads succeeds, while 7 or fewer completely fails, revealing sharp computational thresholds with perfect redundancy among the 16 even heads. SAE analysis reveals the mechanism: format representations separate (10% feature at Layer 7), then re-entangle with different weightings (80% feature at Layer 10), with specific features showing 1.5x amplification in failing formats. We achieve perfect repair using only 25% of attention heads and identify a 60% pattern replacement threshold, demonstrating that apparent full-module requirements hide sophisticated substructure with implications for interpretability and efficiency. All of our code is available at https://github.com/gussand/surgeon.","title":"Even Heads Fix Odd Errors Mechanistic Discovery and Surgical Repair in Transformer Attention"},{"location":"weekly_paper/2025-08-29/#one-joke-to-rule-them-all-on-the-impossibility-of-generalizing-humor","text":"Authors: Mor Turgeman, Chen Shani, Dafna Shahaf 2025-08-26 http://arxiv.org/abs/2508.19402v1 Humor is a broad and complex form of that remains challenging for machines. Despite its broadness, most existing research on computational humor traditionally focused on modeling a specific type of humor. In this work, we wish to understand whether competence on one or more specific humor tasks confers any ability to transfer to novel, unseen types; in other words, is this fragmentation inevitable? This question is especially timely as new humor types continuously emerge in online and social media contexts (e.g., memes, anti-humor, AI fails). If Large Language Models ( s) are to keep up with this evolving landscape, they must be able to generalize across humor types by capturing deeper, transferable mechanisms. To investigate this, we conduct a series of transfer learning experiments across four datasets, representing different humor tasks. We train s under varied diversity settings (1-3 datasets in training, testing on a novel task). Experiments reveal that models are capable of some transfer, and can reach up to 75% accuracy on unseen datasets; training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Further analysis suggests relations between humor types, with Dad Jokes surprisingly emerging as the best enabler of transfer (but is difficult to transfer to). We release data and code.","title":"One Joke to Rule them All? On the (Im)possibility of Generalizing Humor"},{"location":"weekly_paper/2025-08-29/#a-theory-of-goal-oriented-medium-access-protocol-design-and-distributed-bandit-learning","text":"Authors: Federico Chiariotti, Andrea Zanella 2025-08-26 http://arxiv.org/abs/2508.19141v1 The Goal-oriented Communication (GoC) paradigm breaks the separation between and the content of the data, tailoring decisions to the specific needs of the receiver and targeting application performance. While recent studies show impressive encoding performance in point-to-point scenarios, the multi-node distributed scenario is still almost unexplored. Moreover, the few studies to investigate this consider a centralized collision-free approach, where a central scheduler decides the transmission order of the nodes. In this work, we address the Goal-oriented Multiple Access (GoMA) problem, in which multiple intelligent agents must coordinate to share a wireless channel and avoid mutual interference. We propose a theoretical framework for the analysis and optimization of distributed GoMA, as a first step towards its complete characterization. We prove that the problem is non-convex and may admit multiple Nash Equilibrium (NE) solutions. We provide a characterization of each node's best response to others' strategies and propose an optimization approach that provably reaches one such NE, outperforming centralized approaches by up to 100% while also reducing energy consumption. We also design a distributed learning algorithm that operates with limited feedback and no prior knowledge.","title":"A Theory of Goal-Oriented Medium Access Protocol Design and Distributed Bandit Learning"},{"location":"weekly_paper/2025-08-29/#random-forest-based-out-of-distribution-detection-for-robust-lung-cancer-segmentation","text":"Authors: Aneesh Rangnekar, Harini Veeraraghavan 2025-08-26 http://arxiv.org/abs/2508.19112v1 Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution r, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.","title":"Random forest-based out-of-distribution detection for robust lung cancer segmentation"},{"location":"weekly_paper/2025-08-29/#apt-llm-exploiting-arbitrary-precision-tensor-core-computing-for-llm-acceleration","text":"Authors: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang 2025-08-26 http://arxiv.org/abs/2508.19087v1 Large language models ( s) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra- d s at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive scheme for arbitrary precision s, namely APT- . Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different architectures and precision settings. In inference, APT- achieves up to a 3.99 \\times speedup compared to FP16 baselines and a 2.16 \\times speedup over NVIDIA CUTLASS INT4 on RTX 3090. On RTX 4090 and H800, APT- achieves up to 2.44 \\times speedup over FP16 and 1.65 \\times speedup over CUTLASS integer baselines.","title":"APT-LLM Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration"},{"location":"weekly_paper/2025-08-29/#federated-fine-tuning-of-sparsely-activated-large-language-models-on-resource-constrained-devices","text":"Authors: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu 2025-08-26 http://arxiv.org/abs/2508.19078v1 Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models ( s) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model , computation offloading, or expert . However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based s across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) -based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while pre accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.","title":"Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices"},{"location":"weekly_paper/2025-08-29/#a-concurrent-modular-agent-framework-for-autonomous-llm-agents","text":"Authors: Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami 2025-08-26 http://arxiv.org/abs/2508.19042v1 We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model ( )-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an , inter-module , and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.","title":"A Concurrent Modular Agent Framework for Autonomous LLM Agents"},{"location":"weekly_paper/2025-08-29/#sense-of-self-and-time-in-borderline-personality-a-comparative-robustness-study-with-generative-ai","text":"Authors: Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores 2025-08-26 http://arxiv.org/abs/2508.19008v1 This study examines the capacity of large language models ( s) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients' life-story interviews, we compared three s (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p < 0.0001), and was judged as human by blinded experts. All scores strongly correlated (R > 0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias.","title":"Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI"},{"location":"weekly_paper/2025-08-29/#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation","text":"Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li 2025-08-26 http://arxiv.org/abs/2508.19003v1 Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware -based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a encoder- r-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training.","title":"RoofSeg An edge-aware transformer-based network for end-to-end roof plane segmentation"},{"location":"weekly_paper/2025-08-29/#llms-in-the-soc-an-empirical-study-of-human-ai-collaboration-in-security-operations-centres","text":"Authors: Ronal Singh, Shahroz Tariq, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris, Martin Lochner 2025-08-26 http://arxiv.org/abs/2508.18947v1 The integration of Large Language Models ( s) into Security Operations Centres (SOCs) presents a transformative, yet still evolving, opportunity to reduce analyst workload through human-AI collaboration. However, their real-world application in SOCs remains underexplored. To address this gap, we present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over 10 months. Our analysis reveals that analysts use s as on-demand aids for sensemaking and context-building, rather than for making high-stakes determinations, pre analyst decision authority. The majority of queries are related to interpreting low-level telemetry (e.g., commands) and refining technical through short (1-3 turn) interactions. Notably, 93% of queries align with established cybersecurity competencies (NICE Framework), underscoring the relevance of use for SOC-related tasks. Despite variations in tasks and engagement, usage trends indicate a shift from occasional exploration to routine integration, with growing adoption and sustained use among a subset of analysts. We find that s function as flexible, on-demand cognitive aids that augment, rather than replace, SOC expertise. Our study provides actionable guidance for designing context-aware, human-centred AI assistance in security operations, highlighting the need for further in-the-wild research on real-world analyst- collaboration, challenges, and impacts.","title":"LLMs in the SOC An Empirical Study of Human-AI Collaboration in Security Operations Centres"},{"location":"weekly_paper/2025-08-29/#enhancing-model-privacy-in-federated-learning-with-random-masking-and-quantization","text":"Authors: Zhibo Xu, Jianhao Zhu, Jingwen Xu, Changze Lv, Zisu Huang, Xiaohua Wang, Muling Wu, Qi Qian, Xiaoqing Zheng, Xuanjing Huang 2025-08-26 http://arxiv.org/abs/2508.18911v2 The primary goal of traditional federated learning is to protect data privacy by enabling distributed edge devices to collaboratively train a shared global model while keeping raw data decentralized at local clients. The rise of large language models ( s) has introduced new challenges in distributed systems, as their substantial computational requirements and the need for specialized expertise raise critical concerns about protecting intellectual property (IP). This highlights the need for a federated learning approach that can safeguard both sensitive data and proprietary models. To tackle this challenge, we propose FedQSN, a federated learning approach that leverages random masking to obscure a subnetwork of model parameters and applies to the remaining parameters. Consequently, the server transmits only a privacy-pre proxy of the global model to clients during each round, thus enhancing the model's confidentiality. Experimental results across various models and tasks demonstrate that our approach not only maintains strong model performance in federated learning settings but also achieves enhanced protection of model parameters compared to baseline methods.","title":"Enhancing Model Privacy in Federated Learning with Random Masking and Quantization"},{"location":"weekly_paper/2025-08-29/#pyfast-a-modular-pytorch-framework-for-time-series-modeling-with-multi-source-and-sparse-data","text":"Authors: Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu 2025-08-26 http://arxiv.org/abs/2508.18891v1 Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates -inspired architectures for the alignment-free fusion of data sources and offers native metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.","title":"pyFAST A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data"},{"location":"weekly_paper/2025-08-29/#clusterfusion-expanding-operator-fusion-scope-for-llm-inference-via-cluster-level-collective-primitive","text":"Authors: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo 2025-08-26 http://arxiv.org/abs/2508.18850v1 Large language model ( ) suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip . To bridge this software-hardware gap, we introduce two cluster-level primitives, ClusterReduce and ClusterGather, which abstract common patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules and computation jointly to expand operator fusion scope by composing stages such as Q Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.","title":"ClusterFusion Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive"},{"location":"weekly_paper/2025-08-29/#starec-an-efficient-agent-framework-for-recommender-systems-via-autonomous-deliberate-reasoning","text":"Authors: Chenghao Wu, Ruiyang Ren, Junjie Zhang, Ruirui Wang, Zhongrui Ma, Qi Ye, Wayne Xin Zhao 2025-08-26 http://arxiv.org/abs/2508.18812v1 While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model ( )-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in -data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.","title":"STARec An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning"},{"location":"weekly_paper/2025-08-29/#a-survey-on-cloud-edge-terminal-collaborative-intelligence-in-aiot-networks","text":"Authors: Jiaqi Wu, Jing Liu, Yang Liu, Lixu Wang, Zehua Wang, Wei Chen, Zijian Tian, Richard Yu, Victor C. M. Leung 2025-08-26 http://arxiv.org/abs/2508.18803v1 The proliferation of Internet of things (IoT) devices in smart cities, transportation, healthcare, and industrial applications, coupled with the explosive growth of AI-driven services, has increased demands for efficient distributed computing architectures and networks, driving cloud-edge-terminal collaborative intelligence (CETCI) as a fundamental paradigm within the artificial intelligence of things (AIoT) community. With advancements in deep learning, large language models ( s), and edge computing, CETCI has made significant progress with emerging AIoT applications, moving beyond isolated layer optimization to deployable collaborative intelligence systems for AIoT (CISAIOT), a practical research focus in AI, distributed computing, and s. This survey describes foundational architectures, enabling technologies, and scenarios of CETCI paradigms, offering a tutorial-style review for CISAIOT beginners. We systematically analyze architectural components spanning cloud, edge, and terminal layers, examining core technologies including network virtualization, container orchestration, and software-defined networking, while presenting categorizations of collaboration paradigms that cover task offloading, resource allocation, and optimization across heterogeneous infrastructures. Furthermore, we explain intelligent collaboration learning frameworks by reviewing advances in federated learning, distributed deep learning, edge-cloud model evolution, and reinforcement learning-based methods. Finally, we discuss challenges (e.g., scalability, heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum computing, digital twin), highlighting how integration of distributed computing and can address open issues and guide development of robust, efficient, and secure collaborative AIoT systems.","title":"A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks"},{"location":"weekly_paper/2025-08-29/#harnessing-rule-based-reinforcement-learning-for-enhanced-grammatical-error-correction","text":"Authors: Yilin Li, Xunjian Yin, Yilin Chen, Xiaojun Wan 2025-08-26 http://arxiv.org/abs/2508.18780v1 Grammatical error correction is a significant task in NLP. Traditional methods based on encoder- r models have achieved certain success, but the application of s in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train s to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \\textbf{state-of-the-art }performance, with a notable increase in \\textbf{recall}. This result clearly highlights the advantages of using RL to steer s, offering a more controllable and reliable paradigm for future development in GEC.","title":"Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction"},{"location":"weekly_paper/2025-08-29/#ultramemv2-memory-networks-scaling-to-120b-parameters-with-superior-long-context-learning","text":"Authors: Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao 2025-08-26 http://arxiv.org/abs/2508.18756v1 While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient computation.","title":"UltraMemV2 Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning"},{"location":"weekly_paper/2025-08-29/#rethinking-caching-for-llm-serving-systems-beyond-traditional-heuristics","text":"Authors: Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee 2025-08-26 http://arxiv.org/abs/2508.18736v1 Serving Large Language Models ( s) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix s neglect query semantics, while state-of-the-art semantic s remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for . SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71 \\times higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.","title":"Rethinking Caching for LLM Serving Systems Beyond Traditional Heuristics"},{"location":"weekly_paper/2025-08-29/#drawing2cad-sequence-to-sequence-learning-for-cad-generation-from-vectorized-drawings","text":"Authors: Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu 2025-08-26 http://arxiv.org/abs/2508.18733v1 Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, pre geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual- r architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.","title":"Drawing2CAD Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings"},{"location":"weekly_paper/2025-08-29/#optimal-sparsity-of-mixture-of-experts-language-models-for-reasoning-tasks","text":"Authors: Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota 2025-08-26 http://arxiv.org/abs/2508.18672v1 Empirical scaling laws have driven the evolution of large language models ( s), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new dimension that current dense-model frontiers overlook. We investigate how MoE influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top- k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top- k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as . Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal- .","title":"Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks"},{"location":"weekly_paper/2025-08-29/#mua-rl-multi-turn-user-interacting-agent-reinforcement-learning-for-agentic-tool-use","text":"Authors: Weikang Zhao, Xili Wang, Chengdi Ma, Lingbin Kong, Zhaohua Yang, Mingxiang Tuo, Xiaowei Shi, Yitao Zhai, Xunliang Cai 2025-08-26 http://arxiv.org/abs/2508.18669v1 With the recent rapid advancement of Agentic Intelligence, agentic tool use in s has become increasingly important. During multi-turn interactions between agents and users, the dynamic, uncertain, and stochastic nature of user demands poses significant challenges to the agent's tool invocation capabilities. Agents are no longer expected to simply call tools to deliver a result; rather, they must iteratively refine their understanding of user needs through while simultaneously invoking tools to resolve user queries. Existing reinforcement learning (RL) approaches for tool use lack the integration of genuinely dynamic users during the RL training process. To bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use), a novel reinforcement learning framework that, for the first time in the field of agentic tool use, integrates -simulated users into the reinforcement learning loop. MUA-RL aims to enable autonomous learning of models to communicate with users efficiently and use various tools to solve practical problems in dynamic multi-turn interactions. Evaluations are done on several multi-turn tool-using benchmarks (see Figure 1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2 Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench Agent -- outperforming or matching the performance of larger open-source models such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.","title":"MUA-RL Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use"},{"location":"weekly_paper/2025-08-29/#scaling-laws-for-task-stratified-knowledge-in-post-training-quantized-large-language-models","text":"Authors: Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu 2025-08-26 http://arxiv.org/abs/2508.18609v2 Large language models ( s) present significant deployment challenges due to their scale, with post-training (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse knowledge capabilities remains elusive, and existing scaling laws for d models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware strategies that can better preserve targeted cognitive functions.","title":"Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models"},{"location":"weekly_paper/2025-08-29/#history-rhymes-accelerating-llm-reinforcement-learning-with-rhymerl","text":"Authors: Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen 2025-08-26 http://arxiv.org/abs/2508.18588v1 With the rapid advancement of large language models ( s), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of s. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency. Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.","title":"History Rhymes Accelerating LLM Reinforcement Learning with RhymeRL"},{"location":"weekly_paper/2025-08-29/#strata-hierarchical-context-caching-for-long-context-language-model-serving","text":"Authors: Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis 2025-08-26 http://arxiv.org/abs/2508.18572v1 Large Language Models ( s) with expanding context windows face significant performance hurdles. While caching key-value ( ) states is critical for avoiding redundant computation, the storage footprint of long-context s quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large d contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for -loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context . Strata introduces GPU-assisted I/O to combat fragmentation, decoupling GPU and CPU memory layouts and employs -aware request scheduling to balance compute with I/O latency and ping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to v + LMCache and 3.75x speedup over NVIDIA TensorRT- on long-context benchmarks, without degrading short-context performance.","title":"Strata Hierarchical Context Caching for Long Context Language Model Serving"},{"location":"weekly_paper/2025-08-29/#llm-driven-intrinsic-motivation-for-sparse-reward-reinforcement-learning","text":"Authors: Andr\u00e9 Quadros, Cassio Silva, Ronnie Alves 2025-08-25 http://arxiv.org/abs/2508.18420v1 This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational State as Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward state novelty, with an intrinsic reward approach derived from Large Language Models ( s). The s leverage their pre-trained knowledge to generate reward signals based on environment and goal descriptions, guiding the agent. We implemented this combined approach with an Actor-Critic (A2C) agent in the MiniGrid DoorKey environment, a benchmark for rewards. Our empirical results show that this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent, which failed to learn. Analysis of learning curves indicates that the combination effectively complements different aspects of the environment and task: VSIMR drives exploration of new states, while the -derived rewards facilitate progressive exploitation towards goals.","title":"LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning"},{"location":"weekly_paper/2025-08-29/#latent-self-consistency-for-reliable-majority-set-selection-in-short-and-long-answer-reasoning","text":"Authors: Jeong-seok Oh, Jay-yoon Lee 2025-08-25 http://arxiv.org/abs/2508.18395v1 Probabilistic in Large Language Models ( s) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks. We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture. Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats.","title":"Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning"},{"location":"weekly_paper/2025-08-29/#backprompting-leveraging-synthetic-production-data-for-health-advice-guardrails","text":"Authors: Kellen Tan Cheng, Anna Lisa Gentile, Chad DeLuca, Guang-Jie Ren 2025-08-25 http://arxiv.org/abs/2508.18384v1 The pervasiveness of large language models ( s) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering s' input/output text through various detectors. However, developing and maintaining robust detectors faces many challenges, one of which is the difficulty in acquiring production-quality labeled data on real outputs prior to deployment. In this work, we propose backprompting, a simple yet intuitive solution to generate production-like labeled data for health advice guardrails development. Furthermore, we pair our backprompting method with a human-in-the-loop clustering technique to label the generated data. Our aim is to construct a parallel corpus roughly representative of the original dataset yet resembling real output. We then infuse existing datasets with our synthetic examples to produce robust training data for our detector. We test our technique in one of the most difficult and nuanced guardrails: the identification of health advice in output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.73%, despite having 400x less parameters.","title":"Backprompting Leveraging Synthetic Production Data for Health Advice Guardrails"},{"location":"weekly_paper/2025-08-29/#dualsparse-moe-coordinating-tensorneuron-level-sparsity-with-expert-partition-and-reconstruction","text":"Authors: Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang 2025-08-25 http://arxiv.org/abs/2508.18376v1 Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models ( s) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, and activating only a subset for each input. While this improves efficiency, MoE still faces substantial challenges due to their massive computational scale and unpredictable activation patterns. To enable efficient MoE deployment, we identify dual at the tensor and neuron levels in pre-trained MoE modules as a key factor for both accuracy and efficiency. Unlike prior work that increases tensor-level through finer-grained expert design during pre-training, we introduce post-training expert partitioning to induce such without retraining. This preserves the mathematical consistency of model transformations and enhances both efficiency and accuracy in subsequent fine-tuning and inference. Building upon this, we propose DualSparse-MoE, an inference system that integrates dynamic tensor-level computation dropping with static neuron-level reconstruction to deliver significant efficiency gains with minimal accuracy loss. Experimental results show that enforcing an approximate 25% drop rate with our approach reduces average accuracy by only 0.08%-0.28% across three prevailing MoE models, while nearly all degrees of computation dropping consistently yield proportional computational speedups. Furthermore, incorporating load-imbalance awareness into expert parallelism achieves a 1.41x MoE module speedup with just 0.5% average accuracy degradation.","title":"DualSparse-MoE Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction"},{"location":"weekly_paper/2025-08-29/#flash-sparse-attention-an-alternative-efficient-implementation-of-native-sparse-attention-kernel","text":"Authors: Ran Yan, Youhe Jiang, Binhang Yuan 2025-08-25 http://arxiv.org/abs/2508.18224v1 Recent progress in attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models ( s). Native Sparse Attention (NSA), a state-of-the-art approach, introduces natively trainable, hardware-aligned attention that delivers substantial system-level performance gains while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA relies on a query-grouping strategy that is efficient only with large Grouped Query Attention (GQA) sizes, whereas modern s typically adopt much smaller GQA groups, which limits the applicability of this algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), which includes an alternative kernel design that enables efficient NSA computation across a wide range of popular s with varied smaller GQA group sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5 \\times and on average 1.6 \\times kernel-level latency reduction, (ii) up to 1.25 \\times and 1.09 \\times on average end-to-end training speedup on state-of-the-art s, and (iii) up to 1.36 \\times and 1.11 \\times on average end-to-end speedup on state-of-the-art s. The source code is open-sourced and publicly available at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.","title":"Flash Sparse Attention An Alternative Efficient Implementation of Native Sparse Attention Kernel"},{"location":"weekly_paper/2025-08-29/#leveraging-large-language-models-for-accurate-sign-language-translation-in-low-resource-scenarios","text":"Authors: Luana Bulla, Gabriele Tuccio, Misael Mongiov\u00ec, Aldo Gangemi 2025-08-25 http://arxiv.org/abs/2508.18183v1 Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of s for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, s lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in technologies for underrepresented linguistic communities.","title":"Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios"},{"location":"weekly_paper/2025-08-29/#adloco-adaptive-batching-significantly-improves-communications-efficiency-and-convergence-for-large-language-models","text":"Authors: Nikolay Kutuzov, Makar Baderko, Stepan Kulibaba, Artem Dzhalilov, Daniel Bobrov, Maxim Mashtaler, Alexander Gasnikov 2025-08-25 http://arxiv.org/abs/2508.18182v1 Scaling distributed training of Large Language Models ( s) requires not only algorithmic advances but also efficient utilization of heterogeneous hardware resources. While existing methods such as DiLoCo have demonstrated promising results, they often fail to fully exploit computational clusters under dynamic workloads. To address this limitation, we propose a three-stage method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo, and switch mode mechanism. MIT allows individual nodes to run multiple lightweight training streams with different model instances in parallel and merge them to combine knowledge, increasing throughput and reducing idle time. Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance computation and , substantially lowering synchronization delays. Switch mode further stabilizes training by seamlessly introducing gradient accumulation once adaptive batch sizes grow beyond hardware-friendly limits. Together, these innovations improve both convergence speed and system efficiency. We also provide a theoretical estimate of the number of s required for the full convergence of a model trained using our method.","title":"AdLoCo adaptive batching significantly improves communications efficiency and convergence for Large Language Models"},{"location":"weekly_paper/2025-08-29/#hllm-creator-hierarchical-llm-based-personalized-creative-generation","text":"Authors: Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan 2025-08-25 http://arxiv.org/abs/2508.18118v1 AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators' inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a single product may have multiple selling points, with different users focusing on different features. This underscores the significant value of personalized, user-centric creative generation. Effective personalized content generation faces two main challenges: (1) accurately modeling user interests and integrating them into the content generation process while adhering to factual constraints, and (2) ensuring high efficiency and scalability to handle the massive user base in industrial scenarios. Additionally, the scarcity of personalized creative data in practice complicates model training, making data construction another key hurdle. We propose H -Creator, a hierarchical framework for efficient user interest modeling and personalized content generation. During inference, a combination of user clustering and a user-ad-matching-prediction based strategy is employed to significantly enhance generation efficiency and reduce computational overhead, making the approach suitable for large-scale deployment. Moreover, we design a data construction pipeline based on chain-of-thought reasoning, which generates high-quality, user-specific creative titles and ensures factual consistency despite limited personalized data. This pipeline serves as a critical foundation for the effectiveness of our model. Extensive experiments on personalized title generation for Douyin Search Ads show the effectiveness of H -Creator. Online A/B test shows a 0.476% increase on Adss, paving the way for more effective and efficient personalized generation in industrial scenarios. Codes for academic dataset are available at https://github.com/bytedance/H .","title":"HLLM-Creator Hierarchical LLM-based Personalized Creative Generation"},{"location":"weekly_paper/2025-08-29/#the-ai-data-scientist","text":"Authors: Farkhad Akimov, Munachiso Samuel Nwadike, Zangir Iklassov, Martin Tak\u00e1\u010d 2025-08-25 http://arxiv.org/abs/2508.18113v1 Imagine decision-makers uploading data and, within minutes, receiving clear, actionable insights delivered straight to their fingertips. That is the promise of the AI Data Scientist, an autonomous Agent powered by large language models ( s) that closes the gap between evidence and action. Rather than simply writing code or responding to prompts, it reasons through questions, tests ideas, and delivers end-to-end insights at a pace far beyond traditional workflows. Guided by the scientific tenet of the hypothesis, this Agent uncovers explanatory patterns in data, evaluates their statistical significance, and uses them to inform predictive modeling. It then translates these results into recommendations that are both rigorous and accessible. At the core of the AI Data Scientist is a team of specialized Subagents, each responsible for a distinct task such as data cleaning, statistical testing, validation, and plain-language . These Subagents write their own code, reason about causality, and identify when additional data is needed to support sound conclusions. Together, they achieve in minutes what might otherwise take days or weeks, enabling a new kind of interaction that makes deep data science both accessible and actionable.","title":"The AI Data Scientist"},{"location":"weekly_paper/2025-08-29/#ase-a-repository-level-benchmark-for-evaluating-security-in-ai-generated-code","text":"Authors: Keke Lian, Bin Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichuan Chen, Jie Wu, Haoling Li, Zhaoxuan Li, Jiongchi Yu, Hui Li, Dong Zhang 2025-08-25 http://arxiv.org/abs/2508.18106v1 The increasing adoption of large language models ( s) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, pre full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading s on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, fast-thinking'' ![key](https://img.shields.io/badge/decoding-F08080) strategies consistently outperform complex, slow-thinking'' reasoning for security patching.","title":"A.S.E A Repository-Level Benchmark for Evaluating Security in AI-Generated Code"},{"location":"weekly_paper/2025-08-29/#ilre-intermediate-layer-retrieval-for-context-compression-in-causal-language-models","text":"Authors: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li 2025-08-25 http://arxiv.org/abs/2508.17892v1 Large Language Models ( s) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate r layer offline, encodes context by streaming chunked only up to that layer, and recalls tokens by the attention scores between the input query and full key in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the ing complexity from O(L^2) to O(L) , but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single 1M tokens request in less than half a minute (speedup \\approx 180\\times ) and scores RULER- 1M benchmark of \\approx 79.8 with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.","title":"ILRe Intermediate Layer Retrieval for Context Compression in Causal Language Models"},{"location":"weekly_paper/2025-08-29/#lexsembridge-fine-grained-dense-representation-enhancement-through-token-aware-embedding-augmentation","text":"Authors: Shaoxiong Zhan, Hai Lin, Hongming Tan, Xiaodong Cai, Hai-Tao Zheng, Xin Su, Zifei Shan, Ruitong Liu, Hong-Gee Kim 2025-08-25 http://arxiv.org/abs/2508.17858v1 As queries in retrieval-augmented generation (RAG) pipelines powered by large language models ( s) become increasingly complex and diverse, dense retrieval models have demonstrated strong performance in semantic matching. Nevertheless, they often struggle with fine-grained retrieval tasks, where precise keyword alignment and span-level localization are required, even in cases with high lexical that would intuitively suggest easier retrieval. To systematically evaluate this limitation, we introduce two targeted tasks, keyword retrieval and part-of-passage retrieval, designed to simulate practical fine-grained scenarios. Motivated by these observations, we propose LexSemBridge, a unified framework that enhances dense query representations through fine-grained, input-aware vector modulation. LexSemBridge constructs latent enhancement vectors from input tokens using three paradigms: Statistical (SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense embeddings via element-wise interaction. Theoretically, we show that this modulation preserves the semantic direction while selectively amplifying discriminative dimensions. LexSemBridge operates as a plug-in without modifying the backbone encoder and naturally extends to both text and vision modalities. Extensive experiments across semantic and fine-grained retrieval tasks validate the effectiveness and generality of our approach. All code and models are publicly available at https://github.com/Jasaxion/LexSemBridge/","title":"LexSemBridge Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation"},{"location":"weekly_paper/2025-08-29/#speculative-safety-aware-decoding","text":"Authors: Xuekang Wang, Shengyu Zhu, Xueqi Cheng 2025-08-25 http://arxiv.org/abs/2508.17739v1 Despite extensive efforts to align Large Language Models ( s) with human values and safety rules, jailbreak attacks that exploit certain vulnerabilities continuously emerge, highlighting the need to strengthen existing s with additional safety properties to defend against these attacks. However, tuning large models has become increasingly resource-intensive and may have difficulty ensuring consistent performance. We introduce Speculative Safety-Aware Decoding (SSD), a lightweight -time approach that equips s with the desired safety property while accelerating inference. We assume that there exists a small language model that possesses this desired property. SSD integrates speculative sampling during and leverages the match ratio between the small and composite models to quantify jailbreak risks. This enables SSD to dynamically switch between schemes to prioritize utility or safety, to handle the challenge of different model capacities. The output token is then sampled from a new distribution that combines the distributions of the original and the small models. Experimental results show that SSD successfully equips the large model with the desired safety property, and also allows the model to remain helpful to benign queries. Furthermore, SSD accelerates the inference time, thanks to the speculative sampling design.","title":"Speculative Safety-Aware Decoding"},{"location":"weekly_paper/2025-08-29/#cmfdnet-cross-mamba-and-feature-discovery-network-for-polyp-segmentation","text":"Authors: Feng Jiang, Zongfei Zhang, Xin Xu 2025-08-25 http://arxiv.org/abs/2508.17729v1 Automated colonic polyp segmentation is crucial for assisting doctors in screening of precancerous polyps and diagnosis of colorectal neoplasms. Although existing methods have achieved promising results, polyp segmentation remains hindered by the following limitations,including: (1) significant variation in polyp shapes and sizes, (2) indistinct boundaries between polyps and adjacent tissues, and (3) small-sized polyps are easily overlooked during the segmentation process. Driven by these practical difficulties, an innovative architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD module. The CMD module, as an innovative r, introduces a cross-scanning method to reduce blurry boundaries. The MSA module adopts a multi-branch parallel structure to enhance the recognition ability for polyps with diverse geometries and scale distributions. The FD module establishes dependencies among all r features to alleviate the under-detection of polyps with small-scale features. Experimental results show that CMFDNet outperforms six SOTA methods used for comparison, especially on ETIS and ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and 1.55%, respectively.","title":"CMFDNet Cross-Mamba and Feature Discovery Network for Polyp Segmentation"},{"location":"weekly_paper/2025-08-29/#catformer-contrastive-adversarial-transformer-for-image-super-resolution","text":"Authors: Qinyi Tian, Spence Cox, Laura E. Dalton 2025-08-25 http://arxiv.org/abs/2508.17708v1 Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired , which progressively refines latent representations, with an auxiliary branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and d using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent -based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among -, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired s in super-resolution.","title":"CATformer Contrastive Adversarial Transformer for Image Super-Resolution"},{"location":"weekly_paper/2025-08-29/#cocoa-confidence-and-context-aware-adaptive-decoding-for-resolving-knowledge-conflicts-in-large-language-models","text":"Authors: Anant Khandelwal, Manish Gupta, Puneet Agrawal 2025-08-25 http://arxiv.org/abs/2508.17670v2 Faithful generation in large language models ( s) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple s on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation.","title":"CoCoA Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models"}]}