{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EfficientPaper Pruning, Quantization and efficient-inference/training paper list. Table of Contents EfficientPaper Getting Started Paper List keyword year publication institution author References Getting Started git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper } Paper List year 2025 AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress [ ] 2024 Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] Massive Activations in Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ] 2023 Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ] 2022 Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ] 2021 Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ] 2020 Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ] 2019 ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ] 2018 A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ] 2017 DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ] 2016 Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ] 1993 Optimal Brain Surgeon and general network pruning [ ] 1989 Optimal Brain Damage [ ] References Hot https://github.com/horseee/Awesome-Efficient-LLM https://github.com/DefTruth/Awesome-Diffusion-Inference https://github.com/DefTruth/Awesome-LLM-Inference https://github.com/AmberLJC/LLMSys-PaperList https://github.com/Hannibal046/Awesome-LLM https://github.com/AmadeusChan/Awesome-LLM-System-Papers https://github.com/KnowingNothing/compiler-and-arch https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management https://github.com/October2001/Awesome-KV-Cache-Compression Cold https://github.com/he-y/Awesome-Pruning https://github.com/htqin/awesome-model-quantization https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression https://github.com/AojunZhou/Efficient-Deep-Learning https://github.com/chester256/Model-Compression-Papers :arrow_up: Back to top","title":"Home"},{"location":"#efficientpaper","text":"Pruning, Quantization and efficient-inference/training paper list.","title":"EfficientPaper"},{"location":"#table-of-contents","text":"EfficientPaper Getting Started Paper List keyword year publication institution author References","title":"Table of Contents"},{"location":"#getting-started","text":"git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper }","title":"Getting Started"},{"location":"#paper-list","text":"","title":"Paper List"},{"location":"#year","text":"","title":"year"},{"location":"#2025","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress [ ]","title":"2025"},{"location":"#2024","text":"Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] Massive Activations in Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ]","title":"2024"},{"location":"#2023","text":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ]","title":"2023"},{"location":"#2022","text":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ]","title":"2022"},{"location":"#2021","text":"Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ]","title":"2021"},{"location":"#2020","text":"Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ]","title":"2020"},{"location":"#2019","text":"ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ]","title":"2019"},{"location":"#2018","text":"A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ]","title":"2018"},{"location":"#2017","text":"DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ]","title":"2017"},{"location":"#2016","text":"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ]","title":"2016"},{"location":"#1993","text":"Optimal Brain Surgeon and general network pruning [ ]","title":"1993"},{"location":"#1989","text":"Optimal Brain Damage [ ]","title":"1989"},{"location":"#references","text":"","title":"References"},{"location":"#hot","text":"https://github.com/horseee/Awesome-Efficient-LLM https://github.com/DefTruth/Awesome-Diffusion-Inference https://github.com/DefTruth/Awesome-LLM-Inference https://github.com/AmberLJC/LLMSys-PaperList https://github.com/Hannibal046/Awesome-LLM https://github.com/AmadeusChan/Awesome-LLM-System-Papers https://github.com/KnowingNothing/compiler-and-arch https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management https://github.com/October2001/Awesome-KV-Cache-Compression","title":"Hot"},{"location":"#cold","text":"https://github.com/he-y/Awesome-Pruning https://github.com/htqin/awesome-model-quantization https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression https://github.com/AojunZhou/Efficient-Deep-Learning https://github.com/chester256/Model-Compression-Papers :arrow_up: Back to top","title":"Cold"},{"location":"cls_author/","text":"author Aaron Courville Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Abhay Gupta Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Adam Fisch Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Aixin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ajay Jaiswal Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note Amir Gholami Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Amir H. Abdi Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Andr\u00e9 F. T. Martins Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Aojun Zhou Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Arvind Krishnamurthy Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ashish Panwar Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Bairu Hou Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Baris Kasikci Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Bei Feng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Beidi Chen Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note Bin Gao Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Bin Lin Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Bin Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Bing Xue Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bingxuan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bochao Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chang Gao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note Chaojun Xiao Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Chen Chen Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Chen Zhang Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Chengda Lu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chenggang Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengqi Deng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengquan Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note Chengruidong Zhang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Chenyang Song Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Chenyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chong Ruan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Christos Kozyrakis Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note Chuang Gan Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note Clark Barrett Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Cody Hao Yu Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Coleman Hooper Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Damai Dai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dan Alistarh Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Daxin Jiang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Daya Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dejian Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Deli Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dianhai Yu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Dong Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Dongjie Ji Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dongsheng Li Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Dongyang Wang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Eldar Kurtic Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Elias Frantar Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Erhang Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fan Yang Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Fangyun Lin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fei Huang Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Fucong Dai Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fuli Luo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Furu Wei Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note Genghan Zhang Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Gongfan Fang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Guanchen Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Guangbo Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guangxuan Xiao Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Guanting Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guohao Dai Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Guowei Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note H. Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hai Zhao Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Haibin Lin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Haibo Chen Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haifeng Wang Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Han Bao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hanshi Sun Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Hanwei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hao Zhang Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Haocheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Haocheng Xi Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Haofeng Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note Haoli Bai Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Haotian Tang Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Haotong Xie Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haowei Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hayden Kwok-Hay So Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Heung-Yeung Shum Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Honghui Ding Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hongsheng Li Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Hrayr Harutyunyan Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Huajian Xin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huazuo Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Qu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huiqiang Jiang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Iman Mirzadeh Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Ion Stoica Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note J. L. Cai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note J. Zico Kolter Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Jan Kautz Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Jason D. Lee Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Jayashree Mohan Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Jeff Pool Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Jiaming Tang Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Jiaming Xu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note Jian Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jianfei Chen Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note Jianfeng Gao Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Jianxi Ye Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jianyong Wang Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note Jianzhong Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiaqi Ni Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiashi Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiawei Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jie Zhou Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Jin Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jin Fang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jingchang Chen Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jingyang Yuan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Joseph E. Gonzalez Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note Jun Zhu Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note Junjie Qiu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junlong Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junxian Guo Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Junxiao Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junyang Lin Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Kai Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kai Hu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kaige Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kan Zhu Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Kang Guan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kang Zhao Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Ke Hong Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Kehong Yuan Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Kexin Huang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kuai Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kurt Keutzer Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Lean Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lecong Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lefei Zhang Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Lei Chen Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Lei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Leyi Xia Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Li Dong Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Li-Wen Chang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Lian Liu Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Liang Zhao Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lianmin Zheng Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Lili Qiu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Litong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Liyue Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lu Hou Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Mao Yang Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Maosong Sun Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Marcos Treviso Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Mark Kurtz Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Mehrdad Farajtabar Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Meng Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mengdi Wang Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Miaojun Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Michael Goin Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Michael Hassid Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Michael W. Mahoney Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Mingchuan Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghua Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghui Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mingjie Sun Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Mingming Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minmin Sun Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Ning Tian Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ningxin Zheng Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Nipun Kwatra Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Panpan Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pavlo Molchanov Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Peiyi Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Peng Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pengcheng He Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Pengfei Zuo Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Qi Hou Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Qiancheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qianhui Wu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Qihao Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qinyu Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qiushi Du Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. J. Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. L. Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ramachandran Ramjee Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Ramya Prabhu Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Rayan Saab Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Roy Schwartz Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Ruihang Lai Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ruiqi Ge Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruisong Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruizhe Pan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runji Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runxin Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruoyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruyi Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note S. S. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Sangmin Bae Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Saurav Muralidharan Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Sean Lie Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Sehoon Kim Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Shang Yang Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Shanghao Lu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shangyan Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shanhuang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shaoqing Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shengen Yan Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shengfeng Ye Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note Shijie Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Shirong Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shiwei Liu Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Shiyao Li Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shiyu Chang Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Shiyu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shreyas Saxena Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note Shuang Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuiping Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shunfeng Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuo Yang Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Shuting Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Size Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Song Han Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Stephanie Wang Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Surin Ahn Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note T. Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tal Schuster Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Tao Xie Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Tao Yuan Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Tao Yun Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tian Pei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianle Cai Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Tianlong Chen Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Tianqi Chen Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Tianqi Wu Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Tianyu Fu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Tianyu Gao Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note Tianyu Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianzhu Ye Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tim Dettmers Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression Ting Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tong Yang Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Torsten Hoefler Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Tri Dao Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note Tuo Zhao Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Vithursan Thangarasa Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note W. L. Xiao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wangding Zeng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wanjia Zhao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei An Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei Lin Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Wei Wang Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Weilin Zhao Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Weiyu Huang Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Weizhu Chen Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Wen Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenfeng Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenjun Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenlei Bao Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Wenqin Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wentao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Woosuk Kwon Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Wulong Liu Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note X. Q. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiafei Qiu Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Xiandong Zhao Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiang Liu Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note Xiangyu Zhang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Xiangyue Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xianzhi Yu Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Xianzu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiao Bi Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaodong Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaohan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaojin Shen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaosha Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaotao Nie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaowei Li Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiaowen Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaoxiang Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Chen Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Xin Cheng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Jin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Xin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xin Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinchao Wang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Xingchao Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xingkai Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinnan Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinxia Shan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyi Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Zhou Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Xinyuan Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiuhong Li Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Xu Han Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Xu Owen He Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Xuecheng Su Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xuefei Ning Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Xuegui Zheng Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xufang Luo Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Xuheng Lin Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. K. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Q. Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Y. X. Wei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. X. Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yang Li Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note Yang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yanhong Xu Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note Yankai Lin Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yanping Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaofeng Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yehui Tang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yi Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yi Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yichao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yifan Shi Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yikai Zhang Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yiliang Xiong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yilong Zhao Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note Ying He Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ying Sheng Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Ying Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yingfa Chen Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Yinhe Han Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Yishi Piao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yisong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiwu Yao Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note Yixiao Li Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Yixin Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note Yixin Song Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Yixuan Tan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyang Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Ma Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Yizhao Gao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yong Li Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Yongqiang Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yu Cheng Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yu Wang Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Yu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuan Ou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuandong Tian Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Yuchen Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yucheng Li Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yuduan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yue Gong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuezhou Hu Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Yuheng Zou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuhui Xu Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Yujia He Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yujun Lin Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Yukun Zha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yulhwa Kim Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note Yunfan Xiong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yunhe Wang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yunxian Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuqing Xia Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuqing Yang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Yutao Sun Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuting Yan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang Luo Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang You Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxin Wu Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Yuxiong He Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Yuxuan Li Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yuxuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuyang Zhou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. F. Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. Z. Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zefan Cai Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note Zehui Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zeyu Mi Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Zhangli Sha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhangyang Wang Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhe Fu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhean Xu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Dong Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Zhen Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenda Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhengyan Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenyu Zhang Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhewei Yao Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Zhewen Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhibin Gou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhicheng Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhigang Yan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhihang Yuan Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note Zhihong Shao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhilin Yang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipeng Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhixuan Lin Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Zhiyu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhiyuan Liu Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Zhongyu Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhou Yu Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Zhuang Liu Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Zhuomin He Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Zhuoshu Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zihan Wang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Zihao Ye Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ziheng Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Zihui Gu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijia Zhu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijun Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zili Wang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Zilin Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziqing Yang Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note Ziwei Ji Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Ziwei Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zixiao Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Zixuan Zhou Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Ziyang Song Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziyi Gao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zizheng Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zuchao Li Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Zunhai Su Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"By Author"},{"location":"cls_author/#author","text":"","title":"author"},{"location":"cls_author/#aaron-courville","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Aaron Courville"},{"location":"cls_author/#abhay-gupta","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Abhay Gupta"},{"location":"cls_author/#adam-fisch","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Adam Fisch"},{"location":"cls_author/#aixin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Aixin Liu"},{"location":"cls_author/#ajay-jaiswal","text":"Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note","title":"Ajay Jaiswal"},{"location":"cls_author/#amir-gholami","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Amir Gholami"},{"location":"cls_author/#amir-h-abdi","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Amir H. Abdi"},{"location":"cls_author/#andre-f-t-martins","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Andr\u00e9 F. T. Martins"},{"location":"cls_author/#aojun-zhou","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Aojun Zhou"},{"location":"cls_author/#arvind-krishnamurthy","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Arvind Krishnamurthy"},{"location":"cls_author/#ashish-panwar","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ashish Panwar"},{"location":"cls_author/#bairu-hou","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Bairu Hou"},{"location":"cls_author/#baris-kasikci","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Baris Kasikci"},{"location":"cls_author/#bei-feng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bei Feng"},{"location":"cls_author/#beidi-chen","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note","title":"Beidi Chen"},{"location":"cls_author/#bin-gao","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Bin Gao"},{"location":"cls_author/#bin-lin","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Bin Lin"},{"location":"cls_author/#bin-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Bin Wang"},{"location":"cls_author/#bing-xue","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bing Xue"},{"location":"cls_author/#bingxuan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bingxuan Wang"},{"location":"cls_author/#bochao-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bochao Wu"},{"location":"cls_author/#chang-gao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note","title":"Chang Gao"},{"location":"cls_author/#chaojun-xiao","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Chaojun Xiao"},{"location":"cls_author/#chen-chen","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Chen Chen"},{"location":"cls_author/#chen-zhang","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Chen Zhang"},{"location":"cls_author/#chengda-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengda Lu"},{"location":"cls_author/#chenggang-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenggang Zhao"},{"location":"cls_author/#chengqi-deng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengqi Deng"},{"location":"cls_author/#chengquan-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note","title":"Chengquan Jiang"},{"location":"cls_author/#chengruidong-zhang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Chengruidong Zhang"},{"location":"cls_author/#chenyang-song","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Chenyang Song"},{"location":"cls_author/#chenyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenyu Zhang"},{"location":"cls_author/#chong-ruan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chong Ruan"},{"location":"cls_author/#christos-kozyrakis","text":"Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"Christos Kozyrakis"},{"location":"cls_author/#chuang-gan","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note","title":"Chuang Gan"},{"location":"cls_author/#clark-barrett","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Clark Barrett"},{"location":"cls_author/#cody-hao-yu","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Cody Hao Yu"},{"location":"cls_author/#coleman-hooper","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Coleman Hooper"},{"location":"cls_author/#damai-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Damai Dai"},{"location":"cls_author/#dan-alistarh","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Dan Alistarh"},{"location":"cls_author/#daxin-jiang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Daxin Jiang"},{"location":"cls_author/#daya-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Daya Guo"},{"location":"cls_author/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_author/#dejian-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dejian Yang"},{"location":"cls_author/#deli-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Deli Chen"},{"location":"cls_author/#dianhai-yu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Dianhai Yu"},{"location":"cls_author/#dong-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Dong Li"},{"location":"cls_author/#dongjie-ji","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dongjie Ji"},{"location":"cls_author/#dongsheng-li","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Dongsheng Li"},{"location":"cls_author/#dongyang-wang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Dongyang Wang"},{"location":"cls_author/#eldar-kurtic","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Eldar Kurtic"},{"location":"cls_author/#elias-frantar","text":"Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models","title":"Elias Frantar"},{"location":"cls_author/#erhang-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Erhang Li"},{"location":"cls_author/#fan-yang","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Fan Yang"},{"location":"cls_author/#fangyun-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fangyun Lin"},{"location":"cls_author/#fei-huang","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Fei Huang"},{"location":"cls_author/#fucong-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fucong Dai"},{"location":"cls_author/#fuli-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fuli Luo"},{"location":"cls_author/#furu-wei","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note","title":"Furu Wei"},{"location":"cls_author/#genghan-zhang","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Genghan Zhang"},{"location":"cls_author/#gongfan-fang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Gongfan Fang"},{"location":"cls_author/#guanchen-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Guanchen Li"},{"location":"cls_author/#guangbo-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guangbo Hao"},{"location":"cls_author/#guangxuan-xiao","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Guangxuan Xiao"},{"location":"cls_author/#guanting-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guanting Chen"},{"location":"cls_author/#guohao-dai","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Guohao Dai"},{"location":"cls_author/#guowei-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guowei Li"},{"location":"cls_author/#h-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"H. Zhang"},{"location":"cls_author/#hai-zhao","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note","title":"Hai Zhao"},{"location":"cls_author/#haibin-lin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Haibin Lin"},{"location":"cls_author/#haibo-chen","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haibo Chen"},{"location":"cls_author/#haifeng-wang","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Haifeng Wang"},{"location":"cls_author/#han-bao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Han Bao"},{"location":"cls_author/#hanshi-sun","text":"Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Hanshi Sun"},{"location":"cls_author/#hanwei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hanwei Xu"},{"location":"cls_author/#hao-zhang","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Hao Zhang"},{"location":"cls_author/#haocheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haocheng Wang"},{"location":"cls_author/#haocheng-xi","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Haocheng Xi"},{"location":"cls_author/#haofeng-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"Haofeng Huang"},{"location":"cls_author/#haoli-bai","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Haoli Bai"},{"location":"cls_author/#haotian-tang","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Haotian Tang"},{"location":"cls_author/#haotong-xie","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haotong Xie"},{"location":"cls_author/#haowei-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haowei Zhang"},{"location":"cls_author/#hayden-kwok-hay-so","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Hayden Kwok-Hay So"},{"location":"cls_author/#heung-yeung-shum","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Heung-Yeung Shum"},{"location":"cls_author/#honghui-ding","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Honghui Ding"},{"location":"cls_author/#hongsheng-li","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Hongsheng Li"},{"location":"cls_author/#hrayr-harutyunyan","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Hrayr Harutyunyan"},{"location":"cls_author/#huajian-xin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huajian Xin"},{"location":"cls_author/#huazuo-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huazuo Gao"},{"location":"cls_author/#hui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Li"},{"location":"cls_author/#hui-qu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Qu"},{"location":"cls_author/#huiqiang-jiang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Huiqiang Jiang"},{"location":"cls_author/#iman-mirzadeh","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Iman Mirzadeh"},{"location":"cls_author/#ion-stoica","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Ion Stoica"},{"location":"cls_author/#j-l-cai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"J. L. Cai"},{"location":"cls_author/#j-zico-kolter","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"J. Zico Kolter"},{"location":"cls_author/#jan-kautz","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Jan Kautz"},{"location":"cls_author/#jason-d-lee","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Jason D. Lee"},{"location":"cls_author/#jayashree-mohan","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Jayashree Mohan"},{"location":"cls_author/#jeff-pool","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Jeff Pool"},{"location":"cls_author/#jiaming-tang","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Jiaming Tang"},{"location":"cls_author/#jiaming-xu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"Jiaming Xu"},{"location":"cls_author/#jian-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jian Liang"},{"location":"cls_author/#jianfei-chen","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"Jianfei Chen"},{"location":"cls_author/#jianfeng-gao","text":"Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Jianfeng Gao"},{"location":"cls_author/#jianxi-ye","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jianxi Ye"},{"location":"cls_author/#jianyong-wang","text":"Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note","title":"Jianyong Wang"},{"location":"cls_author/#jianzhong-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jianzhong Guo"},{"location":"cls_author/#jiaqi-ni","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiaqi Ni"},{"location":"cls_author/#jiashi-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiashi Li"},{"location":"cls_author/#jiawei-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiawei Wang"},{"location":"cls_author/#jie-zhou","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Jie Zhou"},{"location":"cls_author/#jin-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jin Chen"},{"location":"cls_author/#jin-fang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jin Fang"},{"location":"cls_author/#jingchang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingchang Chen"},{"location":"cls_author/#jingyang-yuan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingyang Yuan"},{"location":"cls_author/#joseph-e-gonzalez","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"Joseph E. Gonzalez"},{"location":"cls_author/#jun-zhu","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"Jun Zhu"},{"location":"cls_author/#junjie-qiu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junjie Qiu"},{"location":"cls_author/#junlong-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junlong Li"},{"location":"cls_author/#junxian-guo","text":"Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Junxian Guo"},{"location":"cls_author/#junxiao-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junxiao Song"},{"location":"cls_author/#junyang-lin","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Junyang Lin"},{"location":"cls_author/#kai-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Dong"},{"location":"cls_author/#kai-hu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Hu"},{"location":"cls_author/#kaige-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kaige Gao"},{"location":"cls_author/#kan-zhu","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note","title":"Kan Zhu"},{"location":"cls_author/#kang-guan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kang Guan"},{"location":"cls_author/#kang-zhao","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Kang Zhao"},{"location":"cls_author/#ke-hong","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Ke Hong"},{"location":"cls_author/#kehong-yuan","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Kehong Yuan"},{"location":"cls_author/#kexin-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kexin Huang"},{"location":"cls_author/#kuai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kuai Yu"},{"location":"cls_author/#kurt-keutzer","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Kurt Keutzer"},{"location":"cls_author/#lean-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lean Wang"},{"location":"cls_author/#lecong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lecong Zhang"},{"location":"cls_author/#lefei-zhang","text":"Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Lefei Zhang"},{"location":"cls_author/#lei-chen","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Lei Chen"},{"location":"cls_author/#lei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lei Xu"},{"location":"cls_author/#leyi-xia","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Leyi Xia"},{"location":"cls_author/#li-dong","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Li Dong"},{"location":"cls_author/#li-wen-chang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Li-Wen Chang"},{"location":"cls_author/#lian-liu","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Lian Liu"},{"location":"cls_author/#liang-zhao","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liang Zhao"},{"location":"cls_author/#lianmin-zheng","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Lianmin Zheng"},{"location":"cls_author/#lili-qiu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Lili Qiu"},{"location":"cls_author/#litong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Litong Wang"},{"location":"cls_author/#liyue-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liyue Zhang"},{"location":"cls_author/#lu-hou","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Lu Hou"},{"location":"cls_author/#mao-yang","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Mao Yang"},{"location":"cls_author/#maosong-sun","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Maosong Sun"},{"location":"cls_author/#marcos-treviso","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Marcos Treviso"},{"location":"cls_author/#mark-kurtz","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Mark Kurtz"},{"location":"cls_author/#mehrdad-farajtabar","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Mehrdad Farajtabar"},{"location":"cls_author/#meng-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Meng Li"},{"location":"cls_author/#mengdi-wang","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Mengdi Wang"},{"location":"cls_author/#miaojun-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Miaojun Wang"},{"location":"cls_author/#michael-goin","text":"Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Michael Goin"},{"location":"cls_author/#michael-hassid","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Michael Hassid"},{"location":"cls_author/#michael-w-mahoney","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Michael W. Mahoney"},{"location":"cls_author/#mingchuan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingchuan Zhang"},{"location":"cls_author/#minghua-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghua Zhang"},{"location":"cls_author/#minghui-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghui Tang"},{"location":"cls_author/#mingjie-sun","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Mingjie Sun"},{"location":"cls_author/#mingming-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingming Li"},{"location":"cls_author/#minmin-sun","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Minmin Sun"},{"location":"cls_author/#ning-tian","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ning Tian"},{"location":"cls_author/#ningxin-zheng","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ningxin Zheng"},{"location":"cls_author/#nipun-kwatra","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Nipun Kwatra"},{"location":"cls_author/#panpan-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Panpan Huang"},{"location":"cls_author/#pavlo-molchanov","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Pavlo Molchanov"},{"location":"cls_author/#peiyi-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peiyi Wang"},{"location":"cls_author/#peng-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peng Zhang"},{"location":"cls_author/#pengcheng-he","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Pengcheng He"},{"location":"cls_author/#pengfei-zuo","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Pengfei Zuo"},{"location":"cls_author/#qi-hou","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Qi Hou"},{"location":"cls_author/#qiancheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiancheng Wang"},{"location":"cls_author/#qianhui-wu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Qianhui Wu"},{"location":"cls_author/#qihao-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qihao Zhu"},{"location":"cls_author/#qinyu-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Qinyu Chen"},{"location":"cls_author/#qiushi-du","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiushi Du"},{"location":"cls_author/#r-j-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. J. Chen"},{"location":"cls_author/#r-l-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. L. Jin"},{"location":"cls_author/#ramachandran-ramjee","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Ramachandran Ramjee"},{"location":"cls_author/#ramya-prabhu","text":"Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ramya Prabhu"},{"location":"cls_author/#rayan-saab","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"Rayan Saab"},{"location":"cls_author/#roy-schwartz","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Roy Schwartz"},{"location":"cls_author/#ruihang-lai","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Ruihang Lai"},{"location":"cls_author/#ruiqi-ge","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruiqi Ge"},{"location":"cls_author/#ruisong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruisong Zhang"},{"location":"cls_author/#ruizhe-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruizhe Pan"},{"location":"cls_author/#runji-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runji Wang"},{"location":"cls_author/#runxin-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runxin Xu"},{"location":"cls_author/#ruoyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruoyu Zhang"},{"location":"cls_author/#ruyi-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruyi Chen"},{"location":"cls_author/#s-s-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"S. S. Li"},{"location":"cls_author/#sangmin-bae","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Sangmin Bae"},{"location":"cls_author/#saurav-muralidharan","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Saurav Muralidharan"},{"location":"cls_author/#sean-lie","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Sean Lie"},{"location":"cls_author/#sehoon-kim","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Sehoon Kim"},{"location":"cls_author/#shang-yang","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Shang Yang"},{"location":"cls_author/#shanghao-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanghao Lu"},{"location":"cls_author/#shangyan-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shangyan Zhou"},{"location":"cls_author/#shanhuang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanhuang Chen"},{"location":"cls_author/#shaoqing-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shaoqing Wu"},{"location":"cls_author/#shengen-yan","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shengen Yan"},{"location":"cls_author/#shengfeng-ye","text":"Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note","title":"Shengfeng Ye"},{"location":"cls_author/#shijie-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Shijie Cao"},{"location":"cls_author/#shirong-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shirong Ma"},{"location":"cls_author/#shiwei-liu","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Shiwei Liu"},{"location":"cls_author/#shiyao-li","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shiyao Li"},{"location":"cls_author/#shiyu-chang","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Shiyu Chang"},{"location":"cls_author/#shiyu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shiyu Wang"},{"location":"cls_author/#shreyas-saxena","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Shreyas Saxena"},{"location":"cls_author/#shuang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuang Zhou"},{"location":"cls_author/#shuiping-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuiping Yu"},{"location":"cls_author/#shunfeng-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shunfeng Zhou"},{"location":"cls_author/#shuo-yang","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Shuo Yang"},{"location":"cls_author/#shuting-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuting Pan"},{"location":"cls_author/#size-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Size Zheng"},{"location":"cls_author/#song-han","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Song Han"},{"location":"cls_author/#stephanie-wang","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Stephanie Wang"},{"location":"cls_author/#surin-ahn","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Surin Ahn"},{"location":"cls_author/#t-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"T. Wang"},{"location":"cls_author/#tal-schuster","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Tal Schuster"},{"location":"cls_author/#tao-xie","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Tao Xie"},{"location":"cls_author/#tao-yuan","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Tao Yuan"},{"location":"cls_author/#tao-yun","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tao Yun"},{"location":"cls_author/#tian-pei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tian Pei"},{"location":"cls_author/#tianle-cai","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Tianle Cai"},{"location":"cls_author/#tianlong-chen","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Tianlong Chen"},{"location":"cls_author/#tianqi-chen","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Tianqi Chen"},{"location":"cls_author/#tianqi-wu","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Tianqi Wu"},{"location":"cls_author/#tianyu-fu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Tianyu Fu"},{"location":"cls_author/#tianyu-gao","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note","title":"Tianyu Gao"},{"location":"cls_author/#tianyu-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tianyu Sun"},{"location":"cls_author/#tianzhu-ye","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Tianzhu Ye"},{"location":"cls_author/#tim-dettmers","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression","title":"Tim Dettmers"},{"location":"cls_author/#ting-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Ting Cao"},{"location":"cls_author/#tong-yang","text":"Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Tong Yang"},{"location":"cls_author/#torsten-hoefler","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"Torsten Hoefler"},{"location":"cls_author/#tri-dao","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"Tri Dao"},{"location":"cls_author/#tuo-zhao","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Tuo Zhao"},{"location":"cls_author/#vithursan-thangarasa","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Vithursan Thangarasa"},{"location":"cls_author/#w-l-xiao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"W. L. Xiao"},{"location":"cls_author/#wangding-zeng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wangding Zeng"},{"location":"cls_author/#wanjia-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wanjia Zhao"},{"location":"cls_author/#wei-an","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wei An"},{"location":"cls_author/#wei-lin","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Wei Lin"},{"location":"cls_author/#wei-wang","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Wei Wang"},{"location":"cls_author/#weilin-zhao","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Weilin Zhao"},{"location":"cls_author/#weiyu-huang","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Weiyu Huang"},{"location":"cls_author/#weizhu-chen","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Weizhu Chen"},{"location":"cls_author/#wen-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wen Liu"},{"location":"cls_author/#wenfeng-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenfeng Liang"},{"location":"cls_author/#wenjun-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenjun Gao"},{"location":"cls_author/#wenlei-bao","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Wenlei Bao"},{"location":"cls_author/#wenqin-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenqin Yu"},{"location":"cls_author/#wentao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wentao Zhang"},{"location":"cls_author/#woosuk-kwon","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"Woosuk Kwon"},{"location":"cls_author/#wulong-liu","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Wulong Liu"},{"location":"cls_author/#x-q-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"X. Q. Li"},{"location":"cls_author/#xiafei-qiu","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Xiafei Qiu"},{"location":"cls_author/#xiandong-zhao","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiandong Zhao"},{"location":"cls_author/#xiang-liu","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note","title":"Xiang Liu"},{"location":"cls_author/#xiangyu-zhang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Xiangyu Zhang"},{"location":"cls_author/#xiangyue-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiangyue Jin"},{"location":"cls_author/#xianzhi-yu","text":"Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Xianzhi Yu"},{"location":"cls_author/#xianzu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xianzu Wang"},{"location":"cls_author/#xiao-bi","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiao Bi"},{"location":"cls_author/#xiaodong-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaodong Liu"},{"location":"cls_author/#xiaohan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaohan Wang"},{"location":"cls_author/#xiaojin-shen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaojin Shen"},{"location":"cls_author/#xiaokang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Chen"},{"location":"cls_author/#xiaokang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Zhang"},{"location":"cls_author/#xiaosha-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaosha Chen"},{"location":"cls_author/#xiaotao-nie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaotao Nie"},{"location":"cls_author/#xiaowei-li","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiaowei Li"},{"location":"cls_author/#xiaowen-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaowen Sun"},{"location":"cls_author/#xiaoxiang-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaoxiang Wang"},{"location":"cls_author/#xin-chen","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note","title":"Xin Chen"},{"location":"cls_author/#xin-cheng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Cheng"},{"location":"cls_author/#xin-jin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Xin Jin"},{"location":"cls_author/#xin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xin Liu"},{"location":"cls_author/#xin-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Xie"},{"location":"cls_author/#xinchao-wang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Xinchao Wang"},{"location":"cls_author/#xingchao-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingchao Liu"},{"location":"cls_author/#xingkai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingkai Yu"},{"location":"cls_author/#xinnan-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinnan Song"},{"location":"cls_author/#xinxia-shan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinxia Shan"},{"location":"cls_author/#xinyi-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyi Zhou"},{"location":"cls_author/#xinyu-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyu Yang"},{"location":"cls_author/#xinyu-zhou","text":"Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Xinyu Zhou"},{"location":"cls_author/#xinyuan-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyuan Li"},{"location":"cls_author/#xiuhong-li","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Xiuhong Li"},{"location":"cls_author/#xu-han","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Xu Han"},{"location":"cls_author/#xu-owen-he","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Xu Owen He"},{"location":"cls_author/#xuecheng-su","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuecheng Su"},{"location":"cls_author/#xuefei-ning","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Xuefei Ning"},{"location":"cls_author/#xuegui-zheng","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xuegui Zheng"},{"location":"cls_author/#xufang-luo","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Xufang Luo"},{"location":"cls_author/#xuheng-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuheng Lin"},{"location":"cls_author/#y-k-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. K. Li"},{"location":"cls_author/#y-q-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. Q. Wang"},{"location":"cls_author/#y-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note","title":"Y. Wu"},{"location":"cls_author/#y-x-wei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Wei"},{"location":"cls_author/#y-x-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Zhu"},{"location":"cls_author/#yang-li","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note","title":"Yang Li"},{"location":"cls_author/#yang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yang Zhang"},{"location":"cls_author/#yanhong-xu","text":"Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note","title":"Yanhong Xu"},{"location":"cls_author/#yankai-lin","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yankai Lin"},{"location":"cls_author/#yanping-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yanping Huang"},{"location":"cls_author/#yao-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Li"},{"location":"cls_author/#yao-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Zhao"},{"location":"cls_author/#yaofeng-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaofeng Sun"},{"location":"cls_author/#yaohui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Li"},{"location":"cls_author/#yaohui-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Wang"},{"location":"cls_author/#yehui-tang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yehui Tang"},{"location":"cls_author/#yi-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Yu"},{"location":"cls_author/#yi-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Zheng"},{"location":"cls_author/#yichao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yichao Zhang"},{"location":"cls_author/#yifan-shi","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yifan Shi"},{"location":"cls_author/#yikai-zhang","text":"Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yikai Zhang"},{"location":"cls_author/#yiliang-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiliang Xiong"},{"location":"cls_author/#yilong-zhao","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note","title":"Yilong Zhao"},{"location":"cls_author/#ying-he","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying He"},{"location":"cls_author/#ying-sheng","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Ying Sheng"},{"location":"cls_author/#ying-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying Tang"},{"location":"cls_author/#yingfa-chen","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Yingfa Chen"},{"location":"cls_author/#yinhe-han","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Yinhe Han"},{"location":"cls_author/#yishi-piao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yishi Piao"},{"location":"cls_author/#yisong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yisong Wang"},{"location":"cls_author/#yiwu-yao","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note","title":"Yiwu Yao"},{"location":"cls_author/#yixiao-li","text":"Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Yixiao Li"},{"location":"cls_author/#yixin-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note","title":"Yixin Dong"},{"location":"cls_author/#yixin-song","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Yixin Song"},{"location":"cls_author/#yixuan-tan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yixuan Tan"},{"location":"cls_author/#yiyang-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyang Ma"},{"location":"cls_author/#yiyuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyuan Liu"},{"location":"cls_author/#yiyuan-ma","text":"Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Yiyuan Ma"},{"location":"cls_author/#yizhao-gao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yizhao Gao"},{"location":"cls_author/#yong-li","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Yong Li"},{"location":"cls_author/#yongqiang-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yongqiang Guo"},{"location":"cls_author/#yu-cheng","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yu Cheng"},{"location":"cls_author/#yu-wang","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Yu Wang"},{"location":"cls_author/#yu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yu Wu"},{"location":"cls_author/#yuan-ou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuan Ou"},{"location":"cls_author/#yuandong-tian","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Yuandong Tian"},{"location":"cls_author/#yuchen-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuchen Zhu"},{"location":"cls_author/#yucheng-li","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yucheng Li"},{"location":"cls_author/#yuduan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuduan Wang"},{"location":"cls_author/#yue-gong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yue Gong"},{"location":"cls_author/#yuezhou-hu","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Yuezhou Hu"},{"location":"cls_author/#yuheng-zou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuheng Zou"},{"location":"cls_author/#yuhui-xu","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Yuhui Xu"},{"location":"cls_author/#yujia-he","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yujia He"},{"location":"cls_author/#yujun-lin","text":"Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Yujun Lin"},{"location":"cls_author/#yukun-zha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yukun Zha"},{"location":"cls_author/#yulhwa-kim","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note","title":"Yulhwa Kim"},{"location":"cls_author/#yunfan-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunfan Xiong"},{"location":"cls_author/#yunhe-wang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yunhe Wang"},{"location":"cls_author/#yunxian-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunxian Ma"},{"location":"cls_author/#yuqing-xia","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yuqing Xia"},{"location":"cls_author/#yuqing-yang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Yuqing Yang"},{"location":"cls_author/#yutao-sun","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yutao Sun"},{"location":"cls_author/#yuting-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuting Yan"},{"location":"cls_author/#yuxiang-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang Luo"},{"location":"cls_author/#yuxiang-you","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang You"},{"location":"cls_author/#yuxin-wu","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Yuxin Wu"},{"location":"cls_author/#yuxiong-he","text":"Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Yuxiong He"},{"location":"cls_author/#yuxuan-li","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yuxuan Li"},{"location":"cls_author/#yuxuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxuan Liu"},{"location":"cls_author/#yuyang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuyang Zhou"},{"location":"cls_author/#z-f-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. F. Wu"},{"location":"cls_author/#z-z-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. Z. Ren"},{"location":"cls_author/#zefan-cai","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note","title":"Zefan Cai"},{"location":"cls_author/#zehui-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zehui Ren"},{"location":"cls_author/#zeyu-mi","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Zeyu Mi"},{"location":"cls_author/#zhangli-sha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhangli Sha"},{"location":"cls_author/#zhangyang-wang","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhangyang Wang"},{"location":"cls_author/#zhe-fu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhe Fu"},{"location":"cls_author/#zhean-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhean Xu"},{"location":"cls_author/#zhen-dong","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Zhen Dong"},{"location":"cls_author/#zhen-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Huang"},{"location":"cls_author/#zhen-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Zhang"},{"location":"cls_author/#zhenda-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhenda Xie"},{"location":"cls_author/#zhengyan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhengyan Zhang"},{"location":"cls_author/#zhenyu-zhang","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhenyu Zhang"},{"location":"cls_author/#zhewei-yao","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Zhewei Yao"},{"location":"cls_author/#zhewen-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhewen Hao"},{"location":"cls_author/#zhibin-gou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhibin Gou"},{"location":"cls_author/#zhicheng-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhicheng Ma"},{"location":"cls_author/#zhigang-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhigang Yan"},{"location":"cls_author/#zhihang-yuan","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note","title":"Zhihang Yuan"},{"location":"cls_author/#zhihong-shao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhihong Shao"},{"location":"cls_author/#zhilin-yang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhilin Yang"},{"location":"cls_author/#zhipeng-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhipeng Xu"},{"location":"cls_author/#zhixuan-lin","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Zhixuan Lin"},{"location":"cls_author/#zhiyu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhiyu Wu"},{"location":"cls_author/#zhiyuan-liu","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zhiyuan Liu"},{"location":"cls_author/#zhongyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhongyu Zhang"},{"location":"cls_author/#zhou-yu","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Zhou Yu"},{"location":"cls_author/#zhuang-liu","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Zhuang Liu"},{"location":"cls_author/#zhuomin-he","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Zhuomin He"},{"location":"cls_author/#zhuoshu-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhuoshu Li"},{"location":"cls_author/#zihan-wang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Zihan Wang"},{"location":"cls_author/#zihao-ye","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Zihao Ye"},{"location":"cls_author/#ziheng-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ziheng Jiang"},{"location":"cls_author/#zihui-gu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zihui Gu"},{"location":"cls_author/#zijia-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijia Zhu"},{"location":"cls_author/#zijun-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijun Liu"},{"location":"cls_author/#zili-wang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Zili Wang"},{"location":"cls_author/#zilin-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zilin Li"},{"location":"cls_author/#ziqing-yang","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"Ziqing Yang"},{"location":"cls_author/#ziwei-ji","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Ziwei Ji"},{"location":"cls_author/#ziwei-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziwei Xie"},{"location":"cls_author/#zixiao-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Zixiao Huang"},{"location":"cls_author/#zixuan-zhou","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zixuan Zhou"},{"location":"cls_author/#ziyang-song","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyang Song"},{"location":"cls_author/#ziyi-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyi Gao"},{"location":"cls_author/#zizheng-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zizheng Pan"},{"location":"cls_author/#zuchao-li","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Zuchao Li"},{"location":"cls_author/#zunhai-su","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Zunhai Su"},{"location":"cls_institution/","text":"institution AMD Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note AWS AI Labs Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Advanced Micro Devices Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Alibaba Cloud Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Alibaba Group Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Apple Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models IFPruning Instruction-Following Pruning for Large Language Models note Beihang University Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning ByteDance Meta Title Cover Publish Code Note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ByteDance Inc Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note 52A7RO95 Mixture of Experts in Large Language Models note ByteDance Seed Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note CMU Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note CPII under InnoHK Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Carnegie Mellon University Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Center for Advanced AI Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Central South University Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Cerebras Systems Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Chinese University of Hong Kong Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note Chongqing University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor City University of Hong Kong Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note Cohere Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Comenius University Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Computer Network Information Center, Chinese Academy of Sciences Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note Cornell University Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note DENSO IT Lab Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note DeepAuto.ai Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeepMind Meta Title Cover Publish Code Note m Fast Sparse ConvNets DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSpeed Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note Delft University of Technology Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Duke University Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note ETH Zurich Meta Title Cover Publish Code Note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ETH Z\u00fcrich Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Eindhoven University of Technology Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Emory University Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note FAIR Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note Fairleigh Dickinson University Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note Fudan University Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Fudan Universitynst2 Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note Gaoling School of Artificial Intelligence, Renmin University of China Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Georgia Institute of Technology Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note Google Meta Title Cover Publish Code Note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note Google Cloud Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google DeepMind Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google Research Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Graphcore Research Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Habana Labs Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients Harbin Institute of Technology Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Harvard University Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Heriot-Watt University Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Hong Kong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note Houmo AI Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Huawei Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Huawei Cloud Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Huawei Noah's Ark Lab Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note Huawei Noah\u2019s Ark Lab Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Huawei Noah\u2019s Ark Lab, Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note Huawei Technologies Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note Huazhong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Hugging Face Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning IST Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey IST Austria Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Imperial College London Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Indian Institute of Science Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Infinigence-AI Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Institute for Advanced Algorithms Research Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Institute of Automation, Chinese Academy of Sciences Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models Institute of Computing Technology Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Institute of Computing Technology, Chinese Academy of Sciences Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Institute of Information Engineering, Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Intel Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Intel Corporation Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration Intellifusion Inc. Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KAIST Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note KAIST AI Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note KAUST Meta Title Cover Publish Code Note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note Key Laboratory of Multimedia Trusted Perception and Efficient Computing Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Kyushu University Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Lanzhou University Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note Leiden University Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note MBZUAI Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note MIT Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note MIT-IBM Watson AI Lab Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note MakerMaker AI Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Massachusetts Institute of Technology Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note Megvii Technology Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Meituan Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Meta Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Meta AI Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Meta AI (FAIR) Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Meta Platforms Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Michigan State University Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Microsoft Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Microsoft Azure Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Microsoft Azure AI Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning Microsoft Research Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Microsoft Research India Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Mila & Universite de Montreal Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MiniCPM Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Ministry of Education of China, Xiamen University Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Mohamed bin Zayed University of AI Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Moonshot AI Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Multimedia Laboratory (MMLab) Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note NVDIA Meta Title Cover Publish Code Note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note NVIDIA Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress note NVIDIA Research Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note NanKai University Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Nanjing University Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Nanyang Technological University Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Cus-Prun Pruning General Large Language Models into Customized Expert Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note National University of Singapore Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Neural Magic Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note New York University Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note Noah\u2019s Ark Lab, Huawei Technologies Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Normal Computing Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note North China Electric Power University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Northeastern University Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers Northwestern University Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Numenta Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note OPPO Research Institute Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note Ohio State University Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note OpenAI Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights OpenGVLab Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models OpenTeams Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Oxford University Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note Peking University Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Perplexity AI Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Princeton University Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note Purdue University Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note PyTorch Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note Qwen Team Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note Renmin University of China Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Rice University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SJTU Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Salesforce AI Research Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Salesforce Research Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note Samsung Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Santa Clara University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note School of Cyber Security, University of Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models SenseTime Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SenseTime Research Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Seoul National University Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note Shanghai AI Laboratory Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Shanghai Artificial Intelligence Laboratory Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Shanghai Artificial Intelligence Laboratorys Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Shanghai Jiao Tong University Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Shanghai Jiao Tong Universtiy Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note Shanghai Jiaotong University Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ShanghaiTech University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS) Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note Singapore University of Technology and Design Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Sogang University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note Stanford Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Stanford University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note StepFun Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note StepFun Inc. Meta Title Cover Publish Code Note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Sun Yat-sen University Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Sungkyunkwan University Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note Synthesia Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Tencent AI Lab Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Tencent Machine Learning Platform Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Tencent Youtu Lab Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note The Chinese University of Hong Kong Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note The Hebrew University of Jerusalem Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note The Hebrew University of Jerusalem, Israel Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey The Hong Kong Polytechnic University Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note The Hong Kong University of Science and Technology Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note The Ohio State University Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note The University of Hong Kong Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note The University of North Carolina Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note The University of Texas at Austin Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Together AI Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note Tongji University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor Tsinghua University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note UC Berkeley Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note UC Santa Barbara Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note UCSD Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Univeristy of Sydney Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note Universidade da Coru\u00f1a Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Universidade de Lisboa Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note University College London Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note University of Basel Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers University of California Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note University of California, Berkeley Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note University of California, San Diego Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note University of Chinese Academy of Sciences Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note University of Connecticut Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm University of Edinburgh Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note University of Electronic Science and Technology of China Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction University of Hong Kong Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note University of Illinois Urbana-Champaign Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note University of Maryland Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note University of Oxford Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note University of Science and Technology Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note University of Science and Technology of China Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note University of Seoul Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note University of Southern California Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note University of St Andrews Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note University of Surrey Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Surrey, UK Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering University of Texas at Austin Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity University of Washington Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note University of Waterloo Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note University of Wisconsin Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Wisconsin-Madison Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note VITA Group Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Vector Institute Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Vizuara AI Labs Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note Vokram Group Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note WeChat AI Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note Wuhan University Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note Xiamen University Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Xiaohongshu Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Yale University Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences Zhe Jiang University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Zhejiang University Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipu.AI Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Zhongguancun Laboratory Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning baidu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note iFLYTEK Research Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note inst1 Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note inst2 Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"By Institution"},{"location":"cls_institution/#institution","text":"","title":"institution"},{"location":"cls_institution/#amd","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note","title":"AMD"},{"location":"cls_institution/#aws-ai-labs","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AWS AI Labs"},{"location":"cls_institution/#advanced-micro-devices","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Advanced Micro Devices"},{"location":"cls_institution/#alibaba-cloud","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Alibaba Cloud"},{"location":"cls_institution/#alibaba-group","text":"Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Alibaba Group"},{"location":"cls_institution/#apple","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models IFPruning Instruction-Following Pruning for Large Language Models note","title":"Apple"},{"location":"cls_institution/#beihang-university","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning","title":"Beihang University"},{"location":"cls_institution/#bytedance","text":"Meta Title Cover Publish Code Note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"ByteDance"},{"location":"cls_institution/#bytedance-inc","text":"Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note 52A7RO95 Mixture of Experts in Large Language Models note","title":"ByteDance Inc"},{"location":"cls_institution/#bytedance-seed","text":"Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"ByteDance Seed"},{"location":"cls_institution/#cmu","text":"Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note","title":"CMU"},{"location":"cls_institution/#cpii-under-innohk","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"CPII under InnoHK"},{"location":"cls_institution/#carnegie-mellon-university","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Carnegie Mellon University"},{"location":"cls_institution/#center-for-advanced-ai","text":"Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Center for Advanced AI"},{"location":"cls_institution/#central-south-university","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Central South University"},{"location":"cls_institution/#cerebras-systems","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Cerebras Systems"},{"location":"cls_institution/#chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note","title":"Chinese University of Hong Kong"},{"location":"cls_institution/#chongqing-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor","title":"Chongqing University"},{"location":"cls_institution/#city-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"City University of Hong Kong"},{"location":"cls_institution/#cohere","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Cohere"},{"location":"cls_institution/#comenius-university","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"Comenius University"},{"location":"cls_institution/#computer-network-information-center-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note","title":"Computer Network Information Center, Chinese Academy of Sciences"},{"location":"cls_institution/#cornell-university","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note","title":"Cornell University"},{"location":"cls_institution/#denso-it-lab","text":"Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note","title":"DENSO IT Lab"},{"location":"cls_institution/#deepautoai","text":"Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"DeepAuto.ai"},{"location":"cls_institution/#deepmind","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets","title":"DeepMind"},{"location":"cls_institution/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_institution/#deepspeed","text":"Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note","title":"DeepSpeed"},{"location":"cls_institution/#delft-university-of-technology","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Delft University of Technology"},{"location":"cls_institution/#duke-university","text":"Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Duke University"},{"location":"cls_institution/#eth-zurich","text":"Meta Title Cover Publish Code Note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"ETH Zurich"},{"location":"cls_institution/#eth-zurich_1","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"ETH Z\u00fcrich"},{"location":"cls_institution/#eindhoven-university-of-technology","text":"Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Eindhoven University of Technology"},{"location":"cls_institution/#emory-university","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note","title":"Emory University"},{"location":"cls_institution/#fair","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"FAIR"},{"location":"cls_institution/#fairleigh-dickinson-university","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"Fairleigh Dickinson University"},{"location":"cls_institution/#fudan-university","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Fudan University"},{"location":"cls_institution/#fudan-universitynst2","text":"Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note","title":"Fudan Universitynst2"},{"location":"cls_institution/#gaoling-school-of-artificial-intelligence-renmin-university-of-china","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Gaoling School of Artificial Intelligence, Renmin University of China"},{"location":"cls_institution/#georgia-institute-of-technology","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note","title":"Georgia Institute of Technology"},{"location":"cls_institution/#google","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note","title":"Google"},{"location":"cls_institution/#google-cloud","text":"Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Cloud"},{"location":"cls_institution/#google-deepmind","text":"Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google DeepMind"},{"location":"cls_institution/#google-research","text":"Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Research"},{"location":"cls_institution/#graphcore-research","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Graphcore Research"},{"location":"cls_institution/#habana-labs","text":"Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients","title":"Habana Labs"},{"location":"cls_institution/#harbin-institute-of-technology","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Harbin Institute of Technology"},{"location":"cls_institution/#harvard-university","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"Harvard University"},{"location":"cls_institution/#heriot-watt-university","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Heriot-Watt University"},{"location":"cls_institution/#hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note","title":"Hong Kong University of Science and Technology"},{"location":"cls_institution/#houmo-ai","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Houmo AI"},{"location":"cls_institution/#huawei","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Huawei"},{"location":"cls_institution/#huawei-cloud","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Huawei Cloud"},{"location":"cls_institution/#huawei-noahs-ark-lab","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note","title":"Huawei Noah's Ark Lab"},{"location":"cls_institution/#huawei-noahs-ark-lab_1","text":"Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Huawei Noah\u2019s Ark Lab"},{"location":"cls_institution/#huawei-noahs-ark-lab_2","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note","title":"Huawei Noah\u2019s Ark Lab,"},{"location":"cls_institution/#huawei-technologies","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"Huawei Technologies"},{"location":"cls_institution/#huazhong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Huazhong University of Science and Technology"},{"location":"cls_institution/#hugging-face","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning","title":"Hugging Face"},{"location":"cls_institution/#ist","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"IST"},{"location":"cls_institution/#ist-austria","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"IST Austria"},{"location":"cls_institution/#imperial-college-london","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Imperial College London"},{"location":"cls_institution/#indian-institute-of-science","text":"Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Indian Institute of Science"},{"location":"cls_institution/#infinigence-ai","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Infinigence-AI"},{"location":"cls_institution/#institute-for-advanced-algorithms-research","text":"Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Institute for Advanced Algorithms Research"},{"location":"cls_institution/#institute-of-automation-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models","title":"Institute of Automation, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-computing-technology","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Institute of Computing Technology"},{"location":"cls_institution/#institute-of-computing-technology-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Institute of Computing Technology, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-information-engineering-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Institute of Information Engineering, Chinese Academy of Sciences"},{"location":"cls_institution/#intel","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note","title":"Intel"},{"location":"cls_institution/#intel-corporation","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"Intel Corporation"},{"location":"cls_institution/#intellifusion-inc","text":"Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note","title":"Intellifusion Inc."},{"location":"cls_institution/#kaist","text":"Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"KAIST"},{"location":"cls_institution/#kaist-ai","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"KAIST AI"},{"location":"cls_institution/#kaust","text":"Meta Title Cover Publish Code Note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note","title":"KAUST"},{"location":"cls_institution/#key-laboratory-of-multimedia-trusted-perception-and-efficient-computing","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Key Laboratory of Multimedia Trusted Perception and Efficient Computing"},{"location":"cls_institution/#kyushu-university","text":"Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note","title":"Kyushu University"},{"location":"cls_institution/#lanzhou-university","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note","title":"Lanzhou University"},{"location":"cls_institution/#leiden-university","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Leiden University"},{"location":"cls_institution/#mbzuai","text":"Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"MBZUAI"},{"location":"cls_institution/#mit","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"MIT"},{"location":"cls_institution/#mit-ibm-watson-ai-lab","text":"Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note","title":"MIT-IBM Watson AI Lab"},{"location":"cls_institution/#makermaker-ai","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"MakerMaker AI"},{"location":"cls_institution/#massachusetts-institute-of-technology","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"Massachusetts Institute of Technology"},{"location":"cls_institution/#megvii-technology","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note","title":"Megvii Technology"},{"location":"cls_institution/#meituan","text":"Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Meituan"},{"location":"cls_institution/#meta","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Meta"},{"location":"cls_institution/#meta-ai","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Meta AI"},{"location":"cls_institution/#meta-ai-fair","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"Meta AI (FAIR)"},{"location":"cls_institution/#meta-platforms-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"Meta Platforms Inc"},{"location":"cls_institution/#michigan-state-university","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Michigan State University"},{"location":"cls_institution/#microsoft","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Microsoft"},{"location":"cls_institution/#microsoft-azure","text":"Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Microsoft Azure"},{"location":"cls_institution/#microsoft-azure-ai","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning","title":"Microsoft Azure AI"},{"location":"cls_institution/#microsoft-research","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Microsoft Research"},{"location":"cls_institution/#microsoft-research-india","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Microsoft Research India"},{"location":"cls_institution/#mila-universite-de-montreal","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Mila &amp; Universite de Montreal"},{"location":"cls_institution/#minicpm","text":"Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"MiniCPM"},{"location":"cls_institution/#ministry-of-education-of-china-xiamen-university","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Ministry of Education of China, Xiamen University"},{"location":"cls_institution/#mohamed-bin-zayed-university-of-ai","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note","title":"Mohamed bin Zayed University of AI"},{"location":"cls_institution/#moonshot-ai","text":"Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Moonshot AI"},{"location":"cls_institution/#multimedia-laboratory-mmlab","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Multimedia Laboratory (MMLab)"},{"location":"cls_institution/#nvdia","text":"Meta Title Cover Publish Code Note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note","title":"NVDIA"},{"location":"cls_institution/#nvidia","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress note","title":"NVIDIA"},{"location":"cls_institution/#nvidia-research","text":"Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"NVIDIA Research"},{"location":"cls_institution/#nankai-university","text":"Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note","title":"NanKai University"},{"location":"cls_institution/#nanjing-university","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Nanjing University"},{"location":"cls_institution/#nanyang-technological-university","text":"Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Cus-Prun Pruning General Large Language Models into Customized Expert Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Nanyang Technological University"},{"location":"cls_institution/#national-university-of-singapore","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"National University of Singapore"},{"location":"cls_institution/#neural-magic","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Neural Magic"},{"location":"cls_institution/#new-york-university","text":"Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note","title":"New York University"},{"location":"cls_institution/#noahs-ark-lab-huawei-technologies","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Noah\u2019s Ark Lab, Huawei Technologies"},{"location":"cls_institution/#normal-computing","text":"Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note","title":"Normal Computing"},{"location":"cls_institution/#north-china-electric-power-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"North China Electric Power University"},{"location":"cls_institution/#northeastern-university","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"Northeastern University"},{"location":"cls_institution/#northwestern-university","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"Northwestern University"},{"location":"cls_institution/#numenta","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Numenta"},{"location":"cls_institution/#oppo-research-institute","text":"Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"OPPO Research Institute"},{"location":"cls_institution/#ohio-state-university","text":"Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Ohio State University"},{"location":"cls_institution/#openai","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights","title":"OpenAI"},{"location":"cls_institution/#opengvlab","text":"Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models","title":"OpenGVLab"},{"location":"cls_institution/#openteams-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"OpenTeams Inc"},{"location":"cls_institution/#oxford-university","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"Oxford University"},{"location":"cls_institution/#peking-university","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Peking University"},{"location":"cls_institution/#perplexity-ai","text":"Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Perplexity AI"},{"location":"cls_institution/#princeton-university","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note","title":"Princeton University"},{"location":"cls_institution/#purdue-university","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Purdue University"},{"location":"cls_institution/#pytorch","text":"Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"PyTorch"},{"location":"cls_institution/#qwen-team","text":"Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note","title":"Qwen Team"},{"location":"cls_institution/#renmin-university-of-china","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Renmin University of China"},{"location":"cls_institution/#rice-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"Rice University"},{"location":"cls_institution/#sjtu","text":"Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"SJTU"},{"location":"cls_institution/#salesforce-ai-research","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Salesforce AI Research"},{"location":"cls_institution/#salesforce-research","text":"Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"Salesforce Research"},{"location":"cls_institution/#samsung","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note","title":"Samsung"},{"location":"cls_institution/#santa-clara-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Santa Clara University"},{"location":"cls_institution/#school-of-cyber-security-university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"School of Cyber Security, University of Chinese Academy of Sciences"},{"location":"cls_institution/#sensetime","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"SenseTime"},{"location":"cls_institution/#sensetime-research","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"SenseTime Research"},{"location":"cls_institution/#seoul-national-university","text":"Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note","title":"Seoul National University"},{"location":"cls_institution/#shanghai-ai-laboratory","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Shanghai AI Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratory","text":"Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Shanghai Artificial Intelligence Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratorys","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Shanghai Artificial Intelligence Laboratorys"},{"location":"cls_institution/#shanghai-jiao-tong-university","text":"Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Shanghai Jiao Tong University"},{"location":"cls_institution/#shanghai-jiao-tong-universtiy","text":"Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note","title":"Shanghai Jiao Tong Universtiy"},{"location":"cls_institution/#shanghai-jiaotong-university","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"Shanghai Jiaotong University"},{"location":"cls_institution/#shanghaitech-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"ShanghaiTech University"},{"location":"cls_institution/#shenzhen-institutes-of-advanced-technologysiat-chinese-academy-of-sciencecas","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS)"},{"location":"cls_institution/#singapore-university-of-technology-and-design","text":"Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Singapore University of Technology and Design"},{"location":"cls_institution/#sogang-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Sogang University"},{"location":"cls_institution/#stanford","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Stanford"},{"location":"cls_institution/#stanford-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note","title":"Stanford University"},{"location":"cls_institution/#stepfun","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note","title":"StepFun"},{"location":"cls_institution/#stepfun-inc","text":"Meta Title Cover Publish Code Note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"StepFun Inc."},{"location":"cls_institution/#sun-yat-sen-university","text":"Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Sun Yat-sen University"},{"location":"cls_institution/#sungkyunkwan-university","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"Sungkyunkwan University"},{"location":"cls_institution/#synthesia","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Synthesia"},{"location":"cls_institution/#tencent-ai-lab","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Tencent AI Lab"},{"location":"cls_institution/#tencent-machine-learning-platform","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Tencent Machine Learning Platform"},{"location":"cls_institution/#tencent-youtu-lab","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Tencent Youtu Lab"},{"location":"cls_institution/#the-chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"The Chinese University of Hong Kong"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"The Hebrew University of Jerusalem"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem-israel","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"The Hebrew University of Jerusalem, Israel"},{"location":"cls_institution/#the-hong-kong-polytechnic-university","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note","title":"The Hong Kong Polytechnic University"},{"location":"cls_institution/#the-hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note","title":"The Hong Kong University of Science and Technology"},{"location":"cls_institution/#the-ohio-state-university","text":"Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note","title":"The Ohio State University"},{"location":"cls_institution/#the-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"The University of Hong Kong"},{"location":"cls_institution/#the-university-of-north-carolina","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"The University of North Carolina"},{"location":"cls_institution/#the-university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"The University of Texas at Austin"},{"location":"cls_institution/#together-ai","text":"Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"Together AI"},{"location":"cls_institution/#tongji-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor","title":"Tongji University"},{"location":"cls_institution/#tsinghua-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Tsinghua University"},{"location":"cls_institution/#uc-berkeley","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"UC Berkeley"},{"location":"cls_institution/#uc-santa-barbara","text":"Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"UC Santa Barbara"},{"location":"cls_institution/#ucsd","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"UCSD"},{"location":"cls_institution/#univeristy-of-sydney","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"Univeristy of Sydney"},{"location":"cls_institution/#universidade-da-coruna","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"Universidade da Coru\u00f1a"},{"location":"cls_institution/#universidade-de-lisboa","text":"Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Universidade de Lisboa"},{"location":"cls_institution/#university-college-london","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"University College London"},{"location":"cls_institution/#university-of-basel","text":"Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers","title":"University of Basel"},{"location":"cls_institution/#university-of-california","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"University of California"},{"location":"cls_institution/#university-of-california-berkeley","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"University of California, Berkeley"},{"location":"cls_institution/#university-of-california-san-diego","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"University of California, San Diego"},{"location":"cls_institution/#university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"University of Chinese Academy of Sciences"},{"location":"cls_institution/#university-of-connecticut","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm","title":"University of Connecticut"},{"location":"cls_institution/#university-of-edinburgh","text":"Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"University of Edinburgh"},{"location":"cls_institution/#university-of-electronic-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction","title":"University of Electronic Science and Technology of China"},{"location":"cls_institution/#university-of-hong-kong","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note","title":"University of Hong Kong"},{"location":"cls_institution/#university-of-illinois-urbana-champaign","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note","title":"University of Illinois Urbana-Champaign"},{"location":"cls_institution/#university-of-maryland","text":"Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"University of Maryland"},{"location":"cls_institution/#university-of-oxford","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"University of Oxford"},{"location":"cls_institution/#university-of-science-and-technology","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"University of Science and Technology"},{"location":"cls_institution/#university-of-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"University of Science and Technology of China"},{"location":"cls_institution/#university-of-seoul","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"University of Seoul"},{"location":"cls_institution/#university-of-southern-california","text":"Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"University of Southern California"},{"location":"cls_institution/#university-of-st-andrews","text":"Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note","title":"University of St Andrews"},{"location":"cls_institution/#university-of-surrey","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Surrey"},{"location":"cls_institution/#university-of-surrey-uk","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering","title":"University of Surrey, UK"},{"location":"cls_institution/#university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"University of Texas at Austin"},{"location":"cls_institution/#university-of-washington","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"University of Washington"},{"location":"cls_institution/#university-of-waterloo","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note","title":"University of Waterloo"},{"location":"cls_institution/#university-of-wisconsin","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Wisconsin"},{"location":"cls_institution/#university-of-wisconsin-madison","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note","title":"University of Wisconsin-Madison"},{"location":"cls_institution/#vita-group","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter","title":"VITA Group"},{"location":"cls_institution/#vector-institute","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note","title":"Vector Institute"},{"location":"cls_institution/#vizuara-ai-labs","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"Vizuara AI Labs"},{"location":"cls_institution/#vokram-group","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Vokram Group"},{"location":"cls_institution/#wechat-ai","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note","title":"WeChat AI"},{"location":"cls_institution/#wuhan-university","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"Wuhan University"},{"location":"cls_institution/#xiamen-university","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note","title":"Xiamen University"},{"location":"cls_institution/#xiaohongshu","text":"Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Xiaohongshu"},{"location":"cls_institution/#yale-university","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences","title":"Yale University"},{"location":"cls_institution/#zhe-jiang-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"Zhe Jiang University"},{"location":"cls_institution/#zhejiang-university","text":"Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhejiang University"},{"location":"cls_institution/#zhipuai","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Zhipu.AI"},{"location":"cls_institution/#zhongguancun-laboratory","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning","title":"Zhongguancun Laboratory"},{"location":"cls_institution/#baidu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note","title":"baidu"},{"location":"cls_institution/#iflytek-research","text":"Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"iFLYTEK Research"},{"location":"cls_institution/#inst1","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"inst1"},{"location":"cls_institution/#inst2","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"inst2"},{"location":"cls_keyword/","text":"keyword 01-Communication-Computation Overlap Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note 02-Sparsity (Attention) Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 03-Sparsity (Activation) Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note 04-Modeling Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note 05-Sparsity (Structured) Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note 06-Sparse/Pruning Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note 07-Quantization Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 08-Survey Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 52A7RO95 Mixture of Experts in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note 09-Low Rank Decomposition Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note 10-Layer Fusion (Reduce IO) Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note 11-Tool Meta Title Cover Publish Code Note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 12-KV Cache Optimization/Efficient Attention Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 13-Efficient Training Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note 14-Network Structure Design Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note 15-Sparsity (Weight) Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note 16-LLM Deployment Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"By Keyword"},{"location":"cls_keyword/#keyword","text":"","title":"keyword"},{"location":"cls_keyword/#01-communication-computation-overlap","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"01-Communication-Computation Overlap"},{"location":"cls_keyword/#02-sparsity-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"02-Sparsity (Attention)"},{"location":"cls_keyword/#03-sparsity-activation","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note","title":"03-Sparsity (Activation)"},{"location":"cls_keyword/#04-modeling","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"04-Modeling"},{"location":"cls_keyword/#05-sparsity-structured","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"05-Sparsity (Structured)"},{"location":"cls_keyword/#06-sparsepruning","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"06-Sparse/Pruning"},{"location":"cls_keyword/#07-quantization","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"07-Quantization"},{"location":"cls_keyword/#08-survey","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 52A7RO95 Mixture of Experts in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"08-Survey"},{"location":"cls_keyword/#09-low-rank-decomposition","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"09-Low Rank Decomposition"},{"location":"cls_keyword/#10-layer-fusion-reduce-io","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"10-Layer Fusion (Reduce IO)"},{"location":"cls_keyword/#11-tool","text":"Meta Title Cover Publish Code Note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"11-Tool"},{"location":"cls_keyword/#12-kv-cache-optimizationefficient-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"12-KV Cache Optimization/Efficient Attention"},{"location":"cls_keyword/#13-efficient-training","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"13-Efficient Training"},{"location":"cls_keyword/#14-network-structure-design","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"14-Network Structure Design"},{"location":"cls_keyword/#15-sparsity-weight","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"15-Sparsity (Weight)"},{"location":"cls_keyword/#16-llm-deployment","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"16-LLM Deployment"},{"location":"cls_publication/","text":"publication AAAI Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note ACL Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note ASPLOS Meta Title Cover Publish Code Note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note ATC Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AutoML Workshop Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Blog Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note COLM Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note CVPR Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note CVPR workshop Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Coling Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note DATE Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note ECCV Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers ENLSP Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note ICCV Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration ICLR Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note ICLR oral Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models ICML Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note ICML Workshop Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note ISCA Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note JMLR Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks KDD Workshop Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note MICRO Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation MLSys Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note NeurIPS Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note Neuromorphic Computing and Engineering Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note SC Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SOSP Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note TACL Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TC Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note TMLR Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note UAI Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models VLDB Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note VLSI Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers arXiv Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note github Meta Title Cover Publish Code Note FT FasterTransformer KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"By Publication"},{"location":"cls_publication/#publication","text":"","title":"publication"},{"location":"cls_publication/#aaai","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"AAAI"},{"location":"cls_publication/#acl","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note","title":"ACL"},{"location":"cls_publication/#asplos","text":"Meta Title Cover Publish Code Note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"ASPLOS"},{"location":"cls_publication/#atc","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note","title":"ATC"},{"location":"cls_publication/#automl-workshop","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AutoML Workshop"},{"location":"cls_publication/#blog","text":"Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note","title":"Blog"},{"location":"cls_publication/#colm","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note","title":"COLM"},{"location":"cls_publication/#cvpr","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note","title":"CVPR"},{"location":"cls_publication/#cvpr-workshop","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine","title":"CVPR workshop"},{"location":"cls_publication/#coling","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note","title":"Coling"},{"location":"cls_publication/#date","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"DATE"},{"location":"cls_publication/#eccv","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"ECCV"},{"location":"cls_publication/#enlsp","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note","title":"ENLSP"},{"location":"cls_publication/#iccv","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"ICCV"},{"location":"cls_publication/#iclr","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"ICLR"},{"location":"cls_publication/#iclr-oral","text":"Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"ICLR oral"},{"location":"cls_publication/#icml","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"ICML"},{"location":"cls_publication/#icml-workshop","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"ICML Workshop"},{"location":"cls_publication/#isca","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"ISCA"},{"location":"cls_publication/#jmlr","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks","title":"JMLR"},{"location":"cls_publication/#kdd-workshop","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"KDD Workshop"},{"location":"cls_publication/#micro","text":"Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation","title":"MICRO"},{"location":"cls_publication/#mlsys","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note","title":"MLSys"},{"location":"cls_publication/#neurips","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note","title":"NeurIPS"},{"location":"cls_publication/#neuromorphic-computing-and-engineering","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Neuromorphic Computing and Engineering"},{"location":"cls_publication/#sc","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"SC"},{"location":"cls_publication/#sosp","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note","title":"SOSP"},{"location":"cls_publication/#tacl","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"TACL"},{"location":"cls_publication/#tc","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"TC"},{"location":"cls_publication/#tmlr","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"TMLR"},{"location":"cls_publication/#uai","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models","title":"UAI"},{"location":"cls_publication/#vldb","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"VLDB"},{"location":"cls_publication/#vlsi","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers","title":"VLSI"},{"location":"cls_publication/#arxiv","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"arXiv"},{"location":"cls_publication/#github","text":"Meta Title Cover Publish Code Note FT FasterTransformer KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"github"},{"location":"cls_year/","text":"year 2025 Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 2024 Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note 2023 Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer 2022 Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models 2021 Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 2020 Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights 2019 Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training 2018 Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers 2017 Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon 2016 Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding 1993 Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning 1989 Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"By Year"},{"location":"cls_year/#year","text":"","title":"year"},{"location":"cls_year/#2025","text":"Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"2025"},{"location":"cls_year/#2024","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"2024"},{"location":"cls_year/#2023","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer","title":"2023"},{"location":"cls_year/#2022","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models","title":"2022"},{"location":"cls_year/#2021","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks","title":"2021"},{"location":"cls_year/#2020","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights","title":"2020"},{"location":"cls_year/#2019","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training","title":"2019"},{"location":"cls_year/#2018","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"2018"},{"location":"cls_year/#2017","text":"Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon","title":"2017"},{"location":"cls_year/#2016","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","title":"2016"},{"location":"cls_year/#1993","text":"Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning","title":"1993"},{"location":"cls_year/#1989","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"1989"},{"location":"notes/2021/CoCoNet/note/","text":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi Abstract Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2021/CoCoNet/note/#breaking-the-computation-and-communication-abstraction-barrier-in-distributed-machine-learning-workloads","text":"Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2021/CoCoNet/note/#abstract","text":"Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Abstract"},{"location":"notes/2022/ComplementarySparsity/note/","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Combine weight sparsity and activation sparsity Table of Contents Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation Method sparse weight and dense activation a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity. sparse weight and sparse activation a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#two-sparsities-are-better-than-one-unlocking-the-performance-benefits-of-sparse-sparse-networks","text":"Combine weight sparsity and activation sparsity","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#table-of-contents","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation","title":"Table of Contents"},{"location":"notes/2022/ComplementarySparsity/note/#method","text":"","title":"Method"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-dense-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity.","title":"sparse weight and dense activation"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-sparse-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"sparse weight and sparse activation"},{"location":"notes/2022/DSA/note/","text":"Transformer Acceleration with Dynamic Sparse Attention Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie Abstract Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#transformer-acceleration-with-dynamic-sparse-attention","text":"Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#abstract","text":"Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Abstract"},{"location":"notes/2022/fisherpruning/note/","text":"A Fast Post-Training Pruning Framework for Transformers Abstract Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#a-fast-post-training-pruning-framework-for-transformers","text":"","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#abstract","text":"Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"Abstract"},{"location":"notes/2022/spdy/","text":"SPDY: Accurate Pruning with Speedup Guarantees \"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2022/spdy/#spdy-accurate-pruning-with-speedup-guarantees","text":"\"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2023/CodeGeeX/note/","text":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang Abstract Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#codegeex-a-pre-trained-model-for-code-generation-with-multilingual-benchmarking-on-humaneval-x","text":"Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#abstract","text":"Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"Abstract"},{"location":"notes/2023/Compresso/note/","text":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance. Method Challenges Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection Training data for pruning We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset Efficient training-based structured pruning Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, $\\alpha$ is the learnable parameter: Mask regularization for the expected pruning ratio: Experiments The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#compresso-structured-pruning-with-collaborative-prompting-learns-compact-large-language-models","text":"This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#method","text":"","title":"Method"},{"location":"notes/2023/Compresso/note/#challenges","text":"Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection","title":"Challenges"},{"location":"notes/2023/Compresso/note/#training-data-for-pruning","text":"We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset","title":"Training data for pruning"},{"location":"notes/2023/Compresso/note/#efficient-training-based-structured-pruning","text":"Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, $\\alpha$ is the learnable parameter: Mask regularization for the expected pruning ratio:","title":"Efficient training-based structured pruning"},{"location":"notes/2023/Compresso/note/#experiments","text":"The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Experiments"},{"location":"notes/2023/FlashDecoding/note/","text":"FlashDecoding Abstract","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#flashdecoding","text":"","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/GBLM-Pruner/note/","text":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models Abstract Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#beyond-size-how-gradients-shape-pruning-decisions-in-large-language-models","text":"","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#abstract","text":"Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Abstract"},{"location":"notes/2023/H2O/note/","text":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models Abstract Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.","title":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models","text":"","title":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#abstract","text":"Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.","title":"Abstract"},{"location":"notes/2023/IHOT8YP4/note/","text":"Efficient Guided Generation for Large Language Models Abstract In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#efficient-guided-generation-for-large-language-models","text":"","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#abstract","text":"In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Abstract"},{"location":"notes/2023/LLM-Pruner/note/","text":"LLM-Pruner: On the Structural Pruning of Large Language Models Abstract Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#llm-pruner-on-the-structural-pruning-of-large-language-models","text":"","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#abstract","text":"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"Abstract"},{"location":"notes/2023/LLM_in_a_flash/note/","text":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory Abstract Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory","text":"","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#abstract","text":"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"Abstract"},{"location":"notes/2023/LLM_shearing/note/","text":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning structured pruning\u7684\u4e24\u4e2a\u56f0\u96be \u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa Method targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#sheared-llama-accelerating-language-model-pre-training-via-structured-pruning","text":"","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#structured-pruning","text":"\u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa","title":"structured pruning\u7684\u4e24\u4e2a\u56f0\u96be"},{"location":"notes/2023/LLM_shearing/note/#method","text":"targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Method"},{"location":"notes/2023/MeZO/note/","text":"Fine-Tuning Language Models with Just Forward Passes Abstract Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#fine-tuning-language-models-with-just-forward-passes","text":"","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#abstract","text":"Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Abstract"},{"location":"notes/2023/PagedAttention/note/","text":"Efficient Memory Management for Large Language Model Serving with PagedAttention Abstract High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#efficient-memory-management-for-large-language-model-serving-with-pagedattention","text":"","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#abstract","text":"High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Abstract"},{"location":"notes/2023/PowerInfer/note/","text":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Abstract This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#powerinfer-fast-large-language-model-serving-with-a-consumer-grade-gpu","text":"","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#abstract","text":"This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"Abstract"},{"location":"notes/2023/RIIWOI3F/note/","text":"RIIWOI3F Abstract","title":"RIIWOI3F"},{"location":"notes/2023/RIIWOI3F/note/#riiwoi3f","text":"","title":"RIIWOI3F"},{"location":"notes/2023/RIIWOI3F/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/SparseViT/","text":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer Method Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity Result \u223c50% latency reduction with 60% sparsity","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#sparsevit-revisiting-activation-sparsity-for-efficient-high-resolution-vision-transformer","text":"","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#method","text":"Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity","title":"Method"},{"location":"notes/2023/SparseViT/#result","text":"\u223c50% latency reduction with 60% sparsity","title":"Result"},{"location":"notes/2023/VENOM/note/","text":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores Abstract The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#venom-a-vectorized-nm-format-for-unleashing-the-power-of-sparse-tensor-cores","text":"","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#abstract","text":"The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"Abstract"},{"location":"notes/2023/grain/","text":"GRAIN","title":"GRAIN"},{"location":"notes/2023/grain/#grain","text":"","title":"GRAIN"},{"location":"notes/2023/k_pruning/note/","text":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining This is a retraning-free structured pruning approach. Method Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where $\\lambda = \\{0.00025, 1\\}$ and $\\mu = 64$. \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c $\\lambda$ \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c $\\lambda$ \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 $\\mu$ \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002 Results","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#knowledge-preserving-pruning-for-pre-trained-language-models-without-retraining","text":"This is a retraning-free structured pruning approach.","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#method","text":"Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where $\\lambda = \\{0.00025, 1\\}$ and $\\mu = 64$. \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c $\\lambda$ \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c $\\lambda$ \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 $\\mu$ \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002","title":"Method"},{"location":"notes/2023/k_pruning/note/#results","text":"","title":"Results"},{"location":"notes/2023/loftq/note/","text":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models Method \u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#loftq-lora-fine-tuning-aware-quantization-for-large-language-models","text":"","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#method","text":"\u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"Method"},{"location":"notes/2023/simple/","text":"Structured Pruning for Efficient Generative Pre-trained Language Models Method Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V $\\ell_{hidden}$ is hidden state distillation loss: Results \u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#structured-pruning-for-efficient-generative-pre-trained-language-models","text":"","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#method","text":"Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V $\\ell_{hidden}$ is hidden state distillation loss:","title":"Method"},{"location":"notes/2023/simple/#results","text":"\u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Results"},{"location":"notes/2024/068ZPAME/note/","text":"A Survey on Inference Optimization Techniques for Mixture of Experts Models Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li Abstract The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#a-survey-on-inference-optimization-techniques-for-mixture-of-experts-models","text":"Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#abstract","text":"The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"Abstract"},{"location":"notes/2024/0Y41U1N2/note/","text":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs Abstract To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#beyond-24-exploring-vnm-sparsity-for-efficient-transformer-inference-on-gpus","text":"","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#abstract","text":"To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Abstract"},{"location":"notes/2024/ADMM-pruning/note/","text":"Fast and Effective Weight Update for Pruned Large Language Models Abstract Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#fast-and-effective-weight-update-for-pruned-large-language-models","text":"","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#abstract","text":"Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Abstract"},{"location":"notes/2024/APEX/note/","text":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino Abstract Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#apex-an-extensible-and-dynamism-aware-simulator-for-automated-parallel-execution-in-llm-serving","text":"Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#abstract","text":"Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"Abstract"},{"location":"notes/2024/AVSS/note/","text":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis Abstract The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#avss-layer-importance-evaluation-in-large-language-models-via-activation-variance-sparsity-analysis","text":"","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#abstract","text":"The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"Abstract"},{"location":"notes/2024/AdaKV/note/","text":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference Abstract Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#ada-kv-optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference","text":"","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#abstract","text":"Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Abstract"},{"location":"notes/2024/Async-TP/note/","text":"Async-TP Abstract","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#async-tp","text":"","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/CATS/note/","text":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models Abstract Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#cats-contextually-aware-thresholding-for-sparsity-in-large-language-models","text":"","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#abstract","text":"Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"Abstract"},{"location":"notes/2024/CHESS/note/","text":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Abstract Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#chess-optimizing-llm-inference-via-channel-wise-thresholding-and-selective-sparsification","text":"","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#abstract","text":"Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"Abstract"},{"location":"notes/2024/CLA/note/","text":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention Abstract Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#reducing-transformer-key-value-cache-size-with-cross-layer-attention","text":"","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#abstract","text":"Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Abstract"},{"location":"notes/2024/CachedAttention/note/","text":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention Abstract Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention","text":"","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#abstract","text":"Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Abstract"},{"location":"notes/2024/ChunkAttention/note/","text":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition Abstract Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#chunkattention-efficient-self-attention-with-prefix-aware-kv-cache-and-two-phase-partition","text":"","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#abstract","text":"Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/CoreInfer/note/","text":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation Abstract Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86$\\alpha$ $\\beta$ \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#coreinfer-accelerating-large-language-model-inference-with-semantics-inspired-adaptive-sparse-activation","text":"","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#abstract","text":"Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86$\\alpha$ $\\beta$ \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"Abstract"},{"location":"notes/2024/DHIB73MC/note/","text":"A Survey on Efficient Inference for Large Language Models Abstract Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#a-survey-on-efficient-inference-for-large-language-models","text":"","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#abstract","text":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"Abstract"},{"location":"notes/2024/DSnoT/note/","text":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs Abstract The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#dynamic-sparse-no-training-training-free-fine-tuning-for-sparse-llms","text":"","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#abstract","text":"The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/","text":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model Abstract We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1 Multi-head Latent Attention (MLA) Preliminaries: Standard Multi-Head Attention \u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002 Low-Rank Key-Value Joint Compression \u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58$C_t^{KV}$\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e$C_t^{KV}$\u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a Decoupled Rotary Position Embedding \u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#deepseek-v2-a-strong-economical-and-efficient-mixture-of-experts-language-model","text":"","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#abstract","text":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/#multi-head-latent-attention-mla","text":"","title":"Multi-head Latent Attention (MLA)"},{"location":"notes/2024/DeepSeek-V2/note/#preliminaries-standard-multi-head-attention","text":"\u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002","title":"Preliminaries: Standard Multi-Head Attention"},{"location":"notes/2024/DeepSeek-V2/note/#low-rank-key-value-joint-compression","text":"\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58$C_t^{KV}$\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e$C_t^{KV}$\u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"Low-Rank Key-Value Joint Compression"},{"location":"notes/2024/DeepSeek-V2/note/#decoupled-rotary-position-embedding","text":"\u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"Decoupled Rotary Position Embedding"},{"location":"notes/2024/DeepSeek-V3/note/","text":"DeepSeek-V3 Technical Report Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#deepseek-v3-technical-report","text":"","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#abstract","text":"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"Abstract"},{"location":"notes/2024/DeepSeekMoE/note/","text":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models Abstract In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#deepseekmoe-towards-ultimate-expert-specialization-in-mixture-of-experts-language-models","text":"","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#abstract","text":"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"Abstract"},{"location":"notes/2024/DistAttention/note/","text":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache Abstract Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#infinite-llm-efficient-llm-service-for-long-context-with-distattention-and-distributed-kvcache","text":"","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#abstract","text":"Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Abstract"},{"location":"notes/2024/DistributedGEMM/note/","text":"DistributedGEMM Abstract","title":"DistributedGEMM"},{"location":"notes/2024/DistributedGEMM/note/#distributedgemm","text":"","title":"DistributedGEMM"},{"location":"notes/2024/DistributedGEMM/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/Domino/note/","text":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase Abstract Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#domino-eliminating-communication-in-llm-training-via-generic-tensor-slicing-and-overlapping","text":"Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#abstract","text":"Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.","title":"Abstract"},{"location":"notes/2024/DoubleSparsity/note/","text":"Post-Training Sparse Attention with Double Sparsity Abstract The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in attention operations and a 1.9$\\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#post-training-sparse-attention-with-double-sparsity","text":"","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#abstract","text":"The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in attention operations and a 1.9$\\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Abstract"},{"location":"notes/2024/DuoAttention/note/","text":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads Abstract Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a $\\alpha$ \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 $\\alpha$ \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#duoattention-efficient-long-context-llm-inference-with-retrieval-and-streaming-heads","text":"","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#abstract","text":"Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a $\\alpha$ \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 $\\alpha$ \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"Abstract"},{"location":"notes/2024/Eagle/note/","text":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty Abstract Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#eagle-speculative-sampling-requires-rethinking-feature-uncertainty","text":"","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#abstract","text":"Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"Abstract"},{"location":"notes/2024/FLUX/note/","text":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu Abstract Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion","text":"Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#abstract","text":"Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"Abstract"},{"location":"notes/2024/FlashMask/note/","text":"FlashMask: Efficient and Rich Mask Extension of FlashAttention Abstract The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#flashmask-efficient-and-rich-mask-extension-of-flashattention","text":"","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#abstract","text":"The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"Abstract"},{"location":"notes/2024/FrameQuant/note/","text":"FrameQuant: Flexible Low-Bit Quantization for Transformers Abstract Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#framequant-flexible-low-bit-quantization-for-transformers","text":"","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#abstract","text":"Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/HYPL7G37/note/","text":"Accelerating Transformer Pre-training with 2:4 Sparsity Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#accelerating-transformer-pre-training-with-24-sparsity","text":"","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#abstract","text":"Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Abstract"},{"location":"notes/2024/JSHWEV0S/note/","text":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption Abstract Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#keep-the-cost-down-a-review-on-methods-to-optimize-llm-s-kv-cache-consumption","text":"","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#abstract","text":"Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Abstract"},{"location":"notes/2024/KVQuant/note/","text":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Abstract LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#kvquant-towards-10-million-context-length-llm-inference-with-kv-cache-quantization","text":"","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#abstract","text":"LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.","title":"Abstract"},{"location":"notes/2024/L4Q/note/","text":"L4Q Method Step1: warm up Q(W) + $\\alpha$ AB Step2: quantize(W + sAB) Experiments memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"L4Q"},{"location":"notes/2024/L4Q/note/#l4q","text":"Method Step1: warm up Q(W) + $\\alpha$ AB Step2: quantize(W + sAB)","title":"L4Q"},{"location":"notes/2024/L4Q/note/#experiments","text":"memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"Experiments"},{"location":"notes/2024/LISA/note/","text":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LISA/note/#lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning","text":"","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LoRA%2B/note/","text":"LoRA+: Efficient Low Rank Adaptation of Large Models Abstract In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#lora-efficient-low-rank-adaptation-of-large-models","text":"","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#abstract","text":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","title":"Abstract"},{"location":"notes/2024/MFA/note/","text":"Multi-matrix Factorization Attention Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang Abstract We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#multi-matrix-factorization-attention","text":"Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#abstract","text":"We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Abstract"},{"location":"notes/2024/MInference/note/","text":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention Abstract The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#minference-10-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention","text":"","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#abstract","text":"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"Abstract"},{"location":"notes/2024/MaskLLM/note/","text":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models Abstract Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#maskllm-learnable-semi-structured-sparsity-for-large-language-models","text":"","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#abstract","text":"Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"Abstract"},{"location":"notes/2024/MiniKV/note/","text":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache Abstract How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#minikv-pushing-the-limits-of-llm-inference-via-2-bit-layer-discriminative-kv-cache","text":"","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#abstract","text":"How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"Abstract"},{"location":"notes/2024/Minitron/note/","text":"Compact Language Models via Pruning and Knowledge Distillation Abstract Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#compact-language-models-via-pruning-and-knowledge-distillation","text":"","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#abstract","text":"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Abstract"},{"location":"notes/2024/MoA/note/","text":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression Abstract Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#moa-mixture-of-sparse-attention-for-automatic-large-language-model-compression","text":"","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#abstract","text":"Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"Abstract"},{"location":"notes/2024/MoD/note/","text":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Abstract Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling. Method Defining a compute budget \u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a$T/2$\uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684$T^2$\u53d8\u4e3a$\\frac{T}{2}^2$\uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684$T$\u53d8\u4e3a$\\frac{T}{2}$\uff0c\u5373\u539f\u6765\u768450%\u3002 Routing schemes Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9 Routing implementation \u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa$f_i(x)$\u589e\u52a0\u4e86\u4e0e$r_i$\u4f5c\u4e3a\u6743\u91cd\u3002 Non-causal problem during sampling \u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684$r_i$\u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk Training Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49 Results \u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models","text":"","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#abstract","text":"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.","title":"Abstract"},{"location":"notes/2024/MoD/note/#method","text":"","title":"Method"},{"location":"notes/2024/MoD/note/#defining-a-compute-budget","text":"\u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a$T/2$\uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684$T^2$\u53d8\u4e3a$\\frac{T}{2}^2$\uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684$T$\u53d8\u4e3a$\\frac{T}{2}$\uff0c\u5373\u539f\u6765\u768450%\u3002","title":"Defining a compute budget"},{"location":"notes/2024/MoD/note/#routing-schemes","text":"Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9","title":"Routing schemes"},{"location":"notes/2024/MoD/note/#routing-implementation","text":"\u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa$f_i(x)$\u589e\u52a0\u4e86\u4e0e$r_i$\u4f5c\u4e3a\u6743\u91cd\u3002","title":"Routing implementation"},{"location":"notes/2024/MoD/note/#non-causal-problem-during-sampling","text":"\u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684$r_i$\u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk","title":"Non-causal problem during sampling"},{"location":"notes/2024/MoD/note/#training","text":"Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49","title":"Training"},{"location":"notes/2024/MoD/note/#results","text":"\u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Results"},{"location":"notes/2024/NanoFlow/note/","text":"NanoFlow: Towards Optimal Large Language Model Serving Throughput Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci Abstract Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2024/NanoFlow/note/#nanoflow-towards-optimal-large-language-model-serving-throughput","text":"Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2024/NanoFlow/note/#abstract","text":"Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"Abstract"},{"location":"notes/2024/OSSCAR/note/","text":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization Abstract Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to $125\\times$ lower test perplexity on WikiText with $2\\times$ inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also $6\\times$ -- $8\\times$ faster. Notably, our work considers models with tens of billions of parameters, which is up to $100\\times$ larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 $\\hat{w}$","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#osscar-one-shot-structured-pruning-in-vision-and-language-models-with-combinatorial-optimization","text":"","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#abstract","text":"Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to $125\\times$ lower test perplexity on WikiText with $2\\times$ inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also $6\\times$ -- $8\\times$ faster. Notably, our work considers models with tens of billions of parameters, which is up to $100\\times$ larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 $\\hat{w}$","title":"Abstract"},{"location":"notes/2024/PWGG5HBE/note/","text":"A Survey on Large Language Model Acceleration based on KV Cache Management Abstract Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#a-survey-on-large-language-model-acceleration-based-on-kv-cache-management","text":"","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#abstract","text":"Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"Abstract"},{"location":"notes/2024/PowerInfer-2/note/","text":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone Abstract This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#powerinfer-2-fast-large-language-model-inference-on-a-smartphone","text":"","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#abstract","text":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"Abstract"},{"location":"notes/2024/ProSparse/note/","text":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models Method There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface. Experiment Real accelerate on hardware Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization Training Dataset Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#prosparse-introducing-and-enhancing-intrinsic-activation-sparsity-within-large-language-models","text":"","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#method","text":"There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface.","title":"Method"},{"location":"notes/2024/ProSparse/note/#experiment","text":"","title":"Experiment"},{"location":"notes/2024/ProSparse/note/#real-accelerate-on-hardware","text":"Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization","title":"Real accelerate on hardware"},{"location":"notes/2024/ProSparse/note/#training-dataset","text":"Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"Training Dataset"},{"location":"notes/2024/Q-Sparse/note/","text":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated Abstract We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#q-sparse-all-large-language-models-can-be-fully-sparsely-activated","text":"","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#abstract","text":"We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Abstract"},{"location":"notes/2024/QA-LoRA/note/","text":"QA-LoRA QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QA-LoRA/note/#qa-lora","text":"QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QServer/note/","text":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Abstract Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving","text":"","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#abstract","text":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"Abstract"},{"location":"notes/2024/Quest/note/","text":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference Abstract As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#quest-query-aware-sparsity-for-efficient-long-context-llm-inference","text":"","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#abstract","text":"As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Abstract"},{"location":"notes/2024/ReLU2/note/","text":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs The paper indicates through experiments that models employing $ReLU^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReLU2/note/#relu2-wins-discovering-efficient-activation-functions-for-sparse-llms","text":"The paper indicates through experiments that models employing $ReLU^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReMoE/note/","text":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing Abstract Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#remoe-fully-differentiable-mixture-of-experts-with-relu-routing","text":"","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"Abstract"},{"location":"notes/2024/RecycledAttention/note/","text":"Recycled Attention: Efficient inference for long-context language models Abstract Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#recycled-attention-efficient-inference-for-long-context-language-models","text":"","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#abstract","text":"Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Abstract"},{"location":"notes/2024/SAS/note/","text":"SAS Abstract \u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"SAS"},{"location":"notes/2024/SAS/note/#sas","text":"","title":"SAS"},{"location":"notes/2024/SAS/note/#abstract","text":"\u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"Abstract"},{"location":"notes/2024/SCAP/note/","text":"Post-Training Statistical Calibration for Higher Activation Sparsity Abstract We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#post-training-statistical-calibration-for-higher-activation-sparsity","text":"","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#abstract","text":"We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Abstract"},{"location":"notes/2024/SCBench/note/","text":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods Abstract Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#scbench-a-kv-cache-centric-analysis-of-long-context-methods","text":"","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#abstract","text":"Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"Abstract"},{"location":"notes/2024/SGLang/note/","text":"SGLang: Efficient Execution of Structured Language Model Programs Abstract Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#sglang-efficient-execution-of-structured-language-model-programs","text":"","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#abstract","text":"Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"Abstract"},{"location":"notes/2024/SIFT/note/","text":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models Abstract With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#sparse-is-enough-in-fine-tuning-pre-trained-large-language-models","text":"","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#abstract","text":"With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Abstract"},{"location":"notes/2024/SMAT/note/","text":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei Abstract Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#unleashing-the-power-of-meta-tuning-for-few-shot-generalization-through-sparse-interpolated-experts","text":"Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#abstract","text":"Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Abstract"},{"location":"notes/2024/SN1PK7EK/note/","text":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark Abstract In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#revisiting-zeroth-order-optimization-for-memory-efficient-llm-fine-tuning-a-benchmark","text":"","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#abstract","text":"In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Abstract"},{"location":"notes/2024/SPP/note/","text":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models Abstract Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#spp-sparsity-preserved-parameter-efficient-fine-tuning-for-large-language-models","text":"","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#abstract","text":"Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"Abstract"},{"location":"notes/2024/SampleAttention/note/","text":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention Abstract Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#sampleattention-near-lossless-acceleration-of-long-context-llm-inference-with-adaptive-structured-sparse-attention","text":"","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#abstract","text":"Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.","title":"Abstract"},{"location":"notes/2024/SeerAttention/note/","text":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs Abstract Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#seerattention-learning-intrinsic-sparse-attention-in-your-llms","text":"","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#abstract","text":"Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"Abstract"},{"location":"notes/2024/ShadowLLM/note/","text":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models Abstract The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#shadowllm-predictor-based-contextual-sparsity-for-large-language-models","text":"","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#abstract","text":"The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"Abstract"},{"location":"notes/2024/SharedAttention/note/","text":"Beyond KV Caching: Shared Attention for Efficient LLMs Abstract The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#beyond-kv-caching-shared-attention-for-efficient-llms","text":"","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#abstract","text":"The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Abstract"},{"location":"notes/2024/SliceGPT/note/","text":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns Abstract Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#slicegpt-compress-large-language-models-by-deleting-rows-and-columns","text":"","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#abstract","text":"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"Abstract"},{"location":"notes/2024/SlimGPT/note/","text":"SlimGPT: Layer-wise Structured Pruning for Large Language Models Abstract Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#slimgpt-layer-wise-structured-pruning-for-large-language-models","text":"","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#abstract","text":"Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"Abstract"},{"location":"notes/2024/SnapKV/note/","text":"SnapKV: LLM Knows What You are Looking for Before Generation Abstract Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#snapkv-llm-knows-what-you-are-looking-for-before-generation","text":"","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#abstract","text":"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"Abstract"},{"location":"notes/2024/SparQ/note/","text":"SparQ Attention: Bandwidth-Efficient LLM Inference Abstract The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#sparq-attention-bandwidth-efficient-llm-inference","text":"","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#abstract","text":"The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"Abstract"},{"location":"notes/2024/Sparse-IFT/note/","text":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Abstract Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#sparse-ift-sparse-iso-flop-transformations-for-maximizing-training-efficiency","text":"","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#abstract","text":"Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Abstract"},{"location":"notes/2024/SparseInfer/note/","text":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference Abstract Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#sparseinfer-training-free-prediction-of-activation-sparsity-for-fast-llm-inference","text":"","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#abstract","text":"Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"Abstract"},{"location":"notes/2024/SparseLLM/note/","text":"SparseLLM: Towards Global Pruning for Pre-trained Language Models Abstract The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#sparsellm-towards-global-pruning-for-pre-trained-language-models","text":"","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#abstract","text":"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"Abstract"},{"location":"notes/2024/SqueezeLLM/note/","text":"SqueezeLLM: Dense-and-Sparse Quantization Abstract Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#squeezellm-dense-and-sparse-quantization","text":"","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#abstract","text":"Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"Abstract"},{"location":"notes/2024/T3/note/","text":"T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair Abstract Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG.","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#t3-transparent-tracking-triggering-for-fine-grained-overlap-of-compute-collectives","text":"Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#abstract","text":"Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG.","title":"Abstract"},{"location":"notes/2024/TOVA/note/","text":"Transformers are Multi-State RNNs Abstract Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#transformers-are-multi-state-rnns","text":"","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#abstract","text":"Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Abstract"},{"location":"notes/2024/TinyTrain/note/","text":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge Abstract On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#tinytrain-resource-aware-task-adaptive-sparse-training-of-dnns-at-the-data-scarce-edge","text":"","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#abstract","text":"On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"Abstract"},{"location":"notes/2024/TurboSparse/note/","text":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Abstract Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#turbo-sparse-achieving-llm-sota-performance-with-minimal-activated-parameters","text":"","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#abstract","text":"Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Abstract"},{"location":"notes/2024/ULY1AZGY/note/","text":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment Abstract Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#enabling-high-sparsity-foundational-llama-models-with-efficient-pretraining-and-deployment","text":"","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#abstract","text":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Abstract"},{"location":"notes/2024/VB8C61V6/note/","text":"Compressing LLMs: The Truth is Rarely Pure and Never Simple Abstract Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#compressing-llms-the-truth-is-rarely-pure-and-never-simple","text":"","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#abstract","text":"Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Abstract"},{"location":"notes/2024/Vidur/note/","text":"Vidur: A Large-Scale Simulation Framework For LLM Inference Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov Abstract Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#vidur-a-large-scale-simulation-framework-for-llm-inference","text":"Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#abstract","text":"Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Abstract"},{"location":"notes/2024/Wanda/note/","text":"A Simple and Effective Pruning Approach for Large Language Models Abstract As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#a-simple-and-effective-pruning-approach-for-large-language-models","text":"","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#abstract","text":"As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"Abstract"},{"location":"notes/2024/XGrammar/note/","text":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models Abstract The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#xgrammar-flexible-and-efficient-structured-generation-engine-for-large-language-models","text":"","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#abstract","text":"The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"Abstract"},{"location":"notes/2024/YS9YTT55/note/","text":"LLM Inference Serving: Survey of Recent Advances and Opportunities Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari Abstract This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#llm-inference-serving-survey-of-recent-advances-and-opportunities","text":"Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#abstract","text":"This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"Abstract"},{"location":"notes/2024/ZigZagKV/note/","text":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty Abstract Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#zigzagkv-dynamic-kv-cache-compression-for-long-context-modeling-based-on-layer-uncertainty","text":"","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#abstract","text":"Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"Abstract"},{"location":"notes/2024/flash_llm/","text":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/flash_llm/#flash-llm-enabling-cost-effective-and-highly-efficient-large-generative-model-inference-with-unstructured-sparsity","text":"Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/massive-activations/note/","text":"Massive Activations in Large Language Models Abstract We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#massive-activations-in-large-language-models","text":"","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#abstract","text":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Abstract"},{"location":"notes/2024/streaming-llm/note/","text":"Efficient Streaming Language Models with Attention Sinks Abstract Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#efficient-streaming-language-models-with-attention-sinks","text":"","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#abstract","text":"Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Abstract"},{"location":"notes/2025/07NWF4VE/note/","text":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching Abstract Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#accelerating-llm-inference-throughput-via-asynchronous-kv-cache-prefetching","text":"","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#abstract","text":"Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Abstract"},{"location":"notes/2025/0VRXJQ3F/note/","text":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving Abstract Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#rethinking-key-value-cache-compression-techniques-for-large-language-model-serving","text":"","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#abstract","text":"Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.","title":"Abstract"},{"location":"notes/2025/1DZIJVBI/note/","text":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan Abstract This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#characterizing-compute-communication-overlap-in-gpu-accelerated-distributed-deep-learning-performance-and-power-implications","text":"Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#abstract","text":"This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Abstract"},{"location":"notes/2025/2ZU1IWL6/note/","text":"Fast and Simplex: 2-Simplicial Attention in Triton Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil Abstract Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#fast-and-simplex-2-simplicial-attention-in-triton","text":"Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#abstract","text":"Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Abstract"},{"location":"notes/2025/52A7RO95/note/","text":"Mixture of Experts in Large Language Models Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao Abstract This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#mixture-of-experts-in-large-language-models","text":"Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#abstract","text":"This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Abstract"},{"location":"notes/2025/ACP/note/","text":"Adaptive Computation Pruning for the Forgetting Transformer Abstract The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#adaptive-computation-pruning-for-the-forgetting-transformer","text":"","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#abstract","text":"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Abstract"},{"location":"notes/2025/AMALI/note/","text":"AMALI Abstract GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a - cycle-accurate simulators - \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 - analytical models - \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a 1. \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 2. \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"AMALI"},{"location":"notes/2025/AMALI/note/#amali","text":"","title":"AMALI"},{"location":"notes/2025/AMALI/note/#abstract","text":"GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a - cycle-accurate simulators - \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 - analytical models - \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a 1. \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 2. \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"Abstract"},{"location":"notes/2025/Acc-SpMM/note/","text":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores Abstract General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#acc-spmm-accelerating-general-purpose-sparse-matrix-matrix-multiplication-with-gpu-tensor-cores","text":"","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#abstract","text":"General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/AdaSkip/note/","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference Abstract Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#adaskip-adaptive-sublayer-skipping-for-accelerating-long-context-llm-inference","text":"","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#abstract","text":"Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"Abstract"},{"location":"notes/2025/AdaSplash/note/","text":"AdaSplash: Adaptive Sparse Flash Attention Abstract The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#adasplash-adaptive-sparse-flash-attention","text":"","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#abstract","text":"The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"Abstract"},{"location":"notes/2025/AdaptiveSparseTrainer/note/","text":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training Abstract The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#pruning-large-language-models-with-semi-structural-adaptive-sparse-training","text":"","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#abstract","text":"The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Abstract"},{"location":"notes/2025/Adrenaline/note/","text":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation Abstract In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#injecting-adrenaline-into-llm-serving-boosting-resource-utilization-and-throughput-via-attention-disaggregation","text":"","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#abstract","text":"In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Abstract"},{"location":"notes/2025/AhaKV/note/","text":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu Abstract Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#ahakv-adaptive-holistic-attention-driven-kv-cache-eviction-for-efficient-inference-of-large-language-models","text":"Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#abstract","text":"Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"Abstract"},{"location":"notes/2025/AmberPruner/note/","text":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang Abstract In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#amber-pruner-leveraging-nm-activation-sparsity-for-efficient-prefill-in-large-language-models","text":"Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#abstract","text":"In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Abstract"},{"location":"notes/2025/AttentionPredictor/note/","text":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference Abstract With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#attentionpredictor-temporal-pattern-matters-for-efficient-llm-inference","text":"","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#abstract","text":"With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"Abstract"},{"location":"notes/2025/BaWA/note/","text":"BaWA","title":"BaWA"},{"location":"notes/2025/BaWA/note/#bawa","text":"","title":"BaWA"},{"location":"notes/2025/BlockFFN/note/","text":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun Abstract To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#blockffn-towards-end-side-acceleration-friendly-mixture-of-experts-with-chunk-level-activation-sparsity","text":"Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#abstract","text":"To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"Abstract"},{"location":"notes/2025/CCQ/note/","text":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang Abstract The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#ccq-convolutional-code-for-extreme-low-bit-quantization-in-llms","text":"Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#abstract","text":"The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"Abstract"},{"location":"notes/2025/COMET/note/","text":"COMET: Towards Partical W4A4KV4 LLMs Serving Abstract Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#comet-towards-partical-w4a4kv4-llms-serving","text":"","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#abstract","text":"Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"Abstract"},{"location":"notes/2025/CateKV/note/","text":"CateKV","title":"CateKV"},{"location":"notes/2025/CateKV/note/#catekv","text":"","title":"CateKV"},{"location":"notes/2025/ChunkKV/note/","text":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Abstract To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#chunkkv-semantic-preserving-kv-cache-compression-for-efficient-long-context-llm-inference","text":"","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#abstract","text":"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"Abstract"},{"location":"notes/2025/CometSeed/note/","text":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu Abstract Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\\times$ and for end-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts","text":"Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#abstract","text":"Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\\times$ and for end-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Abstract"},{"location":"notes/2025/Cus-Prun/note/","text":"Pruning General Large Language Models into Customized Expert Models Abstract Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#pruning-general-large-language-models-into-customized-expert-models","text":"","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#abstract","text":"Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Abstract"},{"location":"notes/2025/DBudgetKV/note/","text":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance Abstract To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#dbudgetkv-dynamic-budget-in-kv-cache-compression-for-ensuring-optimal-performance","text":"","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#abstract","text":"To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"Abstract"},{"location":"notes/2025/DeepSeek-R1/note/","text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning","text":"","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#abstract","text":"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"Abstract"},{"location":"notes/2025/DeltaAttention/note/","text":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction Abstract The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#delta-attention-fast-and-accurate-sparse-attention-inference-by-delta-correction","text":"","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#abstract","text":"The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Abstract"},{"location":"notes/2025/DeltaLLM/note/","text":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen Abstract Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#deltallm-a-training-free-framework-exploiting-temporal-sparsity-for-efficient-edge-llm-inference","text":"Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#abstract","text":"Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"Abstract"},{"location":"notes/2025/FastKV/note/","text":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Abstract While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#fastkv-kv-cache-compression-for-fast-long-context-processing-with-token-selective-propagation","text":"","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#abstract","text":"While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"Abstract"},{"location":"notes/2025/FlashInfer/note/","text":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving Abstract Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#flashinfer-efficient-and-customizable-attention-engine-for-llm-inference-serving","text":"","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#abstract","text":"Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"Abstract"},{"location":"notes/2025/FlashOverlap/note/","text":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang Abstract Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2025/FlashOverlap/note/#flashoverlap-a-lightweight-design-for-efficiently-overlapping-communication-and-computation","text":"Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2025/FlashOverlap/note/#abstract","text":"Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"Abstract"},{"location":"notes/2025/FlexPrefill/note/","text":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference Abstract Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference","text":"","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#abstract","text":"Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"Abstract"},{"location":"notes/2025/FlexiDepth/note/","text":"Adaptive Layer-skipping in Pre-trained LLMs Abstract Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#adaptive-layer-skipping-in-pre-trained-llms","text":"","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#abstract","text":"Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Abstract"},{"location":"notes/2025/FoX/note/","text":"Forgetting Transformer: Softmax Attention with a Forget Gate Abstract An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#forgetting-transformer-softmax-attention-with-a-forget-gate","text":"","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#abstract","text":"An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Abstract"},{"location":"notes/2025/FreqKV/note/","text":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension Abstract Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#freqkv-frequency-domain-key-value-compression-for-efficient-context-window-extension","text":"","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#abstract","text":"Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"Abstract"},{"location":"notes/2025/GLA/note/","text":"Hardware-Efficient Attention for Fast Decoding Abstract LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\\times$. MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#hardware-efficient-attention-for-fast-decoding","text":"","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#abstract","text":"LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\\times$. MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Abstract"},{"location":"notes/2025/HATA/note/","text":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li Abstract Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#hata-trainable-and-hardware-efficient-hash-aware-top-k-attention-for-scalable-large-model-inference","text":"Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#abstract","text":"Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"Abstract"},{"location":"notes/2025/HCAttention/note/","text":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao Abstract Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#hcattention-extreme-kv-cache-compression-via-heterogeneous-attention-computing-for-llms","text":"Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#abstract","text":"Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"Abstract"},{"location":"notes/2025/HashAttention/note/","text":"HashAttention: Semantic Sparsity for Faster Inference Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica Abstract Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\\times$ through task-specific fine-tuning. On A100 GPU, at $32\\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\\times$ in GPT-FAST and $2.54\\times$ in FlashDecode, and achieves up to $3.12\\times$ higher throughput for GPT-FAST.","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#hashattention-semantic-sparsity-for-faster-inference","text":"Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#abstract","text":"Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\\times$ through task-specific fine-tuning. On A100 GPU, at $32\\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\\times$ in GPT-FAST and $2.54\\times$ in FlashDecode, and achieves up to $3.12\\times$ higher throughput for GPT-FAST.","title":"Abstract"},{"location":"notes/2025/HelixParallelism/note/","text":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani Abstract As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#helix-parallelism-rethinking-sharding-strategies-for-interactive-multi-million-token-llm-decoding","text":"Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#abstract","text":"As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Abstract"},{"location":"notes/2025/IFPruning/note/","text":"Instruction-Following Pruning for Large Language Models Abstract With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#instruction-following-pruning-for-large-language-models","text":"","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#abstract","text":"With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Abstract"},{"location":"notes/2025/KVCache-Factory/note/","text":"KVCache-Factory Abstract","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#kvcache-factory","text":"","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/KVLink/note/","text":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse Abstract We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Motivation \u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898 KVLink KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#kvlink-accelerating-large-language-models-via-efficient-kv-cache-reuse","text":"","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#abstract","text":"We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.","title":"Abstract"},{"location":"notes/2025/KVLink/note/#motivation","text":"\u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898","title":"Motivation"},{"location":"notes/2025/KVLink/note/#kvlink","text":"KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink"},{"location":"notes/2025/KVSink/note/","text":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs Zunhai Su, Kehong Yuan Abstract Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#kvsink-understanding-and-enhancing-the-preservation-of-attention-sinks-in-kv-cache-quantization-for-llms","text":"Zunhai Su, Kehong Yuan","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#abstract","text":"Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"Abstract"},{"location":"notes/2025/KeepKV/note/","text":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference Abstract Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#keepkv-eliminating-output-perturbation-in-kv-cache-compression-for-efficient-llms-inference","text":"","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#abstract","text":"Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"Abstract"},{"location":"notes/2025/LIMINAL/note/","text":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis Abstract This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need","text":"Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#abstract","text":"This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Abstract"},{"location":"notes/2025/LServer/note/","text":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Abstract Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention","text":"","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#abstract","text":"Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"Abstract"},{"location":"notes/2025/LaRoSA/note/","text":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu Abstract Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#la-rosa-enhancing-llm-efficiency-via-layerwise-rotated-sparse-activation","text":"Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#abstract","text":"Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"Abstract"},{"location":"notes/2025/LeanK/note/","text":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu Abstract Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#leank-learnable-k-cache-channel-pruning-for-efficient-decoding","text":"Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#abstract","text":"Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"Abstract"},{"location":"notes/2025/LinearPatch/note/","text":"A Simple Linear Patch Revives Layer-Pruned Large Language Models Abstract Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#a-simple-linear-patch-revives-layer-pruned-large-language-models","text":"","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#abstract","text":"Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"Abstract"},{"location":"notes/2025/MIRAGE/note/","text":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar Abstract KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#mirage-kv-cache-optimization-through-parameter-remapping-for-multi-tenant-llm-serving","text":"Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#abstract","text":"KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"Abstract"},{"location":"notes/2025/MMInference/note/","text":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu Abstract The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#mminference-accelerating-pre-filling-for-long-context-vlms-via-modality-aware-permutation-sparse-attention","text":"Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#abstract","text":"The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"Abstract"},{"location":"notes/2025/MegaScale-MoE/note/","text":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu Abstract We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#megascale-moe-large-scale-communication-efficient-training-of-mixture-of-experts-models-in-production","text":"Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#abstract","text":"We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"Abstract"},{"location":"notes/2025/MiniCPM4/note/","text":"MiniCPM4: Ultra-Efficient LLMs on End Devices MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun Abstract This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#minicpm4-ultra-efficient-llms-on-end-devices","text":"MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#abstract","text":"This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"Abstract"},{"location":"notes/2025/MoBA/note/","text":"MoBA: Mixture of Block Attention for Long-Context LLMs Abstract Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#moba-mixture-of-block-attention-for-long-context-llms","text":"","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#abstract","text":"Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"Abstract"},{"location":"notes/2025/MoE-MLA-RoPE/note/","text":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat Abstract We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#unifying-mixture-of-experts-and-multi-head-latent-attention-for-efficient-language-models","text":"Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#abstract","text":"We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Abstract"},{"location":"notes/2025/MoR/note/","text":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun Abstract Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation","text":"Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#abstract","text":"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/MoSA/note/","text":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing Abstract Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#mixture-of-sparse-attention-content-based-learnable-sparse-attention-via-expert-choice-routing","text":"","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#abstract","text":"Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Abstract"},{"location":"notes/2025/Mosaic/note/","text":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs Abstract Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#mosaic-composite-projection-pruning-for-resource-efficient-llms","text":"","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#abstract","text":"Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Abstract"},{"location":"notes/2025/NSA/note/","text":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention Abstract Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention","text":"","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#abstract","text":"Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/POD-Attention/note/","text":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference Abstract Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to $59\\%$ (mean $28\\%$), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#pod-attention-unlocking-full-prefill-decode-overlap-for-faster-llm-inference","text":"","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#abstract","text":"Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to $59\\%$ (mean $28\\%$), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"Abstract"},{"location":"notes/2025/PSA/note/","text":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving Abstract Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV cache selection, which results in a trade-off between accuracy and efficiency. A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$ boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse $\\underline{A}$ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget $k$. This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and increases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$, compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#progressive-sparse-attention-algorithm-and-system-co-design-for-efficient-attention-in-llm-serving","text":"","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#abstract","text":"Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV cache selection, which results in a trade-off between accuracy and efficiency. A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$ boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse $\\underline{A}$ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget $k$. This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and increases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$, compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Abstract"},{"location":"notes/2025/PanguUltra/note/","text":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu Abstract We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#pangu-ultra-pushing-the-limits-of-dense-large-language-models-on-ascend-npus","text":"Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#abstract","text":"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Abstract"},{"location":"notes/2025/PoD/note/","text":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity Abstract The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\\%$ KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#compressing-kv-cache-for-long-context-llm-inference-with-inter-layer-attention-similarity","text":"","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#abstract","text":"The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\\%$ KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Abstract"},{"location":"notes/2025/PowerAttention/note/","text":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention Abstract Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in $d$-layer LLMs, allowing each output token to attend to $2^d$ tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by $5\\sim 40\\%$, especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ($3.0\\times$ faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#powerattention-exponentially-scaling-of-receptive-fields-for-effective-sparse-attention","text":"","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#abstract","text":"Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in $d$-layer LLMs, allowing each output token to attend to $2^d$ tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by $5\\sim 40\\%$, especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ($3.0\\times$ faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"Abstract"},{"location":"notes/2025/QuickSilver/note/","text":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh Abstract Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#quicksilver-speeding-up-llm-inference-through-dynamic-token-halting-kv-skipping-contextual-token-fusion-and-adaptive-matryoshka-quantization","text":"Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#abstract","text":"Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"Abstract"},{"location":"notes/2025/Qwen3/note/","text":"Qwen3 Technical Report An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu Abstract In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#qwen3-technical-report","text":"An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#abstract","text":"In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Abstract"},{"location":"notes/2025/R-KV/note/","text":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu Abstract Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 $\\alpha$ \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#r-kv-redundancy-aware-kv-cache-compression-for-training-free-reasoning-models-acceleration","text":"Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#abstract","text":"Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 $\\alpha$ \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"Abstract"},{"location":"notes/2025/R-Sparse/note/","text":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference Abstract Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#r-sparse-rank-aware-activation-sparsity-for-efficient-llm-inference","text":"","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#abstract","text":"Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"Abstract"},{"location":"notes/2025/RaaS/note/","text":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity Abstract Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#efficient-long-decoding-inference-with-reasoning-aware-attention-sparsity","text":"","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#abstract","text":"Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.","title":"Abstract"},{"location":"notes/2025/RadialAttention/note/","text":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han Abstract Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference.","title":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation","text":"Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han","title":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#abstract","text":"Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference.","title":"Abstract"},{"location":"notes/2025/ReAttention/note/","text":"ReAttention: Training-Free Infinite Context with Finite Attention Scope Abstract The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#reattention-training-free-infinite-context-with-finite-attention-scope","text":"","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#abstract","text":"The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"Abstract"},{"location":"notes/2025/ReSA/note/","text":"Rectified Sparse Attention Abstract Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#rectified-sparse-attention","text":"","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#abstract","text":"Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Abstract"},{"location":"notes/2025/RecursiveTransformers/note/","text":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster Abstract Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#relaxed-recursive-transformers-effective-parameter-sharing-with-layer-wise-lora","text":"Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#abstract","text":"Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Abstract"},{"location":"notes/2025/SALE/note/","text":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling Abstract Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#sale-low-bit-estimation-for-efficient-sparse-attention-in-long-context-llm-prefilling","text":"","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#abstract","text":"Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"Abstract"},{"location":"notes/2025/SDS/note/","text":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism Abstract Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#enhancing-one-shot-pruned-pre-trained-language-models-through-sparse-dense-sparse-mechanism","text":"","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#abstract","text":"Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Abstract"},{"location":"notes/2025/SEAP/note/","text":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models Abstract Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#seap-training-free-sparse-expert-activation-pruning-unlock-the-brainpower-of-large-language-models","text":"","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#abstract","text":"Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"Abstract"},{"location":"notes/2025/SeerAttention-R/note/","text":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning Abstract We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#seerattention-r-sparse-attention-adaptation-for-long-reasoning","text":"","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#abstract","text":"We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"Abstract"},{"location":"notes/2025/Seesaw/note/","text":"Seesaw: High-throughput LLM Inference via Model Re-sharding Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko Abstract To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#seesaw-high-throughput-llm-inference-via-model-re-sharding","text":"Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#abstract","text":"To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/","text":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference Abstract With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV. \u4e24\u4e2a Observation Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks ShadowKV Pre-filling \u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002 Decoding \u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#shadowkv-kv-cache-in-shadows-for-high-throughput-long-context-llm-inference","text":"","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#abstract","text":"With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/#observation","text":"Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks","title":"\u4e24\u4e2a Observation"},{"location":"notes/2025/ShadowKV/note/#shadowkv","text":"","title":"ShadowKV"},{"location":"notes/2025/ShadowKV/note/#pre-filling","text":"\u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002","title":"Pre-filling"},{"location":"notes/2025/ShadowKV/note/#decoding","text":"\u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"Decoding"},{"location":"notes/2025/SharePrefill/note/","text":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing Abstract Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#accelerating-prefilling-for-long-context-llms-via-sparse-pattern-sharing","text":"","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#abstract","text":"Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/SlimLLM/note/","text":"SlimLLM: Accurate Structured Pruning for Large Language Models Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang Abstract Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#slimllm-accurate-structured-pruning-for-large-language-models","text":"Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#abstract","text":"Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"Abstract"},{"location":"notes/2025/SpargeAttn/note/","text":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Abstract An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#spargeattn-accurate-sparse-attention-accelerating-any-model-inference","text":"","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#abstract","text":"An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"Abstract"},{"location":"notes/2025/SparsingLaw/note/","text":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity Abstract Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#sparsing-law-towards-large-language-models-with-greater-activation-sparsity","text":"","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#abstract","text":"Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Abstract"},{"location":"notes/2025/SpecEE/note/","text":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting Abstract Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#specee-accelerating-large-language-model-inference-with-speculative-early-exiting","text":"","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#abstract","text":"Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"Abstract"},{"location":"notes/2025/SpindleKV/note/","text":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang Abstract Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#spindlekv-a-novel-kv-cache-reduction-method-balancing-both-shallow-and-deep-layers","text":"Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#abstract","text":"Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"Abstract"},{"location":"notes/2025/StarAttention/note/","text":"Star Attention: Efficient LLM Inference over Long Sequences Abstract Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#star-attention-efficient-llm-inference-over-long-sequences","text":"","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#abstract","text":"Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Abstract"},{"location":"notes/2025/Step-3/note/","text":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang Abstract Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding","text":"StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#abstract","text":"Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Abstract"},{"location":"notes/2025/Super-Experts-Profilling/note/","text":"Unveiling Super Experts in Mixture-of-Experts Large Language Models Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan Abstract Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#unveiling-super-experts-in-mixture-of-experts-large-language-models","text":"Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Abstract"},{"location":"notes/2025/TEAL/note/","text":"Training-Free Activation Sparsity in Large Language Models Abstract Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#training-free-activation-sparsity-in-large-language-models","text":"","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#abstract","text":"Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Abstract"},{"location":"notes/2025/Task-KV/note/","text":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads Abstract KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#task-kv-task-aware-kv-cache-optimization-via-semantic-differentiation-of-attention-heads","text":"","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#abstract","text":"KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Abstract"},{"location":"notes/2025/TileLink/note/","text":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu Abstract Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives","text":"Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#abstract","text":"Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"Abstract"},{"location":"notes/2025/TokenWeave/note/","text":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference Raja Gond, Nipun Kwatra, Ramachandran Ramjee Abstract Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#tokenweave-efficient-compute-communication-overlap-for-distributed-llm-inference","text":"Raja Gond, Nipun Kwatra, Ramachandran Ramjee","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#abstract","text":"Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"Abstract"},{"location":"notes/2025/TorchAO/note/","text":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 Abstract We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#torchao-pytorch-native-training-to-serving-model-optimization","text":"Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#abstract","text":"We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"Abstract"},{"location":"notes/2025/Triton-distributed/note/","text":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu Abstract In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#triton-distributed-programming-overlapping-kernels-on-distributed-ai-systems-with-the-triton-compiler","text":"Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#abstract","text":"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Abstract"},{"location":"notes/2025/UC0D8DJ6/note/","text":"Characterizing Communication Patterns in Distributed Large Language Model Inference Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda Abstract Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#characterizing-communication-patterns-in-distributed-large-language-model-inference","text":"Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#abstract","text":"Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Abstract"},{"location":"notes/2025/XAttention/note/","text":"XAttention: Block Sparse Attention with Antidiagonal Scoring Abstract Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#xattention-block-sparse-attention-with-antidiagonal-scoring","text":"","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#abstract","text":"Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"Abstract"},{"location":"notes/2025/kvpress/note/","text":"kvpress Abstract","title":"kvpress"},{"location":"notes/2025/kvpress/note/#kvpress","text":"","title":"kvpress"},{"location":"notes/2025/kvpress/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/sparse-frontier/note/","text":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs Abstract Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#the-sparse-frontier-sparse-attention-trade-offs-in-transformer-llms","text":"","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#abstract","text":"Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"Abstract"},{"location":"notes/2025/topk-decoding/note/","text":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs Abstract There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#exploiting-sparsity-for-long-context-inference-million-token-contexts-on-commodity-gpus","text":"","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#abstract","text":"There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/vAttention/note/","text":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention Abstract PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#vattention-dynamic-memory-management-for-serving-llms-without-pagedattention","text":"","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#abstract","text":"PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"Abstract"}]}