{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EfficientPaper Pruning, Quantization and efficient-inference/training paper list. Table of Contents EfficientPaper Getting Started Paper List keyword year publication institution author References Getting Started git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper } Paper List year 2025 AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress [ ] 2024 Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] Massive Activations in Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ] 2023 Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ] 2022 Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ] 2021 Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ] 2020 Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ] 2019 ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ] 2018 A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ] 2017 DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ] 2016 Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ] 1993 Optimal Brain Surgeon and general network pruning [ ] 1989 Optimal Brain Damage [ ] References Hot https://github.com/horseee/Awesome-Efficient-LLM https://github.com/DefTruth/Awesome-Diffusion-Inference https://github.com/DefTruth/Awesome-LLM-Inference https://github.com/AmberLJC/LLMSys-PaperList https://github.com/Hannibal046/Awesome-LLM https://github.com/AmadeusChan/Awesome-LLM-System-Papers https://github.com/KnowingNothing/compiler-and-arch https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management https://github.com/October2001/Awesome-KV-Cache-Compression Cold https://github.com/he-y/Awesome-Pruning https://github.com/htqin/awesome-model-quantization https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression https://github.com/AojunZhou/Efficient-Deep-Learning https://github.com/chester256/Model-Compression-Papers :arrow_up: Back to top","title":"Home"},{"location":"#efficientpaper","text":"Pruning, Quantization and efficient-inference/training paper list.","title":"EfficientPaper"},{"location":"#table-of-contents","text":"EfficientPaper Getting Started Paper List keyword year publication institution author References","title":"Table of Contents"},{"location":"#getting-started","text":"git clone https://github.com/hustzxd/EfficientPaper pip install protobuf==5.27.2 pandas arxiv Add paper information by ./add_paper_info.sh Run ./refresh_readme.sh efficient_paper.prototxt paper { title: \"EfficientPaper: manage your research papers in an efficient way.\" abbr: \"EfficientPaper\" url: \"https://github.com/hustzxd/EfficientPaper\" authors: \"hustzxd\" } pub { where: \"GitHub\" year: 2023 } code { type: \"Pytorch\" url: \"https://github.com/hustzxd/EfficientPaper\" } note { url: \"EfficientPaper.md\" } keyword { words: efficient_paper }","title":"Getting Started"},{"location":"#paper-list","text":"","title":"Paper List"},{"location":"#year","text":"","title":"year"},{"location":"#2025","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference [ ] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training [ ] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention [ ] COMET: Towards Partical W4A4KV4 LLMs Serving [ ] POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference [ ] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention [ ] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity [ ] KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs [ ] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism [ ] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [ ] Forgetting Transformer: Softmax Attention with a Forget Gate [ ] R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference [ ] ReAttention: Training-Free Infinite Context with Finite Attention Scope [ ] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA [ ] Training-Free Activation Sparsity in Large Language Models [ ] AdaSplash: Adaptive Sparse Flash Attention [ ] BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation [ ] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration [ ] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity [ ] HashAttention: Semantic Sparsity for Faster Inference [ ] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation [ ] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention [ ] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference [ ] SlimLLM: Accurate Structured Pruning for Large Language Models [ ] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference [ ] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity [ ] Star Attention: Efficient LLM Inference over Long Sequences [ ] XAttention: Block Sparse Attention with Antidiagonal Scoring [ ] TorchAO: PyTorch-Native Training-to-Serving Model Optimization [ ] AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs [ ] SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting [ ] Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models [ ] A Simple Linear Patch Revives Layer-Pruned Large Language Models [ ] Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores [ ] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching [ ] Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing [ ] Adaptive Computation Pruning for the Forgetting Transformer [ ] Adaptive Layer-skipping in Pre-trained LLMs [ ] AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models [ ] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models [ ] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference [ ] CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs [ ] Characterizing Communication Patterns in Distributed Large Language Model Inference [ ] Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications [ ] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference [ ] Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts [ ] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance [ ] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [ ] Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction [ ] DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference [ ] Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need [ ] Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity [ ] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs [ ] Fast and Simplex: 2-Simplicial Attention in Triton [ ] FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation [ ] FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving [ ] FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation [ ] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension [ ] HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference [ ] HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs [ ] Hardware-Efficient Attention for Fast Decoding [ ] Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding [ ] Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation [ ] Instruction-Following Pruning for Large Language Models [ ] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse [ ] KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference [ ] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention [ ] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding [ ] MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving [ ] MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production [ ] MiniCPM4: Ultra-Efficient LLMs on End Devices [ ] Mixture of Experts in Large Language Models [ ] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing [ ] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation [ ] MoBA: Mixture of Block Attention for Long-Context LLMs [ ] Mosaic: Composite Projection Pruning for Resource-efficient LLMs [ ] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs [ ] PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention [ ] Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving [ ] Pruning General Large Language Models into Customized Expert Models [ ] QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization [ ] Qwen3 Technical Report [ ] R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration [ ] Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation [ ] Rectified Sparse Attention [ ] Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving [ ] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling [ ] SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models [ ] SeerAttention-R: Sparse Attention Adaptation for Long Reasoning [ ] Seesaw: High-throughput LLM Inference via Model Re-sharding [ ] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers [ ] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding [ ] Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads [ ] The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs [ ] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives [ ] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference [ ] Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler [ ] Unveiling Super Experts in Mixture-of-Experts Large Language Models [ ] Unified KV Cache Compression Methods for Auto-Regressive Models [ ] kvpress [ ]","title":"2025"},{"location":"#2024","text":"Fluctuation-based Adaptive Structured Pruning for Large Language Models [ ] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition [ ] T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives [ ] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [ ] A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems [ ] CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models [ ] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption [ ] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference [ ] Post-Training Statistical Calibration for Higher Activation Sparsity [ ] A Simple and Effective Pruning Approach for Large Language Models [ ] Compressing LLMs: The Truth is Rarely Pure and Never Simple [ ] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs [ ] Efficient Streaming Language Models with Attention Sinks [ ] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [ ] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [ ] SAS: Structured Activation Spasification [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns [ ] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models [ ] Accelerating Transformer Pre-training with 2:4 Sparsity [ ] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [ ] FrameQuant: Flexible Low-Bit Quantization for Transformers [ ] LoRA+: Efficient Low Rank Adaptation of Large Models [ ] OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization [ ] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity [ ] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference [ ] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models [ ] SparQ Attention: Bandwidth-Efficient LLM Inference [ ] Sparse is Enough in Fine-tuning Pre-trained Large Language Models [ ] Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] SqueezeLLM: Dense-and-Sparse Quantization [ ] TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge [ ] Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration [ ] Vidur: A Large-Scale Simulation Framework For LLM Inference [ ] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention [ ] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models [ ] SGLang: Efficient Execution of Structured Language Model Programs [ ] SlimGPT: Layer-wise Structured Pruning for Large Language Models [ ] SparseLLM: Towards Global Pruning for Pre-trained Language Models [ ] Fast and Effective Weight Update for Pruned Large Language Models [ ] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity [ ] A Survey on Efficient Inference for Large Language Models [ ] A Survey on Inference Optimization Techniques for Mixture of Experts Models [ ] A Survey on Large Language Model Acceleration based on KV Cache Management [ ] APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving [ ] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis [ ] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference [ ] Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs [ ] Beyond KV Caching: Shared Attention for Efficient LLMs [ ] Compact Language Models via Pruning and Knowledge Distillation [ ] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation [ ] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model [ ] DeepSeek-V3 Technical Report [ ] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [ ] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping [ ] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads [ ] Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment [ ] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes [ ] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion [ ] FlashMask: Efficient and Rich Mask Extension of FlashAttention [ ] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache [ ] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization [ ] L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ [ ] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning [ ] LLM Inference Serving: Survey of Recent Advances and Opportunities [ ] Massive Activations in Large Language Models [ ] MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache [ ] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models [ ] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression [ ] Multi-matrix Factorization Attention [ ] NanoFlow: Towards Optimal Large Language Model Serving Throughput [ ] Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification [ ] Pytorch Post-Training Sparse Attention with Double Sparsity [ ] PowerInfer-2: Fast Large Language Model Inference on a Smartphone [ ] Website ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models [ ] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated [ ] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving [ ] Pytorch ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs [ ] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing [ ] Recycled Attention: Efficient inference for long-context language models [ ] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention [ ] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark [ ] SCBench: A KV Cache-Centric Analysis of Long-Context Methods [ ] SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention [ ] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs [ ] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models [ ] SnapKV: LLM Knows What You are Looking for Before Generation [ ] Transformers are Multi-State RNNs [ ] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters [ ] Pytorch XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models [ ] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty [ ] [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch [ ]","title":"2024"},{"location":"#2023","text":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences [ ] Gradient-based Intra-attention Pruning on Pre-trained Language Models [ ] Pruning Pre-trained Language Models Without Fine-Tuning [ ] Pruning Pre-trained Language Models with Principled Importance and Self-regularization [ ] Structured Pruning for Efficient Generative Pre-trained Language Models [ ] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [ ] Structural Pruning of Large Language Models via Neural Architecture Search [ ] SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer [ ] TorchSparse++: Efficient Point Cloud Engine [ ] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [ ] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [ ] Minimum Variance Unbiased N:M Sparsity for the Neural Gradients [ ] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. [ ] Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation [ ] Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning [ ] ZipLM: Inference-Aware Structured Pruning of Language Models [ ] VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention [ ] Efficient Methods for Natural Language Processing: A Survey [ ] SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [ ] A Survey on Evaluation of Large Language Models [ ] A Survey on Model Compression for Large Language Models [ ] Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models [ ] CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X [ ] Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models [ ] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [ ] Efficient Guided Generation for Large Language Models [ ] Fine-Tuning Language Models with Just Forward Passes [ ] Flash-Decoding for long-context inference [ ] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning [ ] Gradient-Free Structured Pruning with Unlabeled Data [ ] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models [ ] Knowledge-preserving Pruning for Pre-trained Language Models without Retraining [ ] LLM in a flash: Efficient Large Language Model Inference with Limited Memory [ ] LLM-Pruner: On the Structural Pruning of Large Language Models [ ] LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery [ ] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [ ] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [ ] Post-training Quantization for Neural Networks with Provable Guarantees [ ] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [ ] Pruning Large Language Models via Accuracy Predictor [ ] QLoRA: Efficient Finetuning of Quantized LLMs [ ] QuIP: Quantization with Incoherence Processing [ ] RPTQ: Reorder-based Post-training Quantization for Large Language Models [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [ ] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression [ ] Sparse Fine-tuning for Inference Acceleration of Large Language Models [ ] Sparse Iso-FLOP Transformations for Maximizing Training Efficiency [ ] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [ ] Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers [ ] The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter [ ] Training Transformers with 4-bit Integers [ ] Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering [ ] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [ ] FasterTransformer [ ]","title":"2023"},{"location":"#2022","text":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm [ ] TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models [ ] Creating Sparse GPT-3 Models with Iterative Pruning [ ] LoRA: Low-rank adaptation of large language models [ ] SPDY: Accurate Pruning with Speedup Guarantees [ ] Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation [ ] A Fast Post-Training Pruning Framework for Transformers [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [ ] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [ ] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [ ] Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks [ ] Transformer Acceleration with Dynamic Sparse Attention [ ] An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers [ ] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models [ ]","title":"2022"},{"location":"#2021","text":"Post-training deep neural network pruning via layer-wise calibration [ ] BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [ ] Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch [ ] A Greedy Algorithm for Quantizing Neural Networks [ ] Channel Permutations for N:M Sparsity [ ] Accelerating Sparse Deep Neural Networks [ ] Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads [ ] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks [ ]","title":"2021"},{"location":"#2020","text":"Fast Sparse ConvNets [ ] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference [ ] Movement Pruning: Adaptive Sparsity by Fine-Tuning [ ] GPU Kernels for Block-Sparse Weights [ ]","title":"2020"},{"location":"#2019","text":"ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training [ ]","title":"2019"},{"location":"#2018","text":"A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [ ]","title":"2018"},{"location":"#2017","text":"DSD: Dense-Sparse-Dense Training for Deep Neural Networks [ ] Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [ ]","title":"2017"},{"location":"#2016","text":"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [ ]","title":"2016"},{"location":"#1993","text":"Optimal Brain Surgeon and general network pruning [ ]","title":"1993"},{"location":"#1989","text":"Optimal Brain Damage [ ]","title":"1989"},{"location":"#references","text":"","title":"References"},{"location":"#hot","text":"https://github.com/horseee/Awesome-Efficient-LLM https://github.com/DefTruth/Awesome-Diffusion-Inference https://github.com/DefTruth/Awesome-LLM-Inference https://github.com/AmberLJC/LLMSys-PaperList https://github.com/Hannibal046/Awesome-LLM https://github.com/AmadeusChan/Awesome-LLM-System-Papers https://github.com/KnowingNothing/compiler-and-arch https://papercopilot.com/paper-list https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management https://github.com/October2001/Awesome-KV-Cache-Compression","title":"Hot"},{"location":"#cold","text":"https://github.com/he-y/Awesome-Pruning https://github.com/htqin/awesome-model-quantization https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression https://github.com/AojunZhou/Efficient-Deep-Learning https://github.com/chester256/Model-Compression-Papers :arrow_up: Back to top","title":"Cold"},{"location":"cls_author/","text":"author Aaron Courville Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Abhay Gupta Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Adam Fisch Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Aixin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ajay Jaiswal Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note Amir Gholami Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Amir H. Abdi Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Andr\u00e9 F. T. Martins Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Aojun Zhou Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Arvind Krishnamurthy Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ashish Panwar Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Bairu Hou Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Baris Kasikci Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Bei Feng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Beidi Chen Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note Bin Gao Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Bin Lin Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Bin Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Bing Xue Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bingxuan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Bochao Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chang Gao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note Chaojun Xiao Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Chen Chen Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Chen Zhang Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Chengda Lu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chenggang Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengqi Deng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chengquan Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note Chengruidong Zhang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Chenyang Song Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Chenyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Chong Ruan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Christos Kozyrakis Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note Chuang Gan Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note Clark Barrett Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Cody Hao Yu Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note Coleman Hooper Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Damai Dai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dan Alistarh Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Daxin Jiang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Daya Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dejian Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Deli Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dianhai Yu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Dong Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Dongjie Ji Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Dongsheng Li Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Dongyang Wang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Eldar Kurtic Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Elias Frantar Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Erhang Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fan Yang Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Fangyun Lin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fei Huang Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Fucong Dai Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Fuli Luo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Furu Wei Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note Genghan Zhang Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Gongfan Fang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Guanchen Li Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Guangbo Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guangxuan Xiao Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Guanting Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Guohao Dai Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Guowei Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note H. Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hai Zhao Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Haibin Lin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Haibo Chen Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haifeng Wang Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note Han Bao Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hanshi Sun Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Hanwei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hao Zhang Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Haocheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Haocheng Xi Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Haofeng Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note Haoli Bai Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Haotian Tang Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Haotong Xie Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Haowei Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hayden Kwok-Hay So Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Heung-Yeung Shum Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Honghui Ding Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hongsheng Li Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Hrayr Harutyunyan Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Huajian Xin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huazuo Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Hui Qu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Huiqiang Jiang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Iman Mirzadeh Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Ion Stoica Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note J. L. Cai Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note J. Zico Kolter Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Jan Kautz Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Jason D. Lee Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Jayashree Mohan Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Jeff Pool Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Jiaming Tang Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Jiaming Xu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note Jian Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jianfei Chen Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note Jianfeng Gao Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Jianxi Ye Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jianyong Wang Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note Jianzhong Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiaqi Ni Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiashi Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jiawei Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jie Zhou Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Jin Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jin Fang Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Jingchang Chen Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Jingyang Yuan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Joseph E. Gonzalez Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note Jun Zhu Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note Junjie Qiu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junlong Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junxian Guo Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Junxiao Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Junyang Lin Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note Kai Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kai Hu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kaige Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kan Zhu Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Kang Guan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kang Zhao Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Ke Hong Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Kehong Yuan Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Kexin Huang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kuai Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Kurt Keutzer Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Lean Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lecong Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lefei Zhang Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Lei Chen Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Lei Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Leyi Xia Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Li Dong Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Li-Wen Chang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Lian Liu Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Liang Zhao Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lianmin Zheng Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Lili Qiu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Litong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Liyue Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Lu Hou Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Mao Yang Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Maosong Sun Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Marcos Treviso Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note Mark Kurtz Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Mehrdad Farajtabar Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models Meng Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mengdi Wang Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Miaojun Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Michael Goin Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Michael Hassid Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Michael W. Mahoney Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Mingchuan Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghua Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minghui Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Mingjie Sun Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Mingming Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Minmin Sun Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Ning Tian Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ningxin Zheng Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Nipun Kwatra Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Panpan Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pavlo Molchanov Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Peiyi Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Peng Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Pengcheng He Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Pengfei Zuo Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Qi Hou Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Qiancheng Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qianhui Wu Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Qihao Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Qinyu Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qiushi Du Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. J. Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note R. L. Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ramachandran Ramjee Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Ramya Prabhu Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Rayan Saab Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Roy Schwartz Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note Ruihang Lai Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ruiqi Ge Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruisong Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruizhe Pan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runji Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Runxin Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruoyu Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ruyi Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note S. S. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Sangmin Bae Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Saurav Muralidharan Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Sean Lie Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Sehoon Kim Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note Shang Yang Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Shanghao Lu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shangyan Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shanhuang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shaoqing Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shengen Yan Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shengfeng Ye Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note Shijie Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Shirong Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shiwei Liu Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Shiyao Li Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Shiyu Chang Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Shiyu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shreyas Saxena Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note Shuang Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuiping Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shunfeng Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Shuo Yang Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Shuting Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Size Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Song Han Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Stephanie Wang Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Surin Ahn Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note T. Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tal Schuster Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Tao Xie Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Tao Yuan Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Tao Yun Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tian Pei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianle Cai Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Tianlong Chen Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Tianqi Chen Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Tianqi Wu Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Tianyu Fu Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Tianyu Gao Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note Tianyu Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Tianzhu Ye Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tim Dettmers Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression Ting Cao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Tong Yang Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Torsten Hoefler Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note Tri Dao Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note Tuo Zhao Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Vithursan Thangarasa Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note W. L. Xiao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wangding Zeng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wanjia Zhao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei An Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wei Lin Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Wei Wang Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Weilin Zhao Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Weiyu Huang Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Weizhu Chen Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Wen Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenfeng Liang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenjun Gao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wenlei Bao Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Wenqin Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Wentao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Woosuk Kwon Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Wulong Liu Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note X. Q. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiafei Qiu Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note Xiandong Zhao Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiang Liu Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note Xiangyu Zhang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Xiangyue Jin Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xianzhi Yu Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Xianzu Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiao Bi Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaodong Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaohan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaojin Shen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaokang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaosha Chen Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaotao Nie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaowei Li Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Xiaowen Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiaoxiang Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Chen Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Xin Cheng Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xin Jin Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Xin Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xin Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinchao Wang Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Xingchao Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xingkai Yu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinnan Song Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinxia Shan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyi Zhou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Yang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xinyu Zhou Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Xinyuan Li Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xiuhong Li Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Xu Han Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Xu Owen He Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Xuecheng Su Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Xuefei Ning Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note Xuegui Zheng Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Xufang Luo Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note Xuheng Lin Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. K. Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Q. Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. Wu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Y. X. Wei Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Y. X. Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yang Li Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note Yang Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yanhong Xu Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note Yankai Lin Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yanping Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yao Zhao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaofeng Sun Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yaohui Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yehui Tang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yi Yu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yi Zheng Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yichao Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yifan Shi Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yikai Zhang Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yiliang Xiong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yilong Zhao Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note Ying He Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ying Sheng Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Ying Tang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yingfa Chen Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note Yinhe Han Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Yishi Piao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yisong Wang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiwu Yao Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note Yixiao Li Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Yixin Dong Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note Yixin Song Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Yixuan Tan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyang Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yiyuan Ma Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Yizhao Gao Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yong Li Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Yongqiang Guo Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yu Cheng Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yu Wang Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Yu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuan Ou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuandong Tian Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Yuchen Zhu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yucheng Li Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Yuduan Wang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yue Gong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuezhou Hu Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note Yuheng Zou Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuhui Xu Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Yujia He Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yujun Lin Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Yukun Zha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yulhwa Kim Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note Yunfan Xiong Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yunhe Wang Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Yunxian Ma Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuqing Xia Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuqing Yang Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note Yutao Sun Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Yuting Yan Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang Luo Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxiang You Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuxin Wu Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Yuxiong He Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Yuxuan Li Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Yuxuan Liu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Yuyang Zhou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. F. Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Z. Z. Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zefan Cai Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note Zehui Ren Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zeyu Mi Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Zhangli Sha Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhangyang Wang Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhe Fu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhean Xu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Dong Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Zhen Huang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhen Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenda Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhengyan Zhang Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhenyu Zhang Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Zhewei Yao Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation Zhewen Hao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhibin Gou Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhicheng Ma Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhigang Yan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhihang Yuan Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note Zhihong Shao Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhilin Yang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipeng Xu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhixuan Lin Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Zhiyu Wu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhiyuan Liu Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Zhongyu Zhang Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zhou Yu Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Zhuang Liu Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note Zhuomin He Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Zhuoshu Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zihan Wang Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note Zihao Ye Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Ziheng Jiang Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Zihui Gu Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijia Zhu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zijun Liu Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zili Wang Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Zilin Li Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziqing Yang Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note Ziwei Ji Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Ziwei Xie Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zixiao Huang Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Zixuan Zhou Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Ziyang Song Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Ziyi Gao Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zizheng Pan Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note Zuchao Li Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Zunhai Su Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"By Author"},{"location":"cls_author/#author","text":"","title":"author"},{"location":"cls_author/#aaron-courville","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Aaron Courville"},{"location":"cls_author/#abhay-gupta","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Abhay Gupta"},{"location":"cls_author/#adam-fisch","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Adam Fisch"},{"location":"cls_author/#aixin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Aixin Liu"},{"location":"cls_author/#ajay-jaiswal","text":"Meta Title Cover Publish Code Note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note","title":"Ajay Jaiswal"},{"location":"cls_author/#amir-gholami","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Amir Gholami"},{"location":"cls_author/#amir-h-abdi","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Amir H. Abdi"},{"location":"cls_author/#andre-f-t-martins","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Andr\u00e9 F. T. Martins"},{"location":"cls_author/#aojun-zhou","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Aojun Zhou"},{"location":"cls_author/#arvind-krishnamurthy","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Arvind Krishnamurthy"},{"location":"cls_author/#ashish-panwar","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ashish Panwar"},{"location":"cls_author/#bairu-hou","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Bairu Hou"},{"location":"cls_author/#baris-kasikci","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Baris Kasikci"},{"location":"cls_author/#bei-feng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bei Feng"},{"location":"cls_author/#beidi-chen","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note","title":"Beidi Chen"},{"location":"cls_author/#bin-gao","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Bin Gao"},{"location":"cls_author/#bin-lin","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Bin Lin"},{"location":"cls_author/#bin-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Bin Wang"},{"location":"cls_author/#bing-xue","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bing Xue"},{"location":"cls_author/#bingxuan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bingxuan Wang"},{"location":"cls_author/#bochao-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Bochao Wu"},{"location":"cls_author/#chang-gao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Qwen3 Qwen3 Technical Report note","title":"Chang Gao"},{"location":"cls_author/#chaojun-xiao","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Chaojun Xiao"},{"location":"cls_author/#chen-chen","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Chen Chen"},{"location":"cls_author/#chen-zhang","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Chen Zhang"},{"location":"cls_author/#chengda-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengda Lu"},{"location":"cls_author/#chenggang-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenggang Zhao"},{"location":"cls_author/#chengqi-deng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chengqi Deng"},{"location":"cls_author/#chengquan-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note","title":"Chengquan Jiang"},{"location":"cls_author/#chengruidong-zhang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Chengruidong Zhang"},{"location":"cls_author/#chenyang-song","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Chenyang Song"},{"location":"cls_author/#chenyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chenyu Zhang"},{"location":"cls_author/#chong-ruan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Chong Ruan"},{"location":"cls_author/#christos-kozyrakis","text":"Meta Title Cover Publish Code Note SGLang SGLang: Efficient Execution of Structured Language Model Programs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"Christos Kozyrakis"},{"location":"cls_author/#chuang-gan","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note","title":"Chuang Gan"},{"location":"cls_author/#clark-barrett","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Clark Barrett"},{"location":"cls_author/#cody-hao-yu","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note","title":"Cody Hao Yu"},{"location":"cls_author/#coleman-hooper","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Coleman Hooper"},{"location":"cls_author/#damai-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Damai Dai"},{"location":"cls_author/#dan-alistarh","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Dan Alistarh"},{"location":"cls_author/#daxin-jiang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Daxin Jiang"},{"location":"cls_author/#daya-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Daya Guo"},{"location":"cls_author/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_author/#dejian-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dejian Yang"},{"location":"cls_author/#deli-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Deli Chen"},{"location":"cls_author/#dianhai-yu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Dianhai Yu"},{"location":"cls_author/#dong-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Dong Li"},{"location":"cls_author/#dongjie-ji","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Dongjie Ji"},{"location":"cls_author/#dongsheng-li","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Dongsheng Li"},{"location":"cls_author/#dongyang-wang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Dongyang Wang"},{"location":"cls_author/#eldar-kurtic","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Eldar Kurtic"},{"location":"cls_author/#elias-frantar","text":"Meta Title Cover Publish Code Note SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models","title":"Elias Frantar"},{"location":"cls_author/#erhang-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Erhang Li"},{"location":"cls_author/#fan-yang","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Fan Yang"},{"location":"cls_author/#fangyun-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fangyun Lin"},{"location":"cls_author/#fei-huang","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Fei Huang"},{"location":"cls_author/#fucong-dai","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fucong Dai"},{"location":"cls_author/#fuli-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Fuli Luo"},{"location":"cls_author/#furu-wei","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReSA Rectified Sparse Attention note","title":"Furu Wei"},{"location":"cls_author/#genghan-zhang","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Genghan Zhang"},{"location":"cls_author/#gongfan-fang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Gongfan Fang"},{"location":"cls_author/#guanchen-li","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Guanchen Li"},{"location":"cls_author/#guangbo-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guangbo Hao"},{"location":"cls_author/#guangxuan-xiao","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Guangxuan Xiao"},{"location":"cls_author/#guanting-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guanting Chen"},{"location":"cls_author/#guohao-dai","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Guohao Dai"},{"location":"cls_author/#guowei-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Guowei Li"},{"location":"cls_author/#h-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"H. Zhang"},{"location":"cls_author/#hai-zhao","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note","title":"Hai Zhao"},{"location":"cls_author/#haibin-lin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Haibin Lin"},{"location":"cls_author/#haibo-chen","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haibo Chen"},{"location":"cls_author/#haifeng-wang","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"Haifeng Wang"},{"location":"cls_author/#han-bao","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Han Bao"},{"location":"cls_author/#hanshi-sun","text":"Meta Title Cover Publish Code Note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Hanshi Sun"},{"location":"cls_author/#hanwei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hanwei Xu"},{"location":"cls_author/#hao-zhang","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Hao Zhang"},{"location":"cls_author/#haocheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haocheng Wang"},{"location":"cls_author/#haocheng-xi","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Haocheng Xi"},{"location":"cls_author/#haofeng-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"Haofeng Huang"},{"location":"cls_author/#haoli-bai","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Haoli Bai"},{"location":"cls_author/#haotian-tang","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Haotian Tang"},{"location":"cls_author/#haotong-xie","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Haotong Xie"},{"location":"cls_author/#haowei-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Haowei Zhang"},{"location":"cls_author/#hayden-kwok-hay-so","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Hayden Kwok-Hay So"},{"location":"cls_author/#heung-yeung-shum","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Heung-Yeung Shum"},{"location":"cls_author/#honghui-ding","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Honghui Ding"},{"location":"cls_author/#hongsheng-li","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Hongsheng Li"},{"location":"cls_author/#hrayr-harutyunyan","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Hrayr Harutyunyan"},{"location":"cls_author/#huajian-xin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huajian Xin"},{"location":"cls_author/#huazuo-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Huazuo Gao"},{"location":"cls_author/#hui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Li"},{"location":"cls_author/#hui-qu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Hui Qu"},{"location":"cls_author/#huiqiang-jiang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Huiqiang Jiang"},{"location":"cls_author/#iman-mirzadeh","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Iman Mirzadeh"},{"location":"cls_author/#ion-stoica","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Ion Stoica"},{"location":"cls_author/#j-l-cai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"J. L. Cai"},{"location":"cls_author/#j-zico-kolter","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"J. Zico Kolter"},{"location":"cls_author/#jan-kautz","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Jan Kautz"},{"location":"cls_author/#jason-d-lee","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Jason D. Lee"},{"location":"cls_author/#jayashree-mohan","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Jayashree Mohan"},{"location":"cls_author/#jeff-pool","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Jeff Pool"},{"location":"cls_author/#jiaming-tang","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Jiaming Tang"},{"location":"cls_author/#jiaming-xu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"Jiaming Xu"},{"location":"cls_author/#jian-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jian Liang"},{"location":"cls_author/#jianfei-chen","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"Jianfei Chen"},{"location":"cls_author/#jianfeng-gao","text":"Meta Title Cover Publish Code Note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Jianfeng Gao"},{"location":"cls_author/#jianxi-ye","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jianxi Ye"},{"location":"cls_author/#jianyong-wang","text":"Meta Title Cover Publish Code Note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note","title":"Jianyong Wang"},{"location":"cls_author/#jianzhong-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jianzhong Guo"},{"location":"cls_author/#jiaqi-ni","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiaqi Ni"},{"location":"cls_author/#jiashi-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiashi Li"},{"location":"cls_author/#jiawei-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jiawei Wang"},{"location":"cls_author/#jie-zhou","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Jie Zhou"},{"location":"cls_author/#jin-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jin Chen"},{"location":"cls_author/#jin-fang","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Jin Fang"},{"location":"cls_author/#jingchang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingchang Chen"},{"location":"cls_author/#jingyang-yuan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Jingyang Yuan"},{"location":"cls_author/#joseph-e-gonzalez","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"Joseph E. Gonzalez"},{"location":"cls_author/#jun-zhu","text":"Meta Title Cover Publish Code Note m Training Transformers with 4-bit Integers m Accelerating Transformer Pre-training with 2:4 Sparsity note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"Jun Zhu"},{"location":"cls_author/#junjie-qiu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junjie Qiu"},{"location":"cls_author/#junlong-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junlong Li"},{"location":"cls_author/#junxian-guo","text":"Meta Title Cover Publish Code Note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Junxian Guo"},{"location":"cls_author/#junxiao-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Junxiao Song"},{"location":"cls_author/#junyang-lin","text":"Meta Title Cover Publish Code Note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note Qwen3 Qwen3 Technical Report note","title":"Junyang Lin"},{"location":"cls_author/#kai-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Dong"},{"location":"cls_author/#kai-hu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kai Hu"},{"location":"cls_author/#kaige-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kaige Gao"},{"location":"cls_author/#kan-zhu","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note","title":"Kan Zhu"},{"location":"cls_author/#kang-guan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kang Guan"},{"location":"cls_author/#kang-zhao","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Kang Zhao"},{"location":"cls_author/#ke-hong","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Ke Hong"},{"location":"cls_author/#kehong-yuan","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Kehong Yuan"},{"location":"cls_author/#kexin-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kexin Huang"},{"location":"cls_author/#kuai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Kuai Yu"},{"location":"cls_author/#kurt-keutzer","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Kurt Keutzer"},{"location":"cls_author/#lean-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lean Wang"},{"location":"cls_author/#lecong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lecong Zhang"},{"location":"cls_author/#lefei-zhang","text":"Meta Title Cover Publish Code Note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Lefei Zhang"},{"location":"cls_author/#lei-chen","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Lei Chen"},{"location":"cls_author/#lei-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Lei Xu"},{"location":"cls_author/#leyi-xia","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Leyi Xia"},{"location":"cls_author/#li-dong","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Li Dong"},{"location":"cls_author/#li-wen-chang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Li-Wen Chang"},{"location":"cls_author/#lian-liu","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Lian Liu"},{"location":"cls_author/#liang-zhao","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liang Zhao"},{"location":"cls_author/#lianmin-zheng","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Lianmin Zheng"},{"location":"cls_author/#lili-qiu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Lili Qiu"},{"location":"cls_author/#litong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Litong Wang"},{"location":"cls_author/#liyue-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Liyue Zhang"},{"location":"cls_author/#lu-hou","text":"Meta Title Cover Publish Code Note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Lu Hou"},{"location":"cls_author/#mao-yang","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Mao Yang"},{"location":"cls_author/#maosong-sun","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Maosong Sun"},{"location":"cls_author/#marcos-treviso","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Marcos Treviso"},{"location":"cls_author/#mark-kurtz","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Mark Kurtz"},{"location":"cls_author/#mehrdad-farajtabar","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"Mehrdad Farajtabar"},{"location":"cls_author/#meng-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Meng Li"},{"location":"cls_author/#mengdi-wang","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Mengdi Wang"},{"location":"cls_author/#miaojun-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Miaojun Wang"},{"location":"cls_author/#michael-goin","text":"Meta Title Cover Publish Code Note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Michael Goin"},{"location":"cls_author/#michael-hassid","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Michael Hassid"},{"location":"cls_author/#michael-w-mahoney","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Michael W. Mahoney"},{"location":"cls_author/#mingchuan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingchuan Zhang"},{"location":"cls_author/#minghua-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghua Zhang"},{"location":"cls_author/#minghui-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Minghui Tang"},{"location":"cls_author/#mingjie-sun","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Mingjie Sun"},{"location":"cls_author/#mingming-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Mingming Li"},{"location":"cls_author/#minmin-sun","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Minmin Sun"},{"location":"cls_author/#ning-tian","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ning Tian"},{"location":"cls_author/#ningxin-zheng","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ningxin Zheng"},{"location":"cls_author/#nipun-kwatra","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Nipun Kwatra"},{"location":"cls_author/#panpan-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Panpan Huang"},{"location":"cls_author/#pavlo-molchanov","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Pavlo Molchanov"},{"location":"cls_author/#peiyi-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peiyi Wang"},{"location":"cls_author/#peng-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Peng Zhang"},{"location":"cls_author/#pengcheng-he","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Pengcheng He"},{"location":"cls_author/#pengfei-zuo","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Pengfei Zuo"},{"location":"cls_author/#qi-hou","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Qi Hou"},{"location":"cls_author/#qiancheng-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiancheng Wang"},{"location":"cls_author/#qianhui-wu","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Qianhui Wu"},{"location":"cls_author/#qihao-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qihao Zhu"},{"location":"cls_author/#qinyu-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Qinyu Chen"},{"location":"cls_author/#qiushi-du","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Qiushi Du"},{"location":"cls_author/#r-j-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. J. Chen"},{"location":"cls_author/#r-l-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"R. L. Jin"},{"location":"cls_author/#ramachandran-ramjee","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Ramachandran Ramjee"},{"location":"cls_author/#ramya-prabhu","text":"Meta Title Cover Publish Code Note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Ramya Prabhu"},{"location":"cls_author/#rayan-saab","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"Rayan Saab"},{"location":"cls_author/#roy-schwartz","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TOVA Transformers are Multi-State RNNs note","title":"Roy Schwartz"},{"location":"cls_author/#ruihang-lai","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Ruihang Lai"},{"location":"cls_author/#ruiqi-ge","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruiqi Ge"},{"location":"cls_author/#ruisong-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruisong Zhang"},{"location":"cls_author/#ruizhe-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruizhe Pan"},{"location":"cls_author/#runji-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runji Wang"},{"location":"cls_author/#runxin-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Runxin Xu"},{"location":"cls_author/#ruoyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruoyu Zhang"},{"location":"cls_author/#ruyi-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ruyi Chen"},{"location":"cls_author/#s-s-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"S. S. Li"},{"location":"cls_author/#sangmin-bae","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Sangmin Bae"},{"location":"cls_author/#saurav-muralidharan","text":"Meta Title Cover Publish Code Note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note","title":"Saurav Muralidharan"},{"location":"cls_author/#sean-lie","text":"Meta Title Cover Publish Code Note Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Sean Lie"},{"location":"cls_author/#sehoon-kim","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note","title":"Sehoon Kim"},{"location":"cls_author/#shang-yang","text":"Meta Title Cover Publish Code Note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"Shang Yang"},{"location":"cls_author/#shanghao-lu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanghao Lu"},{"location":"cls_author/#shangyan-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shangyan Zhou"},{"location":"cls_author/#shanhuang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shanhuang Chen"},{"location":"cls_author/#shaoqing-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shaoqing Wu"},{"location":"cls_author/#shengen-yan","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shengen Yan"},{"location":"cls_author/#shengfeng-ye","text":"Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note","title":"Shengfeng Ye"},{"location":"cls_author/#shijie-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Shijie Cao"},{"location":"cls_author/#shirong-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shirong Ma"},{"location":"cls_author/#shiwei-liu","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Shiwei Liu"},{"location":"cls_author/#shiyao-li","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Shiyao Li"},{"location":"cls_author/#shiyu-chang","text":"Meta Title Cover Publish Code Note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Shiyu Chang"},{"location":"cls_author/#shiyu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shiyu Wang"},{"location":"cls_author/#shreyas-saxena","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Shreyas Saxena"},{"location":"cls_author/#shuang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuang Zhou"},{"location":"cls_author/#shuiping-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuiping Yu"},{"location":"cls_author/#shunfeng-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shunfeng Zhou"},{"location":"cls_author/#shuo-yang","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Shuo Yang"},{"location":"cls_author/#shuting-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Shuting Pan"},{"location":"cls_author/#size-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Size Zheng"},{"location":"cls_author/#song-han","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Song Han"},{"location":"cls_author/#stephanie-wang","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Stephanie Wang"},{"location":"cls_author/#surin-ahn","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Surin Ahn"},{"location":"cls_author/#t-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"T. Wang"},{"location":"cls_author/#tal-schuster","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Tal Schuster"},{"location":"cls_author/#tao-xie","text":"Meta Title Cover Publish Code Note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Tao Xie"},{"location":"cls_author/#tao-yuan","text":"Meta Title Cover Publish Code Note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note","title":"Tao Yuan"},{"location":"cls_author/#tao-yun","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tao Yun"},{"location":"cls_author/#tian-pei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tian Pei"},{"location":"cls_author/#tianle-cai","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Tianle Cai"},{"location":"cls_author/#tianlong-chen","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Tianlong Chen"},{"location":"cls_author/#tianqi-chen","text":"Meta Title Cover Publish Code Note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Tianqi Chen"},{"location":"cls_author/#tianqi-wu","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Tianqi Wu"},{"location":"cls_author/#tianyu-fu","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Tianyu Fu"},{"location":"cls_author/#tianyu-gao","text":"Meta Title Cover Publish Code Note MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note","title":"Tianyu Gao"},{"location":"cls_author/#tianyu-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Tianyu Sun"},{"location":"cls_author/#tianzhu-ye","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Tianzhu Ye"},{"location":"cls_author/#tim-dettmers","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression","title":"Tim Dettmers"},{"location":"cls_author/#ting-cao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Ting Cao"},{"location":"cls_author/#tong-yang","text":"Meta Title Cover Publish Code Note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Tong Yang"},{"location":"cls_author/#torsten-hoefler","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"Torsten Hoefler"},{"location":"cls_author/#tri-dao","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"Tri Dao"},{"location":"cls_author/#tuo-zhao","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Tuo Zhao"},{"location":"cls_author/#vithursan-thangarasa","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note","title":"Vithursan Thangarasa"},{"location":"cls_author/#w-l-xiao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"W. L. Xiao"},{"location":"cls_author/#wangding-zeng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wangding Zeng"},{"location":"cls_author/#wanjia-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wanjia Zhao"},{"location":"cls_author/#wei-an","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wei An"},{"location":"cls_author/#wei-lin","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Wei Lin"},{"location":"cls_author/#wei-wang","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Wei Wang"},{"location":"cls_author/#weilin-zhao","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Weilin Zhao"},{"location":"cls_author/#weiyu-huang","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Weiyu Huang"},{"location":"cls_author/#weizhu-chen","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Weizhu Chen"},{"location":"cls_author/#wen-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wen Liu"},{"location":"cls_author/#wenfeng-liang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenfeng Liang"},{"location":"cls_author/#wenjun-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenjun Gao"},{"location":"cls_author/#wenlei-bao","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Wenlei Bao"},{"location":"cls_author/#wenqin-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wenqin Yu"},{"location":"cls_author/#wentao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Wentao Zhang"},{"location":"cls_author/#woosuk-kwon","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"Woosuk Kwon"},{"location":"cls_author/#wulong-liu","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Wulong Liu"},{"location":"cls_author/#x-q-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"X. Q. Li"},{"location":"cls_author/#xiafei-qiu","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note","title":"Xiafei Qiu"},{"location":"cls_author/#xiandong-zhao","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiandong Zhao"},{"location":"cls_author/#xiang-liu","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note","title":"Xiang Liu"},{"location":"cls_author/#xiangyu-zhang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Xiangyu Zhang"},{"location":"cls_author/#xiangyue-jin","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiangyue Jin"},{"location":"cls_author/#xianzhi-yu","text":"Meta Title Cover Publish Code Note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Xianzhi Yu"},{"location":"cls_author/#xianzu-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xianzu Wang"},{"location":"cls_author/#xiao-bi","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiao Bi"},{"location":"cls_author/#xiaodong-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaodong Liu"},{"location":"cls_author/#xiaohan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaohan Wang"},{"location":"cls_author/#xiaojin-shen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaojin Shen"},{"location":"cls_author/#xiaokang-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Chen"},{"location":"cls_author/#xiaokang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaokang Zhang"},{"location":"cls_author/#xiaosha-chen","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaosha Chen"},{"location":"cls_author/#xiaotao-nie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaotao Nie"},{"location":"cls_author/#xiaowei-li","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Xiaowei Li"},{"location":"cls_author/#xiaowen-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaowen Sun"},{"location":"cls_author/#xiaoxiang-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xiaoxiang Wang"},{"location":"cls_author/#xin-chen","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note","title":"Xin Chen"},{"location":"cls_author/#xin-cheng","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Cheng"},{"location":"cls_author/#xin-jin","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Xin Jin"},{"location":"cls_author/#xin-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xin Liu"},{"location":"cls_author/#xin-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xin Xie"},{"location":"cls_author/#xinchao-wang","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note","title":"Xinchao Wang"},{"location":"cls_author/#xingchao-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingchao Liu"},{"location":"cls_author/#xingkai-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xingkai Yu"},{"location":"cls_author/#xinnan-song","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinnan Song"},{"location":"cls_author/#xinxia-shan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinxia Shan"},{"location":"cls_author/#xinyi-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyi Zhou"},{"location":"cls_author/#xinyu-yang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyu Yang"},{"location":"cls_author/#xinyu-zhou","text":"Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Xinyu Zhou"},{"location":"cls_author/#xinyuan-li","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xinyuan Li"},{"location":"cls_author/#xiuhong-li","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Xiuhong Li"},{"location":"cls_author/#xu-han","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Xu Han"},{"location":"cls_author/#xu-owen-he","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Xu Owen He"},{"location":"cls_author/#xuecheng-su","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuecheng Su"},{"location":"cls_author/#xuefei-ning","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note","title":"Xuefei Ning"},{"location":"cls_author/#xuegui-zheng","text":"Meta Title Cover Publish Code Note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Xuegui Zheng"},{"location":"cls_author/#xufang-luo","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note","title":"Xufang Luo"},{"location":"cls_author/#xuheng-lin","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Xuheng Lin"},{"location":"cls_author/#y-k-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. K. Li"},{"location":"cls_author/#y-q-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. Q. Wang"},{"location":"cls_author/#y-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note","title":"Y. Wu"},{"location":"cls_author/#y-x-wei","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Wei"},{"location":"cls_author/#y-x-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Y. X. Zhu"},{"location":"cls_author/#yang-li","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note","title":"Yang Li"},{"location":"cls_author/#yang-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yang Zhang"},{"location":"cls_author/#yanhong-xu","text":"Meta Title Cover Publish Code Note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V3 DeepSeek-V3 Technical Report note None DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note","title":"Yanhong Xu"},{"location":"cls_author/#yankai-lin","text":"Meta Title Cover Publish Code Note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yankai Lin"},{"location":"cls_author/#yanping-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yanping Huang"},{"location":"cls_author/#yao-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Li"},{"location":"cls_author/#yao-zhao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yao Zhao"},{"location":"cls_author/#yaofeng-sun","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaofeng Sun"},{"location":"cls_author/#yaohui-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Li"},{"location":"cls_author/#yaohui-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yaohui Wang"},{"location":"cls_author/#yehui-tang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yehui Tang"},{"location":"cls_author/#yi-yu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Yu"},{"location":"cls_author/#yi-zheng","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yi Zheng"},{"location":"cls_author/#yichao-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yichao Zhang"},{"location":"cls_author/#yifan-shi","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yifan Shi"},{"location":"cls_author/#yikai-zhang","text":"Meta Title Cover Publish Code Note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yikai Zhang"},{"location":"cls_author/#yiliang-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiliang Xiong"},{"location":"cls_author/#yilong-zhao","text":"Meta Title Cover Publish Code Note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note","title":"Yilong Zhao"},{"location":"cls_author/#ying-he","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying He"},{"location":"cls_author/#ying-sheng","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Ying Sheng"},{"location":"cls_author/#ying-tang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ying Tang"},{"location":"cls_author/#yingfa-chen","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note","title":"Yingfa Chen"},{"location":"cls_author/#yinhe-han","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Yinhe Han"},{"location":"cls_author/#yishi-piao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yishi Piao"},{"location":"cls_author/#yisong-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yisong Wang"},{"location":"cls_author/#yiwu-yao","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note","title":"Yiwu Yao"},{"location":"cls_author/#yixiao-li","text":"Meta Title Cover Publish Code Note LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Yixiao Li"},{"location":"cls_author/#yixin-dong","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note","title":"Yixin Dong"},{"location":"cls_author/#yixin-song","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Yixin Song"},{"location":"cls_author/#yixuan-tan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yixuan Tan"},{"location":"cls_author/#yiyang-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyang Ma"},{"location":"cls_author/#yiyuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yiyuan Liu"},{"location":"cls_author/#yiyuan-ma","text":"Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note","title":"Yiyuan Ma"},{"location":"cls_author/#yizhao-gao","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yizhao Gao"},{"location":"cls_author/#yong-li","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note","title":"Yong Li"},{"location":"cls_author/#yongqiang-guo","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yongqiang Guo"},{"location":"cls_author/#yu-cheng","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yu Cheng"},{"location":"cls_author/#yu-wang","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Yu Wang"},{"location":"cls_author/#yu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yu Wu"},{"location":"cls_author/#yuan-ou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuan Ou"},{"location":"cls_author/#yuandong-tian","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Yuandong Tian"},{"location":"cls_author/#yuchen-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuchen Zhu"},{"location":"cls_author/#yucheng-li","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Yucheng Li"},{"location":"cls_author/#yuduan-wang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuduan Wang"},{"location":"cls_author/#yue-gong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yue Gong"},{"location":"cls_author/#yuezhou-hu","text":"Meta Title Cover Publish Code Note m Accelerating Transformer Pre-training with 2:4 Sparsity note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"Yuezhou Hu"},{"location":"cls_author/#yuheng-zou","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuheng Zou"},{"location":"cls_author/#yuhui-xu","text":"Meta Title Cover Publish Code Note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Yuhui Xu"},{"location":"cls_author/#yujia-he","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yujia He"},{"location":"cls_author/#yujun-lin","text":"Meta Title Cover Publish Code Note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note","title":"Yujun Lin"},{"location":"cls_author/#yukun-zha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yukun Zha"},{"location":"cls_author/#yulhwa-kim","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note","title":"Yulhwa Kim"},{"location":"cls_author/#yunfan-xiong","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunfan Xiong"},{"location":"cls_author/#yunhe-wang","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Yunhe Wang"},{"location":"cls_author/#yunxian-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yunxian Ma"},{"location":"cls_author/#yuqing-xia","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yuqing Xia"},{"location":"cls_author/#yuqing-yang","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note","title":"Yuqing Yang"},{"location":"cls_author/#yutao-sun","text":"Meta Title Cover Publish Code Note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Yutao Sun"},{"location":"cls_author/#yuting-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuting Yan"},{"location":"cls_author/#yuxiang-luo","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang Luo"},{"location":"cls_author/#yuxiang-you","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxiang You"},{"location":"cls_author/#yuxin-wu","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Yuxin Wu"},{"location":"cls_author/#yuxiong-he","text":"Meta Title Cover Publish Code Note ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Yuxiong He"},{"location":"cls_author/#yuxuan-li","text":"Meta Title Cover Publish Code Note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Yuxuan Li"},{"location":"cls_author/#yuxuan-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuxuan Liu"},{"location":"cls_author/#yuyang-zhou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Yuyang Zhou"},{"location":"cls_author/#z-f-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. F. Wu"},{"location":"cls_author/#z-z-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Z. Z. Ren"},{"location":"cls_author/#zefan-cai","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note","title":"Zefan Cai"},{"location":"cls_author/#zehui-ren","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zehui Ren"},{"location":"cls_author/#zeyu-mi","text":"Meta Title Cover Publish Code Note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Zeyu Mi"},{"location":"cls_author/#zhangli-sha","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhangli Sha"},{"location":"cls_author/#zhangyang-wang","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhangyang Wang"},{"location":"cls_author/#zhe-fu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhe Fu"},{"location":"cls_author/#zhean-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhean Xu"},{"location":"cls_author/#zhen-dong","text":"Meta Title Cover Publish Code Note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Zhen Dong"},{"location":"cls_author/#zhen-huang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Huang"},{"location":"cls_author/#zhen-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhen Zhang"},{"location":"cls_author/#zhenda-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhenda Xie"},{"location":"cls_author/#zhengyan-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhengyan Zhang"},{"location":"cls_author/#zhenyu-zhang","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Zhenyu Zhang"},{"location":"cls_author/#zhewei-yao","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation","title":"Zhewei Yao"},{"location":"cls_author/#zhewen-hao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhewen Hao"},{"location":"cls_author/#zhibin-gou","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhibin Gou"},{"location":"cls_author/#zhicheng-ma","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhicheng Ma"},{"location":"cls_author/#zhigang-yan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhigang Yan"},{"location":"cls_author/#zhihang-yuan","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note m A Survey on Efficient Inference for Large Language Models note","title":"Zhihang Yuan"},{"location":"cls_author/#zhihong-shao","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhihong Shao"},{"location":"cls_author/#zhilin-yang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhilin Yang"},{"location":"cls_author/#zhipeng-xu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhipeng Xu"},{"location":"cls_author/#zhixuan-lin","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Zhixuan Lin"},{"location":"cls_author/#zhiyu-wu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhiyu Wu"},{"location":"cls_author/#zhiyuan-liu","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zhiyuan Liu"},{"location":"cls_author/#zhongyu-zhang","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhongyu Zhang"},{"location":"cls_author/#zhou-yu","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Zhou Yu"},{"location":"cls_author/#zhuang-liu","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note","title":"Zhuang Liu"},{"location":"cls_author/#zhuomin-he","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note","title":"Zhuomin He"},{"location":"cls_author/#zhuoshu-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zhuoshu Li"},{"location":"cls_author/#zihan-wang","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note","title":"Zihan Wang"},{"location":"cls_author/#zihao-ye","text":"Meta Title Cover Publish Code Note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Zihao Ye"},{"location":"cls_author/#ziheng-jiang","text":"Meta Title Cover Publish Code Note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"Ziheng Jiang"},{"location":"cls_author/#zihui-gu","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zihui Gu"},{"location":"cls_author/#zijia-zhu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijia Zhu"},{"location":"cls_author/#zijun-liu","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zijun Liu"},{"location":"cls_author/#zili-wang","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"Zili Wang"},{"location":"cls_author/#zilin-li","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zilin Li"},{"location":"cls_author/#ziqing-yang","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"Ziqing Yang"},{"location":"cls_author/#ziwei-ji","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Ziwei Ji"},{"location":"cls_author/#ziwei-xie","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziwei Xie"},{"location":"cls_author/#zixiao-huang","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Zixiao Huang"},{"location":"cls_author/#zixuan-zhou","text":"Meta Title Cover Publish Code Note m A Survey on Efficient Inference for Large Language Models note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"Zixuan Zhou"},{"location":"cls_author/#ziyang-song","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyang Song"},{"location":"cls_author/#ziyi-gao","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Ziyi Gao"},{"location":"cls_author/#zizheng-pan","text":"Meta Title Cover Publish Code Note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"Zizheng Pan"},{"location":"cls_author/#zuchao-li","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"Zuchao Li"},{"location":"cls_author/#zunhai-su","text":"Meta Title Cover Publish Code Note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Zunhai Su"},{"location":"cls_institution/","text":"institution AMD Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note AWS AI Labs Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Advanced Micro Devices Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Alibaba Cloud Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Alibaba Group Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Apple Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models IFPruning Instruction-Following Pruning for Large Language Models note Beihang University Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning ByteDance Meta Title Cover Publish Code Note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ByteDance Inc Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note 52A7RO95 Mixture of Experts in Large Language Models note ByteDance Seed Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note CMU Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note CPII under InnoHK Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Carnegie Mellon University Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Center for Advanced AI Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note Central South University Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note Cerebras Systems Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Chinese University of Hong Kong Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note Chongqing University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor City University of Hong Kong Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note Cohere Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Comenius University Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Computer Network Information Center, Chinese Academy of Sciences Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note Cornell University Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note DENSO IT Lab Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note DeepAuto.ai Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeepMind Meta Title Cover Publish Code Note m Fast Sparse ConvNets DeepSeek-AI Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeepSpeed Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note Delft University of Technology Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note Duke University Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note ETH Zurich Meta Title Cover Publish Code Note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ETH Z\u00fcrich Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Eindhoven University of Technology Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Emory University Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note FAIR Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note Fairleigh Dickinson University Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note Fudan University Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note Fudan Universitynst2 Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note Gaoling School of Artificial Intelligence, Renmin University of China Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Georgia Institute of Technology Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note Google Meta Title Cover Publish Code Note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note Google Cloud Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google DeepMind Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Google Research Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note Graphcore Research Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Habana Labs Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients Harbin Institute of Technology Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Harvard University Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Heriot-Watt University Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Hong Kong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note Houmo AI Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Huawei Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Huawei Cloud Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Huawei Noah's Ark Lab Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note Huawei Noah\u2019s Ark Lab Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Huawei Noah\u2019s Ark Lab, Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note Huawei Technologies Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note Huazhong University of Science and Technology Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Hugging Face Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning IST Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey IST Austria Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Imperial College London Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note Indian Institute of Science Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note Infinigence-AI Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note Institute for Advanced Algorithms Research Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Institute of Automation, Chinese Academy of Sciences Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models Institute of Computing Technology Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note Institute of Computing Technology, Chinese Academy of Sciences Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Institute of Information Engineering, Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models Intel Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Intel Corporation Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration Intellifusion Inc. Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KAIST Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note KAIST AI Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note KAUST Meta Title Cover Publish Code Note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note Key Laboratory of Multimedia Trusted Perception and Efficient Computing Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Kyushu University Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Lanzhou University Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note Leiden University Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note MBZUAI Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note MIT Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note MIT-IBM Watson AI Lab Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note MakerMaker AI Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note Massachusetts Institute of Technology Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note Megvii Technology Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note Meituan Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note Meta Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note Meta AI Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Meta AI (FAIR) Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Meta Platforms Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Michigan State University Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note Microsoft Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note Microsoft Azure Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note Microsoft Azure AI Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning Microsoft Research Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Microsoft Research India Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Mila & Universite de Montreal Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note MiniCPM Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note Ministry of Education of China, Xiamen University Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note Mohamed bin Zayed University of AI Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Moonshot AI Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Multimedia Laboratory (MMLab) Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note NVDIA Meta Title Cover Publish Code Note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note NVIDIA Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress note NVIDIA Research Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note NanKai University Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Nanjing University Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Nanyang Technological University Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Cus-Prun Pruning General Large Language Models into Customized Expert Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note National University of Singapore Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Neural Magic Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note New York University Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note Noah\u2019s Ark Lab, Huawei Technologies Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note Normal Computing Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note North China Electric Power University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Northeastern University Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers Northwestern University Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Numenta Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note OPPO Research Institute Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note Ohio State University Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note OpenAI Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights OpenGVLab Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models OpenTeams Inc Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note Oxford University Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note Peking University Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Perplexity AI Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note Princeton University Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note Purdue University Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note PyTorch Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note Qwen Team Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note Renmin University of China Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note Rice University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SJTU Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note Salesforce AI Research Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Salesforce Research Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note Samsung Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note Santa Clara University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note School of Cyber Security, University of Chinese Academy of Sciences Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models SenseTime Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SenseTime Research Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch Seoul National University Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note Shanghai AI Laboratory Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note Shanghai Artificial Intelligence Laboratory Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note Shanghai Artificial Intelligence Laboratorys Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note Shanghai Jiao Tong University Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note Shanghai Jiao Tong Universtiy Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note Shanghai Jiaotong University Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ShanghaiTech University Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS) Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note Singapore University of Technology and Design Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note Sogang University Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note Stanford Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Stanford University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note StepFun Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note StepFun Inc. Meta Title Cover Publish Code Note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Sun Yat-sen University Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note Sungkyunkwan University Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note Synthesia Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Tencent AI Lab Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note Tencent Machine Learning Platform Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Tencent Youtu Lab Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note The Chinese University of Hong Kong Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note The Hebrew University of Jerusalem Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note The Hebrew University of Jerusalem, Israel Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey The Hong Kong Polytechnic University Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note The Hong Kong University of Science and Technology Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note The Ohio State University Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note The University of Hong Kong Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note The University of North Carolina Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note The University of Texas at Austin Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note Together AI Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note Tongji University Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor Tsinghua University Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note UC Berkeley Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note UC Santa Barbara Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note UCSD Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees Univeristy of Sydney Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note Universidade da Coru\u00f1a Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Universidade de Lisboa Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note University College London Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note University of Basel Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers University of California Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note University of California, Berkeley Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note University of California, San Diego Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note University of Chinese Academy of Sciences Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note University of Connecticut Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm University of Edinburgh Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note University of Electronic Science and Technology of China Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction University of Hong Kong Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note University of Illinois Urbana-Champaign Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note University of Maryland Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note University of Oxford Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note University of Science and Technology Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note University of Science and Technology of China Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note University of Seoul Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note University of Southern California Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note University of St Andrews Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note University of Surrey Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Surrey, UK Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering University of Texas at Austin Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity University of Washington Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note University of Waterloo Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note University of Wisconsin Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note University of Wisconsin-Madison Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note VITA Group Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Vector Institute Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Vizuara AI Labs Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note Vokram Group Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note WeChat AI Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note Wuhan University Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note Xiamen University Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Xiaohongshu Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Yale University Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences Zhe Jiang University Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Zhejiang University Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Zhipu.AI Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Zhongguancun Laboratory Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning baidu Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note iFLYTEK Research Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note inst1 Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note inst2 Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"By Institution"},{"location":"cls_institution/#institution","text":"","title":"institution"},{"location":"cls_institution/#amd","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note","title":"AMD"},{"location":"cls_institution/#aws-ai-labs","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AWS AI Labs"},{"location":"cls_institution/#advanced-micro-devices","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Advanced Micro Devices"},{"location":"cls_institution/#alibaba-cloud","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Alibaba Cloud"},{"location":"cls_institution/#alibaba-group","text":"Meta Title Cover Publish Code Note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Alibaba Group"},{"location":"cls_institution/#apple","text":"Meta Title Cover Publish Code Note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models IFPruning Instruction-Following Pruning for Large Language Models note","title":"Apple"},{"location":"cls_institution/#beihang-university","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning","title":"Beihang University"},{"location":"cls_institution/#bytedance","text":"Meta Title Cover Publish Code Note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"ByteDance"},{"location":"cls_institution/#bytedance-inc","text":"Meta Title Cover Publish Code Note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note 52A7RO95 Mixture of Experts in Large Language Models note","title":"ByteDance Inc"},{"location":"cls_institution/#bytedance-seed","text":"Meta Title Cover Publish Code Note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"ByteDance Seed"},{"location":"cls_institution/#cmu","text":"Meta Title Cover Publish Code Note massive-activations Massive Activations in Large Language Models note","title":"CMU"},{"location":"cls_institution/#cpii-under-innohk","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"CPII under InnoHK"},{"location":"cls_institution/#carnegie-mellon-university","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Wanda A Simple and Effective Pruning Approach for Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Carnegie Mellon University"},{"location":"cls_institution/#center-for-advanced-ai","text":"Meta Title Cover Publish Code Note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"Center for Advanced AI"},{"location":"cls_institution/#central-south-university","text":"Meta Title Cover Publish Code Note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note","title":"Central South University"},{"location":"cls_institution/#cerebras-systems","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Cerebras Systems"},{"location":"cls_institution/#chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note","title":"Chinese University of Hong Kong"},{"location":"cls_institution/#chongqing-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor","title":"Chongqing University"},{"location":"cls_institution/#city-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"City University of Hong Kong"},{"location":"cls_institution/#cohere","text":"Meta Title Cover Publish Code Note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Cohere"},{"location":"cls_institution/#comenius-university","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"Comenius University"},{"location":"cls_institution/#computer-network-information-center-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note","title":"Computer Network Information Center, Chinese Academy of Sciences"},{"location":"cls_institution/#cornell-university","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning QuIP QuIP: Quantization with Incoherence Processing Recycled Attention Recycled Attention: Efficient inference for long-context language models note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note","title":"Cornell University"},{"location":"cls_institution/#denso-it-lab","text":"Meta Title Cover Publish Code Note SAS SAS: Structured Activation Spasification note","title":"DENSO IT Lab"},{"location":"cls_institution/#deepautoai","text":"Meta Title Cover Publish Code Note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"DeepAuto.ai"},{"location":"cls_institution/#deepmind","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets","title":"DeepMind"},{"location":"cls_institution/#deepseek-ai","text":"Meta Title Cover Publish Code Note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note","title":"DeepSeek-AI"},{"location":"cls_institution/#deepspeed","text":"Meta Title Cover Publish Code Note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note","title":"DeepSpeed"},{"location":"cls_institution/#delft-university-of-technology","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Delft University of Technology"},{"location":"cls_institution/#duke-university","text":"Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Duke University"},{"location":"cls_institution/#eth-zurich","text":"Meta Title Cover Publish Code Note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note","title":"ETH Zurich"},{"location":"cls_institution/#eth-zurich_1","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"ETH Z\u00fcrich"},{"location":"cls_institution/#eindhoven-university-of-technology","text":"Meta Title Cover Publish Code Note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"Eindhoven University of Technology"},{"location":"cls_institution/#emory-university","text":"Meta Title Cover Publish Code Note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note","title":"Emory University"},{"location":"cls_institution/#fair","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"FAIR"},{"location":"cls_institution/#fairleigh-dickinson-university","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"Fairleigh Dickinson University"},{"location":"cls_institution/#fudan-university","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note","title":"Fudan University"},{"location":"cls_institution/#fudan-universitynst2","text":"Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note","title":"Fudan Universitynst2"},{"location":"cls_institution/#gaoling-school-of-artificial-intelligence-renmin-university-of-china","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Gaoling School of Artificial Intelligence, Renmin University of China"},{"location":"cls_institution/#georgia-institute-of-technology","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note","title":"Georgia Institute of Technology"},{"location":"cls_institution/#google","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note","title":"Google"},{"location":"cls_institution/#google-cloud","text":"Meta Title Cover Publish Code Note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Cloud"},{"location":"cls_institution/#google-deepmind","text":"Meta Title Cover Publish Code Note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google DeepMind"},{"location":"cls_institution/#google-research","text":"Meta Title Cover Publish Code Note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"Google Research"},{"location":"cls_institution/#graphcore-research","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Graphcore Research"},{"location":"cls_institution/#habana-labs","text":"Meta Title Cover Publish Code Note MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients","title":"Habana Labs"},{"location":"cls_institution/#harbin-institute-of-technology","text":"Meta Title Cover Publish Code Note TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Harbin Institute of Technology"},{"location":"cls_institution/#harvard-university","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"Harvard University"},{"location":"cls_institution/#heriot-watt-university","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Heriot-Watt University"},{"location":"cls_institution/#hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note","title":"Hong Kong University of Science and Technology"},{"location":"cls_institution/#houmo-ai","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Houmo AI"},{"location":"cls_institution/#huawei","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note","title":"Huawei"},{"location":"cls_institution/#huawei-cloud","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"Huawei Cloud"},{"location":"cls_institution/#huawei-noahs-ark-lab","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note","title":"Huawei Noah's Ark Lab"},{"location":"cls_institution/#huawei-noahs-ark-lab_1","text":"Meta Title Cover Publish Code Note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Huawei Noah\u2019s Ark Lab"},{"location":"cls_institution/#huawei-noahs-ark-lab_2","text":"Meta Title Cover Publish Code Note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note","title":"Huawei Noah\u2019s Ark Lab,"},{"location":"cls_institution/#huawei-technologies","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"Huawei Technologies"},{"location":"cls_institution/#huazhong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Huazhong University of Science and Technology"},{"location":"cls_institution/#hugging-face","text":"Meta Title Cover Publish Code Note Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning","title":"Hugging Face"},{"location":"cls_institution/#ist","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"IST"},{"location":"cls_institution/#ist-austria","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"IST Austria"},{"location":"cls_institution/#imperial-college-london","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Imperial College London"},{"location":"cls_institution/#indian-institute-of-science","text":"Meta Title Cover Publish Code Note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"Indian Institute of Science"},{"location":"cls_institution/#infinigence-ai","text":"Meta Title Cover Publish Code Note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note","title":"Infinigence-AI"},{"location":"cls_institution/#institute-for-advanced-algorithms-research","text":"Meta Title Cover Publish Code Note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Institute for Advanced Algorithms Research"},{"location":"cls_institution/#institute-of-automation-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models","title":"Institute of Automation, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-computing-technology","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note","title":"Institute of Computing Technology"},{"location":"cls_institution/#institute-of-computing-technology-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Institute of Computing Technology, Chinese Academy of Sciences"},{"location":"cls_institution/#institute-of-information-engineering-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"Institute of Information Engineering, Chinese Academy of Sciences"},{"location":"cls_institution/#intel","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note","title":"Intel"},{"location":"cls_institution/#intel-corporation","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"Intel Corporation"},{"location":"cls_institution/#intellifusion-inc","text":"Meta Title Cover Publish Code Note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note","title":"Intellifusion Inc."},{"location":"cls_institution/#kaist","text":"Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note","title":"KAIST"},{"location":"cls_institution/#kaist-ai","text":"Meta Title Cover Publish Code Note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note","title":"KAIST AI"},{"location":"cls_institution/#kaust","text":"Meta Title Cover Publish Code Note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note","title":"KAUST"},{"location":"cls_institution/#key-laboratory-of-multimedia-trusted-perception-and-efficient-computing","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Key Laboratory of Multimedia Trusted Perception and Efficient Computing"},{"location":"cls_institution/#kyushu-university","text":"Meta Title Cover Publish Code Note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note","title":"Kyushu University"},{"location":"cls_institution/#lanzhou-university","text":"Meta Title Cover Publish Code Note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note","title":"Lanzhou University"},{"location":"cls_institution/#leiden-university","text":"Meta Title Cover Publish Code Note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note","title":"Leiden University"},{"location":"cls_institution/#mbzuai","text":"Meta Title Cover Publish Code Note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"MBZUAI"},{"location":"cls_institution/#mit","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"MIT"},{"location":"cls_institution/#mit-ibm-watson-ai-lab","text":"Meta Title Cover Publish Code Note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note","title":"MIT-IBM Watson AI Lab"},{"location":"cls_institution/#makermaker-ai","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"MakerMaker AI"},{"location":"cls_institution/#massachusetts-institute-of-technology","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note TEAL Training-Free Activation Sparsity in Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"Massachusetts Institute of Technology"},{"location":"cls_institution/#megvii-technology","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note","title":"Megvii Technology"},{"location":"cls_institution/#meituan","text":"Meta Title Cover Publish Code Note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Meituan"},{"location":"cls_institution/#meta","text":"Meta Title Cover Publish Code Note Wanda A Simple and Effective Pruning Approach for Large Language Models note massive-activations Massive Activations in Large Language Models note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"Meta"},{"location":"cls_institution/#meta-ai","text":"Meta Title Cover Publish Code Note streaming-llm Efficient Streaming Language Models with Attention Sinks note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"Meta AI"},{"location":"cls_institution/#meta-ai-fair","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"Meta AI (FAIR)"},{"location":"cls_institution/#meta-platforms-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"Meta Platforms Inc"},{"location":"cls_institution/#michigan-state-university","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"Michigan State University"},{"location":"cls_institution/#microsoft","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"Microsoft"},{"location":"cls_institution/#microsoft-azure","text":"Meta Title Cover Publish Code Note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note","title":"Microsoft Azure"},{"location":"cls_institution/#microsoft-azure-ai","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning","title":"Microsoft Azure AI"},{"location":"cls_institution/#microsoft-research","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning m A Survey on Evaluation of Large Language Models EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Microsoft Research"},{"location":"cls_institution/#microsoft-research-india","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note","title":"Microsoft Research India"},{"location":"cls_institution/#mila-universite-de-montreal","text":"Meta Title Cover Publish Code Note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note ACP Adaptive Computation Pruning for the Forgetting Transformer note","title":"Mila &amp; Universite de Montreal"},{"location":"cls_institution/#minicpm","text":"Meta Title Cover Publish Code Note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note","title":"MiniCPM"},{"location":"cls_institution/#ministry-of-education-of-china-xiamen-university","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Ministry of Education of China, Xiamen University"},{"location":"cls_institution/#mohamed-bin-zayed-university-of-ai","text":"Meta Title Cover Publish Code Note GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note","title":"Mohamed bin Zayed University of AI"},{"location":"cls_institution/#moonshot-ai","text":"Meta Title Cover Publish Code Note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Moonshot AI"},{"location":"cls_institution/#multimedia-laboratory-mmlab","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Multimedia Laboratory (MMLab)"},{"location":"cls_institution/#nvdia","text":"Meta Title Cover Publish Code Note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note","title":"NVDIA"},{"location":"cls_institution/#nvidia","text":"Meta Title Cover Publish Code Note m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks FT FasterTransformer DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note streaming-llm Efficient Streaming Language Models with Attention Sinks note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note kvpress kvpress note","title":"NVIDIA"},{"location":"cls_institution/#nvidia-research","text":"Meta Title Cover Publish Code Note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"NVIDIA Research"},{"location":"cls_institution/#nankai-university","text":"Meta Title Cover Publish Code Note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note","title":"NanKai University"},{"location":"cls_institution/#nanjing-university","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note","title":"Nanjing University"},{"location":"cls_institution/#nanyang-technological-university","text":"Meta Title Cover Publish Code Note L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note Cus-Prun Pruning General Large Language Models into Customized Expert Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Nanyang Technological University"},{"location":"cls_institution/#national-university-of-singapore","text":"Meta Title Cover Publish Code Note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"National University of Singapore"},{"location":"cls_institution/#neural-magic","text":"Meta Title Cover Publish Code Note m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note","title":"Neural Magic"},{"location":"cls_institution/#new-york-university","text":"Meta Title Cover Publish Code Note Recycled Attention Recycled Attention: Efficient inference for long-context language models note","title":"New York University"},{"location":"cls_institution/#noahs-ark-lab-huawei-technologies","text":"Meta Title Cover Publish Code Note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note","title":"Noah\u2019s Ark Lab, Huawei Technologies"},{"location":"cls_institution/#normal-computing","text":"Meta Title Cover Publish Code Note m Efficient Guided Generation for Large Language Models note","title":"Normal Computing"},{"location":"cls_institution/#north-china-electric-power-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"North China Electric Power University"},{"location":"cls_institution/#northeastern-university","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"Northeastern University"},{"location":"cls_institution/#northwestern-university","text":"Meta Title Cover Publish Code Note SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"Northwestern University"},{"location":"cls_institution/#numenta","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Numenta"},{"location":"cls_institution/#oppo-research-institute","text":"Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"OPPO Research Institute"},{"location":"cls_institution/#ohio-state-university","text":"Meta Title Cover Publish Code Note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note","title":"Ohio State University"},{"location":"cls_institution/#openai","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights","title":"OpenAI"},{"location":"cls_institution/#opengvlab","text":"Meta Title Cover Publish Code Note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models","title":"OpenGVLab"},{"location":"cls_institution/#openteams-inc","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"OpenTeams Inc"},{"location":"cls_institution/#oxford-university","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"Oxford University"},{"location":"cls_institution/#peking-university","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note m A Survey on Efficient Inference for Large Language Models note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"Peking University"},{"location":"cls_institution/#perplexity-ai","text":"Meta Title Cover Publish Code Note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"Perplexity AI"},{"location":"cls_institution/#princeton-university","text":"Meta Title Cover Publish Code Note AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TEAL Training-Free Activation Sparsity in Large Language Models note GLA Hardware-Efficient Attention for Fast Decoding note","title":"Princeton University"},{"location":"cls_institution/#purdue-university","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Purdue University"},{"location":"cls_institution/#pytorch","text":"Meta Title Cover Publish Code Note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"PyTorch"},{"location":"cls_institution/#qwen-team","text":"Meta Title Cover Publish Code Note Qwen3 Qwen3 Technical Report note","title":"Qwen Team"},{"location":"cls_institution/#renmin-university-of-china","text":"Meta Title Cover Publish Code Note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"Renmin University of China"},{"location":"cls_institution/#rice-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"Rice University"},{"location":"cls_institution/#sjtu","text":"Meta Title Cover Publish Code Note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note","title":"SJTU"},{"location":"cls_institution/#salesforce-ai-research","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Salesforce AI Research"},{"location":"cls_institution/#salesforce-research","text":"Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"Salesforce Research"},{"location":"cls_institution/#samsung","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note","title":"Samsung"},{"location":"cls_institution/#santa-clara-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Santa Clara University"},{"location":"cls_institution/#school-of-cyber-security-university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note m A Survey on Model Compression for Large Language Models","title":"School of Cyber Security, University of Chinese Academy of Sciences"},{"location":"cls_institution/#sensetime","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"SenseTime"},{"location":"cls_institution/#sensetime-research","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch","title":"SenseTime Research"},{"location":"cls_institution/#seoul-national-university","text":"Meta Title Cover Publish Code Note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note","title":"Seoul National University"},{"location":"cls_institution/#shanghai-ai-laboratory","text":"Meta Title Cover Publish Code Note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note","title":"Shanghai AI Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratory","text":"Meta Title Cover Publish Code Note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note","title":"Shanghai Artificial Intelligence Laboratory"},{"location":"cls_institution/#shanghai-artificial-intelligence-laboratorys","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note","title":"Shanghai Artificial Intelligence Laboratorys"},{"location":"cls_institution/#shanghai-jiao-tong-university","text":"Meta Title Cover Publish Code Note PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note","title":"Shanghai Jiao Tong University"},{"location":"cls_institution/#shanghai-jiao-tong-universtiy","text":"Meta Title Cover Publish Code Note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note","title":"Shanghai Jiao Tong Universtiy"},{"location":"cls_institution/#shanghai-jiaotong-university","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m A Survey on Efficient Inference for Large Language Models note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note","title":"Shanghai Jiaotong University"},{"location":"cls_institution/#shanghaitech-university","text":"Meta Title Cover Publish Code Note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"ShanghaiTech University"},{"location":"cls_institution/#shenzhen-institutes-of-advanced-technologysiat-chinese-academy-of-sciencecas","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS)"},{"location":"cls_institution/#singapore-university-of-technology-and-design","text":"Meta Title Cover Publish Code Note Cus-Prun Pruning General Large Language Models into Customized Expert Models note","title":"Singapore University of Technology and Design"},{"location":"cls_institution/#sogang-university","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"Sogang University"},{"location":"cls_institution/#stanford","text":"Meta Title Cover Publish Code Note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note","title":"Stanford"},{"location":"cls_institution/#stanford-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note","title":"Stanford University"},{"location":"cls_institution/#stepfun","text":"Meta Title Cover Publish Code Note MFA Multi-matrix Factorization Attention note","title":"StepFun"},{"location":"cls_institution/#stepfun-inc","text":"Meta Title Cover Publish Code Note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note","title":"StepFun Inc."},{"location":"cls_institution/#sun-yat-sen-university","text":"Meta Title Cover Publish Code Note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note","title":"Sun Yat-sen University"},{"location":"cls_institution/#sungkyunkwan-university","text":"Meta Title Cover Publish Code Note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"Sungkyunkwan University"},{"location":"cls_institution/#synthesia","text":"Meta Title Cover Publish Code Note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note","title":"Synthesia"},{"location":"cls_institution/#tencent-ai-lab","text":"Meta Title Cover Publish Code Note RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note","title":"Tencent AI Lab"},{"location":"cls_institution/#tencent-machine-learning-platform","text":"Meta Title Cover Publish Code Note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note","title":"Tencent Machine Learning Platform"},{"location":"cls_institution/#tencent-youtu-lab","text":"Meta Title Cover Publish Code Note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note","title":"Tencent Youtu Lab"},{"location":"cls_institution/#the-chinese-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note","title":"The Chinese University of Hong Kong"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem","text":"Meta Title Cover Publish Code Note TOVA Transformers are Multi-State RNNs note","title":"The Hebrew University of Jerusalem"},{"location":"cls_institution/#the-hebrew-university-of-jerusalem-israel","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"The Hebrew University of Jerusalem, Israel"},{"location":"cls_institution/#the-hong-kong-polytechnic-university","text":"Meta Title Cover Publish Code Note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note","title":"The Hong Kong Polytechnic University"},{"location":"cls_institution/#the-hong-kong-university-of-science-and-technology","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note","title":"The Hong Kong University of Science and Technology"},{"location":"cls_institution/#the-ohio-state-university","text":"Meta Title Cover Publish Code Note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note","title":"The Ohio State University"},{"location":"cls_institution/#the-university-of-hong-kong","text":"Meta Title Cover Publish Code Note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note","title":"The University of Hong Kong"},{"location":"cls_institution/#the-university-of-north-carolina","text":"Meta Title Cover Publish Code Note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"The University of North Carolina"},{"location":"cls_institution/#the-university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note None m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Recycled Attention Recycled Attention: Efficient inference for long-context language models note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note","title":"The University of Texas at Austin"},{"location":"cls_institution/#together-ai","text":"Meta Title Cover Publish Code Note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"Together AI"},{"location":"cls_institution/#tongji-university","text":"Meta Title Cover Publish Code Note GBDT Pruning Large Language Models via Accuracy Predictor","title":"Tongji University"},{"location":"cls_institution/#tsinghua-university","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note m Training Transformers with 4-bit Integers RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration m A Survey on Efficient Inference for Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note ReSA Rectified Sparse Attention note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"Tsinghua University"},{"location":"cls_institution/#uc-berkeley","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training FisherPruning A Fast Post-Training Pruning Framework for Transformers note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note SGLang SGLang: Efficient Execution of Structured Language Model Programs note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note","title":"UC Berkeley"},{"location":"cls_institution/#uc-santa-barbara","text":"Meta Title Cover Publish Code Note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note","title":"UC Santa Barbara"},{"location":"cls_institution/#ucsd","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees","title":"UCSD"},{"location":"cls_institution/#univeristy-of-sydney","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"Univeristy of Sydney"},{"location":"cls_institution/#universidade-da-coruna","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"Universidade da Coru\u00f1a"},{"location":"cls_institution/#universidade-de-lisboa","text":"Meta Title Cover Publish Code Note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note","title":"Universidade de Lisboa"},{"location":"cls_institution/#university-college-london","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note","title":"University College London"},{"location":"cls_institution/#university-of-basel","text":"Meta Title Cover Publish Code Note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers","title":"University of Basel"},{"location":"cls_institution/#university-of-california","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"University of California"},{"location":"cls_institution/#university-of-california-berkeley","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note","title":"University of California, Berkeley"},{"location":"cls_institution/#university-of-california-san-diego","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note","title":"University of California, San Diego"},{"location":"cls_institution/#university-of-chinese-academy-of-sciences","text":"Meta Title Cover Publish Code Note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note","title":"University of Chinese Academy of Sciences"},{"location":"cls_institution/#university-of-connecticut","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm","title":"University of Connecticut"},{"location":"cls_institution/#university-of-edinburgh","text":"Meta Title Cover Publish Code Note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"University of Edinburgh"},{"location":"cls_institution/#university-of-electronic-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction","title":"University of Electronic Science and Technology of China"},{"location":"cls_institution/#university-of-hong-kong","text":"Meta Title Cover Publish Code Note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note","title":"University of Hong Kong"},{"location":"cls_institution/#university-of-illinois-urbana-champaign","text":"Meta Title Cover Publish Code Note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note","title":"University of Illinois Urbana-Champaign"},{"location":"cls_institution/#university-of-maryland","text":"Meta Title Cover Publish Code Note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note","title":"University of Maryland"},{"location":"cls_institution/#university-of-oxford","text":"Meta Title Cover Publish Code Note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note","title":"University of Oxford"},{"location":"cls_institution/#university-of-science-and-technology","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note","title":"University of Science and Technology"},{"location":"cls_institution/#university-of-science-and-technology-of-china","text":"Meta Title Cover Publish Code Note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note","title":"University of Science and Technology of China"},{"location":"cls_institution/#university-of-seoul","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"University of Seoul"},{"location":"cls_institution/#university-of-southern-california","text":"Meta Title Cover Publish Code Note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note","title":"University of Southern California"},{"location":"cls_institution/#university-of-st-andrews","text":"Meta Title Cover Publish Code Note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note","title":"University of St Andrews"},{"location":"cls_institution/#university-of-surrey","text":"Meta Title Cover Publish Code Note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Surrey"},{"location":"cls_institution/#university-of-surrey-uk","text":"Meta Title Cover Publish Code Note Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering","title":"University of Surrey, UK"},{"location":"cls_institution/#university-of-texas-at-austin","text":"Meta Title Cover Publish Code Note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","title":"University of Texas at Austin"},{"location":"cls_institution/#university-of-washington","text":"Meta Title Cover Publish Code Note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note","title":"University of Washington"},{"location":"cls_institution/#university-of-waterloo","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note","title":"University of Waterloo"},{"location":"cls_institution/#university-of-wisconsin","text":"Meta Title Cover Publish Code Note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note","title":"University of Wisconsin"},{"location":"cls_institution/#university-of-wisconsin-madison","text":"Meta Title Cover Publish Code Note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note","title":"University of Wisconsin-Madison"},{"location":"cls_institution/#vita-group","text":"Meta Title Cover Publish Code Note m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter","title":"VITA Group"},{"location":"cls_institution/#vector-institute","text":"Meta Title Cover Publish Code Note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note","title":"Vector Institute"},{"location":"cls_institution/#vizuara-ai-labs","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"Vizuara AI Labs"},{"location":"cls_institution/#vokram-group","text":"Meta Title Cover Publish Code Note 52A7RO95 Mixture of Experts in Large Language Models note","title":"Vokram Group"},{"location":"cls_institution/#wechat-ai","text":"Meta Title Cover Publish Code Note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note","title":"WeChat AI"},{"location":"cls_institution/#wuhan-university","text":"Meta Title Cover Publish Code Note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note","title":"Wuhan University"},{"location":"cls_institution/#xiamen-university","text":"Meta Title Cover Publish Code Note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note","title":"Xiamen University"},{"location":"cls_institution/#xiaohongshu","text":"Meta Title Cover Publish Code Note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note","title":"Xiaohongshu"},{"location":"cls_institution/#yale-university","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences","title":"Yale University"},{"location":"cls_institution/#zhe-jiang-university","text":"Meta Title Cover Publish Code Note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"Zhe Jiang University"},{"location":"cls_institution/#zhejiang-university","text":"Meta Title Cover Publish Code Note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note","title":"Zhejiang University"},{"location":"cls_institution/#zhipuai","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note","title":"Zhipu.AI"},{"location":"cls_institution/#zhongguancun-laboratory","text":"Meta Title Cover Publish Code Note SMP Pruning Pre-trained Language Models Without Fine-Tuning","title":"Zhongguancun Laboratory"},{"location":"cls_institution/#baidu","text":"Meta Title Cover Publish Code Note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note","title":"baidu"},{"location":"cls_institution/#iflytek-research","text":"Meta Title Cover Publish Code Note GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note","title":"iFLYTEK Research"},{"location":"cls_institution/#inst1","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"inst1"},{"location":"cls_institution/#inst2","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note Flash-Decoding Flash-Decoding for long-context inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note","title":"inst2"},{"location":"cls_keyword/","text":"keyword 01-Communication-Computation Overlap Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note 02-Sparsity (Attention) Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 03-Sparsity (Activation) Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note 04-Modeling Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note 05-Sparsity (Structured) Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note 06-Sparse/Pruning Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note 07-Quantization Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note 08-Survey Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 52A7RO95 Mixture of Experts in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note 09-Low Rank Decomposition Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note 10-Layer Fusion (Reduce IO) Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note 11-Tool Meta Title Cover Publish Code Note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 12-KV Cache Optimization/Efficient Attention Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 13-Efficient Training Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note 14-Network Structure Design Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note 15-Sparsity (Weight) Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note 16-LLM Deployment Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"By Keyword"},{"location":"cls_keyword/#keyword","text":"","title":"keyword"},{"location":"cls_keyword/#01-communication-computation-overlap","text":"Meta Title Cover Publish Code Note CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note","title":"01-Communication-Computation Overlap"},{"location":"cls_keyword/#02-sparsity-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"02-Sparsity (Attention)"},{"location":"cls_keyword/#03-sparsity-activation","text":"Meta Title Cover Publish Code Note SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note SAS SAS: Structured Activation Spasification note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note TEAL Training-Free Activation Sparsity in Large Language Models note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note IFPruning Instruction-Following Pruning for Large Language Models note","title":"03-Sparsity (Activation)"},{"location":"cls_keyword/#04-modeling","text":"Meta Title Cover Publish Code Note Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note","title":"04-Modeling"},{"location":"cls_keyword/#05-sparsity-structured","text":"Meta Title Cover Publish Code Note FisherPruning A Fast Post-Training Pruning Framework for Transformers note SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note KCM Gradient-Free Structured Pruning with Unlabeled Data K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note Minitron Compact Language Models via Pruning and Knowledge Distillation note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note Cus-Prun Pruning General Large Language Models into Customized Expert Models note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note","title":"05-Sparsity (Structured)"},{"location":"cls_keyword/#06-sparsepruning","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage OBS Optimal Brain Surgeon and general network pruning DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights OpenVINO Post-training deep neural network pruning via layer-wise calibration SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note Recycled Attention Recycled Attention: Efficient inference for long-context language models note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note IFPruning Instruction-Following Pruning for Large Language Models note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"06-Sparse/Pruning"},{"location":"cls_keyword/#07-quantization","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction GPFQ A Greedy Algorithm for Quantizing Neural Networks OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression m Training Transformers with 4-bit Integers ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note","title":"07-Quantization"},{"location":"cls_keyword/#08-survey","text":"Meta Title Cover Publish Code Note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks m Efficient Methods for Natural Language Processing: A Survey m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note 52A7RO95 Mixture of Experts in Large Language Models note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note","title":"08-Survey"},{"location":"cls_keyword/#09-low-rank-decomposition","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note","title":"09-Low Rank Decomposition"},{"location":"cls_keyword/#10-layer-fusion-reduce-io","text":"Meta Title Cover Publish Code Note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning GLA Hardware-Efficient Attention for Fast Decoding note","title":"10-Layer Fusion (Reduce IO)"},{"location":"cls_keyword/#11-tool","text":"Meta Title Cover Publish Code Note FT FasterTransformer SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"11-Tool"},{"location":"cls_keyword/#12-kv-cache-optimizationefficient-attention","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note Flash-Decoding Flash-Decoding for long-context inference note H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note streaming-llm Efficient Streaming Language Models with Attention Sinks note Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note SGLang SGLang: Efficient Execution of Structured Language Model Programs note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"12-KV Cache Optimization/Efficient Attention"},{"location":"cls_keyword/#13-efficient-training","text":"Meta Title Cover Publish Code Note LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning MeZO Fine-Tuning Language Models with Just Forward Passes note LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note m Accelerating Transformer Pre-training with 2:4 Sparsity note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note","title":"13-Efficient Training"},{"location":"cls_keyword/#14-network-structure-design","text":"Meta Title Cover Publish Code Note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MFA Multi-matrix Factorization Attention note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note GLA Hardware-Efficient Attention for Fast Decoding note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note Qwen3 Qwen3 Technical Report note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"14-Network Structure Design"},{"location":"cls_keyword/#15-sparsity-weight","text":"Meta Title Cover Publish Code Note oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Wanda A Simple and Effective Pruning Approach for Large Language Models note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"15-Sparsity (Weight)"},{"location":"cls_keyword/#16-llm-deployment","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Guided Generation for Large Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note SGLang SGLang: Efficient Execution of Structured Language Model Programs note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note","title":"16-LLM Deployment"},{"location":"cls_publication/","text":"publication AAAI Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note ACL Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note ASPLOS Meta Title Cover Publish Code Note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note ATC Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note AutoML Workshop Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search Blog Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note COLM Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note CVPR Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note CVPR workshop Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine Coling Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note DATE Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note ECCV Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers ENLSP Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note ICCV Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration ICLR Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note ICLR oral Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models ICML Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note ICML Workshop Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note ISCA Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note JMLR Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks KDD Workshop Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note MICRO Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation MLSys Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note NeurIPS Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note Neuromorphic Computing and Engineering Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note SC Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note SOSP Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note TACL Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey TC Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note TMLR Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note UAI Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models VLDB Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note VLSI Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers arXiv Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note github Meta Title Cover Publish Code Note FT FasterTransformer KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"By Publication"},{"location":"cls_publication/#publication","text":"","title":"publication"},{"location":"cls_publication/#aaai","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note","title":"AAAI"},{"location":"cls_publication/#acl","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note","title":"ACL"},{"location":"cls_publication/#asplos","text":"Meta Title Cover Publish Code Note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note","title":"ASPLOS"},{"location":"cls_publication/#atc","text":"Meta Title Cover Publish Code Note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note","title":"ATC"},{"location":"cls_publication/#automl-workshop","text":"Meta Title Cover Publish Code Note m Structural Pruning of Large Language Models via Neural Architecture Search","title":"AutoML Workshop"},{"location":"cls_publication/#blog","text":"Meta Title Cover Publish Code Note m Creating Sparse GPT-3 Models with Iterative Pruning DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note","title":"Blog"},{"location":"cls_publication/#colm","text":"Meta Title Cover Publish Code Note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note","title":"COLM"},{"location":"cls_publication/#cvpr","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note","title":"CVPR"},{"location":"cls_publication/#cvpr-workshop","text":"Meta Title Cover Publish Code Note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine","title":"CVPR workshop"},{"location":"cls_publication/#coling","text":"Meta Title Cover Publish Code Note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note","title":"Coling"},{"location":"cls_publication/#date","text":"Meta Title Cover Publish Code Note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note","title":"DATE"},{"location":"cls_publication/#eccv","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"ECCV"},{"location":"cls_publication/#enlsp","text":"Meta Title Cover Publish Code Note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note","title":"ENLSP"},{"location":"cls_publication/#iccv","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration","title":"ICCV"},{"location":"cls_publication/#iclr","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch LoRA LoRA: Low-rank adaptation of large language models AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note","title":"ICLR"},{"location":"cls_publication/#iclr-oral","text":"Meta Title Cover Publish Code Note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models","title":"ICLR oral"},{"location":"cls_publication/#icml","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference SPDY SPDY: Accurate Pruning with Speedup Guarantees note Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note","title":"ICML"},{"location":"cls_publication/#icml-workshop","text":"Meta Title Cover Publish Code Note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note","title":"ICML Workshop"},{"location":"cls_publication/#isca","text":"Meta Title Cover Publish Code Note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note","title":"ISCA"},{"location":"cls_publication/#jmlr","text":"Meta Title Cover Publish Code Note GPFQ A Greedy Algorithm for Quantizing Neural Networks","title":"JMLR"},{"location":"cls_publication/#kdd-workshop","text":"Meta Title Cover Publish Code Note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note","title":"KDD Workshop"},{"location":"cls_publication/#micro","text":"Meta Title Cover Publish Code Note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation","title":"MICRO"},{"location":"cls_publication/#mlsys","text":"Meta Title Cover Publish Code Note nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note","title":"MLSys"},{"location":"cls_publication/#neurips","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning m Channel Permutations for N:M Sparsity FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note","title":"NeurIPS"},{"location":"cls_publication/#neuromorphic-computing-and-engineering","text":"Meta Title Cover Publish Code Note Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note","title":"Neuromorphic Computing and Engineering"},{"location":"cls_publication/#sc","text":"Meta Title Cover Publish Code Note VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note","title":"SC"},{"location":"cls_publication/#sosp","text":"Meta Title Cover Publish Code Note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note","title":"SOSP"},{"location":"cls_publication/#tacl","text":"Meta Title Cover Publish Code Note m Efficient Methods for Natural Language Processing: A Survey","title":"TACL"},{"location":"cls_publication/#tc","text":"Meta Title Cover Publish Code Note DSA Transformer Acceleration with Dynamic Sparse Attention note","title":"TC"},{"location":"cls_publication/#tmlr","text":"Meta Title Cover Publish Code Note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note","title":"TMLR"},{"location":"cls_publication/#uai","text":"Meta Title Cover Publish Code Note SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models","title":"UAI"},{"location":"cls_publication/#vldb","text":"Meta Title Cover Publish Code Note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note","title":"VLDB"},{"location":"cls_publication/#vlsi","text":"Meta Title Cover Publish Code Note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers","title":"VLSI"},{"location":"cls_publication/#arxiv","text":"Meta Title Cover Publish Code Note blocksparse GPU Kernels for Block-Sparse Weights NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note","title":"arXiv"},{"location":"cls_publication/#github","text":"Meta Title Cover Publish Code Note FT FasterTransformer KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"github"},{"location":"cls_year/","text":"year 2025 Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note 2024 Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note 2023 Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer 2022 Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models 2021 Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 2020 Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights 2019 Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training 2018 Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers 2017 Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon 2016 Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding 1993 Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning 1989 Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"By Year"},{"location":"cls_year/#year","text":"","title":"year"},{"location":"cls_year/#2025","text":"Meta Title Cover Publish Code Note AdaSkip AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference note AdaptiveSparseTrainer Pruning Large Language Models with Semi-Structural Adaptive Sparse Training note NSA Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention note COMET COMET: Towards Partical W4A4KV4 LLMs Serving note POD-Attention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference note vAttention vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention note BlockFFN BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity note KVSink KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs note SDS Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism note FlexPrefill FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference note FoX Forgetting Transformer: Softmax Attention with a Forget Gate note R-Sparse R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference note ReAttention ReAttention: Training-Free Infinite Context with Finite Attention Scope note RecursiveTransformers Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA note TEAL Training-Free Activation Sparsity in Large Language Models note AdaSplash AdaSplash: Adaptive Sparse Flash Attention note BaWA BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation note CateKV CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration note PoD Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity note HashAttention HashAttention: Semantic Sparsity for Faster Inference note LaRoSA La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation note MMInference MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention note ShadowKV ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference note SlimLLM SlimLLM: Accurate Structured Pruning for Large Language Models note SpargeAttn SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference note SparsingLaw Sparsing Law: Towards Large Language Models with Greater Activation Sparsity note StarAttention Star Attention: Efficient LLM Inference over Long Sequences note XAttention XAttention: Block Sparse Attention with Antidiagonal Scoring note TorchAO TorchAO: PyTorch-Native Training-to-Serving Model Optimization note AMALI AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs note SpecEE SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting note MoE-MLA-RoPE Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models note LinearPatch A Simple Linear Patch Revives Layer-Pruned Large Language Models note Acc-SpMM Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores note 07NWF4VE Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching note SharePrefill Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing note ACP Adaptive Computation Pruning for the Forgetting Transformer note FlexiDepth Adaptive Layer-skipping in Pre-trained LLMs note AhaKV AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models note AmberPruner Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models note AttentionPredictor AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference note CCQ CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs note UC0D8DJ6 Characterizing Communication Patterns in Distributed Large Language Model Inference note 1DZIJVBI Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications note ChunkKV ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference note CometSeed Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts note DBudgetKV DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance note DeepSeek-R1 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning note DeltaAttention Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction note DeltaLLM DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference note LIMINAL Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need note RaaS Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity note topk-decoding Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs note 2ZU1IWL6 Fast and Simplex: 2-Simplicial Attention in Triton note FastKV FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation note FlashInfer FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving note FlashOverlap FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation note FreqKV FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension note HATA HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference note HCAttention HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs note GLA Hardware-Efficient Attention for Fast Decoding note HelixParallelism Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding note Adrenaline Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation note IFPruning Instruction-Following Pruning for Large Language Models note KVLink KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse note KeepKV KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference note LServer LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention note LeanK LeanK: Learnable K Cache Channel Pruning for Efficient Decoding note MIRAGE MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving note MegaScale-MoE MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production note MiniCPM4 MiniCPM4: Ultra-Efficient LLMs on End Devices note 52A7RO95 Mixture of Experts in Large Language Models note MoSA Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing note MoR Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation note MoBA MoBA: Mixture of Block Attention for Long-Context LLMs note Mosaic Mosaic: Composite Projection Pruning for Resource-efficient LLMs note PanguUltra Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs note PowerAttention PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention note PSA Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving note Cus-Prun Pruning General Large Language Models into Customized Expert Models note QuickSilver QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization note Qwen3 Qwen3 Technical Report note R-KV R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration note RadialAttention Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation note ReSA Rectified Sparse Attention note 0VRXJQ3F Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving note SALE SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling note SEAP SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models note SeerAttention-R SeerAttention-R: Sparse Attention Adaptation for Long Reasoning note Seesaw Seesaw: High-throughput LLM Inference via Model Re-sharding note SpindleKV SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers note Step-3 Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding note Task-KV Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads note sparse-frontier The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs note TileLink TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives note TokenWeave TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference note Triton-distributed Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler note Super-Experts-Profilling Unveiling Super Experts in Mixture-of-Experts Large Language Models note KVCache-Factory Unified KV Cache Compression Methods for Auto-Regressive Models note kvpress kvpress note","title":"2025"},{"location":"cls_year/#2024","text":"Meta Title Cover Publish Code Note FLAP Fluctuation-based Adaptive Structured Pruning for Large Language Models ChunkAttention ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition note T3 T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives note CachedAttention Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention note DistributedGEMM A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems note CATS CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models note m Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption note SparseInfer SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference note SCAP Post-Training Statistical Calibration for Higher Activation Sparsity note Wanda A Simple and Effective Pruning Approach for Large Language Models note LLM-KICK Compressing LLMs: The Truth is Rarely Pure and Never Simple note DSnoT Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs note streaming-llm Efficient Streaming Language Models with Attention Sinks note RIA Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models QA-LoRA QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models note SAS SAS: Structured Activation Spasification note SliceGPT SliceGPT: Compress Large Language Models by Deleting Rows and Columns note ReLU Strikes Back ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models m Accelerating Transformer Pre-training with 2:4 Sparsity note EAGLE EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty note FrameQuant FrameQuant: Flexible Low-Bit Quantization for Transformers note LoRA+ LoRA+: Efficient Low Rank Adaptation of Large Models note OSSCAR OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization note OWL Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Quest Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference note SPP SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models note SparQ SparQ Attention: Bandwidth-Efficient LLM Inference note SIFT Sparse is Enough in Fine-tuning Pre-trained Large Language Models note Sparse-IFT Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency note SqueezeLLM SqueezeLLM: Dense-and-Sparse Quantization note TinyTrain TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge note SMAT Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts note AWQ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Vidur Vidur: A Large-Scale Simulation Framework For LLM Inference note MInference MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention note MaskLLM MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models note SGLang SGLang: Efficient Execution of Structured Language Model Programs note SlimGPT SlimGPT: Layer-wise Structured Pruning for Large Language Models note SparseLLM SparseLLM: Towards Global Pruning for Pre-trained Language Models note ADMM-pruning Fast and Effective Weight Update for Pruned Large Language Models note Flash-LLM Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity note m A Survey on Efficient Inference for Large Language Models note 068ZPAME A Survey on Inference Optimization Techniques for Mixture of Experts Models note PWGG5HBE A Survey on Large Language Model Acceleration based on KV Cache Management note APEX APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving note AVSS AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis note AdaKV Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference note m Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs note SharedAttention Beyond KV Caching: Shared Attention for Efficient LLMs note Minitron Compact Language Models via Pruning and Knowledge Distillation note CoreInfer CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation note DeepSeek-V2 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model note DeepSeek-V3 DeepSeek-V3 Technical Report note DeepSeekMoE DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models note Domino Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping note DuoAttention DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads note m Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment note Bonsa Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes FLUX FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion note FlashMask FlashMask: Efficient and Rich Mask Extension of FlashAttention note DistAttention Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache note KVQuant KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization note L4Q L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ note LISA LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning note YS9YTT55 LLM Inference Serving: Survey of Recent Advances and Opportunities note massive-activations Massive Activations in Large Language Models note MiniKV MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache note MoD Mixture-of-Depths: Dynamically allocating compute in transformer-based language models note MoA MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression note MFA Multi-matrix Factorization Attention note NanoFlow NanoFlow: Towards Optimal Large Language Model Serving Throughput note CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Pytorch note DoubleSparsity Post-Training Sparse Attention with Double Sparsity note PowerInfer-2 PowerInfer-2: Fast Large Language Model Inference on a Smartphone Website note ProSparse ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models note Q-Sparse Q-Sparse: All Large Language Models can be Fully Sparsely-Activated note QServe QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Pytorch note ReLU2 ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs note ReMoE ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing note Recycled Attention Recycled Attention: Efficient inference for long-context language models note CLA Reducing Transformer Key-Value Cache Size with Cross-Layer Attention note m Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark note SCBench SCBench: A KV Cache-Centric Analysis of Long-Context Methods note SampleAttention SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention note SeerAttention SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs note ShadowLLM ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models note SnapKV SnapKV: LLM Knows What You are Looking for Before Generation note TOVA Transformers are Multi-State RNNs note Turbo Sparse Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Pytorch note XGrammar XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models note ZigZagKV ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty note Async-TP [Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch note","title":"2024"},{"location":"cls_year/#2023","text":"Meta Title Cover Publish Code Note Diffuser Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences GRAIN Gradient-based Intra-attention Pruning on Pre-trained Language Models note SMP Pruning Pre-trained Language Models Without Fine-Tuning PINS Pruning Pre-trained Language Models with Principled Importance and Self-regularization SIMPLE Structured Pruning for Efficient Generative Pre-trained Language Models note m Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models note m Structural Pruning of Large Language Models via Neural Architecture Search SparseViT SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer note TorchSparse++ TorchSparse++: Efficient Point Cloud Engine AdaLoRA AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning GPTQ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers MVUE Minimum Variance Unbiased N:M Sparsity for the Neural Gradients m The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers Deja Vu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time SparseGPT SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot. LoSparse Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation nmSPARSE Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning ZipLM ZipLM: Inference-Aware Structured Pruning of Language Models VENOM VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores note Paged Attention Efficient Memory Management for Large Language Model Serving with PagedAttention note m Efficient Methods for Natural Language Processing: A Survey SPDF SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models m A Survey on Evaluation of Large Language Models m A Survey on Model Compression for Large Language Models GBLM-Pruner Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models note CodeGeeX CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X note Compresso Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models note Adaptively Sparse Attention Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers m Efficient Guided Generation for Large Language Models note MeZO Fine-Tuning Language Models with Just Forward Passes note Flash-Decoding Flash-Decoding for long-context inference note FlashAttention-2 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning KCM Gradient-Free Structured Pruning with Unlabeled Data H2O H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models note K-pruning Knowledge-preserving Pruning for Pre-trained Language Models without Retraining note LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory note LLM-Pruner LLM-Pruner: On the Structural Pruning of Large Language Models note LoRAShear LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery LoftQ LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models note OmniQuant OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models GPFQv2 Post-training Quantization for Neural Networks with Provable Guarantees PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU note GBDT Pruning Large Language Models via Accuracy Predictor QLoRA QLoRA: Efficient Finetuning of Quantized LLMs QuIP QuIP: Quantization with Incoherence Processing RPTQ RPTQ: Reorder-based Post-training Quantization for Large Language Models note LLM-shearing Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning note SpQR SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression SquareHead Sparse Fine-tuning for Inference Acceleration of Large Language Models Sparse-IFT Sparse Iso-FLOP Transformations for Maximizing Training Efficiency SMS Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging m Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers Essential Sparsity The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter m Training Transformers with 4-bit Integers Selective Context Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering ZeroQuant-V2 ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation FT FasterTransformer","title":"2023"},{"location":"cls_year/#2022","text":"Meta Title Cover Publish Code Note m Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm TextPruner TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models m Creating Sparse GPT-3 Models with Iterative Pruning LoRA LoRA: Low-rank adaptation of large language models SPDY SPDY: Accurate Pruning with Speedup Guarantees note Sprint Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation FisherPruning A Fast Post-Training Pruning Framework for Transformers note FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness OBC Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning ZeroQuant ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers Complementary Sparsity Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks note DSA Transformer Acceleration with Dynamic Sparse Attention note STA An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers oBERT The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models","title":"2022"},{"location":"cls_year/#2021","text":"Meta Title Cover Publish Code Note OpenVINO Post-training deep neural network pruning via layer-wise calibration BRECQ BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction SR-STE Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch GPFQ A Greedy Algorithm for Quantizing Neural Networks m Channel Permutations for N:M Sparsity NMSparse Accelerating Sparse Deep Neural Networks CoCoNet Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads note m Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks","title":"2021"},{"location":"cls_year/#2020","text":"Meta Title Cover Publish Code Note m Fast Sparse ConvNets m Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference Movement Pruning Movement Pruning: Adaptive Sparsity by Fine-Tuning blocksparse GPU Kernels for Block-Sparse Weights","title":"2020"},{"location":"cls_year/#2019","text":"Meta Title Cover Publish Code Note ActNN ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training","title":"2019"},{"location":"cls_year/#2018","text":"Meta Title Cover Publish Code Note ADMM-pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers","title":"2018"},{"location":"cls_year/#2017","text":"Meta Title Cover Publish Code Note DSD DSD: Dense-Sparse-Dense Training for Deep Neural Networks L-OBS Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon","title":"2017"},{"location":"cls_year/#2016","text":"Meta Title Cover Publish Code Note Deep Compression Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","title":"2016"},{"location":"cls_year/#1993","text":"Meta Title Cover Publish Code Note OBS Optimal Brain Surgeon and general network pruning","title":"1993"},{"location":"cls_year/#1989","text":"Meta Title Cover Publish Code Note OBD Optimal Brain Damage","title":"1989"},{"location":"notes/2021/CoCoNet/note/","text":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi Abstract Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2021/CoCoNet/note/#breaking-the-computation-and-communication-abstraction-barrier-in-distributed-machine-learning-workloads","text":"Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi","title":"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads"},{"location":"notes/2021/CoCoNet/note/#abstract","text":"Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.","title":"Abstract"},{"location":"notes/2022/ComplementarySparsity/note/","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Combine weight sparsity and activation sparsity Table of Contents Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation Method sparse weight and dense activation a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity. sparse weight and sparse activation a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#two-sparsities-are-better-than-one-unlocking-the-performance-benefits-of-sparse-sparse-networks","text":"Combine weight sparsity and activation sparsity","title":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks"},{"location":"notes/2022/ComplementarySparsity/note/#table-of-contents","text":"Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks Method sparse weight and dense activation sparse weight and sparse activation","title":"Table of Contents"},{"location":"notes/2022/ComplementarySparsity/note/#method","text":"","title":"Method"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-dense-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense entity. This is done offline as a preprocessing step. (b) Multiply : each element of the activation is multiplied by the corresponding weight elements in the dense entity (Hadamard product). (c) Route : the appropriate element-wise products are routed separately for each output. (d) Sum : routed products are aggregated and summed to form a separate result for each sparse entity.","title":"sparse weight and dense activation"},{"location":"notes/2022/ComplementarySparsity/note/#sparse-weight-and-sparse-activation","text":"a) Combine : multiple sparse weight structures are overlaid to form a single dense structure. This is done offline as a preprocessing step. (b) Select : a k-WTA component is used to determine the top-k activations and their indices. (c) Multiply : each non-zero activation is multiplied by the corresponding weight elements in the dense structure (Hadamard product). (d) Route : the appropriate element-wise products are routed separately for each output. (e) Sum : routed products are aggregated and summed to form a separate result for each sparse matrix.","title":"sparse weight and sparse activation"},{"location":"notes/2022/DSA/note/","text":"Transformer Acceleration with Dynamic Sparse Attention Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie Abstract Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#transformer-acceleration-with-dynamic-sparse-attention","text":"Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, Yuan Xie","title":"Transformer Acceleration with Dynamic Sparse Attention"},{"location":"notes/2022/DSA/note/#abstract","text":"Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution. \u9700\u8981\u8bad\u7ec3\uff0cseerattention\u548c\u8fd9\u4e2a\u8bba\u6587\u601d\u8def\u975e\u5e38\u50cf\u3002","title":"Abstract"},{"location":"notes/2022/fisherpruning/note/","text":"A Fast Post-Training Pruning Framework for Transformers Abstract Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#a-fast-post-training-pruning-framework-for-transformers","text":"","title":"A Fast Post-Training Pruning Framework for Transformers"},{"location":"notes/2022/fisherpruning/note/#abstract","text":"Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.","title":"Abstract"},{"location":"notes/2022/spdy/","text":"SPDY: Accurate Pruning with Speedup Guarantees \"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2022/spdy/#spdy-accurate-pruning-with-speedup-guarantees","text":"\"SPDY can be seen as a fusion of global search based approaches like AMC and layer-wise constraint-solver methods like AdaQuant; combining the respective advantages of both schemes.\" dynamic programming algorithm","title":"SPDY: Accurate Pruning with Speedup Guarantees"},{"location":"notes/2023/CodeGeeX/note/","text":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang Abstract Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#codegeex-a-pre-trained-model-for-code-generation-with-multilingual-benchmarking-on-humaneval-x","text":"Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"location":"notes/2023/CodeGeeX/note/#abstract","text":"Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.","title":"Abstract"},{"location":"notes/2023/Compresso/note/","text":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance. Method Challenges Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection Training data for pruning We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset Efficient training-based structured pruning Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, $\\alpha$ is the learnable parameter: Mask regularization for the expected pruning ratio: Experiments The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#compresso-structured-pruning-with-collaborative-prompting-learns-compact-large-language-models","text":"This is a training-based structured pruning approach. The limitations of one-shot pruning: - It depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. - Error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance.","title":"Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"},{"location":"notes/2023/Compresso/note/#method","text":"","title":"Method"},{"location":"notes/2023/Compresso/note/#challenges","text":"Training-based pruning is resource-intensive. Hard to preserve the generalization capability of LLM. dataset selection","title":"Challenges"},{"location":"notes/2023/Compresso/note/#training-data-for-pruning","text":"We use instruction tuning datasets as pruning data. The distribution of pruning data should align with the pre-training data. However, it is hard to satisfy this. GPT4-Alpaca dataset","title":"Training data for pruning"},{"location":"notes/2023/Compresso/note/#efficient-training-based-structured-pruning","text":"Structured pruning: - attention heads - FFN intermediate dimension - hidden dimension Learning pruning mask values with augmented L0 regularization \u91cd\u53c2\u6570\u5316mask, $\\alpha$ is the learnable parameter: Mask regularization for the expected pruning ratio:","title":"Efficient training-based structured pruning"},{"location":"notes/2023/Compresso/note/#experiments","text":"The impact of pruning data. GPT4-Alpaca is the best from the three datasets. The effectiveness of collaborative pruning. Table 6 indicates that removing the pruning prompt at either stage significantly reduces the performance of pruned LLMs, particularly on commonsense reasoning and reading comprehension tasks. This demonstrates the effectiveness of our proposed collaborative pruning. Pruning prompt \u5f88\u91cd\u8981\uff01 The effectiveness of post fine-tuning. fine-tuning\u53ef\u4ee5\u8f7b\u5fae\u63d0\u9ad8\u8868\u73b0\uff0c\u4f46\u662f\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u7684\u5f71\u54cd\uff1b\u53bb\u6389fine-tuning\u540e\uff0c\u5927\u90e8\u5206\u4efb\u52a1\u7684\u7ed3\u679c\u4f1a\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u662f\u4e5f\u5b58\u5728\u5f02\u5e38\u63d0\u9ad8\u7684\u60c5\u51b5\uff0c\u8bf4\u660efine-tuning\u5e76\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56fine-tuning.","title":"Experiments"},{"location":"notes/2023/FlashDecoding/note/","text":"FlashDecoding Abstract","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#flashdecoding","text":"","title":"FlashDecoding"},{"location":"notes/2023/FlashDecoding/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/GBLM-Pruner/note/","text":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models Abstract Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#beyond-size-how-gradients-shape-pruning-decisions-in-large-language-models","text":"","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"location":"notes/2023/GBLM-Pruner/note/#abstract","text":"Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.","title":"Abstract"},{"location":"notes/2023/H2O/note/","text":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models Abstract Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.","title":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models","text":"","title":"H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"location":"notes/2023/H2O/note/#abstract","text":"Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.","title":"Abstract"},{"location":"notes/2023/IHOT8YP4/note/","text":"Efficient Guided Generation for Large Language Models Abstract In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#efficient-guided-generation-for-large-language-models","text":"","title":"Efficient Guided Generation for Large Language Models"},{"location":"notes/2023/IHOT8YP4/note/#abstract","text":"In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines","title":"Abstract"},{"location":"notes/2023/LLM-Pruner/note/","text":"LLM-Pruner: On the Structural Pruning of Large Language Models Abstract Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#llm-pruner-on-the-structural-pruning-of-large-language-models","text":"","title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"location":"notes/2023/LLM-Pruner/note/#abstract","text":"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner","title":"Abstract"},{"location":"notes/2023/LLM_in_a_flash/note/","text":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory Abstract Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory","text":"","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"location":"notes/2023/LLM_in_a_flash/note/#abstract","text":"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory. slidling windows \u66ff\u6362\u6700\u8fd1\u6ca1\u6709\u4f7f\u7528\u7684weight weight bundling\uff0c\u7531\u4e8emlp\u7684up\u548cdown\u5177\u6709\u5173\u8054\u6027\uff0c\u4ed6\u4eec\u7684\u5bf9\u5e94\u884c\u548c\u5217\u4f1a\u88ab\u540c\u65f6\u6fc0\u6d3b\uff0c\u6240\u4ee5\u5b58\u50a8\u65f6\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u589e\u52a0flash\u8bfb\u53d6\u65f6\u7684block size\uff0c\u63d0\u9ad8flash IO\u6548\u7387\u3002","title":"Abstract"},{"location":"notes/2023/LLM_shearing/note/","text":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning structured pruning\u7684\u4e24\u4e2a\u56f0\u96be \u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa Method targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#sheared-llama-accelerating-language-model-pre-training-via-structured-pruning","text":"","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"location":"notes/2023/LLM_shearing/note/#structured-pruning","text":"\u76ee\u524d\u7684\u65b9\u6cd5\u627e\u5230\u7684\u7ed3\u6784\u662fsuboptimal\u7684 pruned model\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4fdd\u7559\u7684\u80fd\u529b\u4e0d\u540c\uff0c\u76f4\u63a5\u7684\u4f7f\u7528pre-training data\u8fdb\u884c\u8bad\u7ec3\u4e0d\u591f\u9ad8\u6548\u3002 \u9488\u5bf9\u4ee5\u4e0a\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51fa","title":"structured pruning\u7684\u4e24\u4e2a\u56f0\u96be"},{"location":"notes/2023/LLM_shearing/note/#method","text":"targeted structured pruning\uff0c\u4e00\u79cd\u641c\u7d22\u7684\u65b9\u6cd5\u627e\u5230\u66f4\u52a0\u5408\u9002\u7684pruning struture dynamic batch loading\uff0c\u6839\u636edomain loss\u81ea\u52a8\u8c03\u8282domain data\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002","title":"Method"},{"location":"notes/2023/MeZO/note/","text":"Fine-Tuning Language Models with Just Forward Passes Abstract Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#fine-tuning-language-models-with-just-forward-passes","text":"","title":"Fine-Tuning Language Models with Just Forward Passes"},{"location":"notes/2023/MeZO/note/#abstract","text":"Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","title":"Abstract"},{"location":"notes/2023/PagedAttention/note/","text":"Efficient Memory Management for Large Language Model Serving with PagedAttention Abstract High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#efficient-memory-management-for-large-language-model-serving-with-pagedattention","text":"","title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"location":"notes/2023/PagedAttention/note/#abstract","text":"High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm","title":"Abstract"},{"location":"notes/2023/PowerInfer/note/","text":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Abstract This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#powerinfer-fast-large-language-model-serving-with-a-consumer-grade-gpu","text":"","title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"location":"notes/2023/PowerInfer/note/#abstract","text":"This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.","title":"Abstract"},{"location":"notes/2023/RIIWOI3F/note/","text":"RIIWOI3F Abstract","title":"RIIWOI3F"},{"location":"notes/2023/RIIWOI3F/note/#riiwoi3f","text":"","title":"RIIWOI3F"},{"location":"notes/2023/RIIWOI3F/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2023/SparseViT/","text":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer Method Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity Result \u223c50% latency reduction with 60% sparsity","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#sparsevit-revisiting-activation-sparsity-for-efficient-high-resolution-vision-transformer","text":"","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"location":"notes/2023/SparseViT/#method","text":"Window activation pruning Define the L2 activation magnitude as the importance metric of each windows. Only preserve the computation of the more improtant windows. Shared Scoring Compute the window importance score only once per stage and reuse it across all the blocks within the stage. Evolutionary search to efficiently find the optimal layerwise sparsity Search space 0%, 10%, ..., 80% Sparsity-Aware Adaptation Randomly sampling layerwise activation sparsity and updating the model accordingly at each iteration. Evolutionary search Meet the resource constraints Minimize the Flops. Finetuning with Optimal Sparsity","title":"Method"},{"location":"notes/2023/SparseViT/#result","text":"\u223c50% latency reduction with 60% sparsity","title":"Result"},{"location":"notes/2023/VENOM/note/","text":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores Abstract The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#venom-a-vectorized-nm-format-for-unleashing-the-power-of-sparse-tensor-cores","text":"","title":"VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores"},{"location":"notes/2023/VENOM/note/#abstract","text":"The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no loss in accuracy in modern transformers.","title":"Abstract"},{"location":"notes/2023/grain/","text":"GRAIN","title":"GRAIN"},{"location":"notes/2023/grain/#grain","text":"","title":"GRAIN"},{"location":"notes/2023/k_pruning/note/","text":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining This is a retraning-free structured pruning approach. Method Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where $\\lambda = \\{0.00025, 1\\}$ and $\\mu = 64$. \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c $\\lambda$ \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c $\\lambda$ \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 $\\mu$ \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002 Results","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#knowledge-preserving-pruning-for-pre-trained-language-models-without-retraining","text":"This is a retraning-free structured pruning approach.","title":"Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"},{"location":"notes/2023/k_pruning/note/#method","text":"Key idea Selecting pruning targets Neurons and attention heads that minimally reduce the PLM\u2019s knowledge Iterative pruning Use knowledge reconstruction for each sub-layer to handle the distorted inputs by pruning. K-pruning (Knowledge-preserving pruning) knowledge measurement knowledge-preserving mask search knowledge-preserving pruning Transformer Block consists of MHA and MLP. The model-wise predictive knowledge loss is defined as the KL-divergence of logits between the pruned model and the dense model. The sub-layerwise representational knowledge loss is defined as the F-norm (MSE loss) of the outputs. The improtance scores are defined as: where $\\lambda = \\{0.00025, 1\\}$ and $\\mu = 64$. \u4e0d\u540c\u5c42\u4e4b\u95f4\u7684score\u662f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\u7684\u5417\uff1f \u5bf9\u4e8eMLP\uff0c $\\lambda$ \u53d6\u503c\u975e\u5e38\u5c0f\uff0c\u53ea\u770bpredictive loss\uff0c\u53ef\u4ee5\u8de8\u5c42\u6bd4\u8f83 \u4f46\u662f\u5bf9\u4e8eMHA\uff0c $\\lambda$ \u53d6\u503c\u6bd4\u8f83\u5927\uff0cpredictive/representational \u90fd\u770b\uff0c\u4e24\u8005\u517c\u987e MHA \u4e0e MLP \u4e5f\u53ef\u4ee5\u4e92\u76f8\u6bd4\u8f83\uff1f \u901a\u8fc7\u914d\u6bd4 $\\mu$ \u6765\u5b9e\u73b0 \u603b\u4e4b\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u5f15\u5165\u7528\u6765\u5747\u8861\uff0c\u8de8\u5c42\u4e0e\u8de8\u7b97\u5b50\u7684\u6bd4\u8f83\u3002","title":"Method"},{"location":"notes/2023/k_pruning/note/#results","text":"","title":"Results"},{"location":"notes/2023/loftq/note/","text":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models Method \u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#loftq-lora-fine-tuning-aware-quantization-for-large-language-models","text":"","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"location":"notes/2023/loftq/note/#method","text":"\u5bf9QLoRA\u65b9\u6cd5\u7684\u6539\u8fdb\uff0cQLoRA\u5728\u521d\u59cb\u5316\u65f6\uff0c Q = Quantize(W) A = \u6b63\u6001\u5206\u5e03, B = 0 \u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u975e\u5e38\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u5f00\u59cb\u5c31\u504f\u79bb\u6700\u4f18\u89e3\u592a\u591a\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u53d8\u5dee\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528LoftQ\u7b97\u6cd5\u5bf9Q, A, B\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5982\u4e0b\uff1a","title":"Method"},{"location":"notes/2023/simple/","text":"Structured Pruning for Efficient Generative Pre-trained Language Models Method Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V $\\ell_{hidden}$ is hidden state distillation loss: Results \u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#structured-pruning-for-efficient-generative-pre-trained-language-models","text":"","title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"location":"notes/2023/simple/#method","text":"Sparsity-induced Mask Learning Teacher-Student KD loss and L1 regularization to optimize sparse mask. The learnable sparse mask is initialized to 1, and is updated with gradient descent during training. After learning the mask, these masks are binarized according to a threshold determined by a given sparsity. Fine-tuning Fix sparse mask and finetune weights KD Loss + Local KD loss MSE loss of K, V $\\ell_{hidden}$ is hidden state distillation loss:","title":"Method"},{"location":"notes/2023/simple/#results","text":"\u8bba\u6587\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684GPT2\uff0c\u6309\u7167\u6a21\u578b\u5927\u5c0f\u63a8\u6d4b\u5e94\u8be5\u662fGPT2-small\uff0cppl=29\uff0c\u6240\u4ee5\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\u53ef\u80fd\u5728wikitext2\u4e0a\u8fdb\u884c\u4e86finetune","title":"Results"},{"location":"notes/2024/068ZPAME/note/","text":"A Survey on Inference Optimization Techniques for Mixture of Experts Models Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li Abstract The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#a-survey-on-inference-optimization-techniques-for-mixture-of-experts-models","text":"Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li","title":"A Survey on Inference Optimization Techniques for Mixture of Experts Models"},{"location":"notes/2024/068ZPAME/note/#abstract","text":"The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.","title":"Abstract"},{"location":"notes/2024/0Y41U1N2/note/","text":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs Abstract To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#beyond-24-exploring-vnm-sparsity-for-efficient-transformer-inference-on-gpus","text":"","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs"},{"location":"notes/2024/0Y41U1N2/note/#abstract","text":"To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","title":"Abstract"},{"location":"notes/2024/ADMM-pruning/note/","text":"Fast and Effective Weight Update for Pruned Large Language Models Abstract Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#fast-and-effective-weight-update-for-pruned-large-language-models","text":"","title":"Fast and Effective Weight Update for Pruned Large Language Models"},{"location":"notes/2024/ADMM-pruning/note/#abstract","text":"Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.","title":"Abstract"},{"location":"notes/2024/APEX/note/","text":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino Abstract Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#apex-an-extensible-and-dynamism-aware-simulator-for-automated-parallel-execution-in-llm-serving","text":"Yi-Chien Lin, Woosuk Kwon, Ronald Pineda, Fanny Nina Paravecino","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving"},{"location":"notes/2024/APEX/note/#abstract","text":"Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","title":"Abstract"},{"location":"notes/2024/AVSS/note/","text":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis Abstract The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#avss-layer-importance-evaluation-in-large-language-models-via-activation-variance-sparsity-analysis","text":"","title":"AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis"},{"location":"notes/2024/AVSS/note/#abstract","text":"The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures. \u6839\u636elayer activation\u7684\u76f8\u4f3c\u6027\uff0c\u8fdb\u884c layer pruning.","title":"Abstract"},{"location":"notes/2024/AdaKV/note/","text":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference Abstract Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#ada-kv-optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference","text":"","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference"},{"location":"notes/2024/AdaKV/note/#abstract","text":"Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks. \u6bcf\u4e2ahead\u5206\u914d\u4e0d\u540c\u7684 Budget\uff0c\u533a\u522b\u770b\u5f85\u4e0d\u540c\u7684 attention head.","title":"Abstract"},{"location":"notes/2024/Async-TP/note/","text":"Async-TP Abstract","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#async-tp","text":"","title":"Async-TP"},{"location":"notes/2024/Async-TP/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/CATS/note/","text":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models Abstract Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#cats-contextually-aware-thresholding-for-sparsity-in-large-language-models","text":"","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"},{"location":"notes/2024/CATS/note/#abstract","text":"Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B. \u968f\u673a\u5728\u8bad\u7ec3\u96c6\u4e2d\u62bd\u53d6\u4e00\u4e9b\u6570\u636e\uff0c\u8f93\u5165\u5230LLM\u4e2d\uff0c\u5f97\u5230\u6fc0\u6d3b\u7684\u7edf\u8ba1\uff0c\u4ece\u800c\u786e\u5b9a\u9884\u671f\u6fc0\u6d3b\u7a00\u758f\u5ea6\u4e0b\u7684\u9608\u503c\u3002 \u53e6\u5916\uff0c\u91cd\u5199\u4e86activaiton sparse\u7684kernel\uff0c\u80fd\u591f\u4ea7\u751f\u5b9e\u9645\u7684\u52a0\u901f\u3002 \u4e5f\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002","title":"Abstract"},{"location":"notes/2024/CHESS/note/","text":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification Abstract Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#chess-optimizing-llm-inference-via-channel-wise-thresholding-and-selective-sparsification","text":"","title":"CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification"},{"location":"notes/2024/CHESS/note/#abstract","text":"Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.","title":"Abstract"},{"location":"notes/2024/CLA/note/","text":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention Abstract Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#reducing-transformer-key-value-cache-size-with-cross-layer-attention","text":"","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"},{"location":"notes/2024/CLA/note/#abstract","text":"Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","title":"Abstract"},{"location":"notes/2024/CachedAttention/note/","text":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention Abstract Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention","text":"","title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"location":"notes/2024/CachedAttention/note/#abstract","text":"Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.","title":"Abstract"},{"location":"notes/2024/ChunkAttention/note/","text":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition Abstract Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#chunkattention-efficient-self-attention-with-prefix-aware-kv-cache-and-two-phase-partition","text":"","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition"},{"location":"notes/2024/ChunkAttention/note/#abstract","text":"Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. \u4ece\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u5171\u4eabprompt\u7684\u573a\u666f\uff0c\u4e14\u5171\u4eabtoken\u8d8a\u591a\uff0c\u52a0\u901f\u8d8a\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/CoreInfer/note/","text":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation Abstract Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86$\\alpha$ $\\beta$ \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#coreinfer-accelerating-large-language-model-inference-with-semantics-inspired-adaptive-sparse-activation","text":"","title":"CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"},{"location":"notes/2024/CoreInfer/note/#abstract","text":"Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively. \u4e4b\u524d\u5de5\u4f5c\u5bf9activation sparse\u7684\u9884\u6d4b\u662f\u6309\u7167token-wise\u6765\u505a\u7684\uff0c\u8fd9\u6837\u6709\u51e0\u4e2a\u95ee\u9898\uff1a - Irregular and frequent resource calls during decoding - Additional computation costs during decoding; \u4f7f\u7528MLP\u4f5c\u4e3apredictor\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0d\u80fd\u5ffd\u7565 CoreInfer \u5219\u63d0\u51fasentence-wise activation sparsity\u65b9\u5f0f\uff0c\u5e76\u4e14\u6ca1\u6709MLP\u7684predictor\u3002 \u5b9a\u4e49\u5982\u4e0b\uff1a - Token-wise Core Neurons. - \u6bcf\u4e2atoken\u7ecf\u8fc7\u4e00\u4e2alayer\u8ba1\u7b97\u540e\uff0c\u5bf9\u5e94\u591a\u4e2a\u8f93\u51fa\uff08neurons\uff09,\u53d6\u7edd\u5bf9\u503c\u6700\u5927\u7684\u51e0\u4e2a\u4f5c\u4e3aCore Neurons\u3002 - Sentence-wise Core Neurons. - \u4e00\u4e2asentence\u542b\u6709\u591a\u4e2atoken\uff0c\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u6839\u636e\u4ee5\u4e0a\u4e00\u5b9a\u627e\u5230core neurons\uff0c\u901a\u8fc7\u7edf\u8ba1\u6bcf\u4e2aneuron\u4f5c\u4e3atoken-wise core neuron\u7684\u6b21\u6570\uff0c\u6b21\u6570\u6700\u591a\u7684\u51e0\u4e2a\u4f5c\u4e3asentence-wise core neurons\u3002 \u53ef\u4ee5\u770b\u5230\u5206\u522b\u5f15\u5165\u4e86$\\alpha$ $\\beta$ \u4e24\u4e2a\u8d85\u53c2\u6570\uff0c\u7528\u6765\u63a7\u5236core neurons\u7684\u6bd4\u4f8b\u3002 \u901a\u8fc7\u8fd9\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u7528\u8ba1\u7b97\u5c11\u90e8\u5206\u7684neurons\uff0c\u4fbf\u53ef\u4ee5\u7ef4\u6301\u7cbe\u5ea6\u3002 \u53e6\u5916\u89c2\u5bdf\uff0c\u8d8a\u4e34\u8fd1\u7684tokens\u7684core neurons\u8d8a\u76f8\u4f3c\uff0c\u6240\u4ee5\u5177\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002 \u6240\u4ee5\uff0ccoreinfer\u6839\u636e\u5386\u53f2\u7684core neuron\u6765\u5224\u65ad\u672a\u6765\u7684core neuron\u65b9\u6cd5\u5982\u4e0b\uff1a - Stability-guided Prediction\uff0cprefill\u9636\u6bb5\u7edf\u8ba1\uff0cdecoding\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528prefill\u7684\u7edf\u8ba1\u7ed3\u679c - Similarity-guided Prediction\uff0ccoreinfer \u7edf\u8ba1\u4e86training data\uff0c\u5e76\u6309\u7167\u8bed\u4e49\u5bf9\u5176\u8fdb\u884c\u4e86\u805a\u7c7b\uff0c\u4ece\u800c\u53ef\u4ee5\u6839\u636e\u8bed\u4e49\u76f8\u5173\u6027\u6765\u5224\u65adcore neuron \uff08\u6709\u4e9b\u62bd\u8c61\uff09 \u7b2c\u4e00\u79cd\u601d\u60f3\u548caggregated sparsity\u5f88\u50cf\uff0cLLM in a flash.","title":"Abstract"},{"location":"notes/2024/DHIB73MC/note/","text":"A Survey on Efficient Inference for Large Language Models Abstract Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#a-survey-on-efficient-inference-for-large-language-models","text":"","title":"A Survey on Efficient Inference for Large Language Models"},{"location":"notes/2024/DHIB73MC/note/#abstract","text":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","title":"Abstract"},{"location":"notes/2024/DSnoT/note/","text":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs Abstract The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#dynamic-sparse-no-training-training-free-fine-tuning-for-sparse-llms","text":"","title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"location":"notes/2024/DSnoT/note/#abstract","text":"The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/","text":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model Abstract We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1 Multi-head Latent Attention (MLA) Preliminaries: Standard Multi-Head Attention \u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002 Low-Rank Key-Value Joint Compression \u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58$C_t^{KV}$\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e$C_t^{KV}$\u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a Decoupled Rotary Position Embedding \u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#deepseek-v2-a-strong-economical-and-efficient-mixture-of-experts-language-model","text":"","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"location":"notes/2024/DeepSeek-V2/note/#abstract","text":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. https://spaces.ac.cn/archives/10091/comment-page-1","title":"Abstract"},{"location":"notes/2024/DeepSeek-V2/note/#multi-head-latent-attention-mla","text":"","title":"Multi-head Latent Attention (MLA)"},{"location":"notes/2024/DeepSeek-V2/note/#preliminaries-standard-multi-head-attention","text":"\u7ecf\u5178\u7684MHA\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u9700\u8981\u5c06\u5386\u53f2\u7684KV \u8fdb\u884c\u7f13\u5b58\uff0c\u8fd9\u91cc\u7701\u7565\u4e86RoPE\u7f16\u7801\uff0c\u5b9e\u9645\u9996\u5148\u5bf9KV\u8fdb\u884cRoPE\u7f16\u7801\uff0c\u7136\u540e\u518d\u4fdd\u5b58KV Cache\u3002","title":"Preliminaries: Standard Multi-Head Attention"},{"location":"notes/2024/DeepSeek-V2/note/#low-rank-key-value-joint-compression","text":"\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u4f7f\u7528MLA: KV\u7684\u9700\u8981\u7ecf\u8fc7\u4e24\u6b21Linear\u8fd0\u7b97\u624d\u80fd\u5f97\u5230\uff0c\u53ea\u4fdd\u5b58$C_t^{KV}$\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u9996\u5148\u6839\u636e$C_t^{KV}$\u6620\u5c04\u4e3aKV\uff0c\u7136\u540e\u518d\u8fdb\u884cMHA\u7684\u8fd0\u7b97\u3002 \u53e6\u5916\uff0c\u4e5f\u5bf9Q\u8fdb\u884c\u4e86Low-Rank\u8fd0\u7b97\uff0c\u867d\u7136\u4e0d\u80fd\u51cf\u5c11Cache\uff0c\u4f46\u662f\u80fd\u51cf\u5c11training\u4e2d\u7684\u6fc0\u6d3b\u503c\u7684Memory\u3002 \u4fdd\u5b58\u7684KV cache\u5bf9\u6bd4\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"Low-Rank Key-Value Joint Compression"},{"location":"notes/2024/DeepSeek-V2/note/#decoupled-rotary-position-embedding","text":"\u7136\u800c\uff0cLow-Rank\u4e0eRoPE\u4e0d\u517c\u5bb9\uff0c","title":"Decoupled Rotary Position Embedding"},{"location":"notes/2024/DeepSeek-V3/note/","text":"DeepSeek-V3 Technical Report Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#deepseek-v3-technical-report","text":"","title":"DeepSeek-V3 Technical Report"},{"location":"notes/2024/DeepSeek-V3/note/#abstract","text":"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","title":"Abstract"},{"location":"notes/2024/DeepSeekMoE/note/","text":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models Abstract In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#deepseekmoe-towards-ultimate-expert-specialization-in-mixture-of-experts-language-models","text":"","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"location":"notes/2024/DeepSeekMoE/note/#abstract","text":"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","title":"Abstract"},{"location":"notes/2024/DistAttention/note/","text":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache Abstract Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#infinite-llm-efficient-llm-service-for-long-context-with-distattention-and-distributed-kvcache","text":"","title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"location":"notes/2024/DistAttention/note/#abstract","text":"Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.","title":"Abstract"},{"location":"notes/2024/DistributedGEMM/note/","text":"DistributedGEMM Abstract","title":"DistributedGEMM"},{"location":"notes/2024/DistributedGEMM/note/#distributedgemm","text":"","title":"DistributedGEMM"},{"location":"notes/2024/DistributedGEMM/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2024/Domino/note/","text":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase Abstract Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#domino-eliminating-communication-in-llm-training-via-generic-tensor-slicing-and-overlapping","text":"Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase","title":"Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping"},{"location":"notes/2024/Domino/note/#abstract","text":"Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.","title":"Abstract"},{"location":"notes/2024/DoubleSparsity/note/","text":"Post-Training Sparse Attention with Double Sparsity Abstract The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in attention operations and a 1.9$\\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#post-training-sparse-attention-with-double-sparsity","text":"","title":"Post-Training Sparse Attention with Double Sparsity"},{"location":"notes/2024/DoubleSparsity/note/#abstract","text":"The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in attention operations and a 1.9$\\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.","title":"Abstract"},{"location":"notes/2024/DuoAttention/note/","text":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads Abstract Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a $\\alpha$ \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 $\\alpha$ \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#duoattention-efficient-long-context-llm-inference-with-retrieval-and-streaming-heads","text":"","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"},{"location":"notes/2024/DuoAttention/note/#abstract","text":"Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. \u6bcf\u4e2ahead\u8bbe\u7f6e\u4e00\u4e2a $\\alpha$ \uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u9009\u62e9\u6240\u6709\u7684kv cache\u6216\u8005\u91c7\u7528streaming \u7b56\u7565\u3002 $\\alpha$ \u9700\u8981training\u6765\u8fdb\u884c\u786e\u5b9a\u3002","title":"Abstract"},{"location":"notes/2024/Eagle/note/","text":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty Abstract Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#eagle-speculative-sampling-requires-rethinking-feature-uncertainty","text":"","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"location":"notes/2024/Eagle/note/#abstract","text":"Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.","title":"Abstract"},{"location":"notes/2024/FLUX/note/","text":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu Abstract Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion","text":"Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion"},{"location":"notes/2024/FLUX/note/#abstract","text":"Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.","title":"Abstract"},{"location":"notes/2024/FlashMask/note/","text":"FlashMask: Efficient and Rich Mask Extension of FlashAttention Abstract The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#flashmask-efficient-and-rich-mask-extension-of-flashattention","text":"","title":"FlashMask: Efficient and Rich Mask Extension of FlashAttention"},{"location":"notes/2024/FlashMask/note/#abstract","text":"The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.","title":"Abstract"},{"location":"notes/2024/FrameQuant/note/","text":"FrameQuant: Flexible Low-Bit Quantization for Transformers Abstract Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#framequant-flexible-low-bit-quantization-for-transformers","text":"","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers"},{"location":"notes/2024/FrameQuant/note/#abstract","text":"Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13488.pdf \u76f4\u63a5\u5728original weight space\u5bf9\u6743\u91cd\u91cf\u5316\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8bba\u6587\u4e2d\u63d0\u51fa\u5c06weight\u8f6c\u5316\u5230Fusion Frame\u7a7a\u95f4\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5c06weight\u91cf\u5316\u52302bit\uff0c\u6bd4SOTA\u65b9\u6cd5\u6709\u8f83\u5927\u7684\u63d0\u5347\u3002 \u4e0eQuIP\u7684\u533a\u522b\uff0c\u5982\u679c\u8bbe\u7f6eredundancy factor r = 1\uff0c\u4e14\u968f\u673a\u8bbe\u7f6e\u6b63\u4ea4\u77e9\u9635P,\u90a3\u4e48\u5c31\u548cQuIP\u4e00\u81f4\u4e86\u3002 \u662f\u5426\u9002\u5408\u66f4\u9ad8\u7684bit, \u6bd4\u59823-bit, 4-bit\u914d\u7f6e\uff0c\u8bba\u6587\u6ca1\u6709\u7ed9\u51fa\u5bf9\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u6307\u51faOPTQ\u7b49\u65b9\u6cd5\u5df2\u7ecf\u67094-bit\u7684\u7ed3\u679c\u4e86\uff0c\u6240\u4ee5\u63a8\u65ad\u8fd9\u4e2a\u65b9\u6cd5\u57282-bit\u4e0a\u6709\u63d0\u5347\uff0c\u57283/4-bit\u4e0a\u53ef\u80fd\u63d0\u5347\u4e0d\u660e\u663e\u3002","title":"Abstract"},{"location":"notes/2024/HYPL7G37/note/","text":"Accelerating Transformer Pre-training with 2:4 Sparsity Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#accelerating-transformer-pre-training-with-24-sparsity","text":"","title":"Accelerating Transformer Pre-training with 2:4 Sparsity"},{"location":"notes/2024/HYPL7G37/note/#abstract","text":"Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.","title":"Abstract"},{"location":"notes/2024/JSHWEV0S/note/","text":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption Abstract Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#keep-the-cost-down-a-review-on-methods-to-optimize-llm-s-kv-cache-consumption","text":"","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption"},{"location":"notes/2024/JSHWEV0S/note/#abstract","text":"Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.","title":"Abstract"},{"location":"notes/2024/KVQuant/note/","text":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Abstract LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#kvquant-towards-10-million-context-length-llm-inference-with-kv-cache-quantization","text":"","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"location":"notes/2024/KVQuant/note/#abstract","text":"LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.","title":"Abstract"},{"location":"notes/2024/L4Q/note/","text":"L4Q Method Step1: warm up Q(W) + $\\alpha$ AB Step2: quantize(W + sAB) Experiments memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"L4Q"},{"location":"notes/2024/L4Q/note/#l4q","text":"Method Step1: warm up Q(W) + $\\alpha$ AB Step2: quantize(W + sAB)","title":"L4Q"},{"location":"notes/2024/L4Q/note/#experiments","text":"memory\u7684\u5360\u7528\u53d8\u591a\u4e86\u5f88\u591a\uff0c\u8bba\u6587\u4e2d\u8bf4GPTQ+LoRA stores the quantized weights in memory and dequantizate operation\uff0c\u5305\u542b\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u671f\u671b\u901a\u8fc7\u4f18\u5316\u8fbe\u5230QLoRA\u7684\u4f4ememory\u5360\u7528\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u8fd8\u4e0d\u9519\uff0c\u6bd4QLoRA\u8981\u597d\uff0c","title":"Experiments"},{"location":"notes/2024/LISA/note/","text":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LISA/note/#lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning","text":"","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"location":"notes/2024/LoRA%2B/note/","text":"LoRA+: Efficient Low Rank Adaptation of Large Models Abstract In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#lora-efficient-low-rank-adaptation-of-large-models","text":"","title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"location":"notes/2024/LoRA%2B/note/#abstract","text":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","title":"Abstract"},{"location":"notes/2024/MFA/note/","text":"Multi-matrix Factorization Attention Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang Abstract We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#multi-matrix-factorization-attention","text":"Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang","title":"Multi-matrix Factorization Attention"},{"location":"notes/2024/MFA/note/#abstract","text":"We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.","title":"Abstract"},{"location":"notes/2024/MInference/note/","text":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention Abstract The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#minference-10-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention","text":"","title":"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"},{"location":"notes/2024/MInference/note/#abstract","text":"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference. MInference \u9488\u5bf9prefill\u9636\u6bb5 long-context\u573a\u666f\uff0c\u5229\u7528attention\u7684\u8fd0\u7b97\u52a8\u6001\u7a00\u758f\u884c\u8fdb\u884c\u52a0\u901f\u3002 \u8bba\u6587\u4e2d\u5c06attention \u4e2dsparse\u7684\u7a00\u758f\u5206\u4e3a\u4e09\u79cd\u6a21\u5f0f\uff1a 1. A-shape\u7684\u6a21\u5f0f\uff0c\u53ea\u4fdd\u7559\u6700\u5f00\u59cb\u51e0\u5217\u548c\u6700\u8fd1\u7684\u51e0\u5217\u8fd0\u7b97\uff0c\u56e0\u6b64\u4e5f\u662f\u9759\u6001\u7684sparse\uff0c\u76f4\u63a5\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\u5f00\u9500\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u5982\u540d\u79f0\u542b\u4e49\u4e00\u6837\uff0c\u7ad6\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u659c\u7740\u4fdd\u7559\u51e0\u5217\uff0c\u540c\u65f6sparse index\u9700\u8981\u6839\u636e\u8f93\u5165\u6765\u52a8\u6001\u7684\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11\u8fd0\u7b97\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u6309\u7167block\u7c92\u5ea6\u6765\u9009\u62e9\u4e00\u90e8\u5206block\u8fdb\u884c\u7a00\u758f\u8fd0\u7b97\uff1b \u52a0\u901f\u5b9e\u73b0\uff1a attention\u4e00\u822c\u6709\u591a\u4e2ahead\uff0c\u6587\u4e2d\u9996\u5148\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206sample \u6765\u5bf9attention head\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u6bcf\u4e2ahead\u9009\u62e9\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u5176\u4e2d\u4e00\u79cd\uff1b \u5176\u6b21\uff0c\u5bf9\u5e94\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\u7684\u52a0\u901f\u5b9e\u73b0 1. A-shape\u6a21\u5f0f\uff0c\u7531\u4e8e\u662f\u9759\u6001\u7684sparse\uff0c\u6240\u4ee5\u52a0\u901f\u5b9e\u73b0\u6bd4\u8f83\u76f4\u63a5\uff1b 2. Vertial-slash \u6a21\u5f0f\uff0c\u9700\u8981\u52a8\u6001\u7684\u51b3\u5b9a\u4fdd\u7559\u7684index\uff0c\u4f1a\u6709\u989d\u5916\u7684\u8bc4\u4f30\u7684\u5f00\u9500\uff0csparse\u52a0\u901f\u90e8\u5206\u9700\u8981\u5b9e\u73b0\u5bf9\u5e94attention kernel\u8fdb\u884c\u52a0\u901f\uff1b 3. Block-sparse\u6a21\u5f0f\uff0c\u540c\u6837\u9700\u8981\u52a8\u6001\u51b3\u5b9a\u4fdd\u7559\u7684block index\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u540c\u65f6sparse\u52a0\u901f\u4e5f\u9700\u8981\u5bf9\u5e94\u7684attention kernel\u3002 \u7531\u4e8e\u6a21\u5f0f2\u548c3\u5747\u5f15\u5165\u4e86\u989d\u5916\u7684runtime\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u8fd9\u90e8\u5206\u5f00\u9500\u8981\u5c3d\u53ef\u80fd\u9ad8\u6548\uff0c\u8bbe\u8ba1\u5982\u4e0b\uff1a Vertial-slash \u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3a\u6700\u540e\u4e00\u884cQ\u6d89\u53ca\u7684\u8fd0\u7b97 Block-sparse\u6a21\u5f0f\u7684\u8bc4\u4f30\u5f00\u9500\u4e3b\u8981\u4e3aPool\u4e4b\u540e\u7684\u7f29\u5c0f\u540e\u7684QK\u6d89\u53ca\u7684\u8fd0\u7b97 \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u90e8\u5206\u8bc4\u4f30\u5f00\u9500\u76f8\u8f83\u4e8edense attention\u8fd0\u7b97\u662f\u975e\u5e38\u5c0f\u7684\uff0c\u5f53\u83b7\u53d6\u5230\u5f53\u524d\u7684\u52a8\u6001sparse mask\u540e\uff0c\u4f7f\u7528\u5bf9\u5e94\u7684sparse attention kernel\u4fbf\u53ef\u4ee5\u8fdb\u884c\u52a0\u901f\u3002 \u5b9e\u9a8c\u90e8\u5206\uff1a \u7cbe\u5ea6\u7ed3\u679c\uff1a \u7531\u4e8e\u52a0\u901f\u6765\u6e90\u4e8e\u8df3\u8fc7\u4e86\u4e00\u90e8\u5206\u8fd0\u7b97\uff0c\u4e14\u8bc4\u4f30\u8df3\u8fc7\u7684\u4f4d\u7f6e\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0e\u5b9e\u9645\u7a00\u758f\u4f4d\u7f6e\u6709\u504f\u5dee\u7684\u60c5\u51b5\uff0c\u6240\u4ee5\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u53d7\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u6587\u4e2d\u4f7f\u7528InfiniteBench\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0\u7cbe\u5ea6\u76f8\u8f83\u4e8ebaseline\u786e\u5b9e\u6709\u4e00\u4e9b\u6ce2\u52a8\uff0c\u4f46\u662f\u5e73\u5747\u6b63\u786e\u7387\u6ca1\u6709\u4e0b\u964d\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002 \u52a0\u901f\u7ed3\u679c\uff1a \u53ef\u4ee5\u770b\u5230\u968f\u7740context size\u63d0\u5347\uff0c\u6700\u9ad8\u80fd\u670910\u500d\u7684\u52a0\u901f\u3002","title":"Abstract"},{"location":"notes/2024/MaskLLM/note/","text":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models Abstract Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#maskllm-learnable-semi-structured-sparsity-for-large-language-models","text":"","title":"MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"},{"location":"notes/2024/MaskLLM/note/#abstract","text":"Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at \\url{https://github.com/NVlabs/MaskLLM}.","title":"Abstract"},{"location":"notes/2024/MiniKV/note/","text":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache Abstract How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#minikv-pushing-the-limits-of-llm-inference-via-2-bit-layer-discriminative-kv-cache","text":"","title":"MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache"},{"location":"notes/2024/MiniKV/note/#abstract","text":"How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements. KIVI\u91cd\u70b9\u5728\u4e8e\u91cf\u5316\uff0cminiKV\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86kv sparse\uff0c(eviction)","title":"Abstract"},{"location":"notes/2024/Minitron/note/","text":"Compact Language Models via Pruning and Knowledge Distillation Abstract Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#compact-language-models-via-pruning-and-knowledge-distillation","text":"","title":"Compact Language Models via Pruning and Knowledge Distillation"},{"location":"notes/2024/Minitron/note/#abstract","text":"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.","title":"Abstract"},{"location":"notes/2024/MoA/note/","text":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression Abstract Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#moa-mixture-of-sparse-attention-for-automatic-large-language-model-compression","text":"","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"},{"location":"notes/2024/MoA/note/#abstract","text":"Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}. \u548cDuoAttention\u7684\u505a\u6cd5\u975e\u5e38\u50cf\uff0c\u6bcf\u4e2a atention head \u4f7f\u7528\u4e0d\u540c\u7684 sparse pattern\u3002 \u5b9a\u4e49search space\uff0c\u5305\u542b\u662f\u5426\u4e0d\u8fdb\u884csparse\uff0c\u4f7f\u7528streamingLLM\u65b9\u5f0f\u7b49\u3002 calibration data\u91c7\u7528\u4e86dense model\u751f\u6210\u6570\u636e\uff0c\u7528\u6765\u8861\u91cfattention head\u7684\u5f71\u54cd \u4f18\u5316\u914d\u7f6esparse pattern\uff0c\u4ece\u800c\u6ee1\u8db3\u538b\u7f29\u7ea6\u675f\u6761\u4ef6\u4e0bloss\u6700\u5c0f","title":"Abstract"},{"location":"notes/2024/MoD/note/","text":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Abstract Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling. Method Defining a compute budget \u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a$T/2$\uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684$T^2$\u53d8\u4e3a$\\frac{T}{2}^2$\uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684$T$\u53d8\u4e3a$\\frac{T}{2}$\uff0c\u5373\u539f\u6765\u768450%\u3002 Routing schemes Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9 Routing implementation \u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa$f_i(x)$\u589e\u52a0\u4e86\u4e0e$r_i$\u4f5c\u4e3a\u6743\u91cd\u3002 Non-causal problem during sampling \u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684$r_i$\u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk Training Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49 Results \u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models","text":"","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"location":"notes/2024/MoD/note/#abstract","text":"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.","title":"Abstract"},{"location":"notes/2024/MoD/note/#method","text":"","title":"Method"},{"location":"notes/2024/MoD/note/#defining-a-compute-budget","text":"\u5047\u8bbe\u4e00\u5171\u6709T\u4e2atokens\u8f93\u5165\u5230transformer\u4e2d\u8fdb\u884c\u8fd0\u7b97\uff0c\u6b64\u65f6\u7684compute budget\u5219\u4e3aT. MoE\u65b9\u6cd5\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u4e14\u56de\u9009\u62e9\u5176\u4e2d\u4e00\u4e2aexpert\u8fdb\u884c\u8fd0\u7b97\uff0c\u6240\u4ee5\u5e73\u5747\u7684compute budget\u4e5f\u7ea6\u4e3aT\u3002 \u5bf9\u4e8eMoD\u6765\u8bf4\uff0c\u7531\u4e8e\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u8df3\u8fc7\u4e00\u4e9bblock\uff0c\u6240\u4ee5\u6700\u7ec8\u7684compute budget\u4f1a\u5c0f\u4e8eT\u3002\u5047\u8bbe\uff0c\u5b9a\u4e49\u67d0\u4e00\u4e2ablock\u7684compute budget\u4e3a$T/2$\uff0c\u90a3\u4e48\u8fd9\u4e2ablock\u4e2d\u7684self-attention\u7684flops\u4f1a\u7531\u4e4b\u524d\u7684$T^2$\u53d8\u4e3a$\\frac{T}{2}^2$\uff0c\u4e5f\u5c31\u662f\u53d8\u4e3a\u4e86\u539f\u6765\u768425%\u3002\u540c\u7406\uff0c\u5bf9\u4e8eMLP\uff0c\u5219\u7531\u539f\u6765\u7684$T$\u53d8\u4e3a$\\frac{T}{2}$\uff0c\u5373\u539f\u6765\u768450%\u3002","title":"Defining a compute budget"},{"location":"notes/2024/MoD/note/#routing-schemes","text":"Routing\u7684\u65b9\u5f0f\u53ef\u4ee5\u6709\u4ee5\u4e0b\u51e0\u79cd\u9009\u62e9 - \u968f\u673aroute\uff0c\u7c7b\u4f3cdropout\uff0c\u5bf9performance\u5f71\u54cd\u5f88\u5927 - learned routing\uff0c\u8bc1\u660e\u662f\u6bd4\u968f\u673arouting\u66f4\u597d\u7684\u65b9\u6cd5 - token-choice routing - token\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684path\uff0c\u4f46\u662f\u9700\u8981\u5f15\u5165balancing loss\uff0c\u4e0d\u7136\u6240\u6709\u7684token\u9009\u62e9\u7684path\u5bb9\u6613\u8d8b\u5411\u4e0e\u4e00\u81f4 - \u7531\u4e8e\u6ca1\u6709\u5f3a\u5236\u7684\u7ea6\u675f\uff0ctoken-choice routing\u4f1a\u5bfc\u81f4load unbalance - expert-choice routing - \u7ea6\u675f\u6bcf\u4e2apath\u6709top-k\u7684token\u6765\u9009\u62e9\uff0c\u53ef\u4ee5\u4fdd\u8bc1load balance - \u4f46\u662f\u4f1a\u5bfc\u81f4\u67d0\u4e9btoken\u5b9e\u9645\u8ba1\u7b97\u91cf\u6bd4\u6700\u4f18\u9700\u6c42\u9ad8\u6216\u8005\u4f4e\u3002 \u524d\u4e24\u4e2a\u56fe\u662f\u4ee5MoE\u4e3a\u4f8b\uff0c\u7b2c\u4e09\u4e2a\u56fe\u5219\u662fMoD routing\u7684\u65b9\u6cd5\u3002MoE\u4e2d\u6709\u591a\u4e2aexpert\uff0ctoken-choice\u6bcf\u4e2atoken\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684expert\uff0c\u5982\u5de61\u9014\u4e2d\u865a\u7ebf\u6240\u793a\uff0c\u5982\u679cexpert1\u88ab\u9009\u62e9\u7684\u6b21\u6570\u8fc7\u591a\uff0c\u8d85\u8fc7\u4e86\u8bbe\u7f6e\u7684capacity\uff0c\u5219\u53ea\u80fd\u628a\u8fd9\u4e2atoken\u76f4\u63a5\u6254\u6389\u3002\u4e2d\u95f4\u56fe\u5219\u91c7\u7528\u4e86expert-choice \u65b9\u5f0f\uff0c\u6bcf\u4e2aexpert\u5bf9\u5e94\u4e24\u4e2atoken\uff0c\u7531\u4e8e\u6709\u591a\u4e2aexperts\uff0c\u6240\u4ee5\u67d0\u4e9btoken\u53ef\u80fd\u4f1a\u6709\u591a\u4e2aexpert\uff0c\u67d0\u4e9btoken\u5219\u53ef\u80fd\u4e00\u4e2aexpert\u90fd\u6ca1\u6709\uff0c\u76f8\u5f53\u4e8e\u4e0d\u53c2\u4e0e\u8fd9\u4e2ablock\u8fd0\u7b97\u3002\u53f3\u56fe\u5219\u662f\u5bf9MoD\u7684routing\uff0c\u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8981\u4e48\u53c2\u4e0e\u8fd0\u7b97\uff0c\u8981\u4e48\u4e0d\u53c2\u4e0e\u8fd0\u7b97\uff0c\u6240\u4ee5top-2\u7684token\u4f1a\u53c2\u4e0e\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\uff0c\u5176\u4ed6tokens\u5219\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e2ablock\u7684\u8fd0\u7b97\u3002 \u6700\u7ec8\uff0c\u8bba\u6587\u9009\u62e9 expert-choice routing \uff0c\u5177\u4f53\u539f\u56e0\u4e3a\uff1a \u4e0d\u9700\u8981\u589e\u52a0balancing loss\uff0c\u65e2\u53ef\u4ee5\u6ee1\u8db3load balance Top-k\u7684\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u9700\u8981\u53c2\u4e0e\u8fd0\u7b97\u7684tokens\uff0c\u5176\u4ed6tokens\u5219\u9009\u62e9\u8df3\u8fc7\u8fd0\u7b97 \u9009\u62e9\u53ea\u6709\u4e24\u79cd\uff0c\u8ba1\u7b97\u6216\u8005\u8df3\u8fc7\u8ba1\u7b97\uff0ctop-k\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u8fd9\u4e2a\u9009\u62e9","title":"Routing schemes"},{"location":"notes/2024/MoD/note/#routing-implementation","text":"\u53ef\u4ee5\u770b\u5230\uff0c\u9009\u62e9top-k\u7684token embedding\u8fdb\u884c\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6709\u6548\u7684\u51cf\u5c11compute budget \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684block\u7684\u8f93\u51fa$f_i(x)$\u589e\u52a0\u4e86\u4e0e$r_i$\u4f5c\u4e3a\u6743\u91cd\u3002","title":"Routing implementation"},{"location":"notes/2024/MoD/note/#non-causal-problem-during-sampling","text":"\u5728\u81ea\u56de\u5f52sampling\u65f6\uff0c\u786e\u5b9a\u8fd9\u4e2atoken\u662f\u5426\u5728top-k\u4e2d\uff0c\u9700\u8981\u4e0e\u672a\u6765\u7684token\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u662f\u65e0\u6cd5\u83b7\u53d6\u672a\u6765\u7684token\uff0c\u5bfc\u81f4\u56e0\u679c\u903b\u8f91\u6df7\u4e71\u3002\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u4e2a\u89e3\u51b3\u65b9\u6cd5\uff1a - A simple auxiliary loss - \u4f7f\u7528binary cross-entropy loss\u6765\u533a\u5206\u6bcf\u4e2atoken\u7684$r_i$\u662f\u5426\u5c5e\u4e8etopk - A small auxiliary MLP predictor - \u76f8\u5f53\u4e8esecond router\uff0c\u7528\u4e8e\u9884\u6d4b\u662f\u5426\u5c5e\u4e8etopk","title":"Non-causal problem during sampling"},{"location":"notes/2024/MoD/note/#training","text":"Training\u90e8\u5206\u6240\u6709\u7684\u8d85\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\uff0c\u53ea\u4fee\u6539\u4e86layer number\uff0cheads\u548cembedding size\u7b49","title":"Training"},{"location":"notes/2024/MoD/note/#results","text":"\u540c\u7b49\u53c2\u6570\u91cf\u4e0b\uff0cMoD\u6bd4baseline\u8981\u5feb\uff0c\u540c\u7b49\u8bad\u7ec3flops\u548cwall-clock\u4e0b\uff0c\u8bad\u7ec3\u7ed3\u679c\u76f8\u4f3c\uff1b\uff08\u4e5f\u5c31\u662f\u8bf4MoD\u9700\u8981\u8bad\u7ec3\u66f4\u591a\u7684iteration\uff09 Every block routing \u548c Evary other block routing \u5bf9\u6bd4\uff0c\u540e\u8005\u8868\u73b0\u66f4\u597d\u3002","title":"Results"},{"location":"notes/2024/NanoFlow/note/","text":"NanoFlow: Towards Optimal Large Language Model Serving Throughput Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci Abstract Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2024/NanoFlow/note/#nanoflow-towards-optimal-large-language-model-serving-throughput","text":"Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput"},{"location":"notes/2024/NanoFlow/note/#abstract","text":"Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.","title":"Abstract"},{"location":"notes/2024/OSSCAR/note/","text":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization Abstract Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to $125\\times$ lower test perplexity on WikiText with $2\\times$ inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also $6\\times$ -- $8\\times$ faster. Notably, our work considers models with tens of billions of parameters, which is up to $100\\times$ larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 $\\hat{w}$","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#osscar-one-shot-structured-pruning-in-vision-and-language-models-with-combinatorial-optimization","text":"","title":"OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization"},{"location":"notes/2024/OSSCAR/note/#abstract","text":"Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to $125\\times$ lower test perplexity on WikiText with $2\\times$ inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also $6\\times$ -- $8\\times$ faster. Notably, our work considers models with tens of billions of parameters, which is up to $100\\times$ larger than what has been previously considered in the structured pruning literature. structured pruning\u7684\u65b9\u5f0f\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u540c\uff0clayer-by-layer\u4f18\u5316\u6bcf\u5c42\u7684loss\uff0closs\u7531\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6784\u5efa\u3002 \u5982\u4f55\u641c\u7d22mask\uff0c\u4ece\u800c\u6700\u5c0f\u5316loss\uff0c\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u9996\u5148\u662floss\u7684\u8ba1\u7b97\u6bd4\u8f83\u8017\u65f6\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4f7f\u7528\u5386\u53f2\u7684\u8ba1\u7b97\u548c\u53d8\u5316\u7684mask\u4f4d\u7f6e\u6765\u51cf\u5c11loss\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u4e2a\u4e5f\u5bb9\u6613\u7406\u89e3\uff0c\u53ea\u9700\u8981\u8ba1\u7b97\u5dee\u503c\u5c31\u597d\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e00\u4e2alocal search\u7684\u641c\u7d22\u65b9\u6cd5\u3002 \u6587\u4e2d\u6ca1\u6709\u8bf4\u662f\u5426\u4e3aunform pruning ratio\uff0c\u53ea\u7ed9\u4e86\u52a0\u901f\u6bd4\uff0c\u901a\u8fc7\u4ee3\u7801\u53d1\u73b0\u6bcf\u4e00\u5c42\u914d\u7f6e\u7684pruning ratio\u662f\u4e00\u81f4\u7684\u3002 $\\hat{w}$","title":"Abstract"},{"location":"notes/2024/PWGG5HBE/note/","text":"A Survey on Large Language Model Acceleration based on KV Cache Management Abstract Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#a-survey-on-large-language-model-acceleration-based-on-kv-cache-management","text":"","title":"A Survey on Large Language Model Acceleration based on KV Cache Management"},{"location":"notes/2024/PWGG5HBE/note/#abstract","text":"Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.","title":"Abstract"},{"location":"notes/2024/PowerInfer-2/note/","text":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone Abstract This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#powerinfer-2-fast-large-language-model-inference-on-a-smartphone","text":"","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone"},{"location":"notes/2024/PowerInfer-2/note/#abstract","text":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","title":"Abstract"},{"location":"notes/2024/ProSparse/note/","text":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models Method There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface. Experiment Real accelerate on hardware Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization Training Dataset Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#prosparse-introducing-and-enhancing-intrinsic-activation-sparsity-within-large-language-models","text":"","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"},{"location":"notes/2024/ProSparse/note/#method","text":"There are three stages of ProSparse: 1. ReLU replacement - Replace Non-ReLU activation with ReLU and finetune the model. - However, this stage usually does not achieve satisfactory sparsity. 2. Progressive sparsity regularization - Apply sparsity regularization to the output of FFN. - Progressively increase the regularization factor for better performace. - Enhance higher sparsity. 3. Activation threshold shifting - Modify the vanilla ReLU with FAT ReLU. This paper provides two models prosparse-llama-2-7b and prosparse-llama-2-13b in Huggingface.","title":"Method"},{"location":"notes/2024/ProSparse/note/#experiment","text":"","title":"Experiment"},{"location":"notes/2024/ProSparse/note/#real-accelerate-on-hardware","text":"Approximate strategy A predictor to predict the position of activation sparse. However, the final performance depends on the quality of the predictor and the predictor itself introduce an addional overhead. Test on PowerInfer , a c++ library. Accurate strategy GPU kernel operation fusion coalesced memory access vectorization","title":"Real accelerate on hardware"},{"location":"notes/2024/ProSparse/note/#training-dataset","text":"Pretraining Dataset StarCoder Wikipedia Pile other collected data. Instruction tuning dataset UltraChat multiple-choice QA data of P3 PAQ Unnatural Instructions Flan Super-Natural Instructions other collected data. Prosparse achieves better result than Original model. I think the orginal models do not adopt Instruction Finetuning , but Prosparse does.","title":"Training Dataset"},{"location":"notes/2024/Q-Sparse/note/","text":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated Abstract We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#q-sparse-all-large-language-models-can-be-fully-sparsely-activated","text":"","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"},{"location":"notes/2024/Q-Sparse/note/#abstract","text":"We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. Q-Sparse\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u8fdb\u884ctop-K\u7a00\u758f\u5316\u548c\u76f4\u901a\u4f30\u8ba1\u5668\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u5b8c\u5168\u7a00\u758f\u6fc0\u6d3b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u6fc0\u6d3bLLMs\u7684\u63a8\u7406\u6700\u4f18\u7f29\u653e\u5b9a\u5f8b\u3002 \u6ca1\u6709\u771f\u5b9e\u7684\u52a0\u901f\u5b9e\u73b0\uff0c\u540c\u65f6\u4e5f\u6ca1\u6709\u7ed9\u5b9e\u9645\u7684\u52a0\u901f\u6bd4","title":"Abstract"},{"location":"notes/2024/QA-LoRA/note/","text":"QA-LoRA QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QA-LoRA/note/#qa-lora","text":"QLoRA\u4e3b\u8981\u7528\u4e8e\u51cf\u5c11finetuning\u65f6\u7684memory cost\uff0c\u76f8\u8f83\u4e8eLoRA\uff0c\u5b83\u7684\u6027\u80fd\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u662fQLoRA\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6709\u9700\u8981\u628aAB\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u8868\u793a\u7684\u77e9\u9635\u878d\u5408\u5230\u4f4e\u4f4d\u5bbd\u7684\u6743\u91cd\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u878d\u5408\u7684\u6743\u91cd\u8868\u793a\u4e3a\u9ad8\u4f4d\u5bbd\uff0c\u5e76\u4e0d\u80fd\u6ee1\u8db3\u91cf\u5316\u7684\u7ea6\u675f\u3002 \u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff0c\u6bd4QLoRA\u5dee\u4e00\u70b9\uff0c\u4e5f\u7b97\u6b63\u5e38","title":"QA-LoRA"},{"location":"notes/2024/QServer/note/","text":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Abstract Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving","text":"","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"},{"location":"notes/2024/QServer/note/#abstract","text":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","title":"Abstract"},{"location":"notes/2024/Quest/note/","text":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference Abstract As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#quest-query-aware-sparsity-for-efficient-long-context-llm-inference","text":"","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"},{"location":"notes/2024/Quest/note/#abstract","text":"As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .","title":"Abstract"},{"location":"notes/2024/ReLU2/note/","text":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs The paper indicates through experiments that models employing $ReLU^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReLU2/note/#relu2-wins-discovering-efficient-activation-functions-for-sparse-llms","text":"The paper indicates through experiments that models employing $ReLU^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","title":"$ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs"},{"location":"notes/2024/ReMoE/note/","text":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing Abstract Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#remoe-fully-differentiable-mixture-of-experts-with-relu-routing","text":"","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"},{"location":"notes/2024/ReMoE/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","title":"Abstract"},{"location":"notes/2024/RecycledAttention/note/","text":"Recycled Attention: Efficient inference for long-context language models Abstract Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#recycled-attention-efficient-inference-for-long-context-language-models","text":"","title":"Recycled Attention: Efficient inference for long-context language models"},{"location":"notes/2024/RecycledAttention/note/#abstract","text":"Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. In this paper, we propose Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we recycle the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further explore two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. \u65b9\u6cd5\u6bd4\u8f83\u7b80\u5355\uff0cICLR\u5ba1\u7a3f\u610f\u89c1\u4e5f\u8bf4\u65b9\u6cd5\u521b\u65b0\u6027\u4f4e https://openreview.net/forum?id=8qYuxV4lRu","title":"Abstract"},{"location":"notes/2024/SAS/note/","text":"SAS Abstract \u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"SAS"},{"location":"notes/2024/SAS/note/#sas","text":"","title":"SAS"},{"location":"notes/2024/SAS/note/#abstract","text":"\u7a20\u5bc6activation\u6620\u5c04\u4e3a\u7a00\u758factivation\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u540c\u7684activation\u9009\u62e9\u4e0d\u540c\u7684weight\uff0c\u6240\u4ee5\u601d\u60f3\u4e5f\u7c7b\u4f3c\u4e8eMOE","title":"Abstract"},{"location":"notes/2024/SCAP/note/","text":"Post-Training Statistical Calibration for Higher Activation Sparsity Abstract We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#post-training-statistical-calibration-for-higher-activation-sparsity","text":"","title":"Post-Training Statistical Calibration for Higher Activation Sparsity"},{"location":"notes/2024/SCAP/note/#abstract","text":"We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP. FFN\u7684\u8f93\u5165\u4e5f\u7a00\u758f\u4e86\uff1b \u4e4b\u524d\u6309\u71670\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u7a00\u758f\uff0c\u6839\u636e\u7edf\u8ba1\u7ed3\u679c\u627e\u5230Model center\uff0c\u5e76\u6dfb\u52a0\u504f\u79fb\u540e\u7a00\u758f\uff0c\u4ece\u800c\u80fd\u63d0\u9ad8\u6700\u540e\u7684\u7cbe\u5ea6\uff1b","title":"Abstract"},{"location":"notes/2024/SCBench/note/","text":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods Abstract Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#scbench-a-kv-cache-centric-analysis-of-long-context-methods","text":"","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods"},{"location":"notes/2024/SCBench/note/#abstract","text":"Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.","title":"Abstract"},{"location":"notes/2024/SGLang/note/","text":"SGLang: Efficient Execution of Structured Language Model Programs Abstract Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#sglang-efficient-execution-of-structured-language-model-programs","text":"","title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"location":"notes/2024/SGLang/note/#abstract","text":"Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang","title":"Abstract"},{"location":"notes/2024/SIFT/note/","text":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models Abstract With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#sparse-is-enough-in-fine-tuning-pre-trained-large-language-models","text":"","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Models"},{"location":"notes/2024/SIFT/note/#abstract","text":"With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/. \u53ea\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u548clora\u8fdb\u884c\u6bd4\u8f83","title":"Abstract"},{"location":"notes/2024/SMAT/note/","text":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei Abstract Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#unleashing-the-power-of-meta-tuning-for-few-shot-generalization-through-sparse-interpolated-experts","text":"Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"},{"location":"notes/2024/SMAT/note/#abstract","text":"Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available. OOD\u4efb\u52a1\u7684Meta-learning\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5ea6\u65b9\u6cd5\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5212\u5206\uff0c\u7c7b\u4f3c\u5207\u5206\u4e3a\u591a\u4e2aexpert","title":"Abstract"},{"location":"notes/2024/SN1PK7EK/note/","text":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark Abstract In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#revisiting-zeroth-order-optimization-for-memory-efficient-llm-fine-tuning-a-benchmark","text":"","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"},{"location":"notes/2024/SN1PK7EK/note/#abstract","text":"In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","title":"Abstract"},{"location":"notes/2024/SPP/note/","text":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models Abstract Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#spp-sparsity-preserved-parameter-efficient-fine-tuning-for-large-language-models","text":"","title":"SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models"},{"location":"notes/2024/SPP/note/#abstract","text":"Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.","title":"Abstract"},{"location":"notes/2024/SampleAttention/note/","text":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention Abstract Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#sampleattention-near-lossless-acceleration-of-long-context-llm-inference-with-adaptive-structured-sparse-attention","text":"","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"},{"location":"notes/2024/SampleAttention/note/#abstract","text":"Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.","title":"Abstract"},{"location":"notes/2024/SeerAttention/note/","text":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs Abstract Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#seerattention-learning-intrinsic-sparse-attention-in-your-llms","text":"","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"},{"location":"notes/2024/SeerAttention/note/#abstract","text":"Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.","title":"Abstract"},{"location":"notes/2024/ShadowLLM/note/","text":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models Abstract The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#shadowllm-predictor-based-contextual-sparsity-for-large-language-models","text":"","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models"},{"location":"notes/2024/ShadowLLM/note/#abstract","text":"The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}. Dejavu \u7684\u6539\u8fdb\u7248\uff0cpredictor \u4f7f\u7528\u4e86piqa\u6765\u8bad\u7ec3\uff0c\u4f7f\u7528piqa\u6765\u6d4b\u8bd5\uff0c\u6240\u4ee5\u7ed3\u679c\u63d0\u5347\u5f88\u5927\uff0c\u5982\u679c\u4f7f\u7528\u901a\u7528\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u5347\u53ef\u80fd\u5c31\u6709\u9650\u3002","title":"Abstract"},{"location":"notes/2024/SharedAttention/note/","text":"Beyond KV Caching: Shared Attention for Efficient LLMs Abstract The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#beyond-kv-caching-shared-attention-for-efficient-llms","text":"","title":"Beyond KV Caching: Shared Attention for Efficient LLMs"},{"location":"notes/2024/SharedAttention/note/#abstract","text":"The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments. \u76f8\u90bb\u5c42\u5171\u4eabattention score\uff0c\u4fdd\u7559\u81ea\u5df1\u7684v cache","title":"Abstract"},{"location":"notes/2024/SliceGPT/note/","text":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns Abstract Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#slicegpt-compress-large-language-models-by-deleting-rows-and-columns","text":"","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"location":"notes/2024/SliceGPT/note/#abstract","text":"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","title":"Abstract"},{"location":"notes/2024/SlimGPT/note/","text":"SlimGPT: Layer-wise Structured Pruning for Large Language Models Abstract Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#slimgpt-layer-wise-structured-pruning-for-large-language-models","text":"","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models"},{"location":"notes/2024/SlimGPT/note/#abstract","text":"Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. Batched Greedy Pruning \u8fd9\u4e2a\u7b97\u6cd5[TODO] \u5bf9\u7b2c\u4e00\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u6d4b\u91cf\u6bcf\u5c42\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u8d8a\u5230\u6700\u540e\u4e00\u5c42\u8bef\u5dee\u8d8a\u5927\uff0c\u56e0\u6b64\u7b2c\u4e00\u5c42\u6700\u91cd\u8981\uff0c\u6700\u540e\u4e00\u5c42\u6700\u4e0d\u91cd\u8981\uff0c\u6240\u4ee5\u6309\u7167log\u589e\u52a0","title":"Abstract"},{"location":"notes/2024/SnapKV/note/","text":"SnapKV: LLM Knows What You are Looking for Before Generation Abstract Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#snapkv-llm-knows-what-you-are-looking-for-before-generation","text":"","title":"SnapKV: LLM Knows What You are Looking for Before Generation"},{"location":"notes/2024/SnapKV/note/#abstract","text":"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. \u6839\u636e\u6700\u8fd1\u7684\u5386\u53f2token\u8ba1\u7b97\u5f97\u5230attention\u7684score\uff0cpool topk\u9009\u62e9\u91cd\u8981\u7684kv cache\uff0c\u538b\u7f29kv cache\u3002","title":"Abstract"},{"location":"notes/2024/SparQ/note/","text":"SparQ Attention: Bandwidth-Efficient LLM Inference Abstract The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#sparq-attention-bandwidth-efficient-llm-inference","text":"","title":"SparQ Attention: Bandwidth-Efficient LLM Inference"},{"location":"notes/2024/SparQ/note/#abstract","text":"The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.","title":"Abstract"},{"location":"notes/2024/Sparse-IFT/note/","text":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency Abstract Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#sparse-ift-sparse-iso-flop-transformations-for-maximizing-training-efficiency","text":"","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"},{"location":"notes/2024/Sparse-IFT/note/#abstract","text":"Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.","title":"Abstract"},{"location":"notes/2024/SparseInfer/note/","text":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference Abstract Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#sparseinfer-training-free-prediction-of-activation-sparsity-for-fast-llm-inference","text":"","title":"SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference"},{"location":"notes/2024/SparseInfer/note/#abstract","text":"Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p. \u9884\u6d4b\u5668\u4e0d\u7528\u8bad\u7ec3\uff0c\u53ea\u7528\u83b7\u53d6\u6743\u91cd\u7684MSB\uff0cmost significant bits\uff0c\u4e0e\u8f93\u5165\u8fdb\u884cxor\u8fd0\u7b97\uff0c\u4ece\u800c\u9884\u6d4b\u7ed3\u679c\u6b63\u8d1f\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u4e4b\u524d\u7684\u9884\u6d4b\u5668\u3002","title":"Abstract"},{"location":"notes/2024/SparseLLM/note/","text":"SparseLLM: Towards Global Pruning for Pre-trained Language Models Abstract The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#sparsellm-towards-global-pruning-for-pre-trained-language-models","text":"","title":"SparseLLM: Towards Global Pruning for Pre-trained Language Models"},{"location":"notes/2024/SparseLLM/note/#abstract","text":"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. \u4e4b\u524dsparsegpt\uff0cwanda\u7b49\u662f\u6309\u7167layer-wise loss\u6765\u9010\u5c42\u4f18\u5316\u7684\uff0c\u53ef\u4ee5\u79f0\u4e4b\u4e3a local pruning\uff0c\u901a\u5e38\u53ea\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\uff1b\u76f8\u5bf9\u800c\u8a00\uff0cglobal pruning\u8003\u8651\u5168\u5c40\u7684loss\uff0c\u4ece\u800c\u5728\u7406\u8bba\u4e0a\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002global pruning\u4e0d\u53ef\u907f\u514d\u5728\u4f1a\u5bfc\u81f4\u95ee\u9898\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8be5\u5de5\u4f5c\u8003\u8651\u5207\u5206\u4e3a\u591a\u4e2asubproblem\u6765\u7f13\u89e3\uff1b global pruning\u4f7f\u7528\u8f83\u5c11\u7684\u6570\u636e\u96c6\u65f6\u4e5f\u6709overfit\u7684\u98ce\u9669\uff01\uff01 \u7ed3\u679c\u63d0\u5347\u4e0d\u5927","title":"Abstract"},{"location":"notes/2024/SqueezeLLM/note/","text":"SqueezeLLM: Dense-and-Sparse Quantization Abstract Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#squeezellm-dense-and-sparse-quantization","text":"","title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"location":"notes/2024/SqueezeLLM/note/#abstract","text":"Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM. non-uniform quantization \u8282\u7701IO, \u628aweight\u5212\u5206\u4e3adense\u548csparse\u4e24\u90e8\u5206\uff0csparse\u4ec5\u53600.45%\uff0c\u5305\u542b\u4e00\u4e9boutlier\uff0csensitive weight\u7b49\uff0c\u4f7f\u7528fp16\u8868\u793a\uff0c\u5e76\u4f7f\u7528csr\u65b9\u5f0f\u538b\u7f29\u5b58\u50a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5e94\u7684cuda kernel\u52a0\u901f\u5b9e\u73b0\u3002","title":"Abstract"},{"location":"notes/2024/T3/note/","text":"T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair Abstract Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG.","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#t3-transparent-tracking-triggering-for-fine-grained-overlap-of-compute-collectives","text":"Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair","title":"T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives"},{"location":"notes/2024/T3/note/#abstract","text":"Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy. To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG.","title":"Abstract"},{"location":"notes/2024/TOVA/note/","text":"Transformers are Multi-State RNNs Abstract Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#transformers-are-multi-state-rnns","text":"","title":"Transformers are Multi-State RNNs"},{"location":"notes/2024/TOVA/note/#abstract","text":"Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA","title":"Abstract"},{"location":"notes/2024/TinyTrain/note/","text":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge Abstract On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#tinytrain-resource-aware-task-adaptive-sparse-training-of-dnns-at-the-data-scarce-edge","text":"","title":"TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge"},{"location":"notes/2024/TinyTrain/note/#abstract","text":"On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss (>10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098x and 7.68x, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5x faster and 3.5x more energy-efficient training over status-quo approaches, and 2.23x smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms.","title":"Abstract"},{"location":"notes/2024/TurboSparse/note/","text":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters Abstract Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#turbo-sparse-achieving-llm-sota-performance-with-minimal-activated-parameters","text":"","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters"},{"location":"notes/2024/TurboSparse/note/#abstract","text":"Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","title":"Abstract"},{"location":"notes/2024/ULY1AZGY/note/","text":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment Abstract Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#enabling-high-sparsity-foundational-llama-models-with-efficient-pretraining-and-deployment","text":"","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"},{"location":"notes/2024/ULY1AZGY/note/#abstract","text":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","title":"Abstract"},{"location":"notes/2024/VB8C61V6/note/","text":"Compressing LLMs: The Truth is Rarely Pure and Never Simple Abstract Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#compressing-llms-the-truth-is-rarely-pure-and-never-simple","text":"","title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"location":"notes/2024/VB8C61V6/note/#abstract","text":"Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.","title":"Abstract"},{"location":"notes/2024/Vidur/note/","text":"Vidur: A Large-Scale Simulation Framework For LLM Inference Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov Abstract Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#vidur-a-large-scale-simulation-framework-for-llm-inference","text":"Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"},{"location":"notes/2024/Vidur/note/#abstract","text":"Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","title":"Abstract"},{"location":"notes/2024/Wanda/note/","text":"A Simple and Effective Pruning Approach for Large Language Models Abstract As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#a-simple-and-effective-pruning-approach-for-large-language-models","text":"","title":"A Simple and Effective Pruning Approach for Large Language Models"},{"location":"notes/2024/Wanda/note/#abstract","text":"As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.","title":"Abstract"},{"location":"notes/2024/XGrammar/note/","text":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models Abstract The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#xgrammar-flexible-and-efficient-structured-generation-engine-for-large-language-models","text":"","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"},{"location":"notes/2024/XGrammar/note/#abstract","text":"The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","title":"Abstract"},{"location":"notes/2024/YS9YTT55/note/","text":"LLM Inference Serving: Survey of Recent Advances and Opportunities Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari Abstract This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#llm-inference-serving-survey-of-recent-advances-and-opportunities","text":"Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari","title":"LLM Inference Serving: Survey of Recent Advances and Opportunities"},{"location":"notes/2024/YS9YTT55/note/#abstract","text":"This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.","title":"Abstract"},{"location":"notes/2024/ZigZagKV/note/","text":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty Abstract Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#zigzagkv-dynamic-kv-cache-compression-for-long-context-modeling-based-on-layer-uncertainty","text":"","title":"ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty"},{"location":"notes/2024/ZigZagKV/note/#abstract","text":"Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full KV inference while achieving nearly lossless performance.","title":"Abstract"},{"location":"notes/2024/flash_llm/","text":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/flash_llm/#flash-llm-enabling-cost-effective-and-highly-efficient-large-generative-model-inference-with-unstructured-sparsity","text":"Key idea: Load-as-Sparse and Compute-as-Dense","title":"Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"},{"location":"notes/2024/massive-activations/note/","text":"Massive Activations in Large Language Models Abstract We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#massive-activations-in-large-language-models","text":"","title":"Massive Activations in Large Language Models"},{"location":"notes/2024/massive-activations/note/#abstract","text":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","title":"Abstract"},{"location":"notes/2024/streaming-llm/note/","text":"Efficient Streaming Language Models with Attention Sinks Abstract Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#efficient-streaming-language-models-with-attention-sinks","text":"","title":"Efficient Streaming Language Models with Attention Sinks"},{"location":"notes/2024/streaming-llm/note/#abstract","text":"Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","title":"Abstract"},{"location":"notes/2025/07NWF4VE/note/","text":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching Abstract Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#accelerating-llm-inference-throughput-via-asynchronous-kv-cache-prefetching","text":"","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"},{"location":"notes/2025/07NWF4VE/note/#abstract","text":"Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","title":"Abstract"},{"location":"notes/2025/0VRXJQ3F/note/","text":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving Abstract Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#rethinking-key-value-cache-compression-techniques-for-large-language-model-serving","text":"","title":"Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving"},{"location":"notes/2025/0VRXJQ3F/note/#abstract","text":"Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.","title":"Abstract"},{"location":"notes/2025/1DZIJVBI/note/","text":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan Abstract This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#characterizing-compute-communication-overlap-in-gpu-accelerated-distributed-deep-learning-performance-and-power-implications","text":"Seonho Lee, Jihwan Oh, Junkyum Kim, Seokjin Go, Jongse Park, Divya Mahajan","title":"Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications"},{"location":"notes/2025/1DZIJVBI/note/#abstract","text":"This paper provides an in-depth characterization of GPU-accelerated systems, to understand the interplay between overlapping computation and communication which is commonly employed in distributed training settings. Due to the large size of models, distributing them across multiple devices is required. Overlapping strategies, which enable concurrent computation and communication, are critical for mitigating communication bottlenecks and maximizing GPU utilization. However, the current consensus is that we should always and aggressively overlap compute and communication to mitigate the overhead of distribution. By systematically evaluating state-of-the-art GPUs, this study investigates the impact of hardware features such as numeric precision, specialized cores, and power capping on distributed training workloads. Comprehensive experiments and studies showcase the effects of overlapping strategies on performance and power consumption across varying scenarios. We observe that overlapping computation and communication can result in an average computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This slowdown is in comparison to the scenario when no communication was happening with the compute. We consider this an ideal execution scenario, where the communication in parallel has not impact on the compute time. However, performing computation and communication sequentially is, on average, 10.2% slower than overlapped execution, with a maximum slowdown of 26.6%. We further observe, while specialized datapath and optimized numeric precision mitigate certain slowdowns, overlapping execution can lead to resource contention and also increase power consumption under specific configurations. The analysis also uncovers trade-offs introduced by power and frequency capping, emphasizing the importance of balanced strategies to optimize energy efficiency and training throughput. \u7406\u8bba\u60c5\u51b5\u4e0boverlap\u8ba1\u7b97\u548c\u901a\u4fe1\u4e0d\u4e92\u76f8\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528overlap\u5c06\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u53d8\u616220%\uff5e40%\uff0c\u6b64\u5916overlap\u5bfc\u81f4\u8d44\u6e90\u4e89\u593a\u4e5f\u4f1a\u589e\u52a0\u529f\u8017","title":"Abstract"},{"location":"notes/2025/2ZU1IWL6/note/","text":"Fast and Simplex: 2-Simplicial Attention in Triton Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil Abstract Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#fast-and-simplex-2-simplicial-attention-in-triton","text":"Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil","title":"Fast and Simplex: 2-Simplicial Attention in Triton"},{"location":"notes/2025/2ZU1IWL6/note/#abstract","text":"Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.","title":"Abstract"},{"location":"notes/2025/52A7RO95/note/","text":"Mixture of Experts in Large Language Models Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao Abstract This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#mixture-of-experts-in-large-language-models","text":"Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao","title":"Mixture of Experts in Large Language Models"},{"location":"notes/2025/52A7RO95/note/#abstract","text":"This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","title":"Abstract"},{"location":"notes/2025/ACP/note/","text":"Adaptive Computation Pruning for the Forgetting Transformer Abstract The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#adaptive-computation-pruning-for-the-forgetting-transformer","text":"","title":"Adaptive Computation Pruning for the Forgetting Transformer"},{"location":"notes/2025/ACP/note/#abstract","text":"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox. \u57fa\u4e8eForgetting Transformer\u7684\u6a21\u578b\u4f18\u5316","title":"Abstract"},{"location":"notes/2025/AMALI/note/","text":"AMALI Abstract GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a - cycle-accurate simulators - \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 - analytical models - \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a 1. \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 2. \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"AMALI"},{"location":"notes/2025/AMALI/note/#amali","text":"","title":"AMALI"},{"location":"notes/2025/AMALI/note/#abstract","text":"GPU \u7684\u6027\u80fd\u8bc4\u4f30\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff1a - cycle-accurate simulators - \u63d0\u4f9bcycle\u91cf\u7ea7\u7684\u4eff\u771f\uff0c\u4f46\u662f\u5f88\u6162\uff0c\u5f88\u590d\u6742\uff0c\u56e0\u6b64\u4e5f\u96be\u4ee5\u8fdb\u884c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u3002 - analytical models - \u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u80fd\u591f\u5efa\u7acbcycles-per-instruction\u7ed3\u679c\uff0c\u4fbf\u4e8e\u5206\u6790\u67b6\u6784\u7684\u74f6\u9888 \u4f46\u662fanalytical models\u76ee\u524d\u4ecd\u7136\u975e\u5e38\u4e0d\u51c6\u786e\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u9762\u7684\u539f\u56e0\uff1a 1. \u5f53\u524d\u6ca1\u6709\u8003\u8651tensor cores, immediate constant cache, instruction caches\u7b49\u65b0\u7684GPU\u7279\u6027 2. \u6ca1\u6709\u8003\u8651LLM\u5728\u63a8\u7406\u7684\u7279\u6027 \u56e0\u6b64AMALI\u63d0\u51fa \u63d0\u51fa\u4e86Tensor Core model\uff0c\u7528\u4ee5\u7cbe\u786e\u6355\u6349\u7684\u7531\u4e8edtype\u548csize\u4e0d\u540c\u5bfc\u81f4\u7684\u4e0d\u540c\u5ef6\u8fdf \u5bf9kernel lannch \u7684\u5ef6\u8fdf\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u8fd9\u4e9b\u5ef6\u8fdf\u53ef\u80fd\u662f\u7531immediate constant cache misses and instruction cache misses\u5bfc\u81f4 \u5bf9LLM\u63a8\u7406\u65f6\u6d89\u53ca\u7684warp\u7ea7\u6307\u4ee4\u8fdb\u884c\u4e86\u5efa\u6a21","title":"Abstract"},{"location":"notes/2025/Acc-SpMM/note/","text":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores Abstract General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#acc-spmm-accelerating-general-purpose-sparse-matrix-matrix-multiplication-with-gpu-tensor-cores","text":"","title":"Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores"},{"location":"notes/2025/Acc-SpMM/note/#abstract","text":"General-purpose Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental kernel in scientific computing and deep learning. The emergence of new matrix computation units such as Tensor Cores (TCs) brings more opportunities for SpMM acceleration. However, in order to fully unleash the power of hardware performance, systematic optimization is required. In this paper, we propose Acc-SpMM, a high-performance SpMM library on TCs, with multiple optimizations, including data-affinity-based reordering, memory efficient compressed format, high-throughput pipeline, and adaptive sparsity-aware load balancing. In contrast to the state-of-the-art SpMM kernels on various NVIDIA GPU architectures with a diverse range of benchmark matrices, Acc-SpMM achieves significant performance improvements, on average 2.52x (up to 5.11x) speedup on RTX 4090, on average 1.91x (up to 4.68x) speedup on A800, and on average 1.58x (up to 3.60x) speedup on H100 over cuSPARSE. \u901a\u8fc7\u7a00\u758f\u7f16\u7801\u51cf\u5c11\u6574\u5217\u7684IO\u548c\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/AdaSkip/note/","text":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference Abstract Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#adaskip-adaptive-sublayer-skipping-for-accelerating-long-context-llm-inference","text":"","title":"AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference"},{"location":"notes/2025/AdaSkip/note/#abstract","text":"Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.","title":"Abstract"},{"location":"notes/2025/AdaSplash/note/","text":"AdaSplash: Adaptive Sparse Flash Attention Abstract The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#adasplash-adaptive-sparse-flash-attention","text":"","title":"AdaSplash: Adaptive Sparse Flash Attention"},{"location":"notes/2025/AdaSplash/note/#abstract","text":"The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.","title":"Abstract"},{"location":"notes/2025/AdaptiveSparseTrainer/note/","text":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training Abstract The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#pruning-large-language-models-with-semi-structural-adaptive-sparse-training","text":"","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse Training"},{"location":"notes/2025/AdaptiveSparseTrainer/note/#abstract","text":"The remarkable success of Large Language Models (LLMs) relies heavily on their substantial scale, which poses significant challenges during model deployment in terms of latency and memory consumption. Recently, numerous studies have attempted to compress LLMs using one-shot pruning methods. However, these methods often suffer from considerable performance degradation on complex language understanding tasks, raising concerns about the feasibility of pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer (AST), a novel and efficient retraining framework tailored for semi-structured sparse models. AST enables models to learn optimal masks during the weight update process without incurring additional computational overhead. Furthermore, we demonstrate that incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance under fixed computational constraints. Additionally, a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy. AST achieves state-of-the-art performance with minimal training cost. When applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and 1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU hours. Our work demonstrates the feasibility of deploying semi-structured sparse LLMs and offers a promising alternative for achieving highly compressed models when combined with existing quantization techniques.","title":"Abstract"},{"location":"notes/2025/Adrenaline/note/","text":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation Abstract In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#injecting-adrenaline-into-llm-serving-boosting-resource-utilization-and-throughput-via-attention-disaggregation","text":"","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation"},{"location":"notes/2025/Adrenaline/note/#abstract","text":"In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.","title":"Abstract"},{"location":"notes/2025/AhaKV/note/","text":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu Abstract Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#ahakv-adaptive-holistic-attention-driven-kv-cache-eviction-for-efficient-inference-of-large-language-models","text":"Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu","title":"AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models"},{"location":"notes/2025/AhaKV/note/#abstract","text":"Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.","title":"Abstract"},{"location":"notes/2025/AmberPruner/note/","text":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang Abstract In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#amber-pruner-leveraging-nm-activation-sparsity-for-efficient-prefill-in-large-language-models","text":"Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"notes/2025/AmberPruner/note/#abstract","text":"In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. \u5bf9activation\u8fdb\u884cN\uff1aM pruning\uff0c\u65b9\u6cd5\u501f\u9274Wanda\uff0c\u901a\u8fc7weight\u7ed9activation\u7684\u91cd\u8981\u6027\u8fdb\u884cscale\uff0c\u56e0\u4e3aweight\u662f\u9759\u6001\u7684\uff0c\u56e0\u6b64scale\u4e5f\u662f\u9759\u6001\u7684\uff0c\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u52a8\u6001\u7684\u6fc0\u6d3b\u503c\u4e0e\u9759\u6001\u7684scale\u76f8\u4e58\uff0c\u5e76\u4f7f\u7528topk\u5f97\u5230\u52a8\u6001\u7684mask\u3002 \u65b9\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u52a8\u6001\u7684\u7b97mask\u5f00\u9500\u8f83\u5927\u3002","title":"Abstract"},{"location":"notes/2025/AttentionPredictor/note/","text":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference Abstract With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#attentionpredictor-temporal-pattern-matters-for-efficient-llm-inference","text":"","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"},{"location":"notes/2025/AttentionPredictor/note/#abstract","text":"With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.","title":"Abstract"},{"location":"notes/2025/BaWA/note/","text":"BaWA","title":"BaWA"},{"location":"notes/2025/BaWA/note/#bawa","text":"","title":"BaWA"},{"location":"notes/2025/BlockFFN/note/","text":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun Abstract To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#blockffn-towards-end-side-acceleration-friendly-mixture-of-experts-with-chunk-level-activation-sparsity","text":"Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity"},{"location":"notes/2025/BlockFFN/note/#abstract","text":"To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN). \u8fd9\u91cc\u53d6\u8fde\u7eed\u76848\u4e2atoken\u5224\u65ad\u662f\u5426\u9700\u8981\u540c\u65f6\u6fc0\u6d3b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e0e\u6211\u4eec\u7684\u60f3\u6cd5\u57fa\u672c\u4e00\u81f4\u3002","title":"Abstract"},{"location":"notes/2025/CCQ/note/","text":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang Abstract The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#ccq-convolutional-code-for-extreme-low-bit-quantization-in-llms","text":"Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang","title":"CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs"},{"location":"notes/2025/CCQ/note/#abstract","text":"The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.","title":"Abstract"},{"location":"notes/2025/COMET/note/","text":"COMET: Towards Partical W4A4KV4 LLMs Serving Abstract Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#comet-towards-partical-w4a4kv4-llms-serving","text":"","title":"COMET: Towards Partical W4A4KV4 LLMs Serving"},{"location":"notes/2025/COMET/note/#abstract","text":"Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.","title":"Abstract"},{"location":"notes/2025/CateKV/note/","text":"CateKV","title":"CateKV"},{"location":"notes/2025/CateKV/note/#catekv","text":"","title":"CateKV"},{"location":"notes/2025/ChunkKV/note/","text":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Abstract To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#chunkkv-semantic-preserving-kv-cache-compression-for-efficient-long-context-llm-inference","text":"","title":"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"},{"location":"notes/2025/ChunkKV/note/#abstract","text":"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods. \u6309\u7167chunk\u6765\u5224\u65ad\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2atoken\uff0c\u8fd9\u6837\u80fd\u66f4\u597d\u7684\u4fdd\u7559tokens\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\u3002","title":"Abstract"},{"location":"notes/2025/CometSeed/note/","text":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu Abstract Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\\times$ and for end-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts","text":"Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu","title":"Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts"},{"location":"notes/2025/CometSeed/note/#abstract","text":"Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\\times$ and for end-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.","title":"Abstract"},{"location":"notes/2025/Cus-Prun/note/","text":"Pruning General Large Language Models into Customized Expert Models Abstract Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#pruning-general-large-language-models-into-customized-expert-models","text":"","title":"Pruning General Large Language Models into Customized Expert Models"},{"location":"notes/2025/Cus-Prun/note/#abstract","text":"Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes. language domain task \u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u7136\u540e\u6309\u7167\u5b9e\u9645\u7684\u4efb\u52a1\uff0c\u5c06\u4e09\u4e2a\u7ef4\u5ea6\u7684mask\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684mask\u3002","title":"Abstract"},{"location":"notes/2025/DBudgetKV/note/","text":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance Abstract To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#dbudgetkv-dynamic-budget-in-kv-cache-compression-for-ensuring-optimal-performance","text":"","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance"},{"location":"notes/2025/DBudgetKV/note/#abstract","text":"To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods. \u52a8\u6001\u7684\u5bf9\u6bcf\u4e00\u5c42\u7684kv \u7a00\u758f\u5ea6budget\u8fdb\u884c\u8c03\u6574\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4e0d\u964d\u4f4e\u540c\u65f6\u51cf\u5c11\u66f4\u591a\u7684kv cache","title":"Abstract"},{"location":"notes/2025/DeepSeek-R1/note/","text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning","text":"","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"location":"notes/2025/DeepSeek-R1/note/#abstract","text":"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","title":"Abstract"},{"location":"notes/2025/DeltaAttention/note/","text":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction Abstract The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#delta-attention-fast-and-accurate-sparse-attention-inference-by-delta-correction","text":"","title":"Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction"},{"location":"notes/2025/DeltaAttention/note/#abstract","text":"The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills. sparse prefill\u7684output\u548cdense prefill\u7684output\u5206\u5e03\u4f1a\u53d1\u751f\u504f\u79fb\uff0c\u53ef\u4ee5sample\u4e00\u90e8\u5206query\u8ba1\u7b97dense \u548c sparse \u4e4b\u95f4\u7684\u504f\u79fb\u91cf\uff0c\u5e76\u590d\u5236\u6269\u5c55\uff0c\u5bf9sparse prefill \u7684output\u8fdb\u884c\u4fee\u6b63\u3002 \u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728MInference\u548cStreamingLLM\u57fa\u7840\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u4ec5\u6709\u5c11\u91cf\u7684overhead\u3002","title":"Abstract"},{"location":"notes/2025/DeltaLLM/note/","text":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen Abstract Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#deltallm-a-training-free-framework-exploiting-temporal-sparsity-for-efficient-edge-llm-inference","text":"Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen","title":"DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"notes/2025/DeltaLLM/note/#abstract","text":"Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"Abstract"},{"location":"notes/2025/FastKV/note/","text":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Abstract While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#fastkv-kv-cache-compression-for-fast-long-context-processing-with-token-selective-propagation","text":"","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"},{"location":"notes/2025/FastKV/note/#abstract","text":"While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.","title":"Abstract"},{"location":"notes/2025/FlashInfer/note/","text":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving Abstract Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#flashinfer-efficient-and-customizable-attention-engine-for-llm-inference-serving","text":"","title":"FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"},{"location":"notes/2025/FlashInfer/note/#abstract","text":"Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.","title":"Abstract"},{"location":"notes/2025/FlashOverlap/note/","text":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang Abstract Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2025/FlashOverlap/note/#flashoverlap-a-lightweight-design-for-efficiently-overlapping-communication-and-computation","text":"Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation"},{"location":"notes/2025/FlashOverlap/note/#abstract","text":"Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.","title":"Abstract"},{"location":"notes/2025/FlexPrefill/note/","text":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference Abstract Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference","text":"","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference"},{"location":"notes/2025/FlexPrefill/note/#abstract","text":"Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. \u505a\u6cd5\u548cMInfernce\u975e\u5e38\u50cf\uff0c\u53ea\u662f\u9009\u62e9patter\u8bbe\u7f6e\u65f6\u6709\u4e00\u5b9a\u7684\u533a\u522b","title":"Abstract"},{"location":"notes/2025/FlexiDepth/note/","text":"Adaptive Layer-skipping in Pre-trained LLMs Abstract Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#adaptive-layer-skipping-in-pre-trained-llms","text":"","title":"Adaptive Layer-skipping in Pre-trained LLMs"},{"location":"notes/2025/FlexiDepth/note/#abstract","text":"Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration. \u590d\u6742\u4e00\u4e9b\u7684router\u8bbe\u8ba1 \u76f8\u8f83\u4e8e\u5355\u4e2aLinear Layer\uff0c\u6548\u679c\u66f4\u597d Attention skipping \u8df3\u8fc7query\u5bf9\u5e94\u7684\u8fd0\u7b97\uff0c\u4f46\u4ecd\u7136\u8ba1\u7b97KV cache MLP Skipping \u4f7f\u7528\u66f4\u5c0f\u7684MLP\u66ff\u6362\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53bb\u6389","title":"Abstract"},{"location":"notes/2025/FoX/note/","text":"Forgetting Transformer: Softmax Attention with a Forget Gate Abstract An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#forgetting-transformer-softmax-attention-with-a-forget-gate","text":"","title":"Forgetting Transformer: Softmax Attention with a Forget Gate"},{"location":"notes/2025/FoX/note/#abstract","text":"An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer. Linear Transformer \u7cfb\u5217 \u6a21\u578b\u8f83\u5c0f\uff0c\u4f46\u662f\u4ee3\u7801\u5f00\u6e90\uff0c\u80fd\u516c\u5e73\u7684\u5bf9\u6bd4\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8bad\u7ec3\u3002","title":"Abstract"},{"location":"notes/2025/FreqKV/note/","text":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension Abstract Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#freqkv-frequency-domain-key-value-compression-for-efficient-context-window-extension","text":"","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension"},{"location":"notes/2025/FreqKV/note/#abstract","text":"Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method. \u5728context \u6269\u5c55\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9700\u8981\u8bad\u7ec3\uff0c\u5e76\u4e14\u9700\u8981runtime\u5bf9KV \u8fdb\u884c\u538b\u7f29\uff0c\u63d0\u5347\u6709\u9650\uff0cICLR 2025\u6295\u7a3f\u88ab\u62d2\u3002","title":"Abstract"},{"location":"notes/2025/GLA/note/","text":"Hardware-Efficient Attention for Fast Decoding Abstract LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\\times$. MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#hardware-efficient-attention-for-fast-decoding","text":"","title":"Hardware-Efficient Attention for Fast Decoding"},{"location":"notes/2025/GLA/note/#abstract","text":"LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\\times$. MLA\u7684\u6539\u8fdb\uff0c\u5c06latent\u5212\u5206group\uff0c\u7c7b\u4f3cMHA\u5230GQA\u7684\u6539\u8fdb\u3002","title":"Abstract"},{"location":"notes/2025/HATA/note/","text":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li Abstract Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#hata-trainable-and-hardware-efficient-hash-aware-top-k-attention-for-scalable-large-model-inference","text":"Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li","title":"HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference"},{"location":"notes/2025/HATA/note/#abstract","text":"Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA. HATA\u7528\u4e8edecoding\u65f6\u7684\u52a0\u901f\uff0c\u4e4b\u524dkv\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982snapkv\u7528\u5386\u53f2\u7684\u7ed3\u679c\u9884\u6d4b\u5f53\u524d\u7ed3\u679c\uff0cquest\u6309\u7167block pool\u9884\u6d4bblock\u7684\u91cd\u8981\u6027\uff0cseerattention\u901a\u8fc7\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u91cd\u8981\u6027\uff1b HATA\u63d0\u51fa\u5c06QK\u6620\u5c04\u4e3aHash Code\uff0c\u5e76\u5b9e\u7528xor \u64cd\u4f5c\u9ad8\u6548\u7684\u5224\u65adK cache\u7684\u91cd\u8981\u6027\u3002 \u6620\u5c04Hash Code\u7684\u8fc7\u7a0b\u6d89\u53ca\u5230\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u9700\u8981\u4e00\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60 \u548c HashAttention: Semantic Sparsity for Faster Inference \u601d\u8def\u5f88\u50cf","title":"Abstract"},{"location":"notes/2025/HCAttention/note/","text":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao Abstract Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#hcattention-extreme-kv-cache-compression-via-heterogeneous-attention-computing-for-llms","text":"Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao","title":"HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"notes/2025/HCAttention/note/#abstract","text":"Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"Abstract"},{"location":"notes/2025/HashAttention/note/","text":"HashAttention: Semantic Sparsity for Faster Inference Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica Abstract Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\\times$ through task-specific fine-tuning. On A100 GPU, at $32\\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\\times$ in GPT-FAST and $2.54\\times$ in FlashDecode, and achieves up to $3.12\\times$ higher throughput for GPT-FAST.","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#hashattention-semantic-sparsity-for-faster-inference","text":"Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica","title":"HashAttention: Semantic Sparsity for Faster Inference"},{"location":"notes/2025/HashAttention/note/#abstract","text":"Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\\times$ through task-specific fine-tuning. On A100 GPU, at $32\\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\\times$ in GPT-FAST and $2.54\\times$ in FlashDecode, and achieves up to $3.12\\times$ higher throughput for GPT-FAST.","title":"Abstract"},{"location":"notes/2025/HelixParallelism/note/","text":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani Abstract As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#helix-parallelism-rethinking-sharding-strategies-for-interactive-multi-million-token-llm-decoding","text":"Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani","title":"Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding"},{"location":"notes/2025/HelixParallelism/note/#abstract","text":"As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency. We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical. \u957f\u5e8f\u5217\u63a8\u7406\u65f6\uff0cKVP\u5bf9KV\u8fdb\u884c\u5207\u5206\uff0c\u4e0e\u4e4b\u524d\u5de5\u4f5c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u6cbf\u7740senquence \u7ef4\u5ea6\u5207\u5206 \u5bfc\u81f4qK\u8fd0\u7b97\u540e\u7ed3\u679c\u9700\u8981All-to-All\u901a\u4fe1\u8fdb\u884c\u5168\u5c40\u7684softmax \u5982\u4f55\u7f13\u89e3\u4e0a\u9762\u95ee\u9898\uff1f\u91c7\u7528batch-wise computation-communication overlap MLP\u90e8\u5206\u6309\u7167TP\u5207\u5206","title":"Abstract"},{"location":"notes/2025/IFPruning/note/","text":"Instruction-Following Pruning for Large Language Models Abstract With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#instruction-following-pruning-for-large-language-models","text":"","title":"Instruction-Following Pruning for Large Language Models"},{"location":"notes/2025/IFPruning/note/#abstract","text":"With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. \u5f15\u5165\u4e86Sparsity Predictor\u51b3\u5b9aMLP\u7684\u90e8\u5206channel\u88ab\u7a00\u758f\u3002\u8fd9\u4e2aPredictor\u4ec5\u548cprompt\u6709\u5173\uff0c\u6240\u4ee5\u4ecb\u4e8e input-dependent pruning \u4e0e static pruning \u4e4b\u95f4\u3002 \u8fd9\u4e2a\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u8bad\u7ec3Predictor\uff0c\u6bd4\u5982\u201dtranslate English text to French\u201c\u4f5c\u4e3aprompt\uff0c\u591a\u8f6e\u5bf9\u8bdd\u65f6\uff0c\u53ea\u4f7f\u7528\u7b2c\u4e00\u8f6ehuman\u7684message\u4f5c\u4e3aprompt\u3002 \u5728\u63a8\u7406\u65f6\uff0c\u4ec5\u9700\u8981\u6839\u636eprompt\u8f93\u5165\u5230Predictor\u5f97\u5230MLP\u7684\u7a00\u758f\u4f4d\u7f6e\uff0c\u5f97\u5230sub model\uff0c\u4f7f\u7528sub model\u8fdb\u884c\u63a8\u7406\uff0c\u4fbf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002 \u65b0\u7684\u6311\u6218\uff0c\u4e0d\u540ctask\u5c31\u4e0d\u80fd\u8fdb\u884cbatching\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6fc0\u6d3b\u7684\u4f4d\u7f6e\u662f\u4e0d\u540c\u7684\u3002\u4e5f\u5c31\u8981\u6c42\u63d0\u524d\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5f97\u5230\u7a00\u758f\u540e\u7684sub model\uff0c\u4ece\u800c\u53ef\u4ee5\u52a0\u901f\u6bcf\u4e2atask\u3002","title":"Abstract"},{"location":"notes/2025/KVCache-Factory/note/","text":"KVCache-Factory Abstract","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#kvcache-factory","text":"","title":"KVCache-Factory"},{"location":"notes/2025/KVCache-Factory/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/KVLink/note/","text":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse Abstract We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Motivation \u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898 KVLink KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#kvlink-accelerating-large-language-models-via-efficient-kv-cache-reuse","text":"","title":"KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"},{"location":"notes/2025/KVLink/note/#abstract","text":"We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.","title":"Abstract"},{"location":"notes/2025/KVLink/note/#motivation","text":"\u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u5171\u4eab\u4e00\u6bb5\u957f\u6587\u672c\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981decode\u751f\u6210KV cache \u672c\u5de5\u4f5c\u63d0\u51fa\u628a\u6587\u672c\u6309\u7167segment\u63d0\u524d\u8ba1\u7b97\u5f97\u5230KV\uff0c\u5728\u4f7f\u7528\u65f6\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5 \u4f46\u4f1a\u9047\u5230\u4e00\u4e9b\u95ee\u9898","title":"Motivation"},{"location":"notes/2025/KVLink/note/#kvlink","text":"KV cache positional re-encoding\uff0c\u5728Inference\u65f6\u5bf9KV\u8fdb\u884c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 Trainable cross-segment special tokens\uff0c\u4e24\u6bb5\u4e0d\u4f9d\u8d56\u6587\u672c\u62fc\u63a5\u65f6\uff0c\u589e\u52a0\u4e00\u4e9btoken Fine-tuning with a diverse data mixture \u6700\u540e\u589e\u52a0\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u611f\u77e5\u5230\u591a\u4e2asegment\u62fc\u63a5","title":"KVLink"},{"location":"notes/2025/KVSink/note/","text":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs Zunhai Su, Kehong Yuan Abstract Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#kvsink-understanding-and-enhancing-the-preservation-of-attention-sinks-in-kv-cache-quantization-for-llms","text":"Zunhai Su, Kehong Yuan","title":"KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"notes/2025/KVSink/note/#abstract","text":"Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. KV Sink\u73b0\u8c61\u4e00\u822c\u5728\u524d\u51e0\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u4e5f\u5b8c\u5168\u4e00\u6837\uff0c\u672c\u5de5\u4f5c\u901a\u8fc7\u89c2\u5bdfoutlier\u7684\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5f97\u5230stable outlier\uff0c\u80fd\u591f\u9ad8\u6548\u7684\u9884\u6d4bSink\u7684\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u6b64\u8fdb\u884c\u6df7\u5408\u4f4d\u5bbd\u7684\u91cf\u5316\u3002","title":"Abstract"},{"location":"notes/2025/KeepKV/note/","text":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference Abstract Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#keepkv-eliminating-output-perturbation-in-kv-cache-compression-for-efficient-llms-inference","text":"","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference"},{"location":"notes/2025/KeepKV/note/#abstract","text":"Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.","title":"Abstract"},{"location":"notes/2025/LIMINAL/note/","text":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis Abstract This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need","text":"Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis","title":"Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"notes/2025/LIMINAL/note/#abstract","text":"This paper presents a limit study of transformer-based large language model (LLM) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective communication must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of LLM inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of LLM deployment strategies.","title":"Abstract"},{"location":"notes/2025/LServer/note/","text":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Abstract Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention","text":"","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"},{"location":"notes/2025/LServer/note/#abstract","text":"Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. An efficient serving system for long sequence LLMs that leverages hybrid sparse attention.","title":"Abstract"},{"location":"notes/2025/LaRoSA/note/","text":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu Abstract Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#la-rosa-enhancing-llm-efficiency-via-layerwise-rotated-sparse-activation","text":"Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu","title":"La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation"},{"location":"notes/2025/LaRoSA/note/#abstract","text":"Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.","title":"Abstract"},{"location":"notes/2025/LeanK/note/","text":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu Abstract Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#leank-learnable-k-cache-channel-pruning-for-efficient-decoding","text":"Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"notes/2025/LeanK/note/#abstract","text":"Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. static channel mask \u4fdd\u7559attention sink\u548crecent tokens K cache\u4e2d\u95f4\u90e8\u5206\u6309\u7167channel\u8fdb\u884csparse\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0cmask\u8bad\u7ec3\u63d0\u524d\u5f97\u5230\u5e76\u56fa\u5b9a \u6bcf32 \u4e2adecoding step\u66f4\u65b0\u662f\u66f4\u65b0recent tokens\uff0cmask\u4f4d\u7f6e\u5176\u5b9e\u4ecd\u7136\u56fa\u5b9a mask\u5f97\u5230\u65b9\u5f0f\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5f97\u5230\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u56fa\u5b9a","title":"Abstract"},{"location":"notes/2025/LinearPatch/note/","text":"A Simple Linear Patch Revives Layer-Pruned Large Language Models Abstract Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#a-simple-linear-patch-revives-layer-pruned-large-language-models","text":"","title":"A Simple Linear Patch Revives Layer-Pruned Large Language Models"},{"location":"notes/2025/LinearPatch/note/#abstract","text":"Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.","title":"Abstract"},{"location":"notes/2025/MIRAGE/note/","text":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar Abstract KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#mirage-kv-cache-optimization-through-parameter-remapping-for-multi-tenant-llm-serving","text":"Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar","title":"MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving"},{"location":"notes/2025/MIRAGE/note/#abstract","text":"KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.","title":"Abstract"},{"location":"notes/2025/MMInference/note/","text":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu Abstract The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#mminference-accelerating-pre-filling-for-long-context-vlms-via-modality-aware-permutation-sparse-attention","text":"Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention"},{"location":"notes/2025/MMInference/note/#abstract","text":"The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.","title":"Abstract"},{"location":"notes/2025/MegaScale-MoE/note/","text":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu Abstract We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#megascale-moe-large-scale-communication-efficient-training-of-mixture-of-experts-models-in-production","text":"Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu","title":"MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production"},{"location":"notes/2025/MegaScale-MoE/note/#abstract","text":"We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.","title":"Abstract"},{"location":"notes/2025/MiniCPM4/note/","text":"MiniCPM4: Ultra-Efficient LLMs on End Devices MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun Abstract This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#minicpm4-ultra-efficient-llms-on-end-devices","text":"MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices"},{"location":"notes/2025/MiniCPM4/note/#abstract","text":"This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.","title":"Abstract"},{"location":"notes/2025/MoBA/note/","text":"MoBA: Mixture of Block Attention for Long-Context LLMs Abstract Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#moba-mixture-of-block-attention-for-long-context-llms","text":"","title":"MoBA: Mixture of Block Attention for Long-Context LLMs"},{"location":"notes/2025/MoBA/note/#abstract","text":"Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.","title":"Abstract"},{"location":"notes/2025/MoE-MLA-RoPE/note/","text":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat Abstract We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#unifying-mixture-of-experts-and-multi-head-latent-attention-for-efficient-language-models","text":"Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"notes/2025/MoE-MLA-RoPE/note/#abstract","text":"We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Abstract"},{"location":"notes/2025/MoR/note/","text":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun Abstract Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation","text":"Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"location":"notes/2025/MoR/note/#abstract","text":"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost. \u6bcf\u4e00\u5c42\u5faa\u73af\u8ba1\u7b97\u51e0\u6b21\uff0c\u518d\u8ba1\u7b97\u4e0b\u4e00\u5c42\uff0c\u672c\u8d28\u4e0a\u662fparameter sharing\u6216\u8005adaptive computation\uff0c\u4ee3\u8868\u662fRecursive Transformer\uff0cMoR\u5728Recursive Transformer\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u6bcf\u4e2aToken\u91c7\u7528\u4e0d\u540c\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ece\u800c\u4f7f\u5f97Attention\u8ba1\u7b97\u53d8\u6210\u7a00\u758f\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/MoSA/note/","text":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing Abstract Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#mixture-of-sparse-attention-content-based-learnable-sparse-attention-via-expert-choice-routing","text":"","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"},{"location":"notes/2025/MoSA/note/#abstract","text":"Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","title":"Abstract"},{"location":"notes/2025/Mosaic/note/","text":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs Abstract Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#mosaic-composite-projection-pruning-for-resource-efficient-llms","text":"","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs"},{"location":"notes/2025/Mosaic/note/#abstract","text":"Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Uniform Pruning -> Non-uniform Pruning \u4e0d\u4ec5\u6bcf\u4e00\u5c42\u7a00\u758f\u5ea6\u4e0d\u540c\uff0c\u6bcf\u5c42\u4e2d\u7684\u6bcf\u4e2aLinear Projection\u4e5f\u4e0d\u540c\u3002","title":"Abstract"},{"location":"notes/2025/NSA/note/","text":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention Abstract Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention","text":"","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"},{"location":"notes/2025/NSA/note/#abstract","text":"Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. \u548cMinference SeerAttention\u975e\u5e38\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u6216\u8005\u542f\u53d1\u5f0f\u7b97\u6cd5\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981\uff0c\u4ece\u800c\u5f97\u5230sparase attention\u7684\u8fd0\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/POD-Attention/note/","text":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference Abstract Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to $59\\%$ (mean $28\\%$), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#pod-attention-unlocking-full-prefill-decode-overlap-for-faster-llm-inference","text":"","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference"},{"location":"notes/2025/POD-Attention/note/#abstract","text":"Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to $59\\%$ (mean $28\\%$), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.","title":"Abstract"},{"location":"notes/2025/PSA/note/","text":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving Abstract Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV cache selection, which results in a trade-off between accuracy and efficiency. A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$ boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse $\\underline{A}$ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget $k$. This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and increases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$, compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#progressive-sparse-attention-algorithm-and-system-co-design-for-efficient-attention-in-llm-serving","text":"","title":"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving"},{"location":"notes/2025/PSA/note/#abstract","text":"Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV cache selection, which results in a trade-off between accuracy and efficiency. A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$ boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse $\\underline{A}$ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget $k$. This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and increases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$, compared to state-of-the-art DSAes and systems without sparse attention, respectively. \u4e0d\u4f7f\u7528TOPk\u6765\u9009\u62e9KV block\uff0c\u9996\u5148\u5bf9block\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u7136\u540e\u4f9d\u6b21\u8ba1\u7b97\uff0c\u5224\u65ad\u8ba1\u7b97\u7ed3\u679c\u662f\u5426\u6ee1\u8db3\u8981\u6c42\uff0c\u5426\u5219\u5c31\u591a\u7b97\u4e00\u4e2aBlock\uff1b","title":"Abstract"},{"location":"notes/2025/PanguUltra/note/","text":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu Abstract We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#pangu-ultra-pushing-the-limits-of-dense-large-language-models-on-ascend-npus","text":"Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs"},{"location":"notes/2025/PanguUltra/note/#abstract","text":"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.","title":"Abstract"},{"location":"notes/2025/PoD/note/","text":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity Abstract The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\\%$ KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#compressing-kv-cache-for-long-context-llm-inference-with-inter-layer-attention-similarity","text":"","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"},{"location":"notes/2025/PoD/note/#abstract","text":"The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\\%$ KV cache without compromising the performance. Step1: \u67e5\u627e\u76f8\u90bb\u5c42\u76f8\u8fd1\u7684attention score\uff0c\u628a\u591a\u4e2alayer\u7ec4\u6210\u4e00\u4e2ablock\uff1b Step2: block\u5185\u90e8\u5171\u4eabattention socre\uff0c\u56e0\u6b64\u5c31\u51cf\u5c11\u4e86key \u53c2\u4e0e\u7684\u8fd0\u7b97\uff0cblock\u5185\u53ea\u4fdd\u7559\u4e00\u4efdkey\uff0cvalue cache\u4ecd\u7136\u5168\u90e8\u90fd\u8981\u4fdd\u7559\uff1b Step3: \u8bad\u7ec35B tokens\uff0c\u7ed3\u679c\u63d0\u5347","title":"Abstract"},{"location":"notes/2025/PowerAttention/note/","text":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention Abstract Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in $d$-layer LLMs, allowing each output token to attend to $2^d$ tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by $5\\sim 40\\%$, especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ($3.0\\times$ faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#powerattention-exponentially-scaling-of-receptive-fields-for-effective-sparse-attention","text":"","title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention"},{"location":"notes/2025/PowerAttention/note/#abstract","text":"Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in $d$-layer LLMs, allowing each output token to attend to $2^d$ tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by $5\\sim 40\\%$, especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ($3.0\\times$ faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.","title":"Abstract"},{"location":"notes/2025/QuickSilver/note/","text":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh Abstract Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#quicksilver-speeding-up-llm-inference-through-dynamic-token-halting-kv-skipping-contextual-token-fusion-and-adaptive-matryoshka-quantization","text":"Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh","title":"QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"},{"location":"notes/2025/QuickSilver/note/#abstract","text":"Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms: (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length. Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).","title":"Abstract"},{"location":"notes/2025/Qwen3/note/","text":"Qwen3 Technical Report An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu Abstract In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#qwen3-technical-report","text":"An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu","title":"Qwen3 Technical Report"},{"location":"notes/2025/Qwen3/note/#abstract","text":"In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","title":"Abstract"},{"location":"notes/2025/R-KV/note/","text":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu Abstract Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 $\\alpha$ \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#r-kv-redundancy-aware-kv-cache-compression-for-training-free-reasoning-models-acceleration","text":"Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu","title":"R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration"},{"location":"notes/2025/R-KV/note/#abstract","text":"Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets. Snap-KV\u7b49\u65b9\u6cd5\u53ea\u8003\u8651\u957fprompt\u60c5\u51b5\u4e0b\u7684kv \u538b\u7f29\uff0c\u6ca1\u6709\u8003\u8651\u957fgeneration\u4e0b\u7684kv \u538b\u7f29\uff0c\u73b0\u5728Reasoning\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u4e00\u822c\u4f1a\u751f\u6210\u5f88\u957f\u7684\u63a8\u7406\u94fe\u3002 Decoding-time Compression \u4e0d\u6b62\u5173\u6ce8prefill\u65f6\u7684kv \u538b\u7f29\uff0c\u5728decoding\u65f6\uff0c\u6bcf\u6b21decode\u7279\u5b9a\u957f\u5ea6\u7684token\u540e\uff0c\u5bf9kv \u8fdb\u884c\u538b\u7f29 Importance Scoring via Attention Weights \u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u4f3c\uff0c\u6839\u636e\u6700\u8fd1\u7684 $\\alpha$ \u4e2atoken\u5f97\u5230\u7684Attention Weight\u6765\u5224\u65ad\u54ea\u4e9bkv cache\u66f4\u91cd\u8981 Redundancy Estimation via Semantic Similarity \u5bf9K cache\u53bb\u5197\u4f59\uff0c\u5148\u5bf9K \u53d6\u5747\u503c\uff0c\u7136\u540e\u6c42\u6240\u6709K \u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u6570\u503c\u8f83\u5927\u8868\u793a\u8d8a\u5197\u4f59\u3002 Joint Selection Strategy for KV Cache Retention \u4ee5\u4e0a\u4e24\u4e2a\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u7efc\u5408\uff0c\u5bf9KV \u8fdb\u884c\u538b\u7f29","title":"Abstract"},{"location":"notes/2025/R-Sparse/note/","text":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference Abstract Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#r-sparse-rank-aware-activation-sparsity-for-efficient-llm-inference","text":"","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"},{"location":"notes/2025/R-Sparse/note/#abstract","text":"Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","title":"Abstract"},{"location":"notes/2025/RaaS/note/","text":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity Abstract Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#efficient-long-decoding-inference-with-reasoning-aware-attention-sparsity","text":"","title":"Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity"},{"location":"notes/2025/RaaS/note/#abstract","text":"Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.","title":"Abstract"},{"location":"notes/2025/RadialAttention/note/","text":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han Abstract Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference.","title":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation","text":"Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han","title":"Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation"},{"location":"notes/2025/RadialAttention/note/#abstract","text":"Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference.","title":"Abstract"},{"location":"notes/2025/ReAttention/note/","text":"ReAttention: Training-Free Infinite Context with Finite Attention Scope Abstract The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#reattention-training-free-infinite-context-with-finite-attention-scope","text":"","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope"},{"location":"notes/2025/ReAttention/note/#abstract","text":"The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.","title":"Abstract"},{"location":"notes/2025/ReSA/note/","text":"Rectified Sparse Attention Abstract Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#rectified-sparse-attention","text":"","title":"Rectified Sparse Attention"},{"location":"notes/2025/ReSA/note/#abstract","text":"Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. \u4f7f\u7528sparse decoding t\u6b21\u540e\uff0c\u4f7f\u7528\u4e00\u6b21dense prefill\u4fee\u6b63\u4e4b\u524d\u7684\u751f\u6210\u7684t\u4e2atoken\u7684kv cache\uff0c\u548cspeculative decoding\u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fdb\u884creject/accept\u5224\u65ad\u3002","title":"Abstract"},{"location":"notes/2025/RecursiveTransformers/note/","text":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster Abstract Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#relaxed-recursive-transformers-effective-parameter-sharing-with-layer-wise-lora","text":"Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"},{"location":"notes/2025/RecursiveTransformers/note/#abstract","text":"Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","title":"Abstract"},{"location":"notes/2025/SALE/note/","text":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling Abstract Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#sale-low-bit-estimation-for-efficient-sparse-attention-in-long-context-llm-prefilling","text":"","title":"SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling"},{"location":"notes/2025/SALE/note/#abstract","text":"Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. \u4f7f\u75284-bit\u8ba1\u7b97\u8fd1\u4f3cscore\uff0c\u800c\u4e0d\u662f\u7528pooling\u65b9\u5f0f\u8ba1\u7b97block sparsity\u3002","title":"Abstract"},{"location":"notes/2025/SDS/note/","text":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism Abstract Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#enhancing-one-shot-pruned-pre-trained-language-models-through-sparse-dense-sparse-mechanism","text":"","title":"Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism"},{"location":"notes/2025/SDS/note/#abstract","text":"Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.","title":"Abstract"},{"location":"notes/2025/SEAP/note/","text":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models Abstract Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#seap-training-free-sparse-expert-activation-pruning-unlock-the-brainpower-of-large-language-models","text":"","title":"SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models"},{"location":"notes/2025/SEAP/note/#abstract","text":"Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs. \u6bcf\u4e2atask\u7a00\u758f\u8868\u73b0\u4e0d\u540c\uff0c\u56e0\u6b64\u6bcf\u4e2atask\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684mask\uff0cSEAP-gen \u8868\u793a\u7efc\u5408\u6240\u6709task\u7684\u901a\u7528mask\u3002 \u9488\u5bf9\u6bcf\u4e2atask\u7684\u6821\u51c6\u96c6\u662f\u76f4\u63a5\u5728task\u91cc\u9762\u9009\u53d6\u7684\u5417\uff1f","title":"Abstract"},{"location":"notes/2025/SeerAttention-R/note/","text":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning Abstract We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#seerattention-r-sparse-attention-adaptation-for-long-reasoning","text":"","title":"SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"},{"location":"notes/2025/SeerAttention-R/note/#abstract","text":"We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.","title":"Abstract"},{"location":"notes/2025/Seesaw/note/","text":"Seesaw: High-throughput LLM Inference via Model Re-sharding Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko Abstract To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#seesaw-high-throughput-llm-inference-via-model-re-sharding","text":"Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko","title":"Seesaw: High-throughput LLM Inference via Model Re-sharding"},{"location":"notes/2025/Seesaw/note/#abstract","text":"To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/","text":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference Abstract With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV. \u4e24\u4e2a Observation Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks ShadowKV Pre-filling \u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002 Decoding \u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#shadowkv-kv-cache-in-shadows-for-high-throughput-long-context-llm-inference","text":"","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"},{"location":"notes/2025/ShadowKV/note/#abstract","text":"With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.","title":"Abstract"},{"location":"notes/2025/ShadowKV/note/#observation","text":"Pre-RoPE keys \u53ef\u4ee5\u901a\u8fc7SVD\u5206\u89e3\u65b9\u5f0f\uff0c\u538b\u7f296\u500d\uff0c\u4e14\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931 Post-RePE keys \u90bb\u8fd1 tokens\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53bb\u9664\u4e00\u4e9boutlier\u540e\uff0c\u5e76\u6309\u7167chunk\u5bf9\u5176\u8fdb\u884cmean reduce\uff0c\u89c2\u5bdf\u5230\u975e\u5e38\u597d\u7684 cosine \u76f8\u4f3c\u5ea6\uff0c\u56e0\u6b64\u628amean reduce\u540e\u7684key \u5f53\u4f5c landmarks","title":"\u4e24\u4e2a Observation"},{"location":"notes/2025/ShadowKV/note/#shadowkv","text":"","title":"ShadowKV"},{"location":"notes/2025/ShadowKV/note/#pre-filling","text":"\u5bf9Pre-RoPE\u7684K\u8fdb\u884cSVD\u5206\u89e3\uff0c \u5bf9Post-RoPE\u7684K \u6309\u7167chunk\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4fdd\u7559mean \u5982\u679c\u7ec4\u5185\u7684 cosine \u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff0c\u90a3\u4e48\u8fd9\u7ec4\u4f5c\u4e3aoutlier\u6765\u5355\u72ec\u4fdd\u5b58\u3002","title":"Pre-filling"},{"location":"notes/2025/ShadowKV/note/#decoding","text":"\u6839\u636eK\u7684landmark\u4e0eQ\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230softmax\u7684score\uff0c \u9009\u62e9\u6700\u9ad8\u7684\u51e0\u4e2ascore\u5bf9\u5e94\u7684index\uff0c\u6839\u636eindex\u9009\u62e9\u7a00\u758fV \u6839\u636eindex\u548cprefill\u4fdd\u5b58\u7684SVD\u5c0f\u77e9\u9635\uff0c\u5408\u5e76\u4e3a\u7a00\u758fK \u662f\u5426\u9700\u8981\u6839\u636esparse K sparse V\u8ba1\u7b97attention? \u5e94\u8be5\u662f\u9700\u8981\u7684\uff0c\u7b97\u6cd5\u5199\u7684\u6709\u4e9b\u4e0d\u5b8c\u6574 \u6240\u6709\u7684V\u90fd\u4fdd\u5b58\u4e0b\u6765\u4e86\uff0cK\u7684SVD\u5206\u89e3\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4e0d\u505a\uff0c\u4e5f\u53ef\u4ee5\u50cfV\u4e00\u6837\u5b58\u5728CPU Memoy\u4e2d\uff0c\u6839\u636elandmark\u8fdb\u884c\u8ba1\u7b97\uff1b\u9664\u4e86SVD\u5206\u89e3\u7684\u601d\u8def\uff0clandmark\u7684\u601d\u60f3\u5176\u5b9e\u548cMInference\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u95f4\u9694\u7684\u8ba1\u7b97\u6765\u8bc4\u4f30\u54ea\u90e8\u5206\u7684kv\u91cd\u8981\uff0c\u4ece\u800c\u5c06dense attention\u8f6c\u5316\u4e3a sparse attention\uff0c\u8fdb\u800c\u52a0\u901f\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u3002 K\u7684\u4e34\u8fd1\u76f8\u4f3c\u76f8\u548c\u53ef\u4ee5\u5206\u89e3\u7684\u7279\u6027\u3002","title":"Decoding"},{"location":"notes/2025/SharePrefill/note/","text":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing Abstract Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#accelerating-prefilling-for-long-context-llms-via-sparse-pattern-sharing","text":"","title":"Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"},{"location":"notes/2025/SharePrefill/note/#abstract","text":"Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy. \u6839\u636e\u7edf\u8ba1\uff0c\u5c06head \u805a\u7c7b\uff0c\u540c\u4e00\u4e2acluster\u5185\u90e8\u5171\u4eabblock sparse mask\uff0c\u8ba1\u7b97\u65f6\uff0ccluster\u4e2d\u7b2c\u4e00\u4e2ahead\u6309\u7167dense\u8ba1\u7b97\uff0c\u5f97\u5230\u8f93\u5165\u540e\u8ba1\u7b97sparse mask\uff0c\u6b64\u540e\u8fd9\u4e2acluster\u4e2d\u7684\u5176\u4ed6head\uff0c\u4fbf\u53ef\u4ee5\u6839\u636emask\u8fdb\u884csparse \u8ba1\u7b97\u3002","title":"Abstract"},{"location":"notes/2025/SlimLLM/note/","text":"SlimLLM: Accurate Structured Pruning for Large Language Models Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang Abstract Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#slimllm-accurate-structured-pruning-for-large-language-models","text":"Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang","title":"SlimLLM: Accurate Structured Pruning for Large Language Models"},{"location":"notes/2025/SlimLLM/note/#abstract","text":"Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","title":"Abstract"},{"location":"notes/2025/SpargeAttn/note/","text":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Abstract An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#spargeattn-accurate-sparse-attention-accelerating-any-model-inference","text":"","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"},{"location":"notes/2025/SpargeAttn/note/#abstract","text":"An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","title":"Abstract"},{"location":"notes/2025/SparsingLaw/note/","text":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity Abstract Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#sparsing-law-towards-large-language-models-with-greater-activation-sparsity","text":"","title":"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity"},{"location":"notes/2025/SparsingLaw/note/#abstract","text":"Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. \u63d0\u51faPPL-p%\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u5b9e\u5c31\u662f\u5e15\u7d2f\u6258\u66f2\u7ebf\uff0c\u8fd9\u91ccp%\u8868\u793asparsity ratio\uff0c\u8d8a\u9ad8\u4e00\u534a\u6a21\u578b\u7684\u7cbe\u5ea6\u4e5f\u4f1a\u8d8a\u5dee\uff0c\u6bd4\u5982ppl\u8d8a\u9ad8\u3002\u56fe\u4e2d\u7ed9\u7684\u662factivation ratio\uff0c\u6b63\u597d\u4e0ep%\u76f8\u52a0=100%\u3002 PPL-1%\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9sparse\u7684\u5fcd\u53d7\u7a0b\u5ea6 ReLU\uff1a\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a00\u758f SiLU: \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u52a0\u7a20\u5bc6 \uff08\u4f7f\u7528PPL-1%\u6765\u8bc4\u4f30\uff09\uff0c\u8fd9\u5e76\u4e0d\u80fd\u8bf4dense silu model\u6bd4relu model\u5dee\uff0c\u6309\u7167\u7ecf\u9a8c\u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0csilu model\u6bd4relu model\u597d\uff0c\u4f46\u662f\u53bb\u63891%\u7684\u6fc0\u6d3b\u540e\uff0csilu model\u7684\u7cbe\u5ea6\u8868\u73b0\u4f1a\u964d\u4f4e\u5f88\u591a\uff0c\u800c\u4e14\u968f\u7740\u8bad\u7ec3\u7684\u6570\u636e\u8d8a\u591a\uff0c\u7cbe\u5ea6\u964d\u4f4e\u7684\u5e45\u5ea6\u5c31\u8d8a\u5927\uff1b\u8fd9\u4e2a\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ed3\u8bba\u3002 \u6240\u4ee5\u63a8\u8350\u4f7f\u7528ReLU\u6765\u4f5c\u4e3aLLM\u7684\u6fc0\u6d3b\u51fd\u6570 width-depth ratio \u8868\u793a hidden dimension \u4e0e layer number\u7684\u6bd4\u4f8b\uff0c\u8d8a\u9ad8\u8bc1\u660e\u8fd9\u4e2aLLM\u8d8a\u80d6\uff0c\u8fd9\u4e2a\u53c2\u6570\u4e5f\u4e0eactivation sparsity\u8868\u73b0\u6709\u5bc6\u5207\u5173\u7cfb \u4ee50.1B\u6a21\u578b\u8bad\u7ec3\u4e3e\u4f8b Fig.5\u8868\u793aWidth-Depth Ratio\u5728[0, 114]\u4e4b\u95f4\uff0c\u6a21\u578b\u4f1a\u9010\u6e10\u53d8\u80d6\uff0c\u4e14\u6fc0\u6d3b\u6bd4\u4f8b\u8d8a\u6765\u8d8a\u9ad8\uff0c\u6240\u4ee5\u5e0c\u671b\u6a21\u578b\u8d8a\u7626\u8d8a\u597d\uff0c\u6709\u5229\u4e8eactivation sparsity Fig.6\u8868\u793aWidth-Depth Ratio\u5728[74, 182]\u4e4b\u95f4\uff0c\u4e0d\u80d6\u4e0d\u7626\u65f6\u8bad\u7ec3loss\u6700\u597d \u7efc\u4e0a\uff0c\u53ef\u4ee5\u9009\u53d6 Width-Depth Ratio=74\uff0c\u53ef\u4ee5\u6ee1\u8db3training loss \u548c activation sparsity \u9700\u6c42 \u786e\u5b9a\u6a21\u578b\u7684 Width-Depth \uff0c\u4e14training data \u8db3\u591f\u591a\u65f6\uff0cmodel\u7684\u53c2\u6570\u91cf\u5bf9 activation sparsity \u5f71\u54cd\u5f88\u5c0f \u4f46\u662f\u5c0f\u6a21\u578b\u66f4\u52a0\u5bb9\u6613\u6536\u655b \uff08\u4ece activation sparsity\u53d8\u5316\u7684\u89d2\u5ea6\uff09 \u53e6\u5916\uff0c\u4e0d\u540cscale \u7684LLM\uff0cacitvation \u6fc0\u6d3b\u7684\u9891\u7387\u90fd\u662f\u76f8\u4f3c\u7684\uff0c\u4e14\u5bf9\u76f8\u540c\u7684token\u8f93\u5165\uff0c\u4e0d\u540cscale LLM\u6fc0\u6d3b\u7684\u6bd4\u4f8b\u4e5f\u662f\u76f8\u4f3c\u7684\u3002 \u6839\u636e\u4ee5\u4e0a\u89c2\u5bdf\uff1a - LLM Architectural design \u5c3d\u91cf\u4f7f\u7528ReLU\uff0c\u4e14\u6ee1\u8db3loss \u524d\u63d0\u4e0b\uff0c\u5c3d\u91cf\u7626\u4e00\u70b9 - Training-time predictable sparsity\uff0c\u53ef\u4ee5\u6839\u636e\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u9884\u6d4b\u5927\u6a21\u578b\u7684\u7ed3\u679c - Lens for the convergence of neuron specialization\uff0cFig4\u8868\u793atraining loss\u6536\u655b\u540e\uff0cactivation sparsity \u4ecd\u7136\u5728\u9010\u6b65\u7684\u8fdb\u5316\uff0c\u6240\u4ee5\u53ef\u4ee5\u8ba9\u6a21\u578b\u591a\u8bad\u7ec3\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u7684\u795e\u7ecf\u5143\u8fdb\u5316\u4e3a\u4e13\u7528\u795e\u7ecf\u5143\u3002","title":"Abstract"},{"location":"notes/2025/SpecEE/note/","text":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting Abstract Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#specee-accelerating-large-language-model-inference-with-speculative-early-exiting","text":"","title":"SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting"},{"location":"notes/2025/SpecEE/note/#abstract","text":"Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively. \u4f7f\u7528Eagle\u7684draft model\u751f\u6210topk\u7684token \u5927\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u7ecf\u8fc7 topk \u7684\u5c0flm_head\uff0c \u7279\u5f811: \u751f\u6210Speculative Token Logits \u7279\u5f812: \u518d\u751f\u6210Local Probabilities \u7279\u5f813: \u4e0e\u4e0a\u4e00\u5c42\u7684Local Probabilities\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230Probability variation \u6839\u636e\u4e09\u4e2a\u7279\u5f81\uff0c\u7ecf\u8fc7\u9884\u6d4b\u671f\u9884\u6d4b\u662f\u5426\u8981Early Exit \u662f: \u786e\u5b9aEarly Exit\uff0c\u8ba1\u7b97\u5927lm_head\uff0c\u5224\u65ad\u662f\u5426\u4e0e\u5c0flm_head\u7684top1\u4e00\u81f4 \u4e00\u81f4\uff0c\u63a8\u7406\u63d0\u524d\u9000\u51fa \u4e0d\u4e00\u81f4\uff0c\u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42 \u5426: \u7ee7\u7eed\u7b97\u4e0b\u4e00\u5c42","title":"Abstract"},{"location":"notes/2025/SpindleKV/note/","text":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang Abstract Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#spindlekv-a-novel-kv-cache-reduction-method-balancing-both-shallow-and-deep-layers","text":"Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang","title":"SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers"},{"location":"notes/2025/SpindleKV/note/#abstract","text":"Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.","title":"Abstract"},{"location":"notes/2025/StarAttention/note/","text":"Star Attention: Efficient LLM Inference over Long Sequences Abstract Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#star-attention-efficient-llm-inference-over-long-sequences","text":"","title":"Star Attention: Efficient LLM Inference over Long Sequences"},{"location":"notes/2025/StarAttention/note/#abstract","text":"Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy. RingAttention\u7684\u6539\u8fdb\uff0c\u5c06\u957f\u6587\u6863\u5206\u5e03\u5728\u591a\u4e2anode\u4e0a\uff0c\u4f46\u662f\u4e0d\u8fdb\u884c\u901a\u4fe1\uff0c\u76f4\u63a5\u8ba1\u7b97kv cache\uff0c\u5728decode\u65f6\uff0cquery\u9700\u8981global attention\uff0c\u6b64\u65f6\u901a\u4fe1\u91cf\u8f83\u5c11\u3002 \u7cbe\u5ea6\u4f1a\u6709\u4e0b\u964d\uff0c\u901f\u5ea6\u660e\u663e\u63d0\u5347\u3002 \u548cKVLink\u7684\u601d\u60f3\u6709\u4e9b\u7c7b\u4f3c\uff0cKVLink\u7528\u4e8eRAG\u9886\u57df\uff0cStarAttention\u7528\u4e8e\u5206\u5e03\u5f0f\u63a8\u7406\u9886\u57df\u3002","title":"Abstract"},{"location":"notes/2025/Step-3/note/","text":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang Abstract Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding","text":"StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang","title":"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding"},{"location":"notes/2025/Step-3/note/#abstract","text":"Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.","title":"Abstract"},{"location":"notes/2025/Super-Experts-Profilling/note/","text":"Unveiling Super Experts in Mixture-of-Experts Large Language Models Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan Abstract Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#unveiling-super-experts-in-mixture-of-experts-large-language-models","text":"Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"notes/2025/Super-Experts-Profilling/note/#abstract","text":"Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Abstract"},{"location":"notes/2025/TEAL/note/","text":"Training-Free Activation Sparsity in Large Language Models Abstract Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#training-free-activation-sparsity-in-large-language-models","text":"","title":"Training-Free Activation Sparsity in Large Language Models"},{"location":"notes/2025/TEAL/note/#abstract","text":"Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.","title":"Abstract"},{"location":"notes/2025/Task-KV/note/","text":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads Abstract KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#task-kv-task-aware-kv-cache-optimization-via-semantic-differentiation-of-attention-heads","text":"","title":"Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads"},{"location":"notes/2025/Task-KV/note/#abstract","text":"KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. \u5bf9head\u8fdb\u884c\u5206\u7c7b\uff0cheterogeneous heads\u4e0d\u7a00\u758f\uff0cNon-heterogeneous Heads\u7a00\u758f \u770b\u5b9e\u9a8c\u7ed3\u679c\u63d0\u5347\u4e0d\u662f\u5f88\u5927","title":"Abstract"},{"location":"notes/2025/TileLink/note/","text":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu Abstract Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives","text":"Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives"},{"location":"notes/2025/TileLink/note/#abstract","text":"Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone. In this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.","title":"Abstract"},{"location":"notes/2025/TokenWeave/note/","text":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference Raja Gond, Nipun Kwatra, Ramachandran Ramjee Abstract Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#tokenweave-efficient-compute-communication-overlap-for-distributed-llm-inference","text":"Raja Gond, Nipun Kwatra, Ramachandran Ramjee","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference"},{"location":"notes/2025/TokenWeave/note/#abstract","text":"Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLINK. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Further, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead. We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The computation of one subset is then overlapped with the communication of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory bound RMSNorm to be overlapped with the other batch's computation, providing additional gains. Our evaluations demonstrate up to 29% latency gains and up to 26% throughput gains across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed. TokenWeave \u4e3b\u8981\u7528\u5230\u4e86\u4e09\u4e2a\u6280\u672f\u70b9\uff1a 1. Token-Splitting\uff0c\u5c06\u4e00\u4e2a\u5927batch\u7684\u4efb\u52a1\u62c6\u5206\u6210\u4e24\u4e2abatch\uff0c\u901a\u8fc7\u8fd9\u79cd\u53ef\u4ee5\u5c06\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8fdb\u884coverlap\uff1b\u4f46\u7531\u4e8eGPU\u7684wave-quantization\u539f\u56e0\uff0cNaive\u7684\u62c6\u5206\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e24\u4e2a\u5c0f\u7684kernel\u7684\u5f00\u9500\u5927\u4e8e\u539f\u672c\u7684\u4e00\u4e2a\u5927kernel\uff0c\u9020\u6210\u989d\u5916\u7684overhead\uff1b\u8fd9\u91cc\u7684wave\u6307\u7684\u662fGPU\u5e76\u884c\u8fd0\u7b97\u7684\u6ce2\u675f\uff0c\u4e00\u4e2a\u5927\u7684kernel\u901a\u5e38\u80fd\u8dd1\u6ee1GPU\u7684\u5e76\u884c\u8fd0\u7b97\u8d44\u6e90\uff0c\u62c6\u5206\u4e3a\u4e24\u4e2a\u5c0fkernel\u540e\uff0c\u6709\u53ef\u80fd\u9020\u6210\u5c0fkernel\u7684\u6700\u540e\u4e00\u4e2awave\u5e76\u884c\u6548\u7387\u4f4e\uff1bTokenWeave\u63d0\u51fa\u4f7f\u7528smart\u7684\u65b9\u6cd5\u6765\u62c6\u5206\uff0c\u4f7f\u5f97\u62c6\u5206\u540e\u7684\u4e24\u4e2akernel\u6bd4\u4e00\u4e2akernel\u989d\u5916\u5f15\u5165\u7684\u5f00\u9500\u6700\u5c0f\u3002 2. RMSNorm\u4f18\u5316\uff0cTileLink, Flux\u548cNanoflow\u7b49\u5de5\u4f5c\u6ca1\u6709\u8003\u8651\u5bf9Norm\u51fd\u6570\u7684\u4f18\u5316\uff0c\u5b9e\u6d4b\u53d1\u73b0RMSNorm\u65f6\u95f4\u5360\u6bd4\u57288%\u5de6\u53f3\uff1bTokenWeave\u63d0\u51fa\u5c06\u539f\u672c\u7684AllReduce-RMSNorm\u8fd0\u7b97\u6539\u4e3aReduceScatter-RMSNorm-AllGather\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u5b66\u8ba1\u7b97\u7684\u7b49\u4ef7\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11RMSNorm\u7684\u8ba1\u7b97\u5f00\u9500\uff1b\u4f46\u7b80\u5355\u7684\u62c6\u5206\u540e\u7684\u6548\u7387\u4f1a\u53d8\u5dee\uff0cTokenWeave\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684kernel\u6765\u4f18\u5316\u8fd9\u4e2a\u95ee\u9898\u3002 3. \u65b0\u7279\u6027\u7684\u5f15\u5165\u51cf\u5c11SM\u7684\u8d44\u6e90\u5360\u7528\uff0c\u901a\u8fc7\u4f7f\u7528Multimem instruction\u5c06reduce offload\u5230NVSwitch\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86SM\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5c06\u7b2c\u4e8c\u70b9RMSNorm\u7684kernel\u5b9e\u73b0\u7531\u9700\u898116-20 SMs\u51cf\u5c11\u52302-8 SMs TokenWeave\u5728vLLM v1\u7248\u672c\u4e0a\u5b9e\u73b0\uff0c\u5e26\u676529% latency\u6536\u76ca\uff0c\u6700\u591a26%\u7684throughput\u6536\u76ca\u3002","title":"Abstract"},{"location":"notes/2025/TorchAO/note/","text":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 Abstract We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#torchao-pytorch-native-training-to-serving-model-optimization","text":"Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107","title":"TorchAO: PyTorch-Native Training-to-Serving Model Optimization"},{"location":"notes/2025/TorchAO/note/#abstract","text":"We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"Abstract"},{"location":"notes/2025/Triton-distributed/note/","text":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu Abstract In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#triton-distributed-programming-overlapping-kernels-on-distributed-ai-systems-with-the-triton-compiler","text":"Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler"},{"location":"notes/2025/Triton-distributed/note/#abstract","text":"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","title":"Abstract"},{"location":"notes/2025/UC0D8DJ6/note/","text":"Characterizing Communication Patterns in Distributed Large Language Model Inference Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda Abstract Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#characterizing-communication-patterns-in-distributed-large-language-model-inference","text":"Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"notes/2025/UC0D8DJ6/note/#abstract","text":"Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.","title":"Abstract"},{"location":"notes/2025/XAttention/note/","text":"XAttention: Block Sparse Attention with Antidiagonal Scoring Abstract Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#xattention-block-sparse-attention-with-antidiagonal-scoring","text":"","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring"},{"location":"notes/2025/XAttention/note/#abstract","text":"Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention. Block Sparse Attention\u7684mask\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff0cbaseline\u662fMinference\u548cFlexPrefill \u7b97\u6cd5\u590d\u6742\u5ea6\u6bd4FlexPrefill, Minference\u9ad8\uff0c\u8bba\u6587\u4e2d\u7ed3\u679c\u663e\u793a\u52a0\u901f\u6bd4\u5f88\u597d\uff0c\u4f46\u6839\u636e\u5f00\u6e90\u4ee3\u7801\u590d\u73b0\u52a0\u901f\u4e0d\u7406\u60f3\u3002","title":"Abstract"},{"location":"notes/2025/kvpress/note/","text":"kvpress Abstract","title":"kvpress"},{"location":"notes/2025/kvpress/note/#kvpress","text":"","title":"kvpress"},{"location":"notes/2025/kvpress/note/#abstract","text":"","title":"Abstract"},{"location":"notes/2025/sparse-frontier/note/","text":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs Abstract Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#the-sparse-frontier-sparse-attention-trade-offs-in-transformer-llms","text":"","title":"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"},{"location":"notes/2025/sparse-frontier/note/#abstract","text":"Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.","title":"Abstract"},{"location":"notes/2025/topk-decoding/note/","text":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs Abstract There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#exploiting-sparsity-for-long-context-inference-million-token-contexts-on-commodity-gpus","text":"","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs"},{"location":"notes/2025/topk-decoding/note/#abstract","text":"There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard). \u5728CPU\u4e0a\u8ba1\u7b97qk\uff0c\u5f97\u5230attention score\uff0c\u9009\u53d6topk\u642c\u5230gpu\u4e0a\u8ba1\u7b97","title":"Abstract"},{"location":"notes/2025/vAttention/note/","text":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention Abstract PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#vattention-dynamic-memory-management-for-serving-llms-without-pagedattention","text":"","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"location":"notes/2025/vAttention/note/#abstract","text":"PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer. Paged Attention\u7684\u6539\u8fdb Paged Attention\u7684\u7f3a\u70b9 - Requires Re-writing the Attention Kernel - KV Cache\u4e0d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\uff0c\u9700\u8981\u91cd\u5199kernel - Adds Redundancy in the Serving Framework - \u5728runtime\u65f6\uff0c\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u5757\u7684kv cache\uff0c\u4f46\u662f\u5c5e\u4e8evirtual memory\u7684\u4e0d\u540c\u5730\u5740\u3002Virtual Memory\u5b9e\u9645\u4e0a\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u627e\u5230Physical Memory\u6709\u4e00\u4e2a\u76f8\u4f3c\u7684\u64cd\u4f5c\u3002 - Performance Overhead - Runtime overhead on the GPU - \u591a\u4e86\u4e00\u4e9b\u8fd0\u7b97\u903b\u8f91\uff0c\u5bfc\u81f4GPU\u4e0a\u6709\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500 - Runtime overhead on the CPU vAttention \u53d1\u73b0\u4e24\u4e2a\u73b0\u8c61 - KV cache memory requirement is predictable on a per-iteration basis. - \u5728decoding\u65f6\uff0c\u6bcf\u6b21\u589e\u957f\u7684memory\u662f\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u7684 - KV cache does not require high memory allocation bandwidth. - \u7edf\u8ba1\u53d1\u73b0KV Cache\u9700\u8981Memory\u6700\u9ad8\u53ea\u6709750MB/s \u5728\u7cfb\u7edf\u5c42\u9762\u8fdb\u884c\u5206\u9875\uff0c\u4fdd\u8bc1\u5728virtual memory\u4e0a\u662f\u8fde\u7eed\u7684\uff0c\u800c\u4e0d\u662fPagedAttention\u5728\u7528\u6237\u7a7a\u95f4\u4e2d\u5728Virtual Memory\u4e0a\u8fdb\u884c\u5206\u9875\u3002","title":"Abstract"},{"location":"weekly_paper/2025-07-25/","text":"2025-07-25 Table of Contents Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding ICWLM A Multi-Task Wireless Large Model via In-Context Learning NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models LoRA is All You Need for Safety Alignment of Reasoning LLMs Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Benchmarking LLM Privacy Recognition for Social Robot Decision Making TorchAO PyTorch-Native Training-to-Serving Model Optimization On the transferability of Sparse Autoencoders for interpreting compressed models Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Reservoir Computing as a Language Model Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Scaling Decentralized Learning with FLock IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Tiny language models An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Linear Relational Decoding of Morphology in Language Models KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Characterizing Communication Patterns in Distributed Large Language Model Inference DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration KROMA Ontology Matching with Knowledge Retrieval and Large Language Models LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Authors: Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang 2025-07-24 http://arxiv.org/abs/2507.18607v1 Large language models ( s) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods . However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable -based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods. Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Authors: Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci 2025-07-24 http://arxiv.org/abs/2507.18504v1 Large Language Models ( s) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as s' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates dependency graphs into s' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing -based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with s. Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks Authors: Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief 2025-07-24 http://arxiv.org/abs/2507.18328v1 In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for driving assistance.Nevertheless, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwelltime and duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model ( )-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI. StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Authors: Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu 2025-07-24 http://arxiv.org/abs/2507.18294v1 Adapting s to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in s. Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Authors: Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan 2025-07-24 http://arxiv.org/abs/2507.18224v1 Multi-agent systems (MAS) based on large language models ( s) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer. Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation Authors: Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan 2025-07-24 http://arxiv.org/abs/2507.18212v1 Layer has emerged as a promising technique for compressing large language models ( s) while achieving proportional to the ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the original model's question-answering performance, outperforming the baseline by 4.01%. SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding Authors: Linye Wei, Shuzhang Zhong, Songqiang Xu, Runsheng Wang, Ru Huang, Meng Li 2025-07-24 http://arxiv.org/abs/2507.18181v1 Large language model ( )-based automatic speech recognition (ASR) has recently attracted a lot of attention due to its high recognition accuracy and enhanced multi-dialect support. However, the high decoding latency of s challenges the real-time ASR requirements. Although speculative decoding has been explored for better decoding efficiency, they usually ignore the key characteristics of the ASR task and achieve limited speedup. To further reduce the real-time ASR latency, in this paper, we propose a novel speculative decoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed based on our core observation that ASR decoding is audio-conditioned, which results in high output alignment between small and large ASR models, even given output mismatches in intermediate decoding steps. Therefore, SpecASR features an adaptive draft sequence generation process that dynamically modifies the draft sequence length to maximize the token acceptance length. SpecASR further proposes a draft sequence recycling strategy that reuses the previously generated draft sequence to reduce the draft ASR model latency. Moreover, a two-pass token tree generation algorithm is also proposed to balance the latency of draft and target ASR models. With extensive experimental results, we demonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the baseline autoregressive decoding and speculative decoding, respectively, without any loss in recognition accuracy. ICWLM A Multi-Task Wireless Large Model via In-Context Learning Authors: Yuxuan Wen, Xiaoming Chen, Maojun Zhang, Zhaoyang Zhang 2025-07-24 http://arxiv.org/abs/2507.18167v1 The rapid evolution of wireless technologies, particularly massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave), introduces significant network complexity and computational demands. Significant research efforts have been made to improve physical layer performance by resorting to deep learning (DL) methods, which, however, are usually task-specific and struggle with data scarcity and generalization. To address these challenges, we propose a novel In-Context Wireless Large Model (ICWLM), a wireless-native foundation model designed for simultaneous multi-task learning at the physical layer. Unlike conventional methods that adapt wireless data to pre-trained large language models ( s), ICWLM is trained directly on large-scale, mixed wireless datasets from scratch. It jointly solves multiple classical physical layer problems, including multi-user precoding (sum-rate maximization and max-min SINR) and channel prediction. A key innovation of ICWLM is its utilization of in-context learning (ICL), enabling the model to adapt to varying system configurations and channel conditions with minimal demonstration pairs, eliminating the need for extensive retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm to dynamically balance the individual task losses during multi-task training, ensuring efficient and stable learning across diverse objectives. Extensive simulation results demonstrate that ICWLM achieves competitive performance compared to task-specific methods while exhibiting remarkable generalization capabilities to unseen system configurations. This work offers a promising paradigm for developing unified and adaptive AI models for future wireless networks, potentially reducing deployment complexity and enhancing intelligent resource management. NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Authors: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu 2025-07-24 http://arxiv.org/abs/2507.18028v1 Efficiently editing knowledge stored in large language models ( s) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of s and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value ( ) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of s. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work). Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries Authors: Victor Hartman, Petter T\u00f6rnberg 2025-07-23 http://arxiv.org/abs/2507.17636v1 Negative campaigning is a central feature of political competition, yet empirical research has been limited by the high cost and limited scalability of existing classification methods. This study makes two key contributions. First, it introduces zero-shot Large Language Models ( s) as a novel approach for cross-lingual classification of negative campaigning. Using benchmark datasets in ten languages, we demonstrate that s achieve performance on par with native-speaking human coders and outperform conventional supervised machine learning approaches. Second, we leverage this novel method to conduct the largest cross-national study of negative campaigning to date, analyzing 18 million tweets posted by parliamentarians in 19 European countries between 2017 and 2022. The results reveal consistent cross-national patterns: governing parties are less likely to use negative messaging, while ideologically extreme and populist parties -- particularly those on the radical right -- engage in significantly higher levels of negativity. These findings advance our understanding of how party-level characteristics shape strategic in multiparty systems. More broadly, the study demonstrates the potential of s to enable scalable, transparent, and replicable research in political across linguistic and cultural contexts. R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang 2025-07-23 http://arxiv.org/abs/2507.17307v1 Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model ( ) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning. EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models Authors: Haochen Luo, Yuan Zhang, Chen Liu 2025-07-23 http://arxiv.org/abs/2507.17211v1 Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models ( s) to automate the generation and evolution of alpha factors for portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by -generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints. LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Authors: Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato 2025-07-23 http://arxiv.org/abs/2507.17188v1 This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and , we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model ( )-guided heuristic multi-agent reinforcement learning approach ( -HeMARL) for trajectory optimization. -HeMARL efficiently incorporates expert heuristics policy generated by the , enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds. Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Authors: Mariam ALMutairi, Hyungmin Kim 2025-07-23 http://arxiv.org/abs/2507.17134v1 Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model ( ) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by s, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of -driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty. Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models Authors: Andrii Balashov 2025-07-23 http://arxiv.org/abs/2507.17107v1 Reinforcement learning (RL) is a key post-pretraining step for aligning large language models ( s) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update . It arises naturally, without any constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source s). Moreover, the subnetworks updated by RL show substantial across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes through the lens of the lottery ticket hypothesis. LoRA is All You Need for Safety Alignment of Reasoning LLMs Authors: Yihao Xue, Baharan Mirzasoleiman 2025-07-22 http://arxiv.org/abs/2507.17075v1 Reasoning s have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure s do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the \"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe s -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off. Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Authors: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao 2025-07-22 http://arxiv.org/abs/2507.17061v1 Large language model ( ) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent , reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent systems. Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Authors: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass 2025-07-22 http://arxiv.org/abs/2507.16784v1 To break the context limits of large language models ( s) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of s trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask- mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use. Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges Authors: Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li 2025-07-22 http://arxiv.org/abs/2507.16731v1 As large language models ( s) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based s and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, , bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for -SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence. ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Authors: Shreya Saxena, Siva Prasad, Zishan Ahmad, Vishal Vaddina 2025-07-22 http://arxiv.org/abs/2507.16478v1 Code translation is a crucial process in software development and migration projects, enabling interoperability between different programming languages and enhancing software adaptability and thus longevity. Traditional automated translation methods rely heavily on handcrafted transformation rules, which often lack flexibility and scalability. Meanwhile, advanced language models present promising alternatives but are often limited by proprietary, API-based implementations that raise concerns over data security and reliance. In this paper, we present Auto-Train for Code Translation (ACT), an innovative framework that aims to improve code translation capabilities by enabling in-house finetuning of open-source Large Language Models ( s). ACT's automated pipeline significantly boosts the performance of these models, narrowing the gap between open-source accessibility and the high performance of closed-source solutions. Central to ACT is its synthetic data generation module, which builds extensive, high-quality datasets from initial code samples, incorporating unit tests to ensure functional accuracy and diversity. ACT's evaluation framework incorporates execution-level checks, offering a comprehensive assessment of translation quality. A key feature in ACT is its controller module, which manages the entire pipeline by dynamically adjusting hyperparameters, orchestrating iterative data generation, and finetuning based on real-time evaluations. This enables ACT to intelligently optimize when to continue training, generate additional targeted training data, or stop the process. Our results demonstrate that ACT consistently enhances the effectiveness of open-source models, offering businesses and developers a secure and reliable alternative. Additionally, applying our data generation pipeline to industry-scale migration projects has led to a notable increase in developer . Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video Authors: Alessandro Sebastiano Catinello, Giovanni Maria Farinella, Antonino Furnari 2025-07-22 http://arxiv.org/abs/2507.16342v1 This work tackles the problem of Online detection of Take and Release (OTR) of an object in untrimmed egocentric videos. This task is challenging due to severe label imbalance, with temporally positive annotations, and the need for precise temporal predictions. Furthermore, methods need to be computationally efficient in order to be deployed in real-world online settings. To address these challenges, we propose Mamba-OTR, a model based on the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence during inference while being trained on short video clips. To address label imbalance, our training pipeline incorporates the focal loss and a novel regularization scheme that aligns model predictions with the evaluation metric. Extensive experiments on EPIC-KITCHENS-100, the comparisons with -based approach, and the evaluation of different training and test schemes demonstrate the superiority of Mamba-OTR in both accuracy and efficiency. These finding are particularly evident when evaluating full-length videos or high frame-rate sequences, even when trained on short video snippets for computational convenience. The proposed Mamba-OTR achieves a noteworthy mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in streaming mode, versus the 20.32 of a vanilla and 25.16 of a vanilla Mamba, thus providing a strong baseline for OTR. We will publicly release the source code of Mamba-OTR to support future research. CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Authors: Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu 2025-07-22 http://arxiv.org/abs/2507.16872v1 Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models ( s). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood. In this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are , quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets. Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Authors: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov 2025-07-22 http://arxiv.org/abs/2507.16289v1 Modern sequential recommender systems, ranging from lightweight -based variants to large language models, have become increasingly prominent in academia and industry due to their strong performance in the next-item prediction task. Yet common evaluation protocols for sequential recommendations remain insufficiently developed: they often fail to reflect the corresponding recommendation task accurately, or are not aligned with real-world scenarios. Although the widely used leave-one-out split matches next-item prediction, it permits the between training and test periods, which leads to temporal leakage and unrealistically long test horizon, limiting real-world relevance. Global temporal splitting addresses these issues by evaluating on distinct future periods. However, its applications to sequential recommendations remain loosely defined, particularly in terms of selecting target interactions and constructing a validation subset that provides necessary consistency between validation and test metrics. In this paper, we demonstrate that evaluation outcomes can vary significantly across splitting strategies, influencing model rankings and practical deployment decisions. To improve reproducibility in both academic and industrial settings, we systematically compare different splitting strategies for sequential recommendations across multiple datasets and established baselines. Our findings show that prevalent splits, such as leave-one-out, may be insufficiently aligned with more realistic evaluation strategies. Code: https://github.com/monkey0head/time-to-split Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Authors: Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang 2025-07-22 http://arxiv.org/abs/2507.16274v1 The rapid scaling of large language models ( s) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Default GPU memory allocators of popular deep learning frameworks like PyTorch use online strategies without knowledge of tensor lifespans, which can waste up to 43\\% of memory and cause out-of-memory errors, rendering optimization techniques ineffective or even unusable. To address this, we introduce STWeaver, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STWeaver introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by 79.2\\% (up to 100\\%) across both dense and models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves performance by up to 32.5\\%. Benchmarking LLM Privacy Recognition for Social Robot Decision Making Authors: Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz 2025-07-22 http://arxiv.org/abs/2507.16124v1 Social robots are embodied agents that interact with people while following human norms. These robots interact using verbal and non-verbal cues, and share the physical environments of people. While social robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models ( s) presents new opportunities to develop -empowered social robots for enhanced human-robot interaction. To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, s often process sensitive personal information, particularly within home environments. Given the tension between utility and privacy risks, evaluating how current s manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box s are privacy-aware in the context of household social robots. In this study, we present a set of privacy-relevant scenarios crafted through the lens of Contextual Integrity (CI). We first survey users' privacy preferences regarding in-home social robot behaviors and then examine how their privacy orientation affects their choices of these behaviors (N = 450). We then provide the same set of scenarios and questions to state-of-the-art s (N = 10) and find that the agreement between humans and s is low. To further investigate the capabilities of s as a potential privacy controller, we implement four additional prompting strategies and compare their results. Finally, we discuss the implications and potential of AI privacy awareness in human-robot interaction. TorchAO PyTorch-Native Training-to-Serving Model Optimization Authors: Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 2025-07-21 http://arxiv.org/abs/2507.16099v1 We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 , and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, v , SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/. On the transferability of Sparse Autoencoders for interpreting compressed models Authors: Suchit Gupte, Vishnu Kabir Chhabra, Mohammad Mahdi Khalili 2025-07-21 http://arxiv.org/abs/2507.15977v1 Modern s face inference efficiency challenges due to their scale. To address this, many compression methods have been proposed, such as and quantization. However, the effect of compression on a model's interpretability remains elusive. While several model interpretation approaches exist, such as circuit discovery, Sparse Autoencoders (SAEs) have proven particularly effective in decomposing a model's activation space into its feature basis. In this work, we explore the differences in SAEs for the original and compressed models. We find that SAEs trained on the original model can interpret the compressed model albeit with slight performance degradation compared to the trained SAE on the compressed model. Furthermore, simply the original SAE itself achieves performance comparable to training a new SAE on the pruned model. This finding enables us to mitigate the extensive training costs of SAEs. Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Authors: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl 2025-07-21 http://arxiv.org/abs/2507.15826v1 Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models ( s) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment. In this paper, we present JAM (Just Ask for Music), a lightweight and intuitive framework for natural language music recommendation. JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and mixture-of-experts. We also introduce JAMSessions, a new dataset of over 100k user-query-item triples with anonymized user/item embeddings, uniquely combining conversational queries and user long-term preferences. Our results show that JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks. Reservoir Computing as a Language Model Authors: Felix K\u00f6ster, Atsushi Uchida 2025-07-21 http://arxiv.org/abs/2507.15779v1 Large Language Models ( ) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain . In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known -based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that s excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance. Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Authors: Gordon Hew, Ian McCulloh 2025-07-21 http://arxiv.org/abs/2507.16858v1 Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and -based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments. Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Authors: Jun Kang Yap, Vishnu Monn Baskaran, Wen Shan Tan, Ze Yang Ding, Hao Wang, David L. Dowe 2025-07-21 http://arxiv.org/abs/2507.15385v1 The growing integration of renewable energy sources in modern power systems has introduced significant operational challenges due to their intermittent and uncertain outputs. In recent years, mobile energy storage systems (ESSs) have emerged as a popular flexible resource for mitigating these challenges. Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility, enabling cost-effective energy delivery through the transportation network. However, the widespread deployment of mobile ESSs is often hindered by the high investment cost, which has motivated researchers to investigate utilising more readily available alternatives, such as electric vehicles (EVs) as mobile energy storage units instead. Hence, we explore this opportunity with a MIP-based day-ahead electric vehicle joint routing and scheduling problem in this work. However, solving the problem in a practical setting can often be computationally intractable since the existence of binary variables makes it combinatorial challenging. Therefore, we proposed to simplify the problem's solution process for a MIP solver by the solution search space with a -based deep learning (DL) model. This is done by training the model to rapidly predict the optimal binary solutions. In addition, unlike many existing DL approaches that assume fixed problem structures, the proposed model is designed to accommodate problems with EV fleets of any sizes. This flexibility is essential since frequent re-training can introduce significant computational overhead. We evaluated the approach with simulations on the IEEE 33-bus system coupled with the Nguyen-Dupuis transportation network. Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Authors: Elisa Sanchez-Bayona, Rodrigo Agerri 2025-07-21 http://arxiv.org/abs/2507.15357v1 This paper presents a comprehensive evaluation of the capabilities of Large Language Models ( s) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that s' performance is more influenced by features like lexical and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of s to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of s in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available. Scaling Decentralized Learning with FLock Authors: Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo 2025-07-21 http://arxiv.org/abs/2507.15349v1 Fine-tuning the large language models ( s) are prevented by the deficiency of centralized control and the massive computing and overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data. IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry Authors: Junhyeong Lee, Joon-Young Kim, Heekyu Kim, Inhyo Lee, Seunghwa Ryu 2025-07-21 http://arxiv.org/abs/2507.15268v1 The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective . This study introduces IM-Chat, a multi-agent framework based on large language models ( s), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. Overall, these findings demonstrate the viability of multi-agent systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing. CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Authors: Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung 2025-07-21 http://arxiv.org/abs/2507.15189v1 Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion ), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight -based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets. Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence Authors: Andres Navarro, Carlos de Quinto, Jos\u00e9 Alberto Hern\u00e1ndez 2025-07-20 http://arxiv.org/abs/2507.15049v1 Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as agile, intelligent nodes capable of advanced analytics and instantaneous situational awareness. This article introduces a budget-friendly quadcopter platform that unites 5G s, edge-based processing, and AI to tackle core challenges in NTN scenarios. Outfitted with a panoramic camera, robust onboard computation, and s, the drone system delivers seamless object recognition, contextual analysis, and immersive operator experiences through virtual reality VR technology. Field evaluations confirm the platform's ability to process visual streams with low latency and sustain robust 5G links. Adding s further streamlines operations by extracting actionable insights and refining collected data for decision support. Demonstrated use cases, including emergency response, infrastructure assessment, and environmental surveillance, underscore the system's adaptability in demanding contexts. From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Authors: Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi 2025-07-20 http://arxiv.org/abs/2507.14900v2 Large language models ( s) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates ping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of s, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual s (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual s. Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Authors: Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng 2025-07-20 http://arxiv.org/abs/2507.14894v1 Large Language Models ( s) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses. However, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness. In this paper, we first provide an in-depth analysis of unexpected code-switching using autoencoders and find that when s switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose $\\textbf{S}$parse $\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised $\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches s to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities. Tiny language models Authors: Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter 2025-07-20 http://arxiv.org/abs/2507.14871v2 A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward block architectures pre-trained on large language models ( s). However, pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of s. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language. The data and code that support the findings of this study are openly available on https://github.com/Rg32601/Tiny-Language-Models . An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks Authors: Xinyi Wu, Steven Landgraf, Markus Ulrich, Rongjun Qin 2025-07-20 http://arxiv.org/abs/2507.14798v1 State-of-the-art 3D computer vision algorithms continue to advance in handling , unordered image sets. Recently developed foundational models for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction (DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry Grounded Transformer (VGGT), have attracted attention due to their ability to handle very image s. Evaluating DUSt3R/MASt3R/VGGT on typical aerial images matters, as these models may handle extremely low image s, stereo occlusions, and textureless regions. For redundant collections, they can accelerate 3D reconstruction by using extremely sparsified image sets. Despite tests on various computer vision benchmarks, their potential on photogrammetric aerial blocks remains unexplored. This paper conducts a comprehensive evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of the UseGeo dataset for pose estimation and dense 3D reconstruction. Results show these methods can accurately reconstruct dense point clouds from very image sets (fewer than 10 images, up to 518 pixels resolution), with completeness gains up to +50% over COLMAP. VGGT also demonstrates higher computational efficiency, scalability, and more reliable camera pose estimation. However, all exhibit limitations with high-resolution images and large sets, as pose reliability declines with more images and geometric complexity. These findings suggest -based methods cannot fully replace traditional SfM and MVS, but offer promise as complementary approaches, especially in challenging, low-resolution, and scenarios. LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering Authors: Xinxin Dong, Baoyun Peng, Haokai Ma, Yufei Wang, Zixuan Dong, Fei Hu, Xiaodong Wang 2025-07-20 http://arxiv.org/abs/2507.14784v1 Video Question Answering (VideoQA) requires identifying critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages s to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an M to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency. CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories Authors: Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran 2025-07-19 http://arxiv.org/abs/2507.14766v1 In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes. GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Authors: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan 2025-07-19 http://arxiv.org/abs/2507.14758v1 Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of s and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences. Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Authors: Xuetao Lin, Tianhao Peng, Peihong Dai, Yu Liang, Wenjun Wu 2025-07-19 http://arxiv.org/abs/2507.14698v1 EEG-based emotion recognition plays an important role in developing adaptive brain-computer systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal s with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism. Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Authors: Joren Dumoulin, Pouya Houshmand, Vikram Jain, Marian Verhelst 2025-07-19 http://arxiv.org/abs/2507.14651v1 Hybrid vision s combine the elements of conventional neural networks (NN) and vision s (ViT) to enable lightweight and accurate detection. However, several challenges remain for their efficient deployment on resource-constrained edge devices. The hybrid models suffer from a widely diverse set of NN layer types and large intermediate data tensors, hampering efficient hardware . To enable their execution at the edge, this paper proposes innovations across the hardware-scheduling stack: a.) At the lowest level, a configurable PE array supports all hybrid ViT layer types; b.) temporal loop re-ordering within one layer, enabling hardware support for normalization and softmax layers, minimizing on-chip data transfers; c.) further scheduling optimization employs layer fusion across inverted bottleneck layers to drastically reduce off-chip memory transfers. The resulting accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of 1.39 TOPS/W at 25.6 GMACs/s. Linear Relational Decoding of Morphology in Language Models Authors: Eric Xia, Jugal Kalita 2025-07-19 http://arxiv.org/abs/2507.14640v1 A two-part affine approximation has been found to be a good approximation for computations over certain subject object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation Ws, where s is a middle layer representation of a subject token and W is derived from model derivatives, is also able to accurately reproduce final object states for many relations. This linear technique is able to achieve 90% faithfulness on morphological relations, and we show similar findings multi-lingually and across models. Our findings indicate that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space, and are ly encoded by cross-layer linear transformations. KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction Authors: Saleh Alwer, Ronan Fleming 2025-07-19 http://arxiv.org/abs/2507.14639v1 Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis constant ($K_{\\mathrm{M}}$) are essential for modelling enzymatic activity but experimental data remains limited in scale and diversity. Previous methods for predicting enzyme kinetics typically use mean-pooled residue embeddings from a single protein language model to represent the protein. We present KinForm, a machine learning framework designed to improve predictive accuracy and generalisation for kinetic parameters by optimising protein feature representations. KinForm combines several residue-level embeddings (Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and ProtT5-XL-UniRef50), taken from empirically selected intermediate layers and applies weighted pooling based on per-residue binding-site probability. To counter the resulting high dimensionality, we apply dimensionality reduction using principal--component analysis (PCA) on concatenated protein features, and rebalance the training data via a similarity-based oversampling strategy. KinForm outperforms baseline methods on two benchmark datasets. Improvements are most pronounced in low sequence similarity bins. We observe improvements from binding-site probability pooling, intermediate-layer selection, PCA, and oversampling of low-identity proteins. We also find that removing sequence between folds provides a more realistic evaluation of generalisation and should be the standard over random splitting when benchmarking kinetic prediction models. Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Authors: Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang 2025-07-19 http://arxiv.org/abs/2507.14633v1 The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models ( s). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), -based models (TBMs), and s. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks. Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Authors: Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis 2025-07-18 http://arxiv.org/abs/2507.14397v1 This paper presents a limit study of -based large language model ( ) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving s requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of deployment strategies. Characterizing Communication Patterns in Distributed Large Language Model Inference Authors: Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda 2025-07-18 http://arxiv.org/abs/2507.14392v1 Large Language Models ( s) built on architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU creates significant performance constraints that limit service quality in real-world systems. This paper investigates dynamics in distributed serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense -based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production services and identify key opportunities for optimizing inference frameworks and infrastructure. DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration Authors: Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu 2025-07-18 http://arxiv.org/abs/2507.14088v1 Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model ( ) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct . To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system. KROMA Ontology Matching with Knowledge Retrieval and Large Language Models Authors: Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu 2025-07-18 http://arxiv.org/abs/2507.14032v1 Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models ( s) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the overhead from invoking s. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented s significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge -based approaches while keeping overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale. LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan 2025-07-18 http://arxiv.org/abs/2507.13681v1 Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_ s}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates inference across a wide range of long-context dialogue tasks.","title":"2025-07-25"},{"location":"weekly_paper/2025-07-25/#2025-07-25","text":"","title":"2025-07-25"},{"location":"weekly_paper/2025-07-25/#table-of-contents","text":"Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation Prune&Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding ICWLM A Multi-Task Wireless Large Model via In-Context Learning NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models LoRA is All You Need for Safety Alignment of Reasoning LLMs Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges ACT Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training Benchmarking LLM Privacy Recognition for Social Robot Decision Making TorchAO PyTorch-Native Training-to-Serving Model Optimization On the transferability of Sparse Autoencoders for interpreting compressed models Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation Reservoir Computing as a Language Model Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding Scaling Decentralized Learning with FLock IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs Tiny language models An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge Linear Relational Decoding of Morphology in Language Models KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need Characterizing Communication Patterns in Distributed Large Language Model Inference DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration KROMA Ontology Matching with Knowledge Retrieval and Large Language Models LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues","title":"Table of Contents"},{"location":"weekly_paper/2025-07-25/#explainable-mapper-charting-llm-embedding-spaces-using-perturbation-based-explanation-and-verification-agents","text":"Authors: Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang 2025-07-24 http://arxiv.org/abs/2507.18607v1 Large language models ( s) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods . However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable -based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods.","title":"Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents"},{"location":"weekly_paper/2025-07-25/#not-all-features-deserve-attention-graph-guided-dependency-learning-for-tabular-data-generation-with-language-models","text":"Authors: Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci 2025-07-24 http://arxiv.org/abs/2507.18504v1 Large Language Models ( s) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as s' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates dependency graphs into s' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing -based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with s.","title":"Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models"},{"location":"weekly_paper/2025-07-25/#enhanced-velocity-adaptive-scheme-joint-fair-access-and-age-of-information-optimization-in-vehicular-networks","text":"Authors: Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief 2025-07-24 http://arxiv.org/abs/2507.18328v1 In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for driving assistance.Nevertheless, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwelltime and duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model ( )-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI.","title":"Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks"},{"location":"weekly_paper/2025-07-25/#styleadaptedlm-enhancing-instruction-following-models-with-efficient-stylistic-transfer","text":"Authors: Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu 2025-07-24 http://arxiv.org/abs/2507.18294v1 Adapting s to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in s.","title":"StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer"},{"location":"weekly_paper/2025-07-25/#assemble-your-crew-automatic-multi-agent-communication-topology-design-via-autoregressive-graph-generation","text":"Authors: Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan 2025-07-24 http://arxiv.org/abs/2507.18224v1 Multi-agent systems (MAS) based on large language models ( s) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.","title":"Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation"},{"location":"weekly_paper/2025-07-25/#prunecomp-free-lunch-for-layer-pruned-llms-via-iterative-pruning-with-magnitude-compensation","text":"Authors: Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan 2025-07-24 http://arxiv.org/abs/2507.18212v1 Layer has emerged as a promising technique for compressing large language models ( s) while achieving proportional to the ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the original model's question-answering performance, outperforming the baseline by 4.01%.","title":"Prune&amp;Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation"},{"location":"weekly_paper/2025-07-25/#specasr-accelerating-llm-based-automatic-speech-recognition-via-speculative-decoding","text":"Authors: Linye Wei, Shuzhang Zhong, Songqiang Xu, Runsheng Wang, Ru Huang, Meng Li 2025-07-24 http://arxiv.org/abs/2507.18181v1 Large language model ( )-based automatic speech recognition (ASR) has recently attracted a lot of attention due to its high recognition accuracy and enhanced multi-dialect support. However, the high decoding latency of s challenges the real-time ASR requirements. Although speculative decoding has been explored for better decoding efficiency, they usually ignore the key characteristics of the ASR task and achieve limited speedup. To further reduce the real-time ASR latency, in this paper, we propose a novel speculative decoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed based on our core observation that ASR decoding is audio-conditioned, which results in high output alignment between small and large ASR models, even given output mismatches in intermediate decoding steps. Therefore, SpecASR features an adaptive draft sequence generation process that dynamically modifies the draft sequence length to maximize the token acceptance length. SpecASR further proposes a draft sequence recycling strategy that reuses the previously generated draft sequence to reduce the draft ASR model latency. Moreover, a two-pass token tree generation algorithm is also proposed to balance the latency of draft and target ASR models. With extensive experimental results, we demonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the baseline autoregressive decoding and speculative decoding, respectively, without any loss in recognition accuracy.","title":"SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding"},{"location":"weekly_paper/2025-07-25/#icwlm-a-multi-task-wireless-large-model-via-in-context-learning","text":"Authors: Yuxuan Wen, Xiaoming Chen, Maojun Zhang, Zhaoyang Zhang 2025-07-24 http://arxiv.org/abs/2507.18167v1 The rapid evolution of wireless technologies, particularly massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave), introduces significant network complexity and computational demands. Significant research efforts have been made to improve physical layer performance by resorting to deep learning (DL) methods, which, however, are usually task-specific and struggle with data scarcity and generalization. To address these challenges, we propose a novel In-Context Wireless Large Model (ICWLM), a wireless-native foundation model designed for simultaneous multi-task learning at the physical layer. Unlike conventional methods that adapt wireless data to pre-trained large language models ( s), ICWLM is trained directly on large-scale, mixed wireless datasets from scratch. It jointly solves multiple classical physical layer problems, including multi-user precoding (sum-rate maximization and max-min SINR) and channel prediction. A key innovation of ICWLM is its utilization of in-context learning (ICL), enabling the model to adapt to varying system configurations and channel conditions with minimal demonstration pairs, eliminating the need for extensive retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm to dynamically balance the individual task losses during multi-task training, ensuring efficient and stable learning across diverse objectives. Extensive simulation results demonstrate that ICWLM achieves competitive performance compared to task-specific methods while exhibiting remarkable generalization capabilities to unseen system configurations. This work offers a promising paradigm for developing unified and adaptive AI models for future wireless networks, potentially reducing deployment complexity and enhancing intelligent resource management.","title":"ICWLM A Multi-Task Wireless Large Model via In-Context Learning"},{"location":"weekly_paper/2025-07-25/#neuraldb-scaling-knowledge-editing-in-llms-to-100000-facts-with-neural-kv-database","text":"Authors: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu 2025-07-24 http://arxiv.org/abs/2507.18028v1 Efficiently editing knowledge stored in large language models ( s) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of s and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value ( ) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of s. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work).","title":"NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database"},{"location":"weekly_paper/2025-07-25/#who-attacks-and-why-using-llms-to-identify-negative-campaigning-in-18m-tweets-across-19-countries","text":"Authors: Victor Hartman, Petter T\u00f6rnberg 2025-07-23 http://arxiv.org/abs/2507.17636v1 Negative campaigning is a central feature of political competition, yet empirical research has been limited by the high cost and limited scalability of existing classification methods. This study makes two key contributions. First, it introduces zero-shot Large Language Models ( s) as a novel approach for cross-lingual classification of negative campaigning. Using benchmark datasets in ten languages, we demonstrate that s achieve performance on par with native-speaking human coders and outperform conventional supervised machine learning approaches. Second, we leverage this novel method to conduct the largest cross-national study of negative campaigning to date, analyzing 18 million tweets posted by parliamentarians in 19 European countries between 2017 and 2022. The results reveal consistent cross-national patterns: governing parties are less likely to use negative messaging, while ideologically extreme and populist parties -- particularly those on the radical right -- engage in significantly higher levels of negativity. These findings advance our understanding of how party-level characteristics shape strategic in multiparty systems. More broadly, the study demonstrates the potential of s to enable scalable, transparent, and replicable research in political across linguistic and cultural contexts.","title":"Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries"},{"location":"weekly_paper/2025-07-25/#r-stitch-dynamic-trajectory-stitching-for-efficient-reasoning","text":"Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang 2025-07-23 http://arxiv.org/abs/2507.17307v1 Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model ( ) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.","title":"R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning"},{"location":"weekly_paper/2025-07-25/#efs-evolutionary-factor-searching-for-sparse-portfolio-optimization-using-large-language-models","text":"Authors: Haochen Luo, Yuan Zhang, Chen Liu 2025-07-23 http://arxiv.org/abs/2507.17211v1 Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models ( s) to automate the generation and evolution of alpha factors for portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by -generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints.","title":"EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models"},{"location":"weekly_paper/2025-07-25/#llm-meets-the-sky-heuristic-multi-agent-reinforcement-learning-for-secure-heterogeneous-uav-networks","text":"Authors: Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato 2025-07-23 http://arxiv.org/abs/2507.17188v1 This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and , we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model ( )-guided heuristic multi-agent reinforcement learning approach ( -HeMARL) for trajectory optimization. -HeMARL efficiently incorporates expert heuristics policy generated by the , enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.","title":"LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks"},{"location":"weekly_paper/2025-07-25/#resilient-multi-agent-negotiation-for-medical-supply-chainsintegrating-llms-and-blockchain-for-transparent-coordination","text":"Authors: Mariam ALMutairi, Hyungmin Kim 2025-07-23 http://arxiv.org/abs/2507.17134v1 Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model ( ) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by s, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of -driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty.","title":"Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination"},{"location":"weekly_paper/2025-07-25/#reinforcement-learning-fine-tunes-a-sparse-subnetwork-in-large-language-models","text":"Authors: Andrii Balashov 2025-07-23 http://arxiv.org/abs/2507.17107v1 Reinforcement learning (RL) is a key post-pretraining step for aligning large language models ( s) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update . It arises naturally, without any constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source s). Moreover, the subnetworks updated by RL show substantial across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes through the lens of the lottery ticket hypothesis.","title":"Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models"},{"location":"weekly_paper/2025-07-25/#lora-is-all-you-need-for-safety-alignment-of-reasoning-llms","text":"Authors: Yihao Xue, Baharan Mirzasoleiman 2025-07-22 http://arxiv.org/abs/2507.17075v1 Reasoning s have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure s do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the \"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe s -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off.","title":"LoRA is All You Need for Safety Alignment of Reasoning LLMs"},{"location":"weekly_paper/2025-07-25/#parallelism-meets-adaptiveness-scalable-documents-understanding-in-multi-agent-llm-systems","text":"Authors: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao 2025-07-22 http://arxiv.org/abs/2507.17061v1 Large language model ( ) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent , reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent systems.","title":"Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems"},{"location":"weekly_paper/2025-07-25/#beyond-context-limits-subconscious-threads-for-long-horizon-reasoning","text":"Authors: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass 2025-07-22 http://arxiv.org/abs/2507.16784v1 To break the context limits of large language models ( s) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of s trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask- mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.","title":"Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning"},{"location":"weekly_paper/2025-07-25/#collaborative-inference-and-learning-between-edge-slms-and-cloud-llms-a-survey-of-algorithms-execution-and-open-challenges","text":"Authors: Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li 2025-07-22 http://arxiv.org/abs/2507.16731v1 As large language models ( s) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based s and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, , bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for -SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.","title":"Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges"},{"location":"weekly_paper/2025-07-25/#act-bridging-the-gap-in-code-translation-through-synthetic-data-generation-adaptive-training","text":"Authors: Shreya Saxena, Siva Prasad, Zishan Ahmad, Vishal Vaddina 2025-07-22 http://arxiv.org/abs/2507.16478v1 Code translation is a crucial process in software development and migration projects, enabling interoperability between different programming languages and enhancing software adaptability and thus longevity. Traditional automated translation methods rely heavily on handcrafted transformation rules, which often lack flexibility and scalability. Meanwhile, advanced language models present promising alternatives but are often limited by proprietary, API-based implementations that raise concerns over data security and reliance. In this paper, we present Auto-Train for Code Translation (ACT), an innovative framework that aims to improve code translation capabilities by enabling in-house finetuning of open-source Large Language Models ( s). ACT's automated pipeline significantly boosts the performance of these models, narrowing the gap between open-source accessibility and the high performance of closed-source solutions. Central to ACT is its synthetic data generation module, which builds extensive, high-quality datasets from initial code samples, incorporating unit tests to ensure functional accuracy and diversity. ACT's evaluation framework incorporates execution-level checks, offering a comprehensive assessment of translation quality. A key feature in ACT is its controller module, which manages the entire pipeline by dynamically adjusting hyperparameters, orchestrating iterative data generation, and finetuning based on real-time evaluations. This enables ACT to intelligently optimize when to continue training, generate additional targeted training data, or stop the process. Our results demonstrate that ACT consistently enhances the effectiveness of open-source models, offering businesses and developers a secure and reliable alternative. Additionally, applying our data generation pipeline to industry-scale migration projects has led to a notable increase in developer .","title":"ACT Bridging the Gap in Code Translation through Synthetic Data Generation &amp; Adaptive Training"},{"location":"weekly_paper/2025-07-25/#mamba-otr-a-mamba-based-solution-for-online-take-and-release-detection-from-untrimmed-egocentric-video","text":"Authors: Alessandro Sebastiano Catinello, Giovanni Maria Farinella, Antonino Furnari 2025-07-22 http://arxiv.org/abs/2507.16342v1 This work tackles the problem of Online detection of Take and Release (OTR) of an object in untrimmed egocentric videos. This task is challenging due to severe label imbalance, with temporally positive annotations, and the need for precise temporal predictions. Furthermore, methods need to be computationally efficient in order to be deployed in real-world online settings. To address these challenges, we propose Mamba-OTR, a model based on the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence during inference while being trained on short video clips. To address label imbalance, our training pipeline incorporates the focal loss and a novel regularization scheme that aligns model predictions with the evaluation metric. Extensive experiments on EPIC-KITCHENS-100, the comparisons with -based approach, and the evaluation of different training and test schemes demonstrate the superiority of Mamba-OTR in both accuracy and efficiency. These finding are particularly evident when evaluating full-length videos or high frame-rate sequences, even when trained on short video snippets for computational convenience. The proposed Mamba-OTR achieves a noteworthy mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in streaming mode, versus the 20.32 of a vanilla and 25.16 of a vanilla Mamba, thus providing a strong baseline for OTR. We will publicly release the source code of Mamba-OTR to support future research.","title":"Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video"},{"location":"weekly_paper/2025-07-25/#compleak-deep-learning-model-compression-exacerbates-privacy-leakage","text":"Authors: Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu 2025-07-22 http://arxiv.org/abs/2507.16872v1 Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models ( s). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood. In this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are , quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets.","title":"CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage"},{"location":"weekly_paper/2025-07-25/#time-to-split-exploring-data-splitting-strategies-for-offline-evaluation-of-sequential-recommenders","text":"Authors: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov 2025-07-22 http://arxiv.org/abs/2507.16289v1 Modern sequential recommender systems, ranging from lightweight -based variants to large language models, have become increasingly prominent in academia and industry due to their strong performance in the next-item prediction task. Yet common evaluation protocols for sequential recommendations remain insufficiently developed: they often fail to reflect the corresponding recommendation task accurately, or are not aligned with real-world scenarios. Although the widely used leave-one-out split matches next-item prediction, it permits the between training and test periods, which leads to temporal leakage and unrealistically long test horizon, limiting real-world relevance. Global temporal splitting addresses these issues by evaluating on distinct future periods. However, its applications to sequential recommendations remain loosely defined, particularly in terms of selecting target interactions and constructing a validation subset that provides necessary consistency between validation and test metrics. In this paper, we demonstrate that evaluation outcomes can vary significantly across splitting strategies, influencing model rankings and practical deployment decisions. To improve reproducibility in both academic and industrial settings, we systematically compare different splitting strategies for sequential recommendations across multiple datasets and established baselines. Our findings show that prevalent splits, such as leave-one-out, may be insufficiently aligned with more realistic evaluation strategies. Code: https://github.com/monkey0head/time-to-split","title":"Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders"},{"location":"weekly_paper/2025-07-25/#reducing-gpu-memory-fragmentation-via-spatio-temporal-planning-for-efficient-large-scale-model-training","text":"Authors: Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang 2025-07-22 http://arxiv.org/abs/2507.16274v1 The rapid scaling of large language models ( s) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Default GPU memory allocators of popular deep learning frameworks like PyTorch use online strategies without knowledge of tensor lifespans, which can waste up to 43\\% of memory and cause out-of-memory errors, rendering optimization techniques ineffective or even unusable. To address this, we introduce STWeaver, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STWeaver introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by 79.2\\% (up to 100\\%) across both dense and models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves performance by up to 32.5\\%.","title":"Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training"},{"location":"weekly_paper/2025-07-25/#benchmarking-llm-privacy-recognition-for-social-robot-decision-making","text":"Authors: Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz 2025-07-22 http://arxiv.org/abs/2507.16124v1 Social robots are embodied agents that interact with people while following human norms. These robots interact using verbal and non-verbal cues, and share the physical environments of people. While social robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models ( s) presents new opportunities to develop -empowered social robots for enhanced human-robot interaction. To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, s often process sensitive personal information, particularly within home environments. Given the tension between utility and privacy risks, evaluating how current s manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box s are privacy-aware in the context of household social robots. In this study, we present a set of privacy-relevant scenarios crafted through the lens of Contextual Integrity (CI). We first survey users' privacy preferences regarding in-home social robot behaviors and then examine how their privacy orientation affects their choices of these behaviors (N = 450). We then provide the same set of scenarios and questions to state-of-the-art s (N = 10) and find that the agreement between humans and s is low. To further investigate the capabilities of s as a potential privacy controller, we implement four additional prompting strategies and compare their results. Finally, we discuss the implications and potential of AI privacy awareness in human-robot interaction.","title":"Benchmarking LLM Privacy Recognition for Social Robot Decision Making"},{"location":"weekly_paper/2025-07-25/#torchao-pytorch-native-training-to-serving-model-optimization","text":"Authors: Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar Samard\u017ei\u0107 2025-07-21 http://arxiv.org/abs/2507.16099v1 We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 , and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, v , SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at https://github.com/pytorch/ao/.","title":"TorchAO PyTorch-Native Training-to-Serving Model Optimization"},{"location":"weekly_paper/2025-07-25/#on-the-transferability-of-sparse-autoencoders-for-interpreting-compressed-models","text":"Authors: Suchit Gupte, Vishnu Kabir Chhabra, Mohammad Mahdi Khalili 2025-07-21 http://arxiv.org/abs/2507.15977v1 Modern s face inference efficiency challenges due to their scale. To address this, many compression methods have been proposed, such as and quantization. However, the effect of compression on a model's interpretability remains elusive. While several model interpretation approaches exist, such as circuit discovery, Sparse Autoencoders (SAEs) have proven particularly effective in decomposing a model's activation space into its feature basis. In this work, we explore the differences in SAEs for the original and compressed models. We find that SAEs trained on the original model can interpret the compressed model albeit with slight performance degradation compared to the trained SAE on the compressed model. Furthermore, simply the original SAE itself achieves performance comparable to training a new SAE on the pruned model. This finding enables us to mitigate the extensive training costs of SAEs.","title":"On the transferability of Sparse Autoencoders for interpreting compressed models"},{"location":"weekly_paper/2025-07-25/#just-ask-for-music-jam-multimodal-and-personalized-natural-language-music-recommendation","text":"Authors: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl 2025-07-21 http://arxiv.org/abs/2507.15826v1 Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models ( s) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment. In this paper, we present JAM (Just Ask for Music), a lightweight and intuitive framework for natural language music recommendation. JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and mixture-of-experts. We also introduce JAMSessions, a new dataset of over 100k user-query-item triples with anonymized user/item embeddings, uniquely combining conversational queries and user long-term preferences. Our results show that JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks.","title":"Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation"},{"location":"weekly_paper/2025-07-25/#reservoir-computing-as-a-language-model","text":"Authors: Felix K\u00f6ster, Atsushi Uchida 2025-07-21 http://arxiv.org/abs/2507.15779v1 Large Language Models ( ) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain . In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known -based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that s excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance.","title":"Reservoir Computing as a Language Model"},{"location":"weekly_paper/2025-07-25/#who-leads-in-the-shadows-ergm-and-centrality-analysis-of-congressional-democrats-on-bluesky","text":"Authors: Gordon Hew, Ian McCulloh 2025-07-21 http://arxiv.org/abs/2507.16858v1 Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and -based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments.","title":"Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky"},{"location":"weekly_paper/2025-07-25/#transformer-based-deep-learning-model-for-joint-routing-and-scheduling-with-varying-electric-vehicle-numbers","text":"Authors: Jun Kang Yap, Vishnu Monn Baskaran, Wen Shan Tan, Ze Yang Ding, Hao Wang, David L. Dowe 2025-07-21 http://arxiv.org/abs/2507.15385v1 The growing integration of renewable energy sources in modern power systems has introduced significant operational challenges due to their intermittent and uncertain outputs. In recent years, mobile energy storage systems (ESSs) have emerged as a popular flexible resource for mitigating these challenges. Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility, enabling cost-effective energy delivery through the transportation network. However, the widespread deployment of mobile ESSs is often hindered by the high investment cost, which has motivated researchers to investigate utilising more readily available alternatives, such as electric vehicles (EVs) as mobile energy storage units instead. Hence, we explore this opportunity with a MIP-based day-ahead electric vehicle joint routing and scheduling problem in this work. However, solving the problem in a practical setting can often be computationally intractable since the existence of binary variables makes it combinatorial challenging. Therefore, we proposed to simplify the problem's solution process for a MIP solver by the solution search space with a -based deep learning (DL) model. This is done by training the model to rapidly predict the optimal binary solutions. In addition, unlike many existing DL approaches that assume fixed problem structures, the proposed model is designed to accommodate problems with EV fleets of any sizes. This flexibility is essential since frequent re-training can introduce significant computational overhead. We evaluated the approach with simulations on the IEEE 33-bus system coupled with the Nguyen-Dupuis transportation network.","title":"Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers"},{"location":"weekly_paper/2025-07-25/#metaphor-and-large-language-models-when-surface-features-matter-more-than-deep-understanding","text":"Authors: Elisa Sanchez-Bayona, Rodrigo Agerri 2025-07-21 http://arxiv.org/abs/2507.15357v1 This paper presents a comprehensive evaluation of the capabilities of Large Language Models ( s) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that s' performance is more influenced by features like lexical and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of s to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of s in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.","title":"Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding"},{"location":"weekly_paper/2025-07-25/#scaling-decentralized-learning-with-flock","text":"Authors: Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo 2025-07-21 http://arxiv.org/abs/2507.15349v1 Fine-tuning the large language models ( s) are prevented by the deficiency of centralized control and the massive computing and overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.","title":"Scaling Decentralized Learning with FLock"},{"location":"weekly_paper/2025-07-25/#im-chat-a-multi-agent-llm-based-framework-for-knowledge-transfer-in-injection-molding-industry","text":"Authors: Junhyeong Lee, Joon-Young Kim, Heekyu Kim, Inhyo Lee, Seunghwa Ryu 2025-07-21 http://arxiv.org/abs/2507.15268v1 The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective . This study introduces IM-Chat, a multi-agent framework based on large language models ( s), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. Overall, these findings demonstrate the viability of multi-agent systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing.","title":"IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry"},{"location":"weekly_paper/2025-07-25/#chadet-cross-hierarchical-attention-for-depth-completion-using-unsupervised-lightweight-transformer","text":"Authors: Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung 2025-07-21 http://arxiv.org/abs/2507.15189v1 Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion ), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight -based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets.","title":"CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer"},{"location":"weekly_paper/2025-07-25/#beyond-visual-line-of-sight-uavs-with-edge-ai-connected-llms-and-vr-for-autonomous-aerial-intelligence","text":"Authors: Andres Navarro, Carlos de Quinto, Jos\u00e9 Alberto Hern\u00e1ndez 2025-07-20 http://arxiv.org/abs/2507.15049v1 Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as agile, intelligent nodes capable of advanced analytics and instantaneous situational awareness. This article introduces a budget-friendly quadcopter platform that unites 5G s, edge-based processing, and AI to tackle core challenges in NTN scenarios. Outfitted with a panoramic camera, robust onboard computation, and s, the drone system delivers seamless object recognition, contextual analysis, and immersive operator experiences through virtual reality VR technology. Field evaluations confirm the platform's ability to process visual streams with low latency and sustain robust 5G links. Adding s further streamlines operations by extracting actionable insights and refining collected data for decision support. Demonstrated use cases, including emergency response, infrastructure assessment, and environmental surveillance, underscore the system's adaptability in demanding contexts.","title":"Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence"},{"location":"weekly_paper/2025-07-25/#from-neurons-to-semantics-evaluating-cross-linguistic-alignment-capabilities-of-large-language-models-via-neurons-alignment","text":"Authors: Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi 2025-07-20 http://arxiv.org/abs/2507.14900v2 Large language models ( s) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates ping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of s, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual s (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual s.","title":"From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment"},{"location":"weekly_paper/2025-07-25/#sparse-autoencoder-guided-supervised-finetuning-to-mitigate-unexpected-code-switching-in-llms","text":"Authors: Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng 2025-07-20 http://arxiv.org/abs/2507.14894v1 Large Language Models ( s) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses. However, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness. In this paper, we first provide an in-depth analysis of unexpected code-switching using autoencoders and find that when s switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose $\\textbf{S}$parse $\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised $\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches s to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities.","title":"Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs"},{"location":"weekly_paper/2025-07-25/#tiny-language-models","text":"Authors: Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter 2025-07-20 http://arxiv.org/abs/2507.14871v2 A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward block architectures pre-trained on large language models ( s). However, pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of s. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language. The data and code that support the findings of this study are openly available on https://github.com/Rg32601/Tiny-Language-Models .","title":"Tiny language models"},{"location":"weekly_paper/2025-07-25/#an-evaluation-of-dust3rmast3rvggt-3d-reconstruction-on-photogrammetric-aerial-blocks","text":"Authors: Xinyi Wu, Steven Landgraf, Markus Ulrich, Rongjun Qin 2025-07-20 http://arxiv.org/abs/2507.14798v1 State-of-the-art 3D computer vision algorithms continue to advance in handling , unordered image sets. Recently developed foundational models for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction (DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry Grounded Transformer (VGGT), have attracted attention due to their ability to handle very image s. Evaluating DUSt3R/MASt3R/VGGT on typical aerial images matters, as these models may handle extremely low image s, stereo occlusions, and textureless regions. For redundant collections, they can accelerate 3D reconstruction by using extremely sparsified image sets. Despite tests on various computer vision benchmarks, their potential on photogrammetric aerial blocks remains unexplored. This paper conducts a comprehensive evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of the UseGeo dataset for pose estimation and dense 3D reconstruction. Results show these methods can accurately reconstruct dense point clouds from very image sets (fewer than 10 images, up to 518 pixels resolution), with completeness gains up to +50% over COLMAP. VGGT also demonstrates higher computational efficiency, scalability, and more reliable camera pose estimation. However, all exhibit limitations with high-resolution images and large sets, as pose reliability declines with more images and geometric complexity. These findings suggest -based methods cannot fully replace traditional SfM and MVS, but offer promise as complementary approaches, especially in challenging, low-resolution, and scenarios.","title":"An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks"},{"location":"weekly_paper/2025-07-25/#leadqa-llm-driven-context-aware-temporal-grounding-for-video-question-answering","text":"Authors: Xinxin Dong, Baoyun Peng, Haokai Ma, Yufei Wang, Zixuan Dong, Fei Hu, Xiaodong Wang 2025-07-20 http://arxiv.org/abs/2507.14784v1 Video Question Answering (VideoQA) requires identifying critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages s to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an M to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency.","title":"LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering"},{"location":"weekly_paper/2025-07-25/#cxr-tft-multi-modal-temporal-fusion-transformer-for-predicting-chest-x-ray-trajectories","text":"Authors: Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran 2025-07-19 http://arxiv.org/abs/2507.14766v1 In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes.","title":"CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories"},{"location":"weekly_paper/2025-07-25/#grace-generative-recommendation-via-journey-aware-sparse-attention-on-chain-of-thought-tokenization","text":"Authors: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan 2025-07-19 http://arxiv.org/abs/2507.14758v1 Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of s and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.","title":"GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization"},{"location":"weekly_paper/2025-07-25/#spatial-temporal-transformer-with-curriculum-learning-for-eeg-based-emotion-recognition","text":"Authors: Xuetao Lin, Tianhao Peng, Peihong Dai, Yu Liang, Wenjun Wu 2025-07-19 http://arxiv.org/abs/2507.14698v1 EEG-based emotion recognition plays an important role in developing adaptive brain-computer systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal s with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.","title":"Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition"},{"location":"weekly_paper/2025-07-25/#enabling-efficient-hardware-acceleration-of-hybrid-vision-transformer-vit-networks-at-the-edge","text":"Authors: Joren Dumoulin, Pouya Houshmand, Vikram Jain, Marian Verhelst 2025-07-19 http://arxiv.org/abs/2507.14651v1 Hybrid vision s combine the elements of conventional neural networks (NN) and vision s (ViT) to enable lightweight and accurate detection. However, several challenges remain for their efficient deployment on resource-constrained edge devices. The hybrid models suffer from a widely diverse set of NN layer types and large intermediate data tensors, hampering efficient hardware . To enable their execution at the edge, this paper proposes innovations across the hardware-scheduling stack: a.) At the lowest level, a configurable PE array supports all hybrid ViT layer types; b.) temporal loop re-ordering within one layer, enabling hardware support for normalization and softmax layers, minimizing on-chip data transfers; c.) further scheduling optimization employs layer fusion across inverted bottleneck layers to drastically reduce off-chip memory transfers. The resulting accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of 1.39 TOPS/W at 25.6 GMACs/s.","title":"Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge"},{"location":"weekly_paper/2025-07-25/#linear-relational-decoding-of-morphology-in-language-models","text":"Authors: Eric Xia, Jugal Kalita 2025-07-19 http://arxiv.org/abs/2507.14640v1 A two-part affine approximation has been found to be a good approximation for computations over certain subject object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation Ws, where s is a middle layer representation of a subject token and W is derived from model derivatives, is also able to accurately reproduce final object states for many relations. This linear technique is able to achieve 90% faithfulness on morphological relations, and we show similar findings multi-lingually and across models. Our findings indicate that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space, and are ly encoded by cross-layer linear transformations.","title":"Linear Relational Decoding of Morphology in Language Models"},{"location":"weekly_paper/2025-07-25/#kinform-kinetics-informed-feature-optimised-representation-models-for-enzyme-k_cat-and-k_m-prediction","text":"Authors: Saleh Alwer, Ronan Fleming 2025-07-19 http://arxiv.org/abs/2507.14639v1 Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis constant ($K_{\\mathrm{M}}$) are essential for modelling enzymatic activity but experimental data remains limited in scale and diversity. Previous methods for predicting enzyme kinetics typically use mean-pooled residue embeddings from a single protein language model to represent the protein. We present KinForm, a machine learning framework designed to improve predictive accuracy and generalisation for kinetic parameters by optimising protein feature representations. KinForm combines several residue-level embeddings (Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and ProtT5-XL-UniRef50), taken from empirically selected intermediate layers and applies weighted pooling based on per-residue binding-site probability. To counter the resulting high dimensionality, we apply dimensionality reduction using principal--component analysis (PCA) on concatenated protein features, and rebalance the training data via a similarity-based oversampling strategy. KinForm outperforms baseline methods on two benchmark datasets. Improvements are most pronounced in low sequence similarity bins. We observe improvements from binding-site probability pooling, intermediate-layer selection, PCA, and oversampling of low-identity proteins. We also find that removing sequence between folds provides a more realistic evaluation of generalisation and should be the standard over random splitting when benchmarking kinetic prediction models.","title":"KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction"},{"location":"weekly_paper/2025-07-25/#agentic-satellite-augmented-low-altitude-economy-and-terrestrial-networks-a-survey-on-generative-approaches","text":"Authors: Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang 2025-07-19 http://arxiv.org/abs/2507.14633v1 The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models ( s). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), -based models (TBMs), and s. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks.","title":"Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches"},{"location":"weekly_paper/2025-07-25/#efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need","text":"Authors: Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis 2025-07-18 http://arxiv.org/abs/2507.14397v1 This paper presents a limit study of -based large language model ( ) inference, focusing on the fundamental performance bottlenecks imposed by memory bandwidth, memory capacity, and synchronization overhead in distributed inference systems. We develop a hardware-agnostic performance model that abstracts away implementation details, enabling the analysis of a wide range of current and near-future hardware technologies. Our analysis spans from current HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers SRAM-based designs and scaling techniques from distributed clusters with varying numbers of chips to wafer-scale integration. Our key findings for auto-regressive decoding are: i) serving s requires 100s of GB per server to serve a model instance; ii) high memory bandwidth is critical for high per-user throughput; iii) exposed synchronization latencies to achieve collective must be around 1us else they make the memory bandwidth ineffective; iv) DRAM-based designs have a fundamental advantage in terms of system-level efficiency as measured in throughput per cost or watt; and v) hardware designs can easily reach 2000+ user token/sec but getting to 10,000+ tokens/sec will need smaller models, smaller context, or other forms of algorithmic advances. This study provides valuable insights into the fundamental performance limits of inference, highlighting the potential benefits of future hardware advancements and guiding the optimization of deployment strategies.","title":"Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need"},{"location":"weekly_paper/2025-07-25/#characterizing-communication-patterns-in-distributed-large-language-model-inference","text":"Authors: Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda 2025-07-18 http://arxiv.org/abs/2507.14392v1 Large Language Models ( s) built on architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU creates significant performance constraints that limit service quality in real-world systems. This paper investigates dynamics in distributed serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense -based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production services and identify key opportunities for optimizing inference frameworks and infrastructure.","title":"Characterizing Communication Patterns in Distributed Large Language Model Inference"},{"location":"weekly_paper/2025-07-25/#dpmt-dual-process-multi-scale-theory-of-mind-framework-for-real-time-human-ai-collaboration","text":"Authors: Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu 2025-07-18 http://arxiv.org/abs/2507.14088v1 Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model ( ) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct . To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.","title":"DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration"},{"location":"weekly_paper/2025-07-25/#kroma-ontology-matching-with-knowledge-retrieval-and-large-language-models","text":"Authors: Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu 2025-07-18 http://arxiv.org/abs/2507.14032v1 Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models ( s) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the overhead from invoking s. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented s significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge -based approaches while keeping overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.","title":"KROMA Ontology Matching with Knowledge Retrieval and Large Language Models"},{"location":"weekly_paper/2025-07-25/#loopserve-an-adaptive-dual-phase-llm-inference-acceleration-system-for-multi-turn-dialogues","text":"Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan 2025-07-18 http://arxiv.org/abs/2507.13681v1 Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_ s}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates inference across a wide range of long-context dialogue tasks.","title":"LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues"},{"location":"weekly_paper/2025-08-01/","text":"2025-08-01 Table of Contents DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Unveiling Super Experts in Mixture-of-Experts Large Language Models Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Large Language Models for Wireless Communications From Adaptation to Autonomy Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Agentic Web Weaving the Next Web with AI Agents SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Latent Inter-User Difference Modeling for LLM Personalization METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Modeling Professionalism in Expert Questioning through Linguistic Differentiation FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression The Carbon Cost of Conversation, Sustainability in the Age of Language Models CaliDrop KV Cache Compression with Calibration CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation LowKeyEMG Electromyographic typing with a reduced keyset Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers RegScore Scoring Systems for Regression Tasks MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data Authors: Rabeya Tus Sadia, Qiang Cheng 2025-07-31 http://arxiv.org/abs/2507.23676v1 Microbiome data analysis is essential for understanding host health and disease, yet its inherent and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model ( ). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation. MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Authors: Qian Zhao, Zhuo Sun, Bin Guo, Zhiwen Yu 2025-07-31 http://arxiv.org/abs/2507.23633v1 Agent-assisted memory recall is one critical research problem in the field of human-computer interaction. In conventional methods, the agent can retrieve information from its equipped memory module to help the person recall incomplete or vague memories. The limited size of memory module hinders the acquisition of complete memories and impacts the memory recall performance in practice. Memory theories suggest that the person's relevant memory can be proactively activated through some effective cues. Inspired by this, we propose a novel strategy-guided agent-assisted memory recall method, allowing the agent to transform an original query into a cue-rich one via the judiciously designed strategy to help the person recall memories. To this end, there are two key challenges. (1) How to choose the appropriate recall strategy for diverse forgetting scenarios with distinct memory-recall characteristics? (2) How to obtain the high-quality responses leveraging recall strategies, given only abstract and ly annotated strategy patterns? To address the challenges, we propose a Recall Router framework. Specifically, we design a 5W Recall Map to classify memory queries into five typical scenarios and define fifteen recall strategy patterns across the corresponding scenarios. We then propose a hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to optimize the selection of strategy and the generation of strategy responses. We construct an instruction tuning dataset and fine-tune multiple open-source large language models ( s) to develop MemoCue, an agent that excels in providing memory-inspired responses. Experiments on three representative datasets show that MemoCue surpasses -based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications. Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Authors: Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu 2025-07-31 http://arxiv.org/abs/2507.23370v1 Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models ( s), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of -based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, , and selection. We conduct extensive experiments using three leading s on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent. Unveiling Super Experts in Mixture-of-Experts Large Language Models Authors: Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan 2025-07-31 http://arxiv.org/abs/2507.23279v1 Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models ( s). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE s. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE s, and despite their limited number, them leads to a significant decline in model performance (e.g., three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE s rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE . The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling. Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders Authors: Carolina Zheng, Nicolas Beltran-Velez, Sweta Karlekar, Claudia Shi, Achille Nazaret, Asif Mallik, Amir Feder, David M. Blei 2025-07-31 http://arxiv.org/abs/2507.23220v1 Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \\textit{topic judge}, an -based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of outputs. MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Authors: Yuqi Pang, Bowen Yang, Yun Cao, Fan Rong, Xiaoyu Li, Chen He 2025-07-30 http://arxiv.org/abs/2507.22805v1 Vision large language models (V s) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream s (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA. Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques Authors: Weide Liu, Wei Zhou, Jun Liu, Ping Hu, Jun Cheng, Jungong Han, Weisi Lin 2025-07-30 http://arxiv.org/abs/2507.22791v1 Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and -based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions. trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Authors: MohammadAmin Alamalhoda, Arsalan Firoozi, Alessandro Venturino, Sandra Siegert 2025-07-30 http://arxiv.org/abs/2507.22635v1 The shape of a cell contains essential information about its function within the biological system. Segmenting these structures from large-scale 3D microscopy images is challenging, limiting clinical insights especially for microglia, immune-associated cells involved in neurodegenerative diseases. Existing segmentation methods mainly focus on cell bodies, struggle with ping structures, perform poorly on noisy images, require hyperparameter tuning for each new dataset, or rely on tedious semi-automated approaches. We introduce trAIce3D, a deep-learning architecture designed for precise microglia segmentation, capturing both somas and branches. It employs a two-stage approach: first, a 3D U-Net with vision s in the encoder detects somas using a sliding-window technique to cover the entire image. Then, the same architecture, enhanced with cross-attention blocks in skip connections, refines each soma and its branches by using soma coordinates as a prompt and a 3D window around the target cell as input. Training occurs in two phases: self-supervised Soma Segmentation, followed by prompt-based Branch Segmentation, leveraging pre-trained weights from the first phase. Trained and evaluated on a dataset of 41,230 microglial cells, trAIce3D significantly improves segmentation accuracy and generalization, enabling scalable analysis of complex cellular morphologies. While optimized for microglia, its architecture can extend to other intricate cell types, such as neurons and astrocytes, broadening its impact on neurobiological research. Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation Authors: Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann 2025-07-30 http://arxiv.org/abs/2507.22608v1 Large language models ( s) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share ping neurons, reflecting internal representations of linguistic proximity. Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal \"fallback\" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation. MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Authors: Yaolun Zhang, Xiaogeng Liu, Chaowei Xiao 2025-07-30 http://arxiv.org/abs/2507.22606v1 Large Language Models ( s) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks. Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items Authors: Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram 2025-07-29 http://arxiv.org/abs/2507.22268v1 We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage s to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items. CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Authors: Black Sun, Die, Hu 2025-07-29 http://arxiv.org/abs/2507.22205v1 Remote fetal monitoring technologies are becoming increasingly common. Yet, most current systems offer limited interpretability, leaving expectant parents with raw cardiotocography (CTG) data that is difficult to understand. In this work, we present CTG-Insight, a multi-agent system that provides structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals. Drawing from established medical guidelines, CTG-Insight decomposes each CTG trace into five medically defined features: baseline, variability, s, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. A final aggregation agent synthesizes the outputs to deliver a holistic classification of fetal health, accompanied by a natural language explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare it against deep learning models and the single-agent baseline. Results show that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) while producing transparent and interpretable outputs. This work contributes an interpretable and extensible CTG analysis framework. Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles Authors: Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu 2025-07-29 http://arxiv.org/abs/2507.22168v1 Current benchmarks for evaluating Large Language Models ( s) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of patterns exhibited by humans. Thus, it is possible that s, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring performance across linguistic variations. IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Authors: Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim 2025-07-29 http://arxiv.org/abs/2507.22134v1 While large language models ( s) are widely used for writing, users often struggle to express their nuanced and evolving intents through prompt-based interfaces. Intents -- low-level strategies or preferences for achieving a writing goal -- are often vague, fluid, or even subconscious, making it difficult for users to articulate and adjust them. To address this, we present IntentFlow, which supports the of dynamically evolving intents throughout -assisted writing. IntentFlow extracts goals and intents from user prompts and presents them as editable interface components, which users can revise, remove, or refine via direct manipulation or follow-up prompts. Visual links connect each component to the output segments it influences, helping users understand model behavior. In a within-subjects study (N=12), participants using IntentFlow, compared to a chat-based baseline, expressed their intents more easily and in detail, engaged in more meaningful actions to communicate intents, such as adjusting and deleting, and produced outputs that better aligned with their evolving intents. We found that editable intent representations help users refine and consolidate a final set of intents, which can be reused across similar tasks to support consistent and transferable -assisted writing. Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models Authors: Hyunwoo Yoo, Gail L. Rosen 2025-07-29 http://arxiv.org/abs/2507.21980v1 Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models ( s) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate s such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that s not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that s can effectively reason over , heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications. EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Authors: Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang 2025-07-29 http://arxiv.org/abs/2507.21848v1 Large Language Models ( s) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO. Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer Authors: Xie Zhang, Yina Wang, Chenshu Wu 2025-07-29 http://arxiv.org/abs/2507.21799v1 The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box s, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE. MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Authors: YiZhou Li 2025-07-29 http://arxiv.org/abs/2507.21761v1 Vision Transformers (ViTs) have achieved remarkable success in image recognition, yet standard ViT architectures are hampered by substantial parameter redundancy and high computational cost, limiting their practical deployment. While recent efforts on efficient ViTs primarily focus on static model compression or token-level sparsification, they remain constrained by fixed computational depth for all tokens. In this work, we present MoR-ViT, a novel vision framework that, for the first time, incorporates a token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions (MoR) paradigm. This approach enables each token to adaptively determine its processing depth, yielding a flexible and input-dependent allocation of computational resources. Extensive experiments on ImageNet-1K and transfer benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy with up to 70% parameter reduction and 2.5x inference , but also outperforms leading efficient ViT baselines such as DynamicViT and TinyViT under comparable conditions. These results establish dynamic recursion as an effective strategy for efficient vision s and open new avenues for scalable and deployable deep learning models in real-world scenarios. Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation Authors: Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le 2025-07-29 http://arxiv.org/abs/2507.21563v1 Recommendation systems often suffer from data caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models ( s) and item textual descriptions to enrich interaction data. By few-shot prompting s multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines. TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Authors: Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu 2025-07-29 http://arxiv.org/abs/2507.21526v1 Large Language Models ( s) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static attention methods typically degrade accuracy, while dynamic methods introduce additional computational overhead due to runtime index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance inference efficiency. Large Language Models for Wireless Communications From Adaptation to Autonomy Authors: Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li 2025-07-29 http://arxiv.org/abs/2507.21524v1 The emergence of large language models ( s) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless s, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of s in transforming wireless systems across three key directions: adapting pretrained s for core tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic s with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of -based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future. Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess Authors: Zhenwei Tang, Difan Jiao, Eric Xue, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson 2025-07-29 http://arxiv.org/abs/2507.21488v1 As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or ly represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic s, highlighting its potential for broader applications in personalized AI adaptation. An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Authors: Zujie Xie, Zixuan Chen, Jiheng Liang, Xiangyang Yu, Ziru Yu 2025-07-29 http://arxiv.org/abs/2507.21471v1 Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, ping spectral bands that challenge conventional chemometric approaches. Emerging large language models ( s), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an -driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes. Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication Authors: Zhuoran Xiao, Chenhui Ye, Yijia Feng, Yunbo Hu, Tianyu Jiao, Liyu Cai, Guangyi Liu 2025-07-29 http://arxiv.org/abs/2507.21454v1 The rapid advancement in large foundation models is propelling the paradigm shifts across various industries. One significant change is that agents, instead of traditional machines or humans, will be the primary participants in the future production process, which consequently requires a novel AI-native system tailored for agent s. Integrating the ability of large language models ( s) with task-oriented semantic is a potential approach. However, the output of existing is human language, which is highly constrained and sub-optimal for agent-type . In this paper, we innovatively propose a task-oriented agent system. Specifically, we leverage the original to learn a specialized machine language represented by token embeddings. Simultaneously, a multi-modal is trained to comprehend the application task and to extract essential implicit information from multi-modal inputs, subsequently expressing it using machine language tokens. This representation is significantly more efficient for transmission over the air interface. Furthermore, to reduce transmission overhead, we introduce a joint token and channel coding (JTCC) scheme that compresses the token sequence by exploiting its while enhancing robustness against channel noise. Extensive experiments demonstrate that our approach reduces transmission overhead for downstream tasks while enhancing accuracy relative to the SOTA methods. MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Authors: JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao 2025-07-29 http://arxiv.org/abs/2507.21435v1 Brain-computer interface (BCI) spellers can render a new channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world . In this paper, we present MindChat, a large language model ( )-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt s (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\\% keystrokes and over 32\\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by s will potentially lead to truly practical applications. Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization Authors: Yuang Peng, Jiarui Zhong, Yang Zhang, Hong Cai Chen 2025-07-29 http://arxiv.org/abs/2507.21430v1 Parameter extraction for industry-standard device models like ASM-HEMT is crucial in circuit design workflows. However, many manufacturers do not provide such models, leaving users to build them using only datasheets. Unfortunately, datasheets lack sufficient information for standard step-by-step extraction. Moreover, manual data extraction from datasheets is highly time-consuming, and the absence of a fully automated method forces engineers to perform tedious manual work. To address this challenge, this paper introduces a novel, end-to-end framework that fully automates the generation of simulation-ready ASM-HEMT SPICE models directly from PDF datasheets. Our framework is founded on two core innovations: 1) a multi-modal AI pipeline that integrates computer vision with a large language model ( ) to robustly parse heterogeneous datasheet layouts and digitize characteristic curves, and 2) a novel Iterative-Focusing Tree-structured Parzen Estimator (IF-TPE) optimization algorithm is specifically designed for device parameter extraction under the high-dimensional, -data condition by adaptively refining the parameter search space. Experimental validation on a diverse set of 17 commercial HEMT devices from 10 manufacturers confirms the framework's accuracy and robustness. The generated models demonstrate excellent agreement with published DC and RF characteristics. As the first fully automated workflow of its kind, our proposed solution offers a transformative approach to device modeling, poised to significantly accelerate the circuit design cycle by eliminating the need for manual parameter extraction. ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Authors: Chaoyu Li, Yogesh Kulkarni, Pooyan Fazli 2025-07-29 http://arxiv.org/abs/2507.21420v1 The computational cost of training multimodal large language models (M s) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token method for accelerating M training. Specifically, ReGATE adopts a teacher-student framework in which the M being trained serves as the student, and a frozen reference large language model ( ) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon. Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Authors: Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza 2025-07-28 http://arxiv.org/abs/2507.21349v1 Magnetic resonance imaging (MRI) is a crucial medical imaging modality. However, long acquisition times remain a significant challenge, leading to increased costs, and reduced patient comfort. Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans. Integrating this prior information requires registration of the previous scan to the current image reconstruction, which can be time-consuming. We propose a novel deep-learning-based MRI reconstruction framework which consists of an initial reconstruction network, a deep registration model, and a -based enhancement network. We validated our method on a longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18 subjects at four factors (R5, R10, R15, R20). Quantitative metrics confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon signed-rank test). Furthermore, we analyzed the impact of our MRI reconstruction method on the downstream task of brain segmentation and observed improved accuracy and volumetric agreement with reference segmentations. Our approach also achieved a substantial reduction in total reconstruction time compared to methods that use traditional registration algorithms, making it more suitable for real-time clinical applications. The code associated with this work is publicly available at https://github.com/amirshamaei/longitudinal-mri-deep-recon. Agentic Web Weaving the Next Web with AI Agents Authors: Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, Jun Wang 2025-07-28 http://arxiv.org/abs/2507.21206v1 The emergence of AI agents powered by large language models ( s) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web. SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment Authors: Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen 2025-07-28 http://arxiv.org/abs/2507.20984v2 While frontier large language models ( s) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of s natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level structure combining fine-grained Mixture-of-Experts (MoE) with feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid attention mechanism to slash cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger s. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct. The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Authors: Dinh Nam Pham, Eleftherios Avramidis 2025-07-28 http://arxiv.org/abs/2507.20884v2 Non-manual facial features play a crucial role in sign language , yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a -based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR. Latent Inter-User Difference Modeling for LLM Personalization Authors: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng 2025-07-28 http://arxiv.org/abs/2507.20849v1 Large language models ( s) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen . Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP. METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Authors: Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian 2025-07-28 http://arxiv.org/abs/2507.20842v1 Vision encoders serve as the cornerstone of multimodal understanding. Single-encoder architectures like CLIP exhibit inherent constraints in generalizing across diverse multimodal tasks, while recent multi-encoder fusion methods introduce prohibitive computational overhead to achieve superior performance using complementary visual representations from multiple vision encoders. To address this, we propose a progressive framework, namely Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant visual tokens across the encoding, fusion, and decoding stages for multi-encoder M s. For multi-vision encoding, we discard redundant tokens within each encoder via a rank guided collaborative token assignment strategy. Subsequently, for multi-vision fusion, we combine the visual features from different encoders while reducing cross-encoder redundancy with cooperative . Finally, we propose an adaptive token method in the decoding stage to further discard irrelevant tokens based on the text prompts with dynamically adjusting ratios for specific task demands. To our best knowledge, this is the first successful attempt that achieves an efficient multi-encoder based vision language model with multi-stage strategies. Extensive experiments on 11 benchmarks demonstrate the effectiveness of our proposed approach. Compared with EAGLE, a typical multi-encoder M s, METEOR reduces 76% visual tokens with only 0.3% performance drop in average. The code is available at https://github.com/YuchenLiu98/METEOR. Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Authors: Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q. S. Quek 2025-07-28 http://arxiv.org/abs/2507.21199v1 Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models ( s) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple s for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional over wireless networks. The two primary challenges include 1) guiding a single to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community. Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Authors: Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen 2025-07-28 http://arxiv.org/abs/2507.20511v2 Few-shot Learning (FSL), which endeavors to develop the generalization ability for recognizing novel classes using only a few images, faces significant challenges due to data scarcity. Recent CLIP-like methods based on contrastive language-image pertaining mitigate the issue by leveraging textual representation of the class name for unseen image discovery. Despite the achieved success, simply aligning visual representations to class name embeddings would compromise the visual diversity for novel class discrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method (BCT-CLIP) that explores \\textbf{dominating properties} via contrastive learning beyond simply using class tokens. Through leveraging -based prior knowledge, our method pushes forward FSL with comprehensive structural image representations, including both global category representation and the patch-aware property embeddings. In particular, we presented a novel multi-property generator (MPG) with patch-aware cross-attentions to generate multiple visual property tokens, a Large-Language Model ( )-assistant retrieval procedure with clustering-based to obtain dominating property descriptions, and a new contrastive learning strategy for property-token learning. The superior performances on the 11 widely used datasets demonstrate that our investigation of dominating properties advances discriminative class-specific representation learning and few-shot classification. Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Authors: Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot 2025-07-27 http://arxiv.org/abs/2507.20370v1 Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the is prone to hallucinations, which can compromise decision quality. Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation Authors: Abdullah Alabdullah, Lifeng Han, Chenghua Lin 2025-07-27 http://arxiv.org/abs/2507.20301v1 Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models ( s) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies. What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Authors: Katharina Trinley, Toshiki Nakai, Tatiana Anikina, Tanja Baeumel 2025-07-27 http://arxiv.org/abs/2507.20279v1 Large language models ( s) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes internals and inform future cross-lingual transfer research. Modeling Professionalism in Expert Questioning through Linguistic Differentiation Authors: Giulia D'Agostino, Chung-Chi Chen 2025-07-27 http://arxiv.org/abs/2507.20249v1 Professionalism is a crucial yet underexplored dimension of expert , particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model ( )-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling. FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression Authors: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li 2025-07-26 http://arxiv.org/abs/2507.20030v1 The efficacy of Large Language Models ( s) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value ( ) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAED (Frequency-Adaptive Infinite-Window for cache), a novel, training-free cache compression framework that ensures unbiased information retention. FAED operates by transforming the cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAED 's superiority over existing methods by up to 22\\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches. The Carbon Cost of Conversation, Sustainability in the Age of Language Models Authors: Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter 2025-07-26 http://arxiv.org/abs/2507.20018v2 Large language models ( s) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of s, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model , quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being. CaliDrop KV Cache Compression with Calibration Authors: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang 2025-07-26 http://arxiv.org/abs/2507.19906v1 Large Language Models ( s) require substantial computational resources during generation. While the Key-Value ( ) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often , allowing for the removal of less critical entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods. CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation Authors: Zhanhang Xiong, Dongxia Wang, Yuekang Li, Xinyuan An, Wenhai Wang 2025-07-26 http://arxiv.org/abs/2507.19904v1 As large language models ( s) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate s' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an -based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose s and 6 code-oriented s released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: https://anonymous.4open.science/r/crosspl-2814. AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation Authors: Sourena Khanzadeh 2025-07-26 http://arxiv.org/abs/2507.19902v1 Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating -powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their , and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation. HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Authors: Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao 2025-07-26 http://arxiv.org/abs/2507.19823v1 Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value ( ) cache during inference. Existing cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory. Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation Authors: Xin Zhang, Lissette Iturburu, Juan Nicolas Villamizar, Xiaoyu Liu, Manuel Salmeron, Shirley J. Dyke, Julio Ramirez 2025-07-26 http://arxiv.org/abs/2507.19771v1 Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model ( ) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way. LowKeyEMG Electromyographic typing with a reduced keyset Authors: Johannes Y. Lee, Derek Xiao, Shreyas Kaasyap, Nima R. Hadidi, John L. Zhou, Jacob Cunningham, Rakshith R. Gore, Deniz O. Eren, Jonathan C. Kao 2025-07-26 http://arxiv.org/abs/2507.19736v1 We introduce LowKeyEMG, a real-time human-computer interface that enables efficient text entry using only 7 gesture classes decoded from surface electromyography (sEMG). Prior work has attempted full-alphabet decoding from sEMG, but decoding large character sets remains unreliable, especially for individuals with motor impairments. Instead, LowKeyEMG reduces the English alphabet to 4 gesture keys, with 3 more for space and system interaction, to reliably translate simple one-handed gestures into text, leveraging the recurrent -based language model RW for efficient computation. In real-time experiments, participants achieved average one-handed keyboardless typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture efficiency by 17% (relative to typed phrase length). When typing with only 7 keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this low-key typing paradigm can maintain practical rates. Our results have implications for assistive technologies and any interface where input bandwidth is constrained. Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks Authors: Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani 2025-07-25 http://arxiv.org/abs/2507.19699v1 Although s have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models ( s) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA S as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting. \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems Authors: Beining Wu, Jun Huang, Shui Yu 2025-07-25 http://arxiv.org/abs/2507.19657v1 The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/ dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV s, ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ... DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Authors: Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen 2025-07-25 http://arxiv.org/abs/2507.19608v1 Deploying Large Language Models ( s) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present Delta , a training-free framework that exploits temporal in attention patterns to enable efficient inference across both the prefilling and decoding stages, on resource-constrained edge devices. Delta introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal , and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that Delta offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines. Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts Authors: Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel 2025-07-25 http://arxiv.org/abs/2507.19477v1 Many recent papers have studied the development of superforecaster-level event forecasting s. While methodological problems with early studies cast doubt on the use of s for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art s are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting s. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of -based event forecasting training: noisiness- , knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions. GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Authors: Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab 2025-07-25 http://arxiv.org/abs/2507.19457v1 Large language models ( s) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for s, compared with policy gradients derived from , scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two s, and demonstrates promising results as an inference-time search strategy for code optimization. Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Authors: StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang 2025-07-25 http://arxiv.org/abs/2507.19427v1 Large language models ( s) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE , and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for decoding. Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Authors: Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci 2025-07-25 http://arxiv.org/abs/2507.19334v1 Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models ( s) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures dependencies via an -induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over -based baselines. Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers Authors: Yuki Igaue, Hiroaki Aizawa 2025-07-25 http://arxiv.org/abs/2507.19175v1 Multi-head self-attention is a distinctive feature extraction mechanism of vision s that computes pairwise relationships among all input patches, contributing significantly to their high performance. However, it is known to incur a quadratic computational complexity with respect to the number of patches. One promising approach to address this issue is patch , which improves computational efficiency by identifying and removing redundant patches. In this work, we propose a patch strategy that evaluates the importance of each patch based on the variance of attention weights across multiple attention heads. This approach is inspired by the design of multi-head self-attention, which aims to capture diverse attention patterns across different subspaces of feature representations. The proposed method can be easily applied during both training and inference, and achieves improved throughput while maintaining classification accuracy in scenarios such as fine-tuning with pre-trained models. In addition, we also found that using robust statistical measures, such as the median absolute deviation in place of variance, to assess patch importance can similarly lead to strong performance. Furthermore, by introducing ping patch embeddings, our method achieves better performance with comparable throughput to conventional approaches that utilize all patches. RegScore Scoring Systems for Regression Tasks Authors: Michal K. Grzeszczyk, Tomasz Szczepa\u0144ski, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzci\u0144ski, Arkadiusz Sitek 2025-07-25 http://arxiv.org/abs/2507.19155v1 Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, , and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and k- ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at https://github.com/SanoScience/RegScore. MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Authors: Weitian Wang, Rai Shubham, Cecilia De La Parra, Akash Kumar 2025-07-25 http://arxiv.org/abs/2507.19131v1 In this paper, we propose MixA-Q, a mixed-precision activation quantization framework that leverages intra-layer activation (a concept widely explored in activation methods) for efficient inference of quantized window-based vision s. For a given uniform-bit quantization configuration, MixA-Q separates the batched window computations within Swin blocks and assigns a lower bit width to the activations of less important windows, improving the trade-off between model performance and efficiency. We introduce a Two-Branch Swin Block that processes activations separately in high- and low-bit precision, enabling seamless integration of our method with most quantization-aware training (QAT) and post-training quantization (PTQ) methods, or with simple modifications. Our experimental evaluations over the COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x computational speedup without accuracy loss in PTQ configuration. With QAT, MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP drop by incorporating activation . Notably, by reducing the quantization error in important regions, our -aware quantization adaptation improves the mAP of the quantized W4A4 model (with both weights and activations in 4-bit precision) by 0.7%, reducing quantization degradation by 24%. Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events Authors: Tianyi Li, Flavio Tuteri, Michele Buzzicotti, Fabio Bonaccorso, Luca Biferale 2025-07-25 http://arxiv.org/abs/2507.19103v1 Modeling Lagrangian turbulence remains a fundamental challenge due to its multiscale, intermittent, and non-Gaussian nature. Recent advances in data-driven diffusion models have enabled the generation of realistic Lagrangian velocity trajectories that accurately reproduce statistical properties across scales and capture rare extreme events. This study investigates three key aspects of diffusion-based modeling for Lagrangian turbulence. First, we assess architectural robustness by comparing a U-Net backbone with a -based alternative, finding strong consistency in generated trajectories, with only minor discrepancies at small scales. Second, leveraging a deterministic variant of diffusion model formulation, namely the deterministic denoising diffusion implicit model (DDIM), we identify structured features in the initial latent noise that align consistently with extreme events. Third, we explore accelerated generation by reducing the number of diffusion steps, and find that DDIM enables substantial speedups with minimal loss of statistical fidelity. These findings highlight the robustness of diffusion models and their potential for interpretable, scalable modeling of complex turbulent systems.","title":"2025-08-01"},{"location":"weekly_paper/2025-08-01/#2025-08-01","text":"","title":"2025-08-01"},{"location":"weekly_paper/2025-08-01/#table-of-contents","text":"DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling Unveiling Super Experts in Mixture-of-Experts Large Language Models Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling Large Language Models for Wireless Communications From Adaptation to Autonomy Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization ReGATE Learning Faster and Better with Fewer Tokens in MLLMs Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging Agentic Web Weaving the Next Web with AI Agents SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face? Latent Inter-User Difference Modeling for LLM Personalization METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations Modeling Professionalism in Expert Questioning through Linguistic Differentiation FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression The Carbon Cost of Conversation, Sustainability in the Age of Language Models CaliDrop KV Cache Compression with Calibration CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation LowKeyEMG Electromyographic typing with a reduced keyset Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks \"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers RegScore Scoring Systems for Regression Tasks MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events","title":"Table of Contents"},{"location":"weekly_paper/2025-08-01/#depmicrodiff-diffusion-based-dependency-aware-multimodal-imputation-for-microbiome-data","text":"Authors: Rabeya Tus Sadia, Qiang Cheng 2025-07-31 http://arxiv.org/abs/2507.23676v1 Microbiome data analysis is essential for understanding host health and disease, yet its inherent and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model ( ). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.","title":"DepMicroDiff Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data"},{"location":"weekly_paper/2025-08-01/#memocue-empowering-llm-based-agents-for-human-memory-recall-via-strategy-guided-querying","text":"Authors: Qian Zhao, Zhuo Sun, Bin Guo, Zhiwen Yu 2025-07-31 http://arxiv.org/abs/2507.23633v1 Agent-assisted memory recall is one critical research problem in the field of human-computer interaction. In conventional methods, the agent can retrieve information from its equipped memory module to help the person recall incomplete or vague memories. The limited size of memory module hinders the acquisition of complete memories and impacts the memory recall performance in practice. Memory theories suggest that the person's relevant memory can be proactively activated through some effective cues. Inspired by this, we propose a novel strategy-guided agent-assisted memory recall method, allowing the agent to transform an original query into a cue-rich one via the judiciously designed strategy to help the person recall memories. To this end, there are two key challenges. (1) How to choose the appropriate recall strategy for diverse forgetting scenarios with distinct memory-recall characteristics? (2) How to obtain the high-quality responses leveraging recall strategies, given only abstract and ly annotated strategy patterns? To address the challenges, we propose a Recall Router framework. Specifically, we design a 5W Recall Map to classify memory queries into five typical scenarios and define fifteen recall strategy patterns across the corresponding scenarios. We then propose a hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to optimize the selection of strategy and the generation of strategy responses. We construct an instruction tuning dataset and fine-tune multiple open-source large language models ( s) to develop MemoCue, an agent that excels in providing memory-inspired responses. Experiments on three representative datasets show that MemoCue surpasses -based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications.","title":"MemoCue Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying"},{"location":"weekly_paper/2025-08-01/#trae-agent-an-llm-based-agent-for-software-engineering-with-test-time-scaling","text":"Authors: Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu 2025-07-31 http://arxiv.org/abs/2507.23370v1 Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models ( s), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of -based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, , and selection. We conduct extensive experiments using three leading s on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent.","title":"Trae Agent An LLM-based Agent for Software Engineering with Test-time Scaling"},{"location":"weekly_paper/2025-08-01/#unveiling-super-experts-in-mixture-of-experts-large-language-models","text":"Authors: Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan 2025-07-31 http://arxiv.org/abs/2507.23279v1 Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models ( s). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE s. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE s, and despite their limited number, them leads to a significant decline in model performance (e.g., three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE s rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE . The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.","title":"Unveiling Super Experts in Mixture-of-Experts Large Language Models"},{"location":"weekly_paper/2025-08-01/#model-directions-not-words-mechanistic-topic-models-using-sparse-autoencoders","text":"Authors: Carolina Zheng, Nicolas Beltran-Velez, Sweta Karlekar, Claudia Shi, Achille Nazaret, Asif Mallik, Amir Feder, David M. Blei 2025-07-31 http://arxiv.org/abs/2507.23220v1 Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \\textit{topic judge}, an -based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of outputs.","title":"Model Directions, Not Words Mechanistic Topic Models Using Sparse Autoencoders"},{"location":"weekly_paper/2025-08-01/#mocha-advanced-vision-language-reasoning-with-moe-connector-and-hierarchical-group-attention","text":"Authors: Yuqi Pang, Bowen Yang, Yun Cao, Fan Rong, Xiaoyu Li, Chen He 2025-07-30 http://arxiv.org/abs/2507.22805v1 Vision large language models (V s) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream s (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA.","title":"MoCHA Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention"},{"location":"weekly_paper/2025-08-01/#modality-aware-feature-matching-a-comprehensive-review-of-single-and-cross-modality-techniques","text":"Authors: Weide Liu, Wei Zhou, Jun Liu, Ping Hu, Jun Cheng, Jungong Han, Weisi Lin 2025-07-30 http://arxiv.org/abs/2507.22791v1 Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and -based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions.","title":"Modality-Aware Feature Matching A Comprehensive Review of Single- and Cross-Modality Techniques"},{"location":"weekly_paper/2025-08-01/#traice3d-a-prompt-driven-transformer-based-u-net-for-semantic-segmentation-of-microglial-cells-from-large-scale-3d-microscopy-images","text":"Authors: MohammadAmin Alamalhoda, Arsalan Firoozi, Alessandro Venturino, Sandra Siegert 2025-07-30 http://arxiv.org/abs/2507.22635v1 The shape of a cell contains essential information about its function within the biological system. Segmenting these structures from large-scale 3D microscopy images is challenging, limiting clinical insights especially for microglia, immune-associated cells involved in neurodegenerative diseases. Existing segmentation methods mainly focus on cell bodies, struggle with ping structures, perform poorly on noisy images, require hyperparameter tuning for each new dataset, or rely on tedious semi-automated approaches. We introduce trAIce3D, a deep-learning architecture designed for precise microglia segmentation, capturing both somas and branches. It employs a two-stage approach: first, a 3D U-Net with vision s in the encoder detects somas using a sliding-window technique to cover the entire image. Then, the same architecture, enhanced with cross-attention blocks in skip connections, refines each soma and its branches by using soma coordinates as a prompt and a 3D window around the target cell as input. Training occurs in two phases: self-supervised Soma Segmentation, followed by prompt-based Branch Segmentation, leveraging pre-trained weights from the first phase. Trained and evaluated on a dataset of 41,230 microglial cells, trAIce3D significantly improves segmentation accuracy and generalization, enabling scalable analysis of complex cellular morphologies. While optimized for microglia, its architecture can extend to other intricate cell types, such as neurons and astrocytes, broadening its impact on neurobiological research.","title":"trAIce3D A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images"},{"location":"weekly_paper/2025-08-01/#language-arithmetics-towards-systematic-language-neuron-identification-and-manipulation","text":"Authors: Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann 2025-07-30 http://arxiv.org/abs/2507.22608v1 Large language models ( s) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share ping neurons, reflecting internal representations of linguistic proximity. Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal \"fallback\" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.","title":"Language Arithmetics Towards Systematic Language Neuron Identification and Manipulation"},{"location":"weekly_paper/2025-08-01/#metaagent-automatically-constructing-multi-agent-systems-based-on-finite-state-machines","text":"Authors: Yaolun Zhang, Xiaogeng Liu, Chaowei Xiao 2025-07-30 http://arxiv.org/abs/2507.22606v1 Large Language Models ( s) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks.","title":"MetaAgent Automatically Constructing Multi-Agent Systems Based on Finite State Machines"},{"location":"weekly_paper/2025-08-01/#multi-modal-relational-item-representation-learning-for-inferring-substitutable-and-complementary-items","text":"Authors: Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram 2025-07-29 http://arxiv.org/abs/2507.22268v1 We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage s to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items.","title":"Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items"},{"location":"weekly_paper/2025-08-01/#ctg-insight-a-multi-agent-interpretable-llm-framework-for-cardiotocography-analysis-and-classification","text":"Authors: Black Sun, Die, Hu 2025-07-29 http://arxiv.org/abs/2507.22205v1 Remote fetal monitoring technologies are becoming increasingly common. Yet, most current systems offer limited interpretability, leaving expectant parents with raw cardiotocography (CTG) data that is difficult to understand. In this work, we present CTG-Insight, a multi-agent system that provides structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals. Drawing from established medical guidelines, CTG-Insight decomposes each CTG trace into five medically defined features: baseline, variability, s, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. A final aggregation agent synthesizes the outputs to deliver a holistic classification of fetal health, accompanied by a natural language explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare it against deep learning models and the single-agent baseline. Results show that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) while producing transparent and interpretable outputs. This work contributes an interpretable and extensible CTG analysis framework.","title":"CTG-Insight A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification"},{"location":"weekly_paper/2025-08-01/#persona-augmented-benchmarking-evaluating-llms-across-diverse-writing-styles","text":"Authors: Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu 2025-07-29 http://arxiv.org/abs/2507.22168v1 Current benchmarks for evaluating Large Language Models ( s) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of patterns exhibited by humans. Thus, it is possible that s, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring performance across linguistic variations.","title":"Persona-Augmented Benchmarking Evaluating LLMs Across Diverse Writing Styles"},{"location":"weekly_paper/2025-08-01/#intentflow-interactive-support-for-communicating-intent-with-llms-in-writing-tasks","text":"Authors: Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim 2025-07-29 http://arxiv.org/abs/2507.22134v1 While large language models ( s) are widely used for writing, users often struggle to express their nuanced and evolving intents through prompt-based interfaces. Intents -- low-level strategies or preferences for achieving a writing goal -- are often vague, fluid, or even subconscious, making it difficult for users to articulate and adjust them. To address this, we present IntentFlow, which supports the of dynamically evolving intents throughout -assisted writing. IntentFlow extracts goals and intents from user prompts and presents them as editable interface components, which users can revise, remove, or refine via direct manipulation or follow-up prompts. Visual links connect each component to the output segments it influences, helping users understand model behavior. In a within-subjects study (N=12), participants using IntentFlow, compared to a chat-based baseline, expressed their intents more easily and in detail, engaged in more meaningful actions to communicate intents, such as adjusting and deleting, and produced outputs that better aligned with their evolving intents. We found that editable intent representations help users refine and consolidate a final set of intents, which can be reused across similar tasks to support consistent and transferable -assisted writing.","title":"IntentFlow Interactive Support for Communicating Intent with LLMs in Writing Tasks"},{"location":"weekly_paper/2025-08-01/#predicting-microbial-ontology-and-pathogen-risk-from-environmental-metadata-with-large-language-models","text":"Authors: Hyunwoo Yoo, Gail L. Rosen 2025-07-29 http://arxiv.org/abs/2507.21980v1 Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models ( s) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate s such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that s not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that s can effectively reason over , heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.","title":"Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models"},{"location":"weekly_paper/2025-08-01/#edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity","text":"Authors: Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang 2025-07-29 http://arxiv.org/abs/2507.21848v1 Large Language Models ( s) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO.","title":"EDGE-GRPO Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity"},{"location":"weekly_paper/2025-08-01/#unlocking-interpretability-for-rf-sensing-a-complex-valued-white-box-transformer","text":"Authors: Xie Zhang, Yina Wang, Chenshu Wu 2025-07-29 http://arxiv.org/abs/2507.21799v1 The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box s, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.","title":"Unlocking Interpretability for RF Sensing A Complex-Valued White-Box Transformer"},{"location":"weekly_paper/2025-08-01/#mor-vit-efficient-vision-transformer-with-mixture-of-recursions","text":"Authors: YiZhou Li 2025-07-29 http://arxiv.org/abs/2507.21761v1 Vision Transformers (ViTs) have achieved remarkable success in image recognition, yet standard ViT architectures are hampered by substantial parameter redundancy and high computational cost, limiting their practical deployment. While recent efforts on efficient ViTs primarily focus on static model compression or token-level sparsification, they remain constrained by fixed computational depth for all tokens. In this work, we present MoR-ViT, a novel vision framework that, for the first time, incorporates a token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions (MoR) paradigm. This approach enables each token to adaptively determine its processing depth, yielding a flexible and input-dependent allocation of computational resources. Extensive experiments on ImageNet-1K and transfer benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy with up to 70% parameter reduction and 2.5x inference , but also outperforms leading efficient ViT baselines such as DynamicViT and TinyViT under comparable conditions. These results establish dynamic recursion as an effective strategy for efficient vision s and open new avenues for scalable and deployable deep learning models in real-world scenarios.","title":"MOR-VIT Efficient Vision Transformer with Mixture-of-Recursions"},{"location":"weekly_paper/2025-08-01/#enhancing-graph-based-recommendations-with-majority-voting-llm-rerank-augmentation","text":"Authors: Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le 2025-07-29 http://arxiv.org/abs/2507.21563v1 Recommendation systems often suffer from data caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models ( s) and item textual descriptions to enrich interaction data. By few-shot prompting s multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines.","title":"Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation"},{"location":"weekly_paper/2025-08-01/#trianglemix-a-lossless-and-efficient-attention-pattern-for-long-context-prefilling","text":"Authors: Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu 2025-07-29 http://arxiv.org/abs/2507.21526v1 Large Language Models ( s) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static attention methods typically degrade accuracy, while dynamic methods introduce additional computational overhead due to runtime index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance inference efficiency.","title":"TriangleMix A Lossless and Efficient Attention Pattern for Long Context Prefilling"},{"location":"weekly_paper/2025-08-01/#large-language-models-for-wireless-communications-from-adaptation-to-autonomy","text":"Authors: Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li 2025-07-29 http://arxiv.org/abs/2507.21524v1 The emergence of large language models ( s) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless s, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of s in transforming wireless systems across three key directions: adapting pretrained s for core tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic s with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of -based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future.","title":"Large Language Models for Wireless Communications From Adaptation to Autonomy"},{"location":"weekly_paper/2025-08-01/#learning-to-imitate-with-less-efficient-individual-behavior-modeling-in-chess","text":"Authors: Zhenwei Tang, Difan Jiao, Eric Xue, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson 2025-07-29 http://arxiv.org/abs/2507.21488v1 As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or ly represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic s, highlighting its potential for broader applications in personalized AI adaptation.","title":"Learning to Imitate with Less Efficient Individual Behavior Modeling in Chess"},{"location":"weekly_paper/2025-08-01/#an-llm-driven-agent-framework-for-automated-infrared-spectral-multi-task-reasoning","text":"Authors: Zujie Xie, Zixuan Chen, Jiheng Liang, Xiangyang Yu, Ziru Yu 2025-07-29 http://arxiv.org/abs/2507.21471v1 Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, ping spectral bands that challenge conventional chemometric approaches. Emerging large language models ( s), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an -driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes.","title":"An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning"},{"location":"weekly_paper/2025-08-01/#transmission-with-machine-language-tokens-a-paradigm-for-task-oriented-agent-communication","text":"Authors: Zhuoran Xiao, Chenhui Ye, Yijia Feng, Yunbo Hu, Tianyu Jiao, Liyu Cai, Guangyi Liu 2025-07-29 http://arxiv.org/abs/2507.21454v1 The rapid advancement in large foundation models is propelling the paradigm shifts across various industries. One significant change is that agents, instead of traditional machines or humans, will be the primary participants in the future production process, which consequently requires a novel AI-native system tailored for agent s. Integrating the ability of large language models ( s) with task-oriented semantic is a potential approach. However, the output of existing is human language, which is highly constrained and sub-optimal for agent-type . In this paper, we innovatively propose a task-oriented agent system. Specifically, we leverage the original to learn a specialized machine language represented by token embeddings. Simultaneously, a multi-modal is trained to comprehend the application task and to extract essential implicit information from multi-modal inputs, subsequently expressing it using machine language tokens. This representation is significantly more efficient for transmission over the air interface. Furthermore, to reduce transmission overhead, we introduce a joint token and channel coding (JTCC) scheme that compresses the token sequence by exploiting its while enhancing robustness against channel noise. Extensive experiments demonstrate that our approach reduces transmission overhead for downstream tasks while enhancing accuracy relative to the SOTA methods.","title":"Transmission With Machine Language Tokens A Paradigm for Task-Oriented Agent Communication"},{"location":"weekly_paper/2025-08-01/#mindchat-enhancing-bci-spelling-with-large-language-models-in-realistic-scenarios","text":"Authors: JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao 2025-07-29 http://arxiv.org/abs/2507.21435v1 Brain-computer interface (BCI) spellers can render a new channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world . In this paper, we present MindChat, a large language model ( )-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt s (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\\% keystrokes and over 32\\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by s will potentially lead to truly practical applications.","title":"MindChat Enhancing BCI Spelling with Large Language Models in Realistic Scenarios"},{"location":"weekly_paper/2025-08-01/#automated-hemt-model-construction-from-datasheets-via-multi-modal-intelligence-and-prior-knowledge-free-optimization","text":"Authors: Yuang Peng, Jiarui Zhong, Yang Zhang, Hong Cai Chen 2025-07-29 http://arxiv.org/abs/2507.21430v1 Parameter extraction for industry-standard device models like ASM-HEMT is crucial in circuit design workflows. However, many manufacturers do not provide such models, leaving users to build them using only datasheets. Unfortunately, datasheets lack sufficient information for standard step-by-step extraction. Moreover, manual data extraction from datasheets is highly time-consuming, and the absence of a fully automated method forces engineers to perform tedious manual work. To address this challenge, this paper introduces a novel, end-to-end framework that fully automates the generation of simulation-ready ASM-HEMT SPICE models directly from PDF datasheets. Our framework is founded on two core innovations: 1) a multi-modal AI pipeline that integrates computer vision with a large language model ( ) to robustly parse heterogeneous datasheet layouts and digitize characteristic curves, and 2) a novel Iterative-Focusing Tree-structured Parzen Estimator (IF-TPE) optimization algorithm is specifically designed for device parameter extraction under the high-dimensional, -data condition by adaptively refining the parameter search space. Experimental validation on a diverse set of 17 commercial HEMT devices from 10 manufacturers confirms the framework's accuracy and robustness. The generated models demonstrate excellent agreement with published DC and RF characteristics. As the first fully automated workflow of its kind, our proposed solution offers a transformative approach to device modeling, poised to significantly accelerate the circuit design cycle by eliminating the need for manual parameter extraction.","title":"Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization"},{"location":"weekly_paper/2025-08-01/#regate-learning-faster-and-better-with-fewer-tokens-in-mllms","text":"Authors: Chaoyu Li, Yogesh Kulkarni, Pooyan Fazli 2025-07-29 http://arxiv.org/abs/2507.21420v1 The computational cost of training multimodal large language models (M s) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token method for accelerating M training. Specifically, ReGATE adopts a teacher-student framework in which the M being trained serves as the student, and a frozen reference large language model ( ) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon.","title":"ReGATE Learning Faster and Better with Fewer Tokens in MLLMs"},{"location":"weekly_paper/2025-08-01/#enhancing-and-accelerating-brain-mri-through-deep-learning-reconstruction-using-prior-subject-specific-imaging","text":"Authors: Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza 2025-07-28 http://arxiv.org/abs/2507.21349v1 Magnetic resonance imaging (MRI) is a crucial medical imaging modality. However, long acquisition times remain a significant challenge, leading to increased costs, and reduced patient comfort. Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans. Integrating this prior information requires registration of the previous scan to the current image reconstruction, which can be time-consuming. We propose a novel deep-learning-based MRI reconstruction framework which consists of an initial reconstruction network, a deep registration model, and a -based enhancement network. We validated our method on a longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18 subjects at four factors (R5, R10, R15, R20). Quantitative metrics confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon signed-rank test). Furthermore, we analyzed the impact of our MRI reconstruction method on the downstream task of brain segmentation and observed improved accuracy and volumetric agreement with reference segmentations. Our approach also achieved a substantial reduction in total reconstruction time compared to methods that use traditional registration algorithms, making it more suitable for real-time clinical applications. The code associated with this work is publicly available at https://github.com/amirshamaei/longitudinal-mri-deep-recon.","title":"Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging"},{"location":"weekly_paper/2025-08-01/#agentic-web-weaving-the-next-web-with-ai-agents","text":"Authors: Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, Jun Wang 2025-07-28 http://arxiv.org/abs/2507.21206v1 The emergence of AI agents powered by large language models ( s) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.","title":"Agentic Web Weaving the Next Web with AI Agents"},{"location":"weekly_paper/2025-08-01/#smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment","text":"Authors: Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen 2025-07-28 http://arxiv.org/abs/2507.20984v2 While frontier large language models ( s) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of s natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level structure combining fine-grained Mixture-of-Experts (MoE) with feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid attention mechanism to slash cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger s. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.","title":"SmallThinker A Family of Efficient Large Language Models Natively Trained for Local Deployment"},{"location":"weekly_paper/2025-08-01/#the-importance-of-facial-features-in-vision-based-sign-language-recognition-eyes-mouth-or-full-face","text":"Authors: Dinh Nam Pham, Eleftherios Avramidis 2025-07-28 http://arxiv.org/abs/2507.20884v2 Non-manual facial features play a crucial role in sign language , yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a -based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR.","title":"The Importance of Facial Features in Vision-based Sign Language Recognition Eyes, Mouth or Full Face?"},{"location":"weekly_paper/2025-08-01/#latent-inter-user-difference-modeling-for-llm-personalization","text":"Authors: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng 2025-07-28 http://arxiv.org/abs/2507.20849v1 Large language models ( s) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen . Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.","title":"Latent Inter-User Difference Modeling for LLM Personalization"},{"location":"weekly_paper/2025-08-01/#meteor-multi-encoder-collaborative-token-pruning-for-efficient-vision-language-models","text":"Authors: Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian 2025-07-28 http://arxiv.org/abs/2507.20842v1 Vision encoders serve as the cornerstone of multimodal understanding. Single-encoder architectures like CLIP exhibit inherent constraints in generalizing across diverse multimodal tasks, while recent multi-encoder fusion methods introduce prohibitive computational overhead to achieve superior performance using complementary visual representations from multiple vision encoders. To address this, we propose a progressive framework, namely Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant visual tokens across the encoding, fusion, and decoding stages for multi-encoder M s. For multi-vision encoding, we discard redundant tokens within each encoder via a rank guided collaborative token assignment strategy. Subsequently, for multi-vision fusion, we combine the visual features from different encoders while reducing cross-encoder redundancy with cooperative . Finally, we propose an adaptive token method in the decoding stage to further discard irrelevant tokens based on the text prompts with dynamically adjusting ratios for specific task demands. To our best knowledge, this is the first successful attempt that achieves an efficient multi-encoder based vision language model with multi-stage strategies. Extensive experiments on 11 benchmarks demonstrate the effectiveness of our proposed approach. Compared with EAGLE, a typical multi-encoder M s, METEOR reduces 76% visual tokens with only 0.3% performance drop in average. The code is available at https://github.com/YuchenLiu98/METEOR.","title":"METEOR Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models"},{"location":"weekly_paper/2025-08-01/#advancing-compositional-llm-reasoning-with-structured-task-relations-in-interactive-multimodal-communications","text":"Authors: Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q. S. Quek 2025-07-28 http://arxiv.org/abs/2507.21199v1 Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models ( s) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple s for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional over wireless networks. The two primary challenges include 1) guiding a single to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.","title":"Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications"},{"location":"weekly_paper/2025-08-01/#beyond-class-tokens-llm-guided-dominant-property-mining-for-few-shot-classification","text":"Authors: Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen 2025-07-28 http://arxiv.org/abs/2507.20511v2 Few-shot Learning (FSL), which endeavors to develop the generalization ability for recognizing novel classes using only a few images, faces significant challenges due to data scarcity. Recent CLIP-like methods based on contrastive language-image pertaining mitigate the issue by leveraging textual representation of the class name for unseen image discovery. Despite the achieved success, simply aligning visual representations to class name embeddings would compromise the visual diversity for novel class discrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method (BCT-CLIP) that explores \\textbf{dominating properties} via contrastive learning beyond simply using class tokens. Through leveraging -based prior knowledge, our method pushes forward FSL with comprehensive structural image representations, including both global category representation and the patch-aware property embeddings. In particular, we presented a novel multi-property generator (MPG) with patch-aware cross-attentions to generate multiple visual property tokens, a Large-Language Model ( )-assistant retrieval procedure with clustering-based to obtain dominating property descriptions, and a new contrastive learning strategy for property-token learning. The superior performances on the 11 widely used datasets demonstrate that our investigation of dominating properties advances discriminative class-specific representation learning and few-shot classification.","title":"Beyond Class Tokens LLM-guided Dominant Property Mining for Few-shot Classification"},{"location":"weekly_paper/2025-08-01/#advancing-shared-and-multi-agent-autonomy-in-underwater-missions-integrating-knowledge-graphs-and-retrieval-augmented-generation","text":"Authors: Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot 2025-07-27 http://arxiv.org/abs/2507.20370v1 Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the is prone to hallucinations, which can compromise decision quality.","title":"Advancing Shared and Multi-Agent Autonomy in Underwater Missions Integrating Knowledge Graphs and Retrieval-Augmented Generation"},{"location":"weekly_paper/2025-08-01/#advancing-dialectal-arabic-to-modern-standard-arabic-machine-translation","text":"Authors: Abdullah Alabdullah, Lifeng Han, Chenghua Lin 2025-07-27 http://arxiv.org/abs/2507.20301v1 Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models ( s) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.","title":"Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation"},{"location":"weekly_paper/2025-08-01/#what-languages-does-aya-23-think-in-how-multilinguality-affects-internal-language-representations","text":"Authors: Katharina Trinley, Toshiki Nakai, Tatiana Anikina, Tanja Baeumel 2025-07-27 http://arxiv.org/abs/2507.20279v1 Large language models ( s) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes internals and inform future cross-lingual transfer research.","title":"What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations"},{"location":"weekly_paper/2025-08-01/#modeling-professionalism-in-expert-questioning-through-linguistic-differentiation","text":"Authors: Giulia D'Agostino, Chung-Chi Chen 2025-07-27 http://arxiv.org/abs/2507.20249v1 Professionalism is a crucial yet underexplored dimension of expert , particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model ( )-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.","title":"Modeling Professionalism in Expert Questioning through Linguistic Differentiation"},{"location":"weekly_paper/2025-08-01/#faedkv-infinite-window-fourier-transform-for-unbiased-kv-cache-compression","text":"Authors: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li 2025-07-26 http://arxiv.org/abs/2507.20030v1 The efficacy of Large Language Models ( s) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value ( ) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAED (Frequency-Adaptive Infinite-Window for cache), a novel, training-free cache compression framework that ensures unbiased information retention. FAED operates by transforming the cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAED 's superiority over existing methods by up to 22\\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.","title":"FAEDKV Infinite-Window Fourier Transform for Unbiased KV Cache Compression"},{"location":"weekly_paper/2025-08-01/#the-carbon-cost-of-conversation-sustainability-in-the-age-of-language-models","text":"Authors: Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter 2025-07-26 http://arxiv.org/abs/2507.20018v2 Large language models ( s) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of s, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model , quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.","title":"The Carbon Cost of Conversation, Sustainability in the Age of Language Models"},{"location":"weekly_paper/2025-08-01/#calidrop-kv-cache-compression-with-calibration","text":"Authors: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang 2025-07-26 http://arxiv.org/abs/2507.19906v1 Large Language Models ( s) require substantial computational resources during generation. While the Key-Value ( ) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often , allowing for the removal of less critical entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.","title":"CaliDrop KV Cache Compression with Calibration"},{"location":"weekly_paper/2025-08-01/#crosspl-evaluating-large-language-models-on-cross-programming-language-code-generation","text":"Authors: Zhanhang Xiong, Dongxia Wang, Yuekang Li, Xinyuan An, Wenhai Wang 2025-07-26 http://arxiv.org/abs/2507.19904v1 As large language models ( s) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate s' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an -based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose s and 6 code-oriented s released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: https://anonymous.4open.science/r/crosspl-2814.","title":"CrossPL Evaluating Large Language Models on Cross Programming Language Code Generation"},{"location":"weekly_paper/2025-08-01/#agentmesh-a-cooperative-multi-agent-generative-ai-framework-for-software-development-automation","text":"Authors: Sourena Khanzadeh 2025-07-26 http://arxiv.org/abs/2507.19902v1 Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating -powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their , and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation.","title":"AgentMesh A Cooperative Multi-Agent Generative AI Framework for Software Development Automation"},{"location":"weekly_paper/2025-08-01/#hcattention-extreme-kv-cache-compression-via-heterogeneous-attention-computing-for-llms","text":"Authors: Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao 2025-07-26 http://arxiv.org/abs/2507.19823v1 Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value ( ) cache during inference. Existing cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.","title":"HCAttention Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs"},{"location":"weekly_paper/2025-08-01/#large-language-model-agent-for-structural-drawing-generation-using-react-prompt-engineering-and-retrieval-augmented-generation","text":"Authors: Xin Zhang, Lissette Iturburu, Juan Nicolas Villamizar, Xiaoyu Liu, Manuel Salmeron, Shirley J. Dyke, Julio Ramirez 2025-07-26 http://arxiv.org/abs/2507.19771v1 Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model ( ) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.","title":"Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation"},{"location":"weekly_paper/2025-08-01/#lowkeyemg-electromyographic-typing-with-a-reduced-keyset","text":"Authors: Johannes Y. Lee, Derek Xiao, Shreyas Kaasyap, Nima R. Hadidi, John L. Zhou, Jacob Cunningham, Rakshith R. Gore, Deniz O. Eren, Jonathan C. Kao 2025-07-26 http://arxiv.org/abs/2507.19736v1 We introduce LowKeyEMG, a real-time human-computer interface that enables efficient text entry using only 7 gesture classes decoded from surface electromyography (sEMG). Prior work has attempted full-alphabet decoding from sEMG, but decoding large character sets remains unreliable, especially for individuals with motor impairments. Instead, LowKeyEMG reduces the English alphabet to 4 gesture keys, with 3 more for space and system interaction, to reliably translate simple one-handed gestures into text, leveraging the recurrent -based language model RW for efficient computation. In real-time experiments, participants achieved average one-handed keyboardless typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture efficiency by 17% (relative to typed phrase length). When typing with only 7 keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this low-key typing paradigm can maintain practical rates. Our results have implications for assistive technologies and any interface where input bandwidth is constrained.","title":"LowKeyEMG Electromyographic typing with a reduced keyset"},{"location":"weekly_paper/2025-08-01/#towards-inclusive-nlp-assessing-compressed-multilingual-transformers-across-diverse-language-benchmarks","text":"Authors: Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani 2025-07-25 http://arxiv.org/abs/2507.19699v1 Although s have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models ( s) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA S as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting.","title":"Towards Inclusive NLP Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks"},{"location":"weekly_paper/2025-08-01/#x-of-information-continuum-a-survey-on-ai-driven-multi-dimensional-metrics-for-next-generation-networked-systems","text":"Authors: Beining Wu, Jun Huang, Shui Yu 2025-07-25 http://arxiv.org/abs/2507.19657v1 The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/ dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV s, ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ...","title":"\"X of Information'' Continuum A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems"},{"location":"weekly_paper/2025-08-01/#deltallm-a-training-free-framework-exploiting-temporal-sparsity-for-efficient-edge-llm-inference","text":"Authors: Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen 2025-07-25 http://arxiv.org/abs/2507.19608v1 Deploying Large Language Models ( s) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present Delta , a training-free framework that exploits temporal in attention patterns to enable efficient inference across both the prefilling and decoding stages, on resource-constrained edge devices. Delta introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal , and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that Delta offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.","title":"DeltaLLM A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference"},{"location":"weekly_paper/2025-08-01/#advancing-event-forecasting-through-massive-training-of-large-language-models-challenges-solutions-and-broader-impacts","text":"Authors: Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel 2025-07-25 http://arxiv.org/abs/2507.19477v1 Many recent papers have studied the development of superforecaster-level event forecasting s. While methodological problems with early studies cast doubt on the use of s for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art s are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting s. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of -based event forecasting training: noisiness- , knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.","title":"Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions, and Broader Impacts"},{"location":"weekly_paper/2025-08-01/#gepa-reflective-prompt-evolution-can-outperform-reinforcement-learning","text":"Authors: Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab 2025-07-25 http://arxiv.org/abs/2507.19457v1 Large language models ( s) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for s, compared with policy gradients derived from , scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two s, and demonstrates promising results as an inference-time search strategy for code optimization.","title":"GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning"},{"location":"weekly_paper/2025-08-01/#step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding","text":"Authors: StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang 2025-07-25 http://arxiv.org/abs/2507.19427v1 Large language models ( s) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE , and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for decoding.","title":"Step-3 is Large yet Affordable Model-system Co-design for Cost-effective Decoding"},{"location":"weekly_paper/2025-08-01/#doubling-your-data-in-minutes-ultra-fast-tabular-data-generation-via-llm-induced-dependency-graphs","text":"Authors: Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci 2025-07-25 http://arxiv.org/abs/2507.19334v1 Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models ( s) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures dependencies via an -induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over -based baselines.","title":"Doubling Your Data in Minutes Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs"},{"location":"weekly_paper/2025-08-01/#patch-pruning-strategy-based-on-robust-statistical-measures-of-attention-weight-diversity-in-vision-transformers","text":"Authors: Yuki Igaue, Hiroaki Aizawa 2025-07-25 http://arxiv.org/abs/2507.19175v1 Multi-head self-attention is a distinctive feature extraction mechanism of vision s that computes pairwise relationships among all input patches, contributing significantly to their high performance. However, it is known to incur a quadratic computational complexity with respect to the number of patches. One promising approach to address this issue is patch , which improves computational efficiency by identifying and removing redundant patches. In this work, we propose a patch strategy that evaluates the importance of each patch based on the variance of attention weights across multiple attention heads. This approach is inspired by the design of multi-head self-attention, which aims to capture diverse attention patterns across different subspaces of feature representations. The proposed method can be easily applied during both training and inference, and achieves improved throughput while maintaining classification accuracy in scenarios such as fine-tuning with pre-trained models. In addition, we also found that using robust statistical measures, such as the median absolute deviation in place of variance, to assess patch importance can similarly lead to strong performance. Furthermore, by introducing ping patch embeddings, our method achieves better performance with comparable throughput to conventional approaches that utilize all patches.","title":"Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers"},{"location":"weekly_paper/2025-08-01/#regscore-scoring-systems-for-regression-tasks","text":"Authors: Michal K. Grzeszczyk, Tomasz Szczepa\u0144ski, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzci\u0144ski, Arkadiusz Sitek 2025-07-25 http://arxiv.org/abs/2507.19155v1 Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, , and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and k- ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at https://github.com/SanoScience/RegScore.","title":"RegScore Scoring Systems for Regression Tasks"},{"location":"weekly_paper/2025-08-01/#mixa-q-revisiting-activation-sparsity-for-vision-transformers-from-a-mixed-precision-quantization-perspective","text":"Authors: Weitian Wang, Rai Shubham, Cecilia De La Parra, Akash Kumar 2025-07-25 http://arxiv.org/abs/2507.19131v1 In this paper, we propose MixA-Q, a mixed-precision activation quantization framework that leverages intra-layer activation (a concept widely explored in activation methods) for efficient inference of quantized window-based vision s. For a given uniform-bit quantization configuration, MixA-Q separates the batched window computations within Swin blocks and assigns a lower bit width to the activations of less important windows, improving the trade-off between model performance and efficiency. We introduce a Two-Branch Swin Block that processes activations separately in high- and low-bit precision, enabling seamless integration of our method with most quantization-aware training (QAT) and post-training quantization (PTQ) methods, or with simple modifications. Our experimental evaluations over the COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x computational speedup without accuracy loss in PTQ configuration. With QAT, MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP drop by incorporating activation . Notably, by reducing the quantization error in important regions, our -aware quantization adaptation improves the mAP of the quantized W4A4 model (with both weights and activations in 4-bit precision) by 0.7%, reducing quantization degradation by 24%.","title":"MixA-Q Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective"},{"location":"weekly_paper/2025-08-01/#deterministic-diffusion-models-for-lagrangian-turbulence-robustness-and-encoding-of-extreme-events","text":"Authors: Tianyi Li, Flavio Tuteri, Michele Buzzicotti, Fabio Bonaccorso, Luca Biferale 2025-07-25 http://arxiv.org/abs/2507.19103v1 Modeling Lagrangian turbulence remains a fundamental challenge due to its multiscale, intermittent, and non-Gaussian nature. Recent advances in data-driven diffusion models have enabled the generation of realistic Lagrangian velocity trajectories that accurately reproduce statistical properties across scales and capture rare extreme events. This study investigates three key aspects of diffusion-based modeling for Lagrangian turbulence. First, we assess architectural robustness by comparing a U-Net backbone with a -based alternative, finding strong consistency in generated trajectories, with only minor discrepancies at small scales. Second, leveraging a deterministic variant of diffusion model formulation, namely the deterministic denoising diffusion implicit model (DDIM), we identify structured features in the initial latent noise that align consistently with extreme events. Third, we explore accelerated generation by reducing the number of diffusion steps, and find that DDIM enables substantial speedups with minimal loss of statistical fidelity. These findings highlight the robustness of diffusion models and their potential for interpretable, scalable modeling of complex turbulent systems.","title":"Deterministic diffusion models for Lagrangian turbulence robustness and encoding of extreme events"},{"location":"weekly_paper/2025-08-08/","text":"2025-08-08 Table of Contents MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Automatic LLM Red Teaming Evaluating, Synthesizing, and Enhancing for Customer Support Conversation FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG S$^2$Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Compressing Chain-of-Thought in LLMs via Step Entropy Do language models accommodate their users? A study of linguistic convergence Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS AgentSME for Simulating Diverse Communication Modes in Smart Education Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives LOST Low-rank and Sparse Pre-training for Large Language Models Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Isolating Culture Neurons in Multilingual Large Language Models Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor LeanK Learnable K Cache Channel Pruning for Efficient Decoding Whispering Agents An event-driven covert communication protocol for the Internet of Agents Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models A Survey on AgentOps Categorization, Challenges, and Future Directions AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes IAUNet Instance-Aware U-Net Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Session-Based Recommendation with Validated and Enriched LLM Intents ReaGAN Node-as-Agent-Reasoning Graph Agentic Network EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Systematic Evaluation of Optimization Techniques for Long-Context Language Models Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Authors: Amit Kumar Das, Klaus Mueller 2025-08-06 http://arxiv.org/abs/2508.04679v1 Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models ( s) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming -based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data . Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Authors: Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu 2025-08-06 http://arxiv.org/abs/2508.04664v1 Large Language Models ( s) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment s' capabilities, we propose a complementary approach: empowering s with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips s with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables s to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information- benchmarks-PI- (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging s' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale. Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning Authors: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis 2025-08-06 http://arxiv.org/abs/2508.04581v1 Large language models ( s) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head ), while the repetitive layered structure of s implies significant inter-block redundancy - a dimension largely unexplored beyond key-value ( ) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained s to reduce their number of parameters without experiencing any significant drop in their performance. TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models Authors: Xinkui Zhao, Haode Li, Yifan Zhang, Guanjie Cheng, Yueshen Xu 2025-08-06 http://arxiv.org/abs/2508.04474v1 Recent advances in large language models ( s) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting s with external, interpretable memory. Nevertheless, most existing methods that combine s with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and of new facts. This plug-and-play architecture facilitates seamless integration with various s, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning. CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Authors: Enyu Zhou, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04462v1 Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for inference . However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD. Automatic LLM Red Teaming Authors: Roman Belaire, Arunesh Sinha, Pradeep Varakantham 2025-08-06 http://arxiv.org/abs/2508.04451v1 Red teaming is critical for identifying vulnerabilities and building trust in current s. However, current automated methods for Large Language Models ( s) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment. Evaluating, Synthesizing, and Enhancing for Customer Support Conversation Authors: Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong 2025-08-06 http://arxiv.org/abs/2508.04423v1 Effective customer support requires not only accurate problem solving but also structured and empathetic aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using s to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using -powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong s on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin. FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design Authors: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04405v1 Large Language Models ( s) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit . In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.05. The proposed kernel achieves an average 1.39$\\times$ speedup over ABQ- on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference and 1.21$\\times$ memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ. KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs Authors: Zunhai Su, Kehong Yuan 2025-08-06 http://arxiv.org/abs/2508.04257v1 Key-Value ( ) cache quantization has become a widely adopted optimization technique for efficient large language models ( s) inference by reducing cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of s for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{ Sink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that Sink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during cache quantization. Moreover, when applied to the well-established Quant method, Sink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers. ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Authors: Fatemeh Nazary, Ali Tourani, Yashar Deldjoo, Tommaso Di Noia 2025-08-06 http://arxiv.org/abs/2508.04206v1 Recommending long-form video content demands joint modeling of visual, audio, and textual modalities, yet most benchmarks address only raw features or narrow fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for -augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K, it aligns dense item embeddings from three modalities: audio (block-level, i-vector), visual (CNN, AVF), and text. Missing or metadata is automatically enriched using state-of-the-art s (e.g., OpenAI Ada), generating high-quality synopses for thousands of movies. All text (raw or augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5), producing multiple ready-to-use sets. The pipeline supports interchangeable early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are fully declarative via a single YAML file. Evaluation spans accuracy (Recall, nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty, diversity, fairness. Results show -based augmentation and strong text embeddings boost cold-start and coverage, especially when fused with audio-visual features. Systematic benchmarking reveals universal versus backbone- or metric-specific combinations. Open-source code, embeddings, and configs enable reproducible, fair multimodal RS research and advance principled generative AI integration in large-scale recommendation. Code: https://recsys-lab.github.io/ViLLA-MMBench Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Authors: Millicent Ochieng, Anja Thieme, Ignatius Ezeani, Risa Ueno, Samuel Maina, Keshet Ronen, Javier Gonzalez, Jacki O'Neill 2025-08-06 http://arxiv.org/abs/2508.04199v1 Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models ( s) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate s outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier s demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world . Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Authors: Teng Shi, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu 2025-08-06 http://arxiv.org/abs/2508.04145v1 In modern online platforms, search and recommendation (S&R) often coexist, offering opportunities for performance improvement through search-enhanced approaches. Existing studies show that incorporating search signals boosts recommendation performance. However, the effectiveness of these methods relies heavily on rich search interactions. They primarily benefit a small subset of users with abundant search behavior, while offering limited improvements for the majority of users who exhibit only search activity. To address the problem of search data in search-enhanced recommendation, we face two key challenges: (1) how to learn useful search features for users with search interactions, and (2) how to design effective training objectives under conditions. Our idea is to leverage the features of users with rich search interactions to enhance those of users with search interactions. Based on this idea, we propose GSERec, a method that utilizes message passing on the User-Code Graphs to alleviate data in Search-Enhanced Recommendation. Specifically, we utilize Large Language Models ( s) with vector quantization to generate discrete codes, which connect similar users and thereby construct the graph. Through message passing on this graph, embeddings of users with rich search data are propagated to enhance the embeddings of users with interactions. To further ensure that the message passing captures meaningful information from truly similar users, we introduce a contrastive loss to better model user similarities. The enhanced user representations are then integrated into downstream search-enhanced recommendation models. Experiments on three real-world datasets show that GSERec consistently outperforms baselines, especially for users with search behaviors. Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement Authors: Zheng Cheng, Wenri Wang, Guangyong Chen, Yakun Ju, Yihua Cheng, Zhisong Liu, Yanda Meng, Jintao Song 2025-08-06 http://arxiv.org/abs/2508.04123v1 Underwater image enhancement (UIE) techniques aim to improve visual quality of images captured in aquatic environments by addressing degradation issues caused by light absorption and scattering effects, including color distortion, blurring, and low contrast. Current mainstream solutions predominantly employ multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction quality through multi-resolution feature fusion. However, our extensive experiments demonstrate that high-quality image reconstruction does not necessarily rely on multi-scale feature fusion. Contrary to popular belief, our experiments show that single-scale feature extraction alone can match or surpass the performance of multi-scale methods, significantly reducing complexity. To comprehensively explore single-scale feature potential in underwater enhancement, we propose an innovative Single-Scale Decomposition Network (SSD-Net). This architecture introduces an asymmetrical decomposition mechanism that disentangles input image into clean layer along with degradation layer. The former contains scene-intrinsic information and the latter encodes medium-induced interference. It uniquely combines CNN's local feature extraction capabilities with Transformer's global modeling strengths through two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing dual-branch feature space decoupling via efficient attention operations and adaptive ; 2) Bidirectional Feature Communication Block (BFCB), enabling cross-layer residual interactions for complementary feature mining and fusion. This synergistic design preserves feature decomposition independence while establishing dynamic cross-layer information pathways, effectively enhancing degradation decoupling capacity. TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation Authors: Zunhui Xia, Hongxing Li, Libin Lan 2025-08-06 http://arxiv.org/abs/2508.04058v1 In recent years, -based methods have achieved remarkable progress in medical image segmentation due to their superior ability to capture long-range dependencies. However, these methods typically suffer from two major limitations. First, their computational complexity scales quadratically with the input sequences. Second, the feed-forward network (FFN) modules in vanilla Transformers typically rely on fully connected layers, which limits models' ability to capture local contextual information and multiscale features critical for precise semantic segmentation. To address these issues, we propose an efficient medical image segmentation network, named TCSAFormer. The proposed TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention (CA) module, which combines token compression and pixel-level attention to dynamically focus on the most relevant key-value pairs for each query. This is achieved by globally irrelevant tokens and merging redundant ones, significantly reducing computational complexity while enhancing the model's ability to capture relationships between tokens. Second, it introduces a Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the standard FFN to capture local contextual features and multiscale information, thereby strengthening the model's feature representation capability. We conduct extensive experiments on three publicly available medical image segmentation datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation performance of TCSAFormer. Experimental results demonstrate that TCSAFormer achieves superior performance compared to existing state-of-the-art (SOTA) methods, while maintaining lower computational overhead, thus achieving an optimal trade-off between efficiency and accuracy. PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG Authors: Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang 2025-08-06 http://arxiv.org/abs/2508.04057v1 Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models ( s) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the 's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average. S$^2$Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Authors: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu 2025-08-06 http://arxiv.org/abs/2508.04016v2 Diffusion s have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that leverages Salient data and Sparse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT achieves lossless performance while delivering $3.9\\times$ model compression and $1.3\\times$ inference . Code will be available at https://github.com/wlfeng0509/s2q-vdit. Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency Authors: Md Arafat Sultan, Ram\u00f3n Fernandez Astudillo 2025-08-06 http://arxiv.org/abs/2508.03979v1 Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis . Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five s on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases. MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Authors: Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang 2025-08-05 http://arxiv.org/abs/2508.03553v1 Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models ( s). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \\textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG. Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Authors: Wenxin Mao, Zhitao Wang, Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin 2025-08-05 http://arxiv.org/abs/2508.03379v2 Large language models ( s) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with s' excellent mathematical strengths. Additional static parsing and dependency further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency. Compressing Chain-of-Thought in LLMs via Step Entropy Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu 2025-08-05 http://arxiv.org/abs/2508.03346v1 Large Language Models ( s) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy , which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables s to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances inference efficiency while rigorously preserving accuracy, offering profound implications for practical deployment and a deeper understanding of reasoning structures. Do language models accommodate their users? A study of linguistic convergence Authors: Terra Blevins, Susanne Schmalwieser, Benjamin Roth 2025-08-05 http://arxiv.org/abs/2508.03276v1 While large language models ( s) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language , asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different. Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS Authors: Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang 2025-08-05 http://arxiv.org/abs/2508.03125v1 Large language model-based multi-agent systems ( -MAS) effectively accomplish complex and dynamic tasks through inter-agent , but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting -MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, architectures, and s demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust safeguards in -MAS. AgentSME for Simulating Diverse Communication Modes in Smart Education Authors: Wen-Xi Yang, Tian-Fang Zhao 2025-08-05 http://arxiv.org/abs/2508.03109v1 Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human . To address this issue, this paper proposes AgentSME, a unified generative agent framework powered by . Three directional modes are considered in the models, namely Solo, Mono, and Echo, reflecting different types of agency autonomy and communicative reciprocity. Accuracy is adopted as the primary evaluation metric, complemented by three diversity indices designed to assess the diversity of reasoning contents. Six widely used s are tested to validate the robustness of modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations. The results show that generative agents that employ the Echo mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models. Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives Authors: Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens 2025-08-04 http://arxiv.org/abs/2508.02853v1 We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address demographic coverage, we test whether -generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives. LOST Low-rank and Sparse Pre-training for Large Language Models Authors: Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang 2025-08-04 http://arxiv.org/abs/2508.02668v1 While large language models ( s) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for s, a novel method that ingeniously integrates low-rank and structures to enable effective training of s from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise components to complement the expressiveness of low-rank training. We evaluate LOST on pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST Repo} Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks Authors: Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda 2025-08-04 http://arxiv.org/abs/2508.02556v1 Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into ping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than -based models, making them well-suited for real-world deployment. xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Authors: Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qin Zhang, Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang 2025-08-04 http://arxiv.org/abs/2508.02520v4 The rise of scaled-out s and scaled-up SuperPods signals a new era in large-scale AI infrastructure. s continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs. Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms Authors: Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu 2025-08-04 http://arxiv.org/abs/2508.02506v1 Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models ( s) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments. CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Authors: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang 2025-08-04 http://arxiv.org/abs/2508.02401v1 Recent advances in large language models ( s) have significantly boosted long-context processing. However, the increasing key-value ( ) cache size poses critical challenges to memory and execution efficiency. Most cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based s. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of s. To address the issue above, instead of using all the attention heads in GQA-based s to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive cache allocation strategy. Experimental results demonstrate the proposed Compress consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/Compress .git. Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Authors: Zuxin Ma, Yunhe Cui, Yongbin Qin 2025-08-04 http://arxiv.org/abs/2508.02381v2 Non-uniform structured network methods can effectively reduce Large Language Model ( ) size by eliminating redundant channels or layers, offering lower performance degradation than uniform strategies. However, existing non-uniform methods rely heavily on manually designed policies (e.g., layer importance and scaling factors), and therefore cannot efficiently adapt to scenarios with dynamic ratio requirements. Additionly, a critical bottleneck -- the time-consuming evaluation of policies -- further limits the feasibility of iteratively and dynamically finding optimal policies. To address these limitations, we propose PPF (Predictive Pruning Framework), a novel framework for s that eliminates manual design dependencies via second-level performance prediction. PPF not only supports real-time decisions under dynamic ratios but is also applicable to static scenarios. It employs an agent for producing adaptive and real-time actions, while a lightweight performance predictor that can evaluate a policy in seconds, significantly speeding up the iterative optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can generate dynamic/static policies and it reduces perplexity by up to 33.4% (dynamic ) and 84.78% (static ) over existing methods, outperforming manually designed policies. The performance predictor achieves second-level performance prediction with high accuracy (prediction error < 0.0011). It reduces the mean evaluation latency from minute-level (1 minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52 seconds), achieving over 64 times speedup. Our code will be available at https://github.com/Ma-zx/PPF . Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems Authors: Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang 2025-08-04 http://arxiv.org/abs/2508.02344v1 Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models ( s) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent -based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection through its self-iteration and a new synchronous network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at https://huggingface.co/Season998/Traffic-R1. CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis Authors: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che 2025-08-04 http://arxiv.org/abs/2508.02322v1 Large Language Models ( s) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level , merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU. VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Authors: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu 2025-08-04 http://arxiv.org/abs/2508.02317v3 Recent advances in large language models ( s) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal s remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal s. VeOmni introduces model-centric distributed recipes that decouples from computation, enabling efficient 3D parallelism on omni-modal s. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal s. Isolating Culture Neurons in Multilingual Large Language Models Authors: Danial Namazifard, Lukas Galke 2025-08-04 http://arxiv.org/abs/2508.02241v1 Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that s encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons . Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor Authors: Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu 2025-08-04 http://arxiv.org/abs/2508.02240v2 Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based . First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.} LeanK Learnable K Cache Channel Pruning for Efficient Decoding Authors: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu 2025-08-04 http://arxiv.org/abs/2508.02215v1 Large language models ( s) enable long-context tasks but face efficiency challenges due to the growing key-value ( ) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel . With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK. Whispering Agents An event-driven covert communication protocol for the Internet of Agents Authors: Kaibo Huang, Yukun Wei, WanSheng Wu, Tianhua Zhang, Zhongliang Yang, Linna Zhou 2025-08-04 http://arxiv.org/abs/2508.02188v1 The emergence of the Internet of Agents (IoA) introduces critical challenges for privacy in sensitive, high-stakes domains. While standard Agent-to-Agent (A2A) protocols secure message content, they are not designed to protect the act of itself, leaving agents vulnerable to surveillance and traffic analysis. We find that the rich, event-driven nature of agent dialogues provides a powerful, yet untapped, medium for covert . To harness this potential, we introduce and formalize the Covert Event Channel, the first unified model for agent covert driven by three interconnected dimensions, which consist of the Storage, Timing,and Behavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a novel protocol that operationalizes this event-driven paradigm. Our comprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and robustness while remaining imperceptible to powerful -based wardens, establishing its practical viability. By systematically engineering this channel, our work provides the foundational understanding essential for developing the next generation of monitoring systems and defensive protocols for a secure and trustworthy IoA. Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Authors: Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad 2025-08-04 http://arxiv.org/abs/2508.02148v1 Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic (RKD-SC) framework is proposed to enable efficient and \\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods. Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models Authors: Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang 2025-08-04 http://arxiv.org/abs/2508.02128v1 In the era of large language models ( s), N:M has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight , it often suffers from significant accuracy degradation. Activation , though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation method designed specifically for the prefill stage, targeting the of linear projection layers in s. Extensive experiments across multiple models and ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding- , a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation , providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems. A Survey on AgentOps Categorization, Challenges, and Future Directions Authors: Zexin Wang, Jingjing Li, Quan Zhou, Haotian Si, Yuanhao Liu, Jianhui Li, Gaogang Xie, Fei Sun, Dan Pei, Changhua Pei 2025-08-04 http://arxiv.org/abs/2508.02121v1 As the reasoning capabilities of Large Language Models ( s) continue to advance, -based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is . To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution. AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Authors: Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha 2025-08-04 http://arxiv.org/abs/2508.02079v1 Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models ( s). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation. Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games Authors: Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen 2025-08-04 http://arxiv.org/abs/2508.02076v1 Coordinating multiple large language models ( s) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi- ensembles. In MAC-SPGG, agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation. CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes Authors: Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot 2025-08-03 http://arxiv.org/abs/2508.01936v1 We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view , deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection. IAUNet Instance-Aware U-Net Authors: Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman 2025-08-03 http://arxiv.org/abs/2508.01928v1 Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of ping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, -based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language Authors: Jaskaranjeet Singh, Rakesh Thakur 2025-08-03 http://arxiv.org/abs/2508.01918v1 Despite the rapid advancement of large language models ( s), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs. As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization Authors: Zicong Ye, Kunming Zhang, Guoming Tang 2025-08-03 http://arxiv.org/abs/2508.01744v1 The explosive growth of interactive Large Language Models ( s) has placed unprecedented demands for low latency on cloud GPUs, forcing them into high-power modes and causing escalating energy costs. Real-time inference workloads exhibit significant dynamic volatility, presenting substantial energy-saving opportunities. However, traditional static or rule-based power management strategies struggle to exploit these opportunities without compromising peak performance. To address this challenge, we propose AGFT (An Adaptive GPU Frequency Tuner), a framework that employs online reinforcement learning to autonomously learn an optimal frequency tuning policy. By monitoring real-time features like request load and latency, AGFT utilizes fine-grained frequency control for precise adjustments and intelligent action space for stable, efficient decision-making. This creates a robust, automated energy management solution. We comprehensively evaluated AGFT in an environment simulating realistic, fluctuating inference requests. The experimental results demonstrate that AGFT successfully saves 44.3% of GPU energy consumption while introducing a minimal performance latency overhead of under 10%. This achievement translates into a comprehensive Energy-Delay Product (EDP) optimization of up to 40.3%, clearly showing that our framework can significantly enhance the energy efficiency and economic benefits of existing inference clusters without compromising service quality. SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference Authors: Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu 2025-08-03 http://arxiv.org/abs/2508.02751v1 cache eviction has emerged as an effective solution to alleviate resource constraints faced by s in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between s of different scales. We propose Small , a small model assisted compensation method for cache compression. Small can maintain attention matching between different-scale s to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of Small . Moreover, efficiency evaluations show that Small achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant inference in resource constrained environments. EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models Authors: Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng 2025-08-03 http://arxiv.org/abs/2508.01625v1 Mixture-of-Experts (MoE) has demonstrated promising potential in scaling s. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE- s, which deeply aligns with the characteristics of MoE from the perspectives of quantization and , and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE- s. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation. RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale Authors: Zhilong Chen, Chengzong Zhao, Boyuan Chen, Dayi Lin, Yihao Chen, Arthur Leung, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Ahmed E. Hassan 2025-08-03 http://arxiv.org/abs/2508.01550v1 Training software engineering (SWE) s is bottlenecked by expensive infrastructure, inefficient evaluation pipelines, scarce training data, and costly quality control. We present RepoForge, an autonomous, end-to-end pipeline that generates, evaluates, and trains SWE agents at scale. Our key contributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on SWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new state-of-the-art for $\\leq$8B non-thinking s; (2) 7,304 executable environments auto-generated from real GitHub commits with zero manual intervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per instance) via intelligent dependency management and image ; (4) $>$70\\% faster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge harness; (5) 19,000$\\times$ cheaper labeling through our automated SPICE~\\citep{spice2024} difficulty assessment technique. By unifying storage-efficient sandboxing, Ray-powered evaluation harness, automated data generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate that even $\\leq$8B models can reach new state-of-the-art performance on demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical bottlenecks in SWE agent training: high storage costs of container-based evaluation, inefficient sequential reward pipelines, limited availability of high-quality training data, expensive manual labeling, and multi-turn RL pipeline bottlenecks. BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Authors: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan 2025-08-02 http://arxiv.org/abs/2508.01332v2 The rapid adoption of agentic AI, powered by large language models ( s), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in -driven multi-agent systems (MASes): fragmented identity frameworks, insecure channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, -based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production -based MAS environments. Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Authors: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat 2025-08-02 http://arxiv.org/abs/2508.01261v1 We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla s while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference . Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment. Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Authors: Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen 2025-08-02 http://arxiv.org/abs/2508.01159v1 This study evaluates the capacity of large language models ( s) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that s can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician . Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation Authors: Leyao Wang, Xutao Mao, Xuhui Zhan, Yuying Zhao, Bo Ni, Ryan A. Rossi, Nesreen K. Ahmed, Tyler Derr 2025-08-02 http://arxiv.org/abs/2508.01128v1 Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and -based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations. REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Authors: Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu 2025-08-01 http://arxiv.org/abs/2508.01057v1 Collisions caused by human error are the most common type of multi-vehicle crash, highlighting the critical need for autonomous driving (AD) systems to leverage cooperative perception through Vehicle-to-Everything (V2X) . This capability extends situational awareness beyond the limitations of onboard sensors. However, current -based V2X frameworks suffer from limited generalization, shallow contextual reasoning, and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced reasoning and multimodal integration but typically fall short of real-time performance requirements in safety-critical applications. This paper presents REACT, a real-time, V2X-integrated trajectory optimization framework built upon a fine-tuned lightweight VLM. REACT integrates a set of specialized modules that process multimodal inputs into optimized, risk-aware trajectories. To ensure real-time performance on edge devices, REACT incorporates edge adaptation strategies that reduce model complexity and accelerate inference. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results demonstrate the feasibility of lightweight VLMs for real-time edge-based cooperative planning and showcase the potential of language-guided contextual reasoning to improve safety and responsiveness in autonomous driving. Session-Based Recommendation with Validated and Enriched LLM Intents Authors: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang 2025-08-01 http://arxiv.org/abs/2508.00570v1 Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models ( s), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched -generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of -generated intents with a global intent pool to constrain the 's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability. ReaGAN Node-as-Agent-Reasoning Graph Agentic Network Authors: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang 2025-08-01 http://arxiv.org/abs/2508.00429v2 Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain . Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning. EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Authors: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun 2025-08-01 http://arxiv.org/abs/2508.00370v2 Deploying Transformer-based large language models ( s) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value ( ) cache demands. While existing cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token . Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices. Systematic Evaluation of Optimization Techniques for Long-Context Language Models Authors: Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar 2025-08-01 http://arxiv.org/abs/2508.00305v1 Large language models ( s) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like , quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations. Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study Authors: Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek 2025-08-01 http://arxiv.org/abs/2508.00256v1 Low-altitude wireless networks (LAWNs) have the potential to revolutionize s by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure s in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure s in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models ( s) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.","title":"2025-08-08"},{"location":"weekly_paper/2025-08-08/#2025-08-08","text":"","title":"2025-08-08"},{"location":"weekly_paper/2025-08-08/#table-of-contents","text":"MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models Sculptor Empowering LLMs with Cognitive Agency via Active Context Management Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference Automatic LLM Red Teaming Evaluating, Synthesizing, and Enhancing for Customer Support Conversation FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG S$^2$Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams Compressing Chain-of-Thought in LLMs via Step Entropy Do language models accommodate their users? A study of linguistic convergence Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS AgentSME for Simulating Diverse Communication Modes in Smart Education Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives LOST Low-rank and Sparse Pre-training for Large Language Models Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks xDeepServe Model-as-a-Service on Huawei CloudMatrix384 Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo Isolating Culture Neurons in Multilingual Large Language Models Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor LeanK Learnable K Cache Channel Pruning for Efficient Decoding Whispering Agents An event-driven covert communication protocol for the Internet of Agents Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models A Survey on AgentOps Categorization, Challenges, and Future Directions AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes IAUNet Instance-Aware U-Net Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System Session-Based Recommendation with Validated and Enriched LLM Intents ReaGAN Node-as-Agent-Reasoning Graph Agentic Network EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices Systematic Evaluation of Optimization Techniques for Long-Context Language Models Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study","title":"Table of Contents"},{"location":"weekly_paper/2025-08-08/#misvisfix-an-interactive-dashboard-for-detecting-explaining-and-correcting-misleading-visualizations-using-large-language-models","text":"Authors: Amit Kumar Das, Klaus Mueller 2025-08-06 http://arxiv.org/abs/2508.04679v1 Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models ( s) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming -based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data .","title":"MisVisFix An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models"},{"location":"weekly_paper/2025-08-08/#sculptor-empowering-llms-with-cognitive-agency-via-active-context-management","text":"Authors: Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu 2025-08-06 http://arxiv.org/abs/2508.04664v1 Large Language Models ( s) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment s' capabilities, we propose a complementary approach: empowering s with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips s with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables s to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information- benchmarks-PI- (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging s' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.","title":"Sculptor Empowering LLMs with Cognitive Agency via Active Context Management"},{"location":"weekly_paper/2025-08-08/#share-your-attention-transformer-weight-sharing-via-matrix-based-dictionary-learning","text":"Authors: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis 2025-08-06 http://arxiv.org/abs/2508.04581v1 Large language models ( s) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head ), while the repetitive layered structure of s implies significant inter-block redundancy - a dimension largely unexplored beyond key-value ( ) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained s to reduce their number of parameters without experiencing any significant drop in their performance.","title":"Share Your Attention Transformer Weight Sharing via Matrix-based Dictionary Learning"},{"location":"weekly_paper/2025-08-08/#trail-joint-inference-and-refinement-of-knowledge-graphs-with-large-language-models","text":"Authors: Xinkui Zhao, Haode Li, Yifan Zhang, Guanjie Cheng, Yueshen Xu 2025-08-06 http://arxiv.org/abs/2508.04474v1 Recent advances in large language models ( s) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting s with external, interpretable memory. Nevertheless, most existing methods that combine s with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and of new facts. This plug-and-play architecture facilitates seamless integration with various s, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning.","title":"TRAIL Joint Inference and Refinement of Knowledge Graphs with Large Language Models"},{"location":"weekly_paper/2025-08-08/#card-cache-assisted-parallel-speculative-decoding-for-efficient-large-language-model-inference","text":"Authors: Enyu Zhou, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04462v1 Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for inference . However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD.","title":"CARD Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference"},{"location":"weekly_paper/2025-08-08/#automatic-llm-red-teaming","text":"Authors: Roman Belaire, Arunesh Sinha, Pradeep Varakantham 2025-08-06 http://arxiv.org/abs/2508.04451v1 Red teaming is critical for identifying vulnerabilities and building trust in current s. However, current automated methods for Large Language Models ( s) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment.","title":"Automatic LLM Red Teaming"},{"location":"weekly_paper/2025-08-08/#evaluating-synthesizing-and-enhancing-for-customer-support-conversation","text":"Authors: Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong 2025-08-06 http://arxiv.org/abs/2508.04423v1 Effective customer support requires not only accurate problem solving but also structured and empathetic aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using s to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using -powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong s on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.","title":"Evaluating, Synthesizing, and Enhancing for Customer Support Conversation"},{"location":"weekly_paper/2025-08-08/#flexq-efficient-post-training-int6-quantization-for-llm-serving-via-algorithm-system-co-design","text":"Authors: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He 2025-08-06 http://arxiv.org/abs/2508.04405v1 Large Language Models ( s) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit . In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.05. The proposed kernel achieves an average 1.39$\\times$ speedup over ABQ- on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference and 1.21$\\times$ memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ.","title":"FlexQ Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design"},{"location":"weekly_paper/2025-08-08/#kvsink-understanding-and-enhancing-the-preservation-of-attention-sinks-in-kv-cache-quantization-for-llms","text":"Authors: Zunhai Su, Kehong Yuan 2025-08-06 http://arxiv.org/abs/2508.04257v1 Key-Value ( ) cache quantization has become a widely adopted optimization technique for efficient large language models ( s) inference by reducing cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of s for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{ Sink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that Sink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during cache quantization. Moreover, when applied to the well-established Quant method, Sink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.","title":"KVSink Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"},{"location":"weekly_paper/2025-08-08/#villa-mmbench-a-unified-benchmark-suite-for-llm-augmented-multimodal-movie-recommendation","text":"Authors: Fatemeh Nazary, Ali Tourani, Yashar Deldjoo, Tommaso Di Noia 2025-08-06 http://arxiv.org/abs/2508.04206v1 Recommending long-form video content demands joint modeling of visual, audio, and textual modalities, yet most benchmarks address only raw features or narrow fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for -augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K, it aligns dense item embeddings from three modalities: audio (block-level, i-vector), visual (CNN, AVF), and text. Missing or metadata is automatically enriched using state-of-the-art s (e.g., OpenAI Ada), generating high-quality synopses for thousands of movies. All text (raw or augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5), producing multiple ready-to-use sets. The pipeline supports interchangeable early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are fully declarative via a single YAML file. Evaluation spans accuracy (Recall, nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty, diversity, fairness. Results show -based augmentation and strong text embeddings boost cold-start and coverage, especially when fused with audio-visual features. Systematic benchmarking reveals universal versus backbone- or metric-specific combinations. Open-source code, embeddings, and configs enable reproducible, fair multimodal RS research and advance principled generative AI integration in large-scale recommendation. Code: https://recsys-lab.github.io/ViLLA-MMBench","title":"ViLLA-MMBench A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation"},{"location":"weekly_paper/2025-08-08/#reasoning-beyond-labels-measuring-llm-sentiment-in-low-resource-culturally-nuanced-contexts","text":"Authors: Millicent Ochieng, Anja Thieme, Ignatius Ezeani, Risa Ueno, Samuel Maina, Keshet Ronen, Javier Gonzalez, Jacki O'Neill 2025-08-06 http://arxiv.org/abs/2508.04199v1 Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models ( s) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate s outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier s demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world .","title":"Reasoning Beyond Labels Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts"},{"location":"weekly_paper/2025-08-08/#benefit-from-rich-tackling-search-interaction-sparsity-in-search-enhanced-recommendation","text":"Authors: Teng Shi, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu 2025-08-06 http://arxiv.org/abs/2508.04145v1 In modern online platforms, search and recommendation (S&R) often coexist, offering opportunities for performance improvement through search-enhanced approaches. Existing studies show that incorporating search signals boosts recommendation performance. However, the effectiveness of these methods relies heavily on rich search interactions. They primarily benefit a small subset of users with abundant search behavior, while offering limited improvements for the majority of users who exhibit only search activity. To address the problem of search data in search-enhanced recommendation, we face two key challenges: (1) how to learn useful search features for users with search interactions, and (2) how to design effective training objectives under conditions. Our idea is to leverage the features of users with rich search interactions to enhance those of users with search interactions. Based on this idea, we propose GSERec, a method that utilizes message passing on the User-Code Graphs to alleviate data in Search-Enhanced Recommendation. Specifically, we utilize Large Language Models ( s) with vector quantization to generate discrete codes, which connect similar users and thereby construct the graph. Through message passing on this graph, embeddings of users with rich search data are propagated to enhance the embeddings of users with interactions. To further ensure that the message passing captures meaningful information from truly similar users, we introduce a contrastive loss to better model user similarities. The enhanced user representations are then integrated into downstream search-enhanced recommendation models. Experiments on three real-world datasets show that GSERec consistently outperforms baselines, especially for users with search behaviors.","title":"Benefit from Rich Tackling Search Interaction Sparsity in Search Enhanced Recommendation"},{"location":"weekly_paper/2025-08-08/#excavate-the-potential-of-single-scale-features-a-decomposition-network-for-water-related-optical-image-enhancement","text":"Authors: Zheng Cheng, Wenri Wang, Guangyong Chen, Yakun Ju, Yihua Cheng, Zhisong Liu, Yanda Meng, Jintao Song 2025-08-06 http://arxiv.org/abs/2508.04123v1 Underwater image enhancement (UIE) techniques aim to improve visual quality of images captured in aquatic environments by addressing degradation issues caused by light absorption and scattering effects, including color distortion, blurring, and low contrast. Current mainstream solutions predominantly employ multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction quality through multi-resolution feature fusion. However, our extensive experiments demonstrate that high-quality image reconstruction does not necessarily rely on multi-scale feature fusion. Contrary to popular belief, our experiments show that single-scale feature extraction alone can match or surpass the performance of multi-scale methods, significantly reducing complexity. To comprehensively explore single-scale feature potential in underwater enhancement, we propose an innovative Single-Scale Decomposition Network (SSD-Net). This architecture introduces an asymmetrical decomposition mechanism that disentangles input image into clean layer along with degradation layer. The former contains scene-intrinsic information and the latter encodes medium-induced interference. It uniquely combines CNN's local feature extraction capabilities with Transformer's global modeling strengths through two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing dual-branch feature space decoupling via efficient attention operations and adaptive ; 2) Bidirectional Feature Communication Block (BFCB), enabling cross-layer residual interactions for complementary feature mining and fusion. This synergistic design preserves feature decomposition independence while establishing dynamic cross-layer information pathways, effectively enhancing degradation decoupling capacity.","title":"Excavate the potential of Single-Scale Features A Decomposition Network for Water-Related Optical Image Enhancement"},{"location":"weekly_paper/2025-08-08/#tcsaformer-efficient-vision-transformer-with-token-compression-and-sparse-attention-for-medical-image-segmentation","text":"Authors: Zunhui Xia, Hongxing Li, Libin Lan 2025-08-06 http://arxiv.org/abs/2508.04058v1 In recent years, -based methods have achieved remarkable progress in medical image segmentation due to their superior ability to capture long-range dependencies. However, these methods typically suffer from two major limitations. First, their computational complexity scales quadratically with the input sequences. Second, the feed-forward network (FFN) modules in vanilla Transformers typically rely on fully connected layers, which limits models' ability to capture local contextual information and multiscale features critical for precise semantic segmentation. To address these issues, we propose an efficient medical image segmentation network, named TCSAFormer. The proposed TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention (CA) module, which combines token compression and pixel-level attention to dynamically focus on the most relevant key-value pairs for each query. This is achieved by globally irrelevant tokens and merging redundant ones, significantly reducing computational complexity while enhancing the model's ability to capture relationships between tokens. Second, it introduces a Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the standard FFN to capture local contextual features and multiscale information, thereby strengthening the model's feature representation capability. We conduct extensive experiments on three publicly available medical image segmentation datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation performance of TCSAFormer. Experimental results demonstrate that TCSAFormer achieves superior performance compared to existing state-of-the-art (SOTA) methods, while maintaining lower computational overhead, thus achieving an optimal trade-off between efficiency and accuracy.","title":"TCSAFormer Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation"},{"location":"weekly_paper/2025-08-08/#pairs-parametric-verified-adaptive-information-retrieval-and-selection-for-efficient-rag","text":"Authors: Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang 2025-08-06 http://arxiv.org/abs/2508.04057v1 Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models ( s) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the 's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average.","title":"PAIRS Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG"},{"location":"weekly_paper/2025-08-08/#s2q-vdit-accurate-quantized-video-diffusion-transformer-with-salient-data-and-sparse-token-distillation","text":"Authors: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu 2025-08-06 http://arxiv.org/abs/2508.04016v2 Diffusion s have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that leverages Salient data and Sparse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT achieves lossless performance while delivering $3.9\\times$ model compression and $1.3\\times$ inference . Code will be available at https://github.com/wlfeng0509/s2q-vdit.","title":"S$^2$Q-VDiT Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation"},{"location":"weekly_paper/2025-08-08/#confidence-weighted-token-set-cover-for-early-hypothesis-pruning-in-self-consistency","text":"Authors: Md Arafat Sultan, Ram\u00f3n Fernandez Astudillo 2025-08-06 http://arxiv.org/abs/2508.03979v1 Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis . Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five s on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.","title":"Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency"},{"location":"weekly_paper/2025-08-08/#multirag-a-knowledge-guided-framework-for-mitigating-hallucination-in-multi-source-retrieval-augmented-generation","text":"Authors: Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang 2025-08-05 http://arxiv.org/abs/2508.03553v1 Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models ( s). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \\textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.","title":"MultiRAG A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation"},{"location":"weekly_paper/2025-08-08/#data-dependency-inference-for-industrial-code-generation-based-on-uml-sequence-diagrams","text":"Authors: Wenxin Mao, Zhitao Wang, Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin 2025-08-05 http://arxiv.org/abs/2508.03379v2 Large language models ( s) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with s' excellent mathematical strengths. Additional static parsing and dependency further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.","title":"Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams"},{"location":"weekly_paper/2025-08-08/#compressing-chain-of-thought-in-llms-via-step-entropy","text":"Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu 2025-08-05 http://arxiv.org/abs/2508.03346v1 Large Language Models ( s) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy , which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables s to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances inference efficiency while rigorously preserving accuracy, offering profound implications for practical deployment and a deeper understanding of reasoning structures.","title":"Compressing Chain-of-Thought in LLMs via Step Entropy"},{"location":"weekly_paper/2025-08-08/#do-language-models-accommodate-their-users-a-study-of-linguistic-convergence","text":"Authors: Terra Blevins, Susanne Schmalwieser, Benjamin Roth 2025-08-05 http://arxiv.org/abs/2508.03276v1 While large language models ( s) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language , asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different.","title":"Do language models accommodate their users? A study of linguistic convergence"},{"location":"weekly_paper/2025-08-08/#attack-the-messages-not-the-agents-a-multi-round-adaptive-stealthy-tampering-framework-for-llm-mas","text":"Authors: Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang 2025-08-05 http://arxiv.org/abs/2508.03125v1 Large language model-based multi-agent systems ( -MAS) effectively accomplish complex and dynamic tasks through inter-agent , but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting -MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, architectures, and s demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust safeguards in -MAS.","title":"Attack the Messages, Not the Agents A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS"},{"location":"weekly_paper/2025-08-08/#agentsme-for-simulating-diverse-communication-modes-in-smart-education","text":"Authors: Wen-Xi Yang, Tian-Fang Zhao 2025-08-05 http://arxiv.org/abs/2508.03109v1 Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human . To address this issue, this paper proposes AgentSME, a unified generative agent framework powered by . Three directional modes are considered in the models, namely Solo, Mono, and Echo, reflecting different types of agency autonomy and communicative reciprocity. Accuracy is adopted as the primary evaluation metric, complemented by three diversity indices designed to assess the diversity of reasoning contents. Six widely used s are tested to validate the robustness of modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations. The results show that generative agents that employ the Echo mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models.","title":"AgentSME for Simulating Diverse Communication Modes in Smart Education"},{"location":"weekly_paper/2025-08-08/#modeling-annotator-disagreement-with-demographic-aware-experts-and-synthetic-perspectives","text":"Authors: Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens 2025-08-04 http://arxiv.org/abs/2508.02853v1 We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address demographic coverage, we test whether -generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.","title":"Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives"},{"location":"weekly_paper/2025-08-08/#lost-low-rank-and-sparse-pre-training-for-large-language-models","text":"Authors: Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang 2025-08-04 http://arxiv.org/abs/2508.02668v1 While large language models ( s) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for s, a novel method that ingeniously integrates low-rank and structures to enable effective training of s from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise components to complement the expressiveness of low-rank training. We evaluate LOST on pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST Repo}","title":"LOST Low-rank and Sparse Pre-training for Large Language Models"},{"location":"weekly_paper/2025-08-08/#automated-snomed-ct-concept-annotation-in-clinical-text-using-bi-gru-neural-networks","text":"Authors: Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda 2025-08-04 http://arxiv.org/abs/2508.02556v1 Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into ping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than -based models, making them well-suited for real-world deployment.","title":"Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks"},{"location":"weekly_paper/2025-08-08/#xdeepserve-model-as-a-service-on-huawei-cloudmatrix384","text":"Authors: Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qin Zhang, Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang 2025-08-04 http://arxiv.org/abs/2508.02520v4 The rise of scaled-out s and scaled-up SuperPods signals a new era in large-scale AI infrastructure. s continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs.","title":"xDeepServe Model-as-a-Service on Huawei CloudMatrix384"},{"location":"weekly_paper/2025-08-08/#decomposed-reasoning-with-reinforcement-learning-for-relevance-assessment-in-ugc-platforms","text":"Authors: Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu 2025-08-04 http://arxiv.org/abs/2508.02506v1 Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models ( s) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments.","title":"Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms"},{"location":"weekly_paper/2025-08-08/#compresskv-semantic-retrieval-heads-know-what-tokens-are-not-important-before-generation","text":"Authors: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang 2025-08-04 http://arxiv.org/abs/2508.02401v1 Recent advances in large language models ( s) have significantly boosted long-context processing. However, the increasing key-value ( ) cache size poses critical challenges to memory and execution efficiency. Most cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based s. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of s. To address the issue above, instead of using all the attention heads in GQA-based s to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive cache allocation strategy. Experimental results demonstrate the proposed Compress consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/Compress .git.","title":"CompressKV Semantic Retrieval Heads Know What Tokens are Not Important Before Generation"},{"location":"weekly_paper/2025-08-08/#beyond-manually-designed-pruning-policies-with-second-level-performance-prediction-a-pruning-framework-for-llms","text":"Authors: Zuxin Ma, Yunhe Cui, Yongbin Qin 2025-08-04 http://arxiv.org/abs/2508.02381v2 Non-uniform structured network methods can effectively reduce Large Language Model ( ) size by eliminating redundant channels or layers, offering lower performance degradation than uniform strategies. However, existing non-uniform methods rely heavily on manually designed policies (e.g., layer importance and scaling factors), and therefore cannot efficiently adapt to scenarios with dynamic ratio requirements. Additionly, a critical bottleneck -- the time-consuming evaluation of policies -- further limits the feasibility of iteratively and dynamically finding optimal policies. To address these limitations, we propose PPF (Predictive Pruning Framework), a novel framework for s that eliminates manual design dependencies via second-level performance prediction. PPF not only supports real-time decisions under dynamic ratios but is also applicable to static scenarios. It employs an agent for producing adaptive and real-time actions, while a lightweight performance predictor that can evaluate a policy in seconds, significantly speeding up the iterative optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can generate dynamic/static policies and it reduces perplexity by up to 33.4% (dynamic ) and 84.78% (static ) over existing methods, outperforming manually designed policies. The performance predictor achieves second-level performance prediction with high accuracy (prediction error < 0.0011). It reduces the mean evaluation latency from minute-level (1 minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52 seconds), achieving over 64 times speedup. Our code will be available at https://github.com/Ma-zx/PPF .","title":"Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction A Pruning Framework for LLMs"},{"location":"weekly_paper/2025-08-08/#traffic-r1-reinforced-llms-bring-human-like-reasoning-to-traffic-signal-control-systems","text":"Authors: Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang 2025-08-04 http://arxiv.org/abs/2508.02344v1 Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models ( s) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent -based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection through its self-iteration and a new synchronous network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at https://huggingface.co/Season998/Traffic-R1.","title":"Traffic-R1 Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems"},{"location":"weekly_paper/2025-08-08/#camera-multi-matrix-joint-compression-for-moe-models-via-micro-expert-redundancy-analysis","text":"Authors: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che 2025-08-04 http://arxiv.org/abs/2508.02322v1 Large Language Models ( s) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level , merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.","title":"CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis"},{"location":"weekly_paper/2025-08-08/#veomni-scaling-any-modality-model-training-with-model-centric-distributed-recipe-zoo","text":"Authors: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu 2025-08-04 http://arxiv.org/abs/2508.02317v3 Recent advances in large language models ( s) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal s remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal s. VeOmni introduces model-centric distributed recipes that decouples from computation, enabling efficient 3D parallelism on omni-modal s. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal s.","title":"VeOmni Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo"},{"location":"weekly_paper/2025-08-08/#isolating-culture-neurons-in-multilingual-large-language-models","text":"Authors: Danial Namazifard, Lukas Galke 2025-08-04 http://arxiv.org/abs/2508.02241v1 Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that s encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .","title":"Isolating Culture Neurons in Multilingual Large Language Models"},{"location":"weekly_paper/2025-08-08/#forecasting-when-to-forecast-accelerating-diffusion-models-with-confidence-gated-taylor","text":"Authors: Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu 2025-08-04 http://arxiv.org/abs/2508.02240v2 Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based . First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}","title":"Forecasting When to Forecast Accelerating Diffusion Models with Confidence-Gated Taylor"},{"location":"weekly_paper/2025-08-08/#leank-learnable-k-cache-channel-pruning-for-efficient-decoding","text":"Authors: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu 2025-08-04 http://arxiv.org/abs/2508.02215v1 Large language models ( s) enable long-context tasks but face efficiency challenges due to the growing key-value ( ) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel . With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.","title":"LeanK Learnable K Cache Channel Pruning for Efficient Decoding"},{"location":"weekly_paper/2025-08-08/#whispering-agents-an-event-driven-covert-communication-protocol-for-the-internet-of-agents","text":"Authors: Kaibo Huang, Yukun Wei, WanSheng Wu, Tianhua Zhang, Zhongliang Yang, Linna Zhou 2025-08-04 http://arxiv.org/abs/2508.02188v1 The emergence of the Internet of Agents (IoA) introduces critical challenges for privacy in sensitive, high-stakes domains. While standard Agent-to-Agent (A2A) protocols secure message content, they are not designed to protect the act of itself, leaving agents vulnerable to surveillance and traffic analysis. We find that the rich, event-driven nature of agent dialogues provides a powerful, yet untapped, medium for covert . To harness this potential, we introduce and formalize the Covert Event Channel, the first unified model for agent covert driven by three interconnected dimensions, which consist of the Storage, Timing,and Behavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a novel protocol that operationalizes this event-driven paradigm. Our comprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and robustness while remaining imperceptible to powerful -based wardens, establishing its practical viability. By systematically engineering this channel, our work provides the foundational understanding essential for developing the next generation of monitoring systems and defensive protocols for a secure and trustworthy IoA.","title":"Whispering Agents An event-driven covert communication protocol for the Internet of Agents"},{"location":"weekly_paper/2025-08-08/#large-scale-model-enabled-semantic-communication-based-on-robust-knowledge-distillation","text":"Authors: Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad 2025-08-04 http://arxiv.org/abs/2508.02148v1 Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic (RKD-SC) framework is proposed to enable efficient and \\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods.","title":"Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation"},{"location":"weekly_paper/2025-08-08/#amber-pruner-leveraging-nm-activation-sparsity-for-efficient-prefill-in-large-language-models","text":"Authors: Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang 2025-08-04 http://arxiv.org/abs/2508.02128v1 In the era of large language models ( s), N:M has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight , it often suffers from significant accuracy degradation. Activation , though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation method designed specifically for the prefill stage, targeting the of linear projection layers in s. Extensive experiments across multiple models and ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding- , a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation , providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems.","title":"Amber Pruner Leveraging NM Activation Sparsity for Efficient Prefill in Large Language Models"},{"location":"weekly_paper/2025-08-08/#a-survey-on-agentops-categorization-challenges-and-future-directions","text":"Authors: Zexin Wang, Jingjing Li, Quan Zhou, Haotian Si, Yuanhao Liu, Jianhui Li, Gaogang Xie, Fei Sun, Dan Pei, Changhua Pei 2025-08-04 http://arxiv.org/abs/2508.02121v1 As the reasoning capabilities of Large Language Models ( s) continue to advance, -based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is . To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution.","title":"A Survey on AgentOps Categorization, Challenges, and Future Directions"},{"location":"weekly_paper/2025-08-08/#alignguard-lora-alignment-preserving-fine-tuning-via-fisher-guided-decomposition-and-riemannian-geodesic-collision-regularization","text":"Authors: Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha 2025-08-04 http://arxiv.org/abs/2508.02079v1 Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models ( s). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.","title":"AlignGuard-LoRA Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization"},{"location":"weekly_paper/2025-08-08/#everyone-contributes-incentivizing-strategic-cooperation-in-multi-llm-systems-via-sequential-public-goods-games","text":"Authors: Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen 2025-08-04 http://arxiv.org/abs/2508.02076v1 Coordinating multiple large language models ( s) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi- ensembles. In MAC-SPGG, agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation.","title":"Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games"},{"location":"weekly_paper/2025-08-08/#cvd-sfm-a-cross-view-deep-front-end-structure-from-motion-system-for-sparse-localization-in-multi-altitude-scenes","text":"Authors: Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot 2025-08-03 http://arxiv.org/abs/2508.01936v1 We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view , deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection.","title":"CVD-SfM A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes"},{"location":"weekly_paper/2025-08-08/#iaunet-instance-aware-u-net","text":"Authors: Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman 2025-08-03 http://arxiv.org/abs/2508.01928v1 Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of ping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, -based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet","title":"IAUNet Instance-Aware U-Net"},{"location":"weekly_paper/2025-08-08/#quantum-rag-and-pungpt2-advancing-low-resource-language-generation-and-retrieval-for-the-punjabi-language","text":"Authors: Jaskaranjeet Singh, Rakesh Thakur 2025-08-03 http://arxiv.org/abs/2508.01918v1 Despite the rapid advancement of large language models ( s), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs. As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP","title":"Quantum-RAG and PunGPT2 Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language"},{"location":"weekly_paper/2025-08-08/#agft-an-adaptive-gpu-frequency-tuner-for-real-time-llm-inference-optimization","text":"Authors: Zicong Ye, Kunming Zhang, Guoming Tang 2025-08-03 http://arxiv.org/abs/2508.01744v1 The explosive growth of interactive Large Language Models ( s) has placed unprecedented demands for low latency on cloud GPUs, forcing them into high-power modes and causing escalating energy costs. Real-time inference workloads exhibit significant dynamic volatility, presenting substantial energy-saving opportunities. However, traditional static or rule-based power management strategies struggle to exploit these opportunities without compromising peak performance. To address this challenge, we propose AGFT (An Adaptive GPU Frequency Tuner), a framework that employs online reinforcement learning to autonomously learn an optimal frequency tuning policy. By monitoring real-time features like request load and latency, AGFT utilizes fine-grained frequency control for precise adjustments and intelligent action space for stable, efficient decision-making. This creates a robust, automated energy management solution. We comprehensively evaluated AGFT in an environment simulating realistic, fluctuating inference requests. The experimental results demonstrate that AGFT successfully saves 44.3% of GPU energy consumption while introducing a minimal performance latency overhead of under 10%. This achievement translates into a comprehensive Energy-Delay Product (EDP) optimization of up to 40.3%, clearly showing that our framework can significantly enhance the energy efficiency and economic benefits of existing inference clusters without compromising service quality.","title":"AGFT An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization"},{"location":"weekly_paper/2025-08-08/#smallkv-small-model-assisted-compensation-of-kv-cache-compression-for-efficient-llm-inference","text":"Authors: Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu 2025-08-03 http://arxiv.org/abs/2508.02751v1 cache eviction has emerged as an effective solution to alleviate resource constraints faced by s in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between s of different scales. We propose Small , a small model assisted compensation method for cache compression. Small can maintain attention matching between different-scale s to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of Small . Moreover, efficiency evaluations show that Small achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant inference in resource constrained environments.","title":"SmallKV Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference"},{"location":"weekly_paper/2025-08-08/#eac-moe-expert-selection-aware-compressor-for-mixture-of-experts-large-language-models","text":"Authors: Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng 2025-08-03 http://arxiv.org/abs/2508.01625v1 Mixture-of-Experts (MoE) has demonstrated promising potential in scaling s. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE- s, which deeply aligns with the characteristics of MoE from the perspectives of quantization and , and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE- s. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.","title":"EAC-MoE Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models"},{"location":"weekly_paper/2025-08-08/#repoforge-training-a-sota-fast-thinking-swe-agent-with-an-end-to-end-data-curation-pipeline-synergizing-sft-and-rl-at-scale","text":"Authors: Zhilong Chen, Chengzong Zhao, Boyuan Chen, Dayi Lin, Yihao Chen, Arthur Leung, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Ahmed E. Hassan 2025-08-03 http://arxiv.org/abs/2508.01550v1 Training software engineering (SWE) s is bottlenecked by expensive infrastructure, inefficient evaluation pipelines, scarce training data, and costly quality control. We present RepoForge, an autonomous, end-to-end pipeline that generates, evaluates, and trains SWE agents at scale. Our key contributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on SWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new state-of-the-art for $\\leq$8B non-thinking s; (2) 7,304 executable environments auto-generated from real GitHub commits with zero manual intervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per instance) via intelligent dependency management and image ; (4) $>$70\\% faster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge harness; (5) 19,000$\\times$ cheaper labeling through our automated SPICE~\\citep{spice2024} difficulty assessment technique. By unifying storage-efficient sandboxing, Ray-powered evaluation harness, automated data generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate that even $\\leq$8B models can reach new state-of-the-art performance on demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical bottlenecks in SWE agent training: high storage costs of container-based evaluation, inefficient sequential reward pipelines, limited availability of high-quality training data, expensive manual labeling, and multi-turn RL pipeline bottlenecks.","title":"RepoForge Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale"},{"location":"weekly_paper/2025-08-08/#blocka2a-towards-secure-and-verifiable-agent-to-agent-interoperability","text":"Authors: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan 2025-08-02 http://arxiv.org/abs/2508.01332v2 The rapid adoption of agentic AI, powered by large language models ( s), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in -driven multi-agent systems (MASes): fragmented identity frameworks, insecure channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, -based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production -based MAS environments.","title":"BlockA2A Towards Secure and Verifiable Agent-to-Agent Interoperability"},{"location":"weekly_paper/2025-08-08/#unifying-mixture-of-experts-and-multi-head-latent-attention-for-efficient-language-models","text":"Authors: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat 2025-08-02 http://arxiv.org/abs/2508.01261v1 We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization. Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla s while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference . Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.","title":"Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models"},{"location":"weekly_paper/2025-08-08/#asking-the-right-questions-benchmarking-large-language-models-in-the-development-of-clinical-consultation-templates","text":"Authors: Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen 2025-08-02 http://arxiv.org/abs/2508.01159v1 This study evaluates the capacity of large language models ( s) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that s can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician .","title":"Asking the Right Questions Benchmarking Large Language Models in the Development of Clinical Consultation Templates"},{"location":"weekly_paper/2025-08-08/#towards-bridging-review-sparsity-in-recommendation-with-textual-edge-graph-representation","text":"Authors: Leyao Wang, Xutao Mao, Xuhui Zhan, Yuying Zhao, Bo Ni, Ryan A. Rossi, Nesreen K. Ahmed, Tyler Derr 2025-08-02 http://arxiv.org/abs/2508.01128v1 Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and -based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations.","title":"Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation"},{"location":"weekly_paper/2025-08-08/#react-a-real-time-edge-ai-based-v2x-framework-for-accident-avoidance-in-autonomous-driving-system","text":"Authors: Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu 2025-08-01 http://arxiv.org/abs/2508.01057v1 Collisions caused by human error are the most common type of multi-vehicle crash, highlighting the critical need for autonomous driving (AD) systems to leverage cooperative perception through Vehicle-to-Everything (V2X) . This capability extends situational awareness beyond the limitations of onboard sensors. However, current -based V2X frameworks suffer from limited generalization, shallow contextual reasoning, and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced reasoning and multimodal integration but typically fall short of real-time performance requirements in safety-critical applications. This paper presents REACT, a real-time, V2X-integrated trajectory optimization framework built upon a fine-tuned lightweight VLM. REACT integrates a set of specialized modules that process multimodal inputs into optimized, risk-aware trajectories. To ensure real-time performance on edge devices, REACT incorporates edge adaptation strategies that reduce model complexity and accelerate inference. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results demonstrate the feasibility of lightweight VLMs for real-time edge-based cooperative planning and showcase the potential of language-guided contextual reasoning to improve safety and responsiveness in autonomous driving.","title":"REACT A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System"},{"location":"weekly_paper/2025-08-08/#session-based-recommendation-with-validated-and-enriched-llm-intents","text":"Authors: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang 2025-08-01 http://arxiv.org/abs/2508.00570v1 Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models ( s), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched -generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of -generated intents with a global intent pool to constrain the 's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability.","title":"Session-Based Recommendation with Validated and Enriched LLM Intents"},{"location":"weekly_paper/2025-08-08/#reagan-node-as-agent-reasoning-graph-agentic-network","text":"Authors: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang 2025-08-01 http://arxiv.org/abs/2508.00429v2 Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain . Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.","title":"ReaGAN Node-as-Agent-Reasoning Graph Agentic Network"},{"location":"weekly_paper/2025-08-08/#edgeinfinite-instruct-bridging-sft-based-optimization-and-npu-level-efficiency-for-edge-devices","text":"Authors: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun 2025-08-01 http://arxiv.org/abs/2508.00370v2 Deploying Transformer-based large language models ( s) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value ( ) cache demands. While existing cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token . Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.","title":"EdgeInfinite-Instruct Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices"},{"location":"weekly_paper/2025-08-08/#systematic-evaluation-of-optimization-techniques-for-long-context-language-models","text":"Authors: Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar 2025-08-01 http://arxiv.org/abs/2508.00305v1 Large language models ( s) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like , quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.","title":"Systematic Evaluation of Optimization Techniques for Long-Context Language Models"},{"location":"weekly_paper/2025-08-08/#large-ai-model-enabled-secure-communications-in-low-altitude-wireless-networks-concepts-perspectives-and-case-study","text":"Authors: Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek 2025-08-01 http://arxiv.org/abs/2508.00256v1 Low-altitude wireless networks (LAWNs) have the potential to revolutionize s by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure s in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure s in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models ( s) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.","title":"Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks Concepts, Perspectives and Case Study"},{"location":"weekly_paper/latest/","text":"2025-08-12 Table of Contents BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts GLiClass Generalist Lightweight Model for Sequence Classification Tasks HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs LET-US Long Event-Text Understanding of Scenes Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning Aligning Effective Tokens with Video Anomaly in Large Language Models M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC KV Cache Compression for Inference Efficiency in LLMs A Review MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime Pragmatics beyond humans meaning, communication, and LLMs LLM Serving Optimization with Variable Prefill and Decode Lengths You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang 2025-08-11 http://arxiv.org/abs/2508.08127v1 The security of -based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard. TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake 2025-08-11 http://arxiv.org/abs/2508.08115v1 We present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models ( s). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.'s \"Big Five\" model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop , and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the task's requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains. ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon 2025-08-11 http://arxiv.org/abs/2508.08101v1 Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal , lateral , and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of -powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions. Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang 2025-08-11 http://arxiv.org/abs/2508.08001v1 \"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an -based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty decoding module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal. EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li 2025-08-11 http://arxiv.org/abs/2508.07809v1 Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models ( s) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes , limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger s for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration. We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables s to stably learn from initially unsolved hard problems under rewards. We apply EvoCoT to multiple families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables s to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research. Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li 2025-08-11 http://arxiv.org/abs/2508.07785v1 The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models ( s). MoE models facilitate scalability by enabling parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter s developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size. GLiClass Generalist Lightweight Model for Sequence Classification Tasks Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko 2025-08-11 http://arxiv.org/abs/2508.07662v1 Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative s have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data- conditions or from human feedback. HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han 2025-08-11 http://arxiv.org/abs/2508.07602v1 Invoking external tools enables Large Language Models ( s) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of s and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the . Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs Authors: Dom Huh, Prasant Mohapatra 2025-08-10 http://arxiv.org/abs/2508.07466v1 Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models ( s) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models ( s), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations. LET-US Long Event-Text Understanding of Scenes Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu 2025-08-10 http://arxiv.org/abs/2508.07401v1 Event cameras output event streams as , asynchronous data with microsecond-level temporal resolution, enabling visual perception with low latency and a high dynamic range. While existing Multimodal Large Language Models (M s) have achieved significant success in understanding and analyzing RGB video content, they either fail to interpret event streams effectively or remain constrained to very short sequences. In this paper, we introduce LET-US, a framework for long event-stream--text comprehension that employs an adaptive compression mechanism to reduce the volume of input events while preserving critical visual details. LET-US thus establishes a new frontier in cross-modal inferential understanding over extended event sequences. To bridge the substantial modality gap between event streams and textual representations, we adopt a two-stage optimization paradigm that progressively equips our model with the capacity to interpret event-based scenes. To handle the voluminous temporal information inherent in long event streams, we leverage text-guided cross-modal queries for feature reduction, augmented by hierarchical clustering and similarity computation to distill the most representative event features. Moreover, we curate and construct a large-scale event-text aligned dataset to train our model, achieving tighter alignment of event features within the embedding space. We also develop a comprehensive benchmark covering a diverse set of tasks -- reasoning, captioning, classification, temporal localization and moment retrieval. Experimental results demonstrate that LET-US outperforms prior state-of-the-art M s in both descriptive accuracy and semantic comprehension on long-duration event streams. All datasets, codes, and models will be publicly available. Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang 2025-08-10 http://arxiv.org/abs/2508.07329v1 With the breakthrough progress of large language models ( s) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved. BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang 2025-08-10 http://arxiv.org/abs/2508.07300v1 Real-time semantic segmentation presents the dual challenge of designing efficient architectures that capture large receptive fields for semantic understanding while also refining detailed contours. Vision s model long-range dependencies effectively but incur high computational cost. To address these challenges, we introduce the Large Kernel Attention (LKA) mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet) expands the receptive field to capture contextual information and extracts visual and structural features using Sparse Decomposed Large Separable Kernel Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism dynamically adapts the receptive field to further enhance performance. Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches contextual features by synergistically combining dilated convolutions and large kernel attention. The bilateral architecture facilitates frequent branch , and the Boundary Guided Adaptive Fusion (BGAF) module enhances boundary delineation by integrating spatial and semantic features under boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding 79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet pretraining, demonstrating state-of-the-art performance. The code and model is available at https://github.com/maomao0819/BEVANet. LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07227v1 inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token and dynamic workload scheduling to accelerate speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits. DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan 2025-08-10 http://arxiv.org/abs/2508.07185v1 Large Language Models ( s) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables s to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a knowledge attention mechanism, which allows the to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building s that can stay current with the ever-changing world. How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak 2025-08-10 http://arxiv.org/abs/2508.07127v1 Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and ly annotated datasets remains a non-trivial task. Recently, s has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned s to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how s can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of s in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care. From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya 2025-08-09 http://arxiv.org/abs/2508.07117v1 Graph Neural Networks (GNNs) have emerged as powerful tools for learning over structured data, including text-attributed graphs, which are common in domains such as citation networks, social platforms, and knowledge graphs. GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language. In this work, we introduce LOGIC, a lightweight, post-hoc framework that uses large language models ( s) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure. This enables the to reason about GNN internal representations and produce natural language explanations along with concise explanation subgraphs. Our experiments across four real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off between fidelity and , while significantly improving human-centric metrics such as insightfulness. LOGIC sets a new direction for -based explainability in graph learning by aligning GNN internals with human reasoning. Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV Authors: Roberto Balestri, Guglielmo Pescatore 2025-08-09 http://arxiv.org/abs/2508.07010v1 Serialized television narratives present significant analytical challenges due to their complex, temporally distributed storylines that necessitate sophisticated information management. This paper introduces a multi-agent system (MAS) designed to extract and analyze narrative arcs by implementing principles of computational memory architectures. The system conceptualizes narrative understanding through analogues of human memory: Large Language Models ( s) provide a form of semantic memory for general narrative patterns, while a vector database stores specific arc progressions as episodic memories. A multi-agent workflow simulates working memory processes to integrate these information types. Tested on the first season of Grey's Anatomy (ABC 2005-), the MAS identifies three arc types: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific. These arcs and their episodic developments are stored in a vector database, facilitating structured analysis and semantic comparison. To bridge automation with critical interpretation, a graphical interface enables human oversight and refinement of the system's narrative memory. While demonstrating strong performance in identifying Anthology Arcs and character entities, the system's reliance on textual paratexts (episode summaries) revealed limitations in discerning ping arcs and opaque dynamics, underscoring the challenges in computational memory consolidation versus human holistic understanding. This memory-centric approach highlights the potential of combining AI-driven memory processing with human expertise. Beyond television, it offers promise for serialized written formats where narrative is entirely text-based. Future work will focus on integrating multimodal inputs to enrich episodic memory, refining memory integration mechanisms within the MAS, and expanding testing across diverse genres. SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn 2025-08-09 http://arxiv.org/abs/2508.06978v1 Large Language Models ( s) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical decode stage of inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to ~12x compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude. Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan 2025-08-09 http://arxiv.org/abs/2508.06765v1 Collaboratively fine-tuning (FT) large language models ( s) over heterogeneous mobile devices fosters immense potential applications of personalized intelligence. However, such a vision faces critical system challenges. Conventional federated FT approaches place prohibitive computational and memory burdens on mobile hardware, and their synchronous model aggregation protocols stall for slower devices. In this paper, we propose Fed Mobi , a novel design to facilitate efficient federated FT across mobile devices with diverse computing/ speeds and local model architectures. In particular, Fed Mobi implements a pioneering server-assisted federated side-tuning paradigm. Briefly, mobile devices perform lightweight forward propagation computations on local data using their frozen pre-scaled backbone s, and then upload selected intermediate activations. The server trains a shared side-network independently, eliminating client-side backpropagation and enabling asynchronous updates. To bridge model heterogeneity across different devices, we introduce an adaptive layer-wise feature alignment method, which ensures consistent representations for collaboratively tuning a shared side network. Extensive experimental results demonstrate that Fed Mobi can maintain robust fine-tuning performance while achieving extremely low on-device memory, with at least 95.2% reduction in computation overhead, 93.2% reduction in costs and 5.1x faster convergence compared to existing methods, validating its efficacy for practical adaptation over heterogeneous mobile devices. SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning Authors: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang 2025-08-08 http://arxiv.org/abs/2508.06447v1 Long-context inference for Large Language Models ( s) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that s can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise naturally enables an asynchronous cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token (TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance. Aligning Effective Tokens with Video Anomaly in Large Language Models Authors: Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu 2025-08-08 http://arxiv.org/abs/2508.06350v1 Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (M s) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models ( s), we propose VA-GPT, a novel M designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and s through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware M s, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks. M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang 2025-08-08 http://arxiv.org/abs/2508.06328v1 Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models ( s), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation. Here, we introduce M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. Central to our framework is an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image selection and placement in a controllable and semantically aligned manner. Empirical results show that our lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency. Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC Authors: Ruichong Zhang 2025-08-08 http://arxiv.org/abs/2508.06309v1 In recent years, concerns about intellectual property (IP) in large language models ( s) have grown significantly. Plagiarizing other s (through direct weight copying, upcycling, , or continual pretraining) and claiming authorship without properly attributing to the original license, is a serious misconduct that can lead to significant financial and reputational harm to the original developers. However, existing methods for detecting plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related. To address these limitations, we propose Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory. MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference. Experimental results demonstrate that MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible. KV Cache Compression for Inference Efficiency in LLMs A Review Authors: Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang 2025-08-08 http://arxiv.org/abs/2508.06297v1 Withtherapid advancement of large language models ( s), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value ( ) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models. MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration Authors: Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu 2025-08-08 http://arxiv.org/abs/2508.06189v1 With the of urbanization, criminal behavior in public scenes poses an increasingly serious threat to social security. Traditional anomaly detection methods based on feature recognition struggle to capture high-level behavioral semantics from historical information, while generative approaches based on Large Language Models ( s) often fail to meet real-time requirements. To address these challenges, we propose MA-CBP, a criminal behavior prediction framework based on multi-agent asynchronous collaboration. This framework transforms real-time video streams into frame-level semantic descriptions, constructs causally consistent historical summaries, and fuses adjacent image frames to perform joint reasoning over long- and short-term contexts. The resulting behavioral decisions include key elements such as event subjects, locations, and causes, enabling early warning of potential criminal activity. In addition, we construct a high-quality criminal behavior dataset that provides multi-scale language supervision, including frame-level, summary-level, and event-level semantic annotations. Experimental results demonstrate that our method achieves superior performance on multiple datasets and offers a promising solution for risk warning in urban public safety scenarios. Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime Authors: Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira 2025-08-08 http://arxiv.org/abs/2508.06178v1 Large language models ( s) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into s and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in s with limited data at https://github.com/hugoabonizio/knowledge-injection-methods. Pragmatics beyond humans meaning, communication, and LLMs Authors: V\u00edt Gvo\u017ediak 2025-08-08 http://arxiv.org/abs/2508.06167v1 The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models ( s) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of s. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like s. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for involving generative AI. LLM Serving Optimization with Variable Prefill and Decode Lengths Authors: Meixuan Wang, Yinyu Ye, Zijie Zhou 2025-08-08 http://arxiv.org/abs/2508.06133v1 We study the problem of serving (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency. You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures Authors: Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang 2025-08-08 http://arxiv.org/abs/2508.06105v1 Large language models ( s) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \\textbf{\\underline{Logic}}-aware \\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented \\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph to reduce redundant retrieval and uses context to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines. ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline Authors: Morris Alper, Moran Yanuka, Raja Giryes, Ga\u0161per Begu\u0161 2025-08-08 http://arxiv.org/abs/2508.06094v1 Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international . Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern s as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages s' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise. Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale Authors: Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez 2025-08-08 http://arxiv.org/abs/2508.05938v1 Detecting prosociality in text-- intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best -based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\\sim$35\\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving high precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.","title":"Weekly Paper"},{"location":"weekly_paper/latest/#2025-08-12","text":"","title":"2025-08-12"},{"location":"weekly_paper/latest/#table-of-contents","text":"BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts GLiClass Generalist Lightweight Model for Sequence Classification Tasks HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs LET-US Long Event-Text Understanding of Scenes Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction? From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning Aligning Effective Tokens with Video Anomaly in Large Language Models M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC KV Cache Compression for Inference Efficiency in LLMs A Review MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime Pragmatics beyond humans meaning, communication, and LLMs LLM Serving Optimization with Variable Prefill and Decode Lengths You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale","title":"Table of Contents"},{"location":"weekly_paper/latest/#blindguard-safeguarding-llm-based-multi-agent-systems-under-unknown-attacks","text":"Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang 2025-08-11 http://arxiv.org/abs/2508.08127v1 The security of -based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.","title":"BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"},{"location":"weekly_paper/latest/#teammedagents-enhancing-medical-decision-making-of-llms-through-structured-teamwork","text":"Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake 2025-08-11 http://arxiv.org/abs/2508.08115v1 We present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models ( s). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.'s \"Big Five\" model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop , and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the task's requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains.","title":"TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"},{"location":"weekly_paper/latest/#chatgpt-on-the-road-leveraging-large-language-model-powered-in-vehicle-conversational-agents-for-safer-and-more-enjoyable-driving-experience","text":"Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon 2025-08-11 http://arxiv.org/abs/2508.08101v1 Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal , lateral , and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of -powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions.","title":"ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience"},{"location":"weekly_paper/latest/#interpreting-fedspeak-with-confidence-a-llm-based-uncertainty-aware-framework-guided-by-monetary-policy-transmission-paths","text":"Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang 2025-08-11 http://arxiv.org/abs/2508.08001v1 \"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an -based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty decoding module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal.","title":"Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"},{"location":"weekly_paper/latest/#evocot-overcoming-the-exploration-bottleneck-in-reinforcement-learning","text":"Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li 2025-08-11 http://arxiv.org/abs/2508.07809v1 Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models ( s) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes , limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger s for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration. We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables s to stably learn from initially unsolved hard problems under rewards. We apply EvoCoT to multiple families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables s to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.","title":"EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning"},{"location":"weekly_paper/latest/#grove-moe-towards-efficient-and-superior-moe-llms-with-adjugate-experts","text":"Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li 2025-08-11 http://arxiv.org/abs/2508.07785v1 The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models ( s). MoE models facilitate scalability by enabling parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter s developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.","title":"Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts"},{"location":"weekly_paper/latest/#gliclass-generalist-lightweight-model-for-sequence-classification-tasks","text":"Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko 2025-08-11 http://arxiv.org/abs/2508.07662v1 Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative s have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data- conditions or from human feedback.","title":"GLiClass Generalist Lightweight Model for Sequence Classification Tasks"},{"location":"weekly_paper/latest/#hgmf-a-hierarchical-gaussian-mixture-framework-for-scalable-tool-invocation-within-the-model-context-protocol","text":"Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han 2025-08-11 http://arxiv.org/abs/2508.07602v1 Invoking external tools enables Large Language Models ( s) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of s and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the . Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries.","title":"HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol"},{"location":"weekly_paper/latest/#grounding-natural-language-for-multi-agent-decision-making-with-multi-agentic-llms","text":"Authors: Dom Huh, Prasant Mohapatra 2025-08-10 http://arxiv.org/abs/2508.07466v1 Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models ( s) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models ( s), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations.","title":"Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"},{"location":"weekly_paper/latest/#let-us-long-event-text-understanding-of-scenes","text":"Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu 2025-08-10 http://arxiv.org/abs/2508.07401v1 Event cameras output event streams as , asynchronous data with microsecond-level temporal resolution, enabling visual perception with low latency and a high dynamic range. While existing Multimodal Large Language Models (M s) have achieved significant success in understanding and analyzing RGB video content, they either fail to interpret event streams effectively or remain constrained to very short sequences. In this paper, we introduce LET-US, a framework for long event-stream--text comprehension that employs an adaptive compression mechanism to reduce the volume of input events while preserving critical visual details. LET-US thus establishes a new frontier in cross-modal inferential understanding over extended event sequences. To bridge the substantial modality gap between event streams and textual representations, we adopt a two-stage optimization paradigm that progressively equips our model with the capacity to interpret event-based scenes. To handle the voluminous temporal information inherent in long event streams, we leverage text-guided cross-modal queries for feature reduction, augmented by hierarchical clustering and similarity computation to distill the most representative event features. Moreover, we curate and construct a large-scale event-text aligned dataset to train our model, achieving tighter alignment of event features within the embedding space. We also develop a comprehensive benchmark covering a diverse set of tasks -- reasoning, captioning, classification, temporal localization and moment retrieval. Experimental results demonstrate that LET-US outperforms prior state-of-the-art M s in both descriptive accuracy and semantic comprehension on long-duration event streams. All datasets, codes, and models will be publicly available.","title":"LET-US Long Event-Text Understanding of Scenes"},{"location":"weekly_paper/latest/#efficient-edge-llms-deployment-via-hessianaware-quantization-and-cpu-gpu-collaborative","text":"Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang 2025-08-10 http://arxiv.org/abs/2508.07329v1 With the breakthrough progress of large language models ( s) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.","title":"Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative"},{"location":"weekly_paper/latest/#bevanet-bilateral-efficient-visual-attention-network-for-real-time-semantic-segmentation","text":"Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang 2025-08-10 http://arxiv.org/abs/2508.07300v1 Real-time semantic segmentation presents the dual challenge of designing efficient architectures that capture large receptive fields for semantic understanding while also refining detailed contours. Vision s model long-range dependencies effectively but incur high computational cost. To address these challenges, we introduce the Large Kernel Attention (LKA) mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet) expands the receptive field to capture contextual information and extracts visual and structural features using Sparse Decomposed Large Separable Kernel Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism dynamically adapts the receptive field to further enhance performance. Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches contextual features by synergistically combining dilated convolutions and large kernel attention. The bilateral architecture facilitates frequent branch , and the Boundary Guided Adaptive Fusion (BGAF) module enhances boundary delineation by integrating spatial and semantic features under boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding 79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet pretraining, demonstrating state-of-the-art performance. The code and model is available at https://github.com/maomao0819/BEVANet.","title":"BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"},{"location":"weekly_paper/latest/#lp-spec-leveraging-lpddr-pim-for-efficient-llm-mobile-speculative-inference-with-architecture-dataflow-co-optimization","text":"Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia 2025-08-10 http://arxiv.org/abs/2508.07227v1 inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token and dynamic workload scheduling to accelerate speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits.","title":"LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization"},{"location":"weekly_paper/latest/#dysk-attn-a-framework-for-efficient-real-time-knowledge-updating-in-large-language-models-via-dynamic-sparse-knowledge-attention","text":"Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan 2025-08-10 http://arxiv.org/abs/2508.07185v1 Large Language Models ( s) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables s to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a knowledge attention mechanism, which allows the to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building s that can stay current with the ever-changing world.","title":"DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"},{"location":"weekly_paper/latest/#how-effectively-can-large-language-models-connect-snp-variants-and-ecg-phenotypes-for-cardiovascular-risk-prediction","text":"Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak 2025-08-10 http://arxiv.org/abs/2508.07127v1 Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and ly annotated datasets remains a non-trivial task. Recently, s has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned s to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how s can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of s in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.","title":"How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?"},{"location":"weekly_paper/latest/#from-nodes-to-narratives-explaining-graph-neural-networks-with-llms-and-graph-context","text":"Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya 2025-08-09 http://arxiv.org/abs/2508.07117v1 Graph Neural Networks (GNNs) have emerged as powerful tools for learning over structured data, including text-attributed graphs, which are common in domains such as citation networks, social platforms, and knowledge graphs. GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language. In this work, we introduce LOGIC, a lightweight, post-hoc framework that uses large language models ( s) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure. This enables the to reason about GNN internal representations and produce natural language explanations along with concise explanation subgraphs. Our experiments across four real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off between fidelity and , while significantly improving human-centric metrics such as insightfulness. LOGIC sets a new direction for -based explainability in graph learning by aligning GNN internals with human reasoning.","title":"From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context"},{"location":"weekly_paper/latest/#narrative-memory-in-machines-multi-agent-arc-extraction-in-serialized-tv","text":"Authors: Roberto Balestri, Guglielmo Pescatore 2025-08-09 http://arxiv.org/abs/2508.07010v1 Serialized television narratives present significant analytical challenges due to their complex, temporally distributed storylines that necessitate sophisticated information management. This paper introduces a multi-agent system (MAS) designed to extract and analyze narrative arcs by implementing principles of computational memory architectures. The system conceptualizes narrative understanding through analogues of human memory: Large Language Models ( s) provide a form of semantic memory for general narrative patterns, while a vector database stores specific arc progressions as episodic memories. A multi-agent workflow simulates working memory processes to integrate these information types. Tested on the first season of Grey's Anatomy (ABC 2005-), the MAS identifies three arc types: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific. These arcs and their episodic developments are stored in a vector database, facilitating structured analysis and semantic comparison. To bridge automation with critical interpretation, a graphical interface enables human oversight and refinement of the system's narrative memory. While demonstrating strong performance in identifying Anthology Arcs and character entities, the system's reliance on textual paratexts (episode summaries) revealed limitations in discerning ping arcs and opaque dynamics, underscoring the challenges in computational memory consolidation versus human holistic understanding. This memory-centric approach highlights the potential of combining AI-driven memory processing with human expertise. Beyond television, it offers promise for serialized written formats where narrative is entirely text-based. Future work will focus on integrating multimodal inputs to enrich episodic memory, refining memory integration mechanisms within the MAS, and expanding testing across diverse genres.","title":"Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV"},{"location":"weekly_paper/latest/#ssd-offloading-for-llm-mixture-of-experts-weights-considered-harmful-in-energy-efficiency","text":"Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn 2025-08-09 http://arxiv.org/abs/2508.06978v1 Large Language Models ( s) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical decode stage of inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to ~12x compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude.","title":"SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency"},{"location":"weekly_paper/latest/#fed-mobillm-efficient-federated-llm-fine-tuning-over-heterogeneous-mobile-devices-via-server-assisted-side-tuning","text":"Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan 2025-08-09 http://arxiv.org/abs/2508.06765v1 Collaboratively fine-tuning (FT) large language models ( s) over heterogeneous mobile devices fosters immense potential applications of personalized intelligence. However, such a vision faces critical system challenges. Conventional federated FT approaches place prohibitive computational and memory burdens on mobile hardware, and their synchronous model aggregation protocols stall for slower devices. In this paper, we propose Fed Mobi , a novel design to facilitate efficient federated FT across mobile devices with diverse computing/ speeds and local model architectures. In particular, Fed Mobi implements a pioneering server-assisted federated side-tuning paradigm. Briefly, mobile devices perform lightweight forward propagation computations on local data using their frozen pre-scaled backbone s, and then upload selected intermediate activations. The server trains a shared side-network independently, eliminating client-side backpropagation and enabling asynchronous updates. To bridge model heterogeneity across different devices, we introduce an adaptive layer-wise feature alignment method, which ensures consistent representations for collaboratively tuning a shared side network. Extensive experimental results demonstrate that Fed Mobi can maintain robust fine-tuning performance while achieving extremely low on-device memory, with at least 95.2% reduction in computation overhead, 93.2% reduction in costs and 5.1x faster convergence compared to existing methods, validating its efficacy for practical adaptation over heterogeneous mobile devices.","title":"Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"},{"location":"weekly_paper/latest/#sliminfer-accelerating-long-context-llm-inference-via-dynamic-token-pruning","text":"Authors: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang 2025-08-08 http://arxiv.org/abs/2508.06447v1 Long-context inference for Large Language Models ( s) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that s can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise naturally enables an asynchronous cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token (TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.","title":"SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning"},{"location":"weekly_paper/latest/#aligning-effective-tokens-with-video-anomaly-in-large-language-models","text":"Authors: Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu 2025-08-08 http://arxiv.org/abs/2508.06350v1 Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (M s) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models ( s), we propose VA-GPT, a novel M designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and s through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware M s, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks.","title":"Aligning Effective Tokens with Video Anomaly in Large Language Models"},{"location":"weekly_paper/latest/#m2io-r1-an-efficient-rl-enhanced-reasoning-framework-for-multimodal-retrieval-augmented-multimodal-generation","text":"Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang 2025-08-08 http://arxiv.org/abs/2508.06328v1 Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models ( s), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation. Here, we introduce M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. Central to our framework is an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image selection and placement in a controllable and semantically aligned manner. Empirical results show that our lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency.","title":"M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation"},{"location":"weekly_paper/latest/#matrix-driven-instant-review-confident-detection-and-reconstruction-of-llm-plagiarism-on-pc","text":"Authors: Ruichong Zhang 2025-08-08 http://arxiv.org/abs/2508.06309v1 In recent years, concerns about intellectual property (IP) in large language models ( s) have grown significantly. Plagiarizing other s (through direct weight copying, upcycling, , or continual pretraining) and claiming authorship without properly attributing to the original license, is a serious misconduct that can lead to significant financial and reputational harm to the original developers. However, existing methods for detecting plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related. To address these limitations, we propose Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory. MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference. Experimental results demonstrate that MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible.","title":"Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC"},{"location":"weekly_paper/latest/#kv-cache-compression-for-inference-efficiency-in-llms-a-review","text":"Authors: Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang 2025-08-08 http://arxiv.org/abs/2508.06297v1 Withtherapid advancement of large language models ( s), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value ( ) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.","title":"KV Cache Compression for Inference Efficiency in LLMs A Review"},{"location":"weekly_paper/latest/#ma-cbp-a-criminal-behavior-prediction-framework-based-on-multi-agent-asynchronous-collaboration","text":"Authors: Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu 2025-08-08 http://arxiv.org/abs/2508.06189v1 With the of urbanization, criminal behavior in public scenes poses an increasingly serious threat to social security. Traditional anomaly detection methods based on feature recognition struggle to capture high-level behavioral semantics from historical information, while generative approaches based on Large Language Models ( s) often fail to meet real-time requirements. To address these challenges, we propose MA-CBP, a criminal behavior prediction framework based on multi-agent asynchronous collaboration. This framework transforms real-time video streams into frame-level semantic descriptions, constructs causally consistent historical summaries, and fuses adjacent image frames to perform joint reasoning over long- and short-term contexts. The resulting behavioral decisions include key elements such as event subjects, locations, and causes, enabling early warning of potential criminal activity. In addition, we construct a high-quality criminal behavior dataset that provides multi-scale language supervision, including frame-level, summary-level, and event-level semantic annotations. Experimental results demonstrate that our method achieves superior performance on multiple datasets and offers a promising solution for risk warning in urban public safety scenarios.","title":"MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration"},{"location":"weekly_paper/latest/#comparing-knowledge-injection-methods-for-llms-in-a-low-resource-regime","text":"Authors: Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira 2025-08-08 http://arxiv.org/abs/2508.06178v1 Large language models ( s) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into s and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in s with limited data at https://github.com/hugoabonizio/knowledge-injection-methods.","title":"Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime"},{"location":"weekly_paper/latest/#pragmatics-beyond-humans-meaning-communication-and-llms","text":"Authors: V\u00edt Gvo\u017ediak 2025-08-08 http://arxiv.org/abs/2508.06167v1 The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models ( s) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of s. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like s. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for involving generative AI.","title":"Pragmatics beyond humans meaning, communication, and LLMs"},{"location":"weekly_paper/latest/#llm-serving-optimization-with-variable-prefill-and-decode-lengths","text":"Authors: Meixuan Wang, Yinyu Ye, Zijie Zhou 2025-08-08 http://arxiv.org/abs/2508.06133v1 We study the problem of serving (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.","title":"LLM Serving Optimization with Variable Prefill and Decode Lengths"},{"location":"weekly_paper/latest/#you-dont-need-pre-built-graphs-for-rag-retrieval-augmented-generation-with-adaptive-reasoning-structures","text":"Authors: Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang 2025-08-08 http://arxiv.org/abs/2508.06105v1 Large language models ( s) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \\textbf{\\underline{Logic}}-aware \\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented \\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph to reduce redundant retrieval and uses context to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.","title":"You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures"},{"location":"weekly_paper/latest/#conlangcrafter-constructing-languages-with-a-multi-hop-llm-pipeline","text":"Authors: Morris Alper, Moran Yanuka, Raja Giryes, Ga\u0161per Begu\u0161 2025-08-08 http://arxiv.org/abs/2508.06094v1 Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international . Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern s as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages s' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.","title":"ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline"},{"location":"weekly_paper/latest/#prosocial-behavior-detection-in-player-game-chat-from-aligning-human-ai-definitions-to-efficient-annotation-at-scale","text":"Authors: Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez 2025-08-08 http://arxiv.org/abs/2508.05938v1 Detecting prosociality in text-- intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best -based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\\sim$35\\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving high precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.","title":"Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale"}]}