# 2024-10-11

# Table of Contents
* [Retrieval-Augmented Decision Transformer External Memory for In-context RL](#Retrieval-Augmented-Decision-Transformer-External-Memory-for-In-context-RL)
* [Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models](#Sparse-Autoencoders-Reveal-Universal-Feature-Spaces-Across-Large-Language-Models)
* [SWIFT On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration](#SWIFT-On-the-Fly-Self-Speculative-Decoding-for-LLM-Inference-Acceleration)
* [Learning a generalized multiscale prolongation operator](#Learning-a-generalized-multiscale-prolongation-operator)
* [DreamMesh4D Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation](#DreamMesh4D-Video-to-4D-Generation-with-Sparse-Controlled-Gaussian-Mesh-Hybrid-Representation)
* [Scaling Laws for Mixed quantization in Large Language Models](#Scaling-Laws-for-Mixed-quantization-in-Large-Language-Models)
* [Towards Universality Studying Mechanistic Similarity Across Language Model Architectures](#Towards-Universality-Studying-Mechanistic-Similarity-Across-Language-Model-Architectures)
* [Chip-Tuning Classify Before Language Models Say](#Chip-Tuning-Classify-Before-Language-Models-Say)
* [Transformer-assisted Parametric CSI Feedback for mmWave Massive MIMO Systems](#Transformer-assisted-Parametric-CSI-Feedback-for-mmWave-Massive-MIMO-Systems)
* [LLM Compression with Neural Architecture Search](#LLM-Compression-with-Neural-Architecture-Search)
* [Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs](#Functional-level-Uncertainty-Quantification-for-Calibrated-Fine-tuning-on-LLMs)


## Retrieval-Augmented Decision Transformer External Memory for In-context RL

>Authors: Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter

>2024-10-09

> http://arxiv.org/abs/2410.07071v1

In-context learning (ICL) is the ability of a model to learn a new task by
observing a few exemplars in its context. While prevalent in NLP, this
capability has recently also been observed in Reinforcement Learning (RL)
settings. Prior in-context RL methods, however, require entire episodes in the
agent's context. Given that complex environments typically lead to long
episodes with sparse rewards, these methods are constrained to simple
environments with short episodes. To address these challenges, we introduce
Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external
memory mechanism to store past experiences from which it retrieves only
sub-trajectories relevant for the current situation. The retrieval component in
RA-DT does not require training and can be entirely domain-agnostic. We
evaluate the capabilities of RA-DT on grid-world environments, robotics
simulations, and procedurally-generated video games. On grid-worlds, RA-DT
outperforms baselines, while using only a fraction of their context length.
Furthermore, we illuminate the limitations of current in-context RL methods on
complex environments and discuss future directions. To facilitate future
research, we release datasets for four of the considered environments.


## Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models

>Authors: Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez

>2024-10-09

> http://arxiv.org/abs/2410.06981v1

We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality.


## SWIFT On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration

>Authors: Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li

>2024-10-09

> http://arxiv.org/abs/2410.06916v1

Speculative decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by first employing a compact model to draft multiple tokens
efficiently and then using the target LLM to verify them in parallel. While
this technique has achieved notable speedups, most existing approaches
necessitate either additional parameters or extensive training to construct
effective draft models, thereby restricting their applicability across
different LLMs and tasks. To address this limitation, we explore a novel
plug-and-play SD solution with layer-skipping, which skips intermediate layers
of the target LLM as the compact draft model. Our analysis reveals that LLMs
exhibit great potential for self-acceleration through layer sparsity and the
task-specific nature of this sparsity. Building on these insights, we introduce
SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively
selects intermediate layers of LLMs to skip during inference. SWIFT does not
require auxiliary models or additional training, making it a plug-and-play
solution for accelerating LLM inference across diverse input data streams. Our
extensive experiments across a wide range of models and downstream tasks
demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving
the original distribution of the generated text.


## Learning a generalized multiscale prolongation operator

>Authors: Yucheng Liu, Shubin Fu, Yingjie Zhou, Changqing Ye, Eric T. Chung

>2024-10-09

> http://arxiv.org/abs/2410.06832v1

Multigrid preconditioners are one of the most powerful techniques for solving
large sparse linear systems. In this research, we address Darcy flow problems
with random permeability using the conjugate gradient method, enhanced by a
two-grid preconditioner based on a generalized multiscale prolongation
operator, which has been demonstrated to be stable for high contrast profiles.
To circumvent the need for repeatedly solving spectral problems with varying
coefficients, we harness deep learning techniques to expedite the construction
of the generalized multiscale prolongation operator. Considering linear
transformations on multiscale basis have no impact on the performance of the
preconditioner, we devise a loss function by the coefficient-based distance
between subspaces instead of $l^2$-norm of the difference of the corresponding
multiscale bases. We discover that leveraging the inherent symmetry in the
local spectral problem can effectively accelerate the neural network training
process. In scenarios where training data are limited, we utilize the
Karhunen-Lo\`eve expansion to augment the dataset. Extensive numerical
experiments with various types of random coefficient models are exhibited,
showing that the proposed method can significantly reduce the time required to
generate the prolongation operator while maintaining the original efficiency of
the two-grid preconditioner.


## DreamMesh4D Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation

>Authors: Zhiqi Li, Yiming Chen, Peidong Liu

>2024-10-09

> http://arxiv.org/abs/2410.06756v1

Recent advancements in 2D/3D generative techniques have facilitated the
generation of dynamic 3D objects from monocular videos. Previous methods mainly
rely on the implicit neural radiance fields (NeRF) or explicit Gaussian
Splatting as the underlying representation, and struggle to achieve
satisfactory spatial-temporal consistency and surface appearance. Drawing
inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a
novel framework combining mesh representation with geometric skinning technique
to generate high-quality 4D object from a monocular video. Instead of utilizing
classical texture map for appearance, we bind Gaussian splats to triangle face
of mesh for differentiable optimization of both the texture and mesh vertices.
In particular, DreamMesh4D begins with a coarse mesh obtained through an
image-to-3D generation procedure. Sparse points are then uniformly sampled
across the mesh surface, and are used to build a deformation graph to drive the
motion of the 3D object for the sake of computational efficiency and providing
additional constraint. For each step, transformations of sparse control points
are predicted using a deformation network, and the mesh vertices as well as the
surface Gaussians are deformed via a novel geometric skinning algorithm, which
is a hybrid approach combining LBS (linear blending skinning) and DQS
(dual-quaternion skinning), mitigating drawbacks associated with both
approaches. The static surface Gaussians and mesh vertices as well as the
deformation network are learned via reference view photometric loss, score
distillation loss as well as other regularizers in a two-stage manner.
Extensive experiments demonstrate superior performance of our method.
Furthermore, our method is compatible with modern graphic pipelines, showcasing
its potential in the 3D gaming and film industry.


## Scaling Laws for Mixed quantization in Large Language Models

>Authors: Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao

>2024-10-09

> http://arxiv.org/abs/2410.06722v1

Post-training quantization of Large Language Models (LLMs) has proven
effective in reducing the computational requirements for running inference on
these models. In this study, we focus on a straightforward question: When
aiming for a specific accuracy or perplexity target for low-precision
quantization, how many high-precision numbers or calculations are required to
preserve as we scale LLMs to larger sizes? We first introduce a critical metric
named the quantization ratio, which compares the number of parameters quantized
to low-precision arithmetic against the total parameter count. Through
extensive and carefully controlled experiments across different model families,
arithmetic types, and quantization granularities (e.g. layer-wise,
matmul-wise), we identify two central phenomenons. 1) The larger the models,
the better they can preserve performance with an increased quantization ratio,
as measured by perplexity in pre-training tasks or accuracy in downstream
tasks. 2) The finer the granularity of mixed-precision quantization (e.g.,
matmul-wise), the more the model can increase the quantization ratio. We
believe these observed phenomena offer valuable insights for future AI hardware
design and the development of advanced Efficient AI algorithms.


## Towards Universality Studying Mechanistic Similarity Across Language Model Architectures

>Authors: Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu

>2024-10-09

> http://arxiv.org/abs/2410.06672v2

The hypothesis of Universality in interpretability suggests that different
neural networks may converge to implement similar algorithms on similar tasks.
In this work, we investigate two mainstream architectures for language
modeling, namely Transformers and Mambas, to explore the extent of their
mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate
interpretable features from these models and show that most features are
similar in these two models. We also validate the correlation between feature
similarity and Universality. We then delve into the circuit-level analysis of
Mamba models and find that the induction circuits in Mamba are structurally
analogous to those in Transformers. We also identify a nuanced difference we
call \emph{Off-by-One motif}: The information of one token is written into the
SSM state in its next position. Whilst interaction between tokens in
Transformers does not exhibit such trend.


## Chip-Tuning Classify Before Language Models Say

>Authors: Fangwei Zhu, Dian Li, Jiajun Huang, Gang Liu, Hui Wang, Zhifang Sui

>2024-10-09

> http://arxiv.org/abs/2410.06541v1

The rapid development in the performance of large language models (LLMs) is
accompanied by the escalation of model size, leading to the increasing cost of
model training and inference. Previous research has discovered that certain
layers in LLMs exhibit redundancy, and removing these layers brings only
marginal loss in model performance. In this paper, we adopt the probing
technique to explain the layer redundancy in LLMs and demonstrate that language
models can be effectively pruned with probing classifiers. We propose
chip-tuning, a simple and effective structured pruning framework specialized
for classification problems. Chip-tuning attaches tiny probing classifiers
named chips to different layers of LLMs, and trains chips with the backbone
model frozen. After selecting a chip for classification, all layers subsequent
to the attached layer could be removed with marginal performance loss.
Experimental results on various LLMs and datasets demonstrate that chip-tuning
significantly outperforms previous state-of-the-art baselines in both accuracy
and pruning ratio, achieving a pruning ratio of up to 50%. We also find that
chip-tuning could be applied on multimodal models, and could be combined with
model finetuning, proving its excellent compatibility.


## Transformer-assisted Parametric CSI Feedback for mmWave Massive MIMO Systems

>Authors: Hyungyu Ju, Seokhyun Jeong, Seungnyun Kim, Byungju Lee, Byonghyo Shim

>2024-10-09

> http://arxiv.org/abs/2410.06504v1

As a key technology to meet the ever-increasing data rate demand in beyond 5G
and 6G communications, millimeter-wave (mmWave) massive multiple-input
multiple-output (MIMO) systems have gained much attention recently.To make the
most of mmWave massive MIMO systems, acquisition of accurate channel state
information (CSI) at the base station (BS) is crucial. However, this task is by
no means easy due to the CSI feedback overhead induced by the large number of
antennas. In this paper, we propose a parametric CSI feedback technique for
mmWave massive MIMO systems. Key idea of the proposed technique is to compress
the mmWave MIMO channel matrix into a few geometric channel parameters (e.g.,
angles, delays, and path gains). Due to the limited scattering of mmWave
signal, the number of channel parameters is much smaller than the number of
antennas, thereby reducing the CSI feedback overhead significantly. Moreover,
by exploiting the deep learning (DL) technique for the channel parameter
extraction and the MIMO channel reconstruction, we can effectively suppress the
channel quantization error. From the numerical results, we demonstrate that the
proposed technique outperforms the conventional CSI feedback techniques in
terms of normalized mean square error (NMSE) and bit error rate (BER).


## LLM Compression with Neural Architecture Search

>Authors: Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein

>2024-10-09

> http://arxiv.org/abs/2410.06479v1

Large language models (LLMs) exhibit remarkable reasoning abilities, allowing
them to generalize across a wide range of downstream tasks, such as commonsense
reasoning or instruction following. However, as LLMs scale, inference costs
become increasingly prohibitive, accumulating significantly over their life
cycle. This poses the question: Can we compress pre-trained LLMs to meet
diverse size and latency requirements? We leverage Neural Architecture Search
(NAS) to compress LLMs by pruning structural components, such as attention
heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between
performance and efficiency. While NAS already achieved promising results on
small language models in previous work, in this paper we propose various
extensions that allow us to scale to LLMs. Compared to structural pruning
baselines, we show that NAS improves performance up to 3.4% on MMLU with an
on-device latency speedup.


## Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs

>Authors: Ruijia Niu, Dongxia Wu, Rose Yu, Yi-An Ma

>2024-10-09

> http://arxiv.org/abs/2410.06431v1

From common-sense reasoning to domain-specific tasks, parameter-efficient
fine tuning (PEFT) methods for large language models (LLMs) have showcased
significant performance improvements on downstream tasks. However, fine-tuned
LLMs often struggle with overconfidence in uncertain predictions, particularly
due to sparse training data. This overconfidence reflects poor epistemic
uncertainty calibration, which arises from limitations in the model's ability
to generalize with limited data. Existing PEFT uncertainty quantification
methods for LLMs focus on the post fine-tuning stage and thus have limited
capability in calibrating epistemic uncertainty. To address these limitations,
we propose Functional-Level Uncertainty Quantification for Calibrated
Fine-Tuning (UQ4CT), which captures and calibrates functional-level epistemic
uncertainty during the fine-tuning stage via a mixture-of-expert framework. We
show that UQ4CT reduces Expected Calibration Error (ECE) by more than $25\%$
while maintaining high accuracy across $5$ benchmarks. Furthermore, UQ4CT
maintains superior ECE performance with high accuracy under distribution shift,
showcasing improved generalizability.

