# 2024-11-08

# Table of Contents
* [Low-depth quantum symmetrization](#Low-depth-quantum-symmetrization)
* [ParaGAN A Scalable Distributed Training Framework for Generative Adversarial Networks](#ParaGAN-A-Scalable-Distributed-Training-Framework-for-Generative-Adversarial-Networks)
* [Interactions Across Blocks in Post-Training Quantization of Large Language Models](#Interactions-Across-Blocks-in-Post-Training-Quantization-of-Large-Language-Models)
* [Polynomial Composition Activations Unleashing the Dynamics of Large Language Models](#Polynomial-Composition-Activations-Unleashing-the-Dynamics-of-Large-Language-Models)
* [Large Generative Model-assisted Talking-face Semantic Communication System](#Large-Generative-Model-assisted-Talking-face-Semantic-Communication-System)
* [An Edge Computing-Based Solution for Real-Time Leaf Disease Classification using Thermal Imaging](#An-Edge-Computing-Based-Solution-for-Real-Time-Leaf-Disease-Classification-using-Thermal-Imaging)
* [From Novice to Expert LLM Agent Policy Optimization via Step-wise Reinforcement Learning](#From-Novice-to-Expert-LLM-Agent-Policy-Optimization-via-Step-wise-Reinforcement-Learning)
* [Automating Exploratory Proteomics Research via Language Models](#Automating-Exploratory-Proteomics-Research-via-Language-Models)
* [These Maps Are Made by Propagation Adapting Deep Stereo Networks to Road Scenarios with Decisive Disparity Diffusion](#These-Maps-Are-Made-by-Propagation-Adapting-Deep-Stereo-Networks-to-Road-Scenarios-with-Decisive-Disparity-Diffusion)
* [TATAA Programmable Mixed-Precision Transformer Acceleration with a Transformable Arithmetic Architecture](#TATAA-Programmable-Mixed-Precision-Transformer-Acceleration-with-a-Transformable-Arithmetic-Architecture)
* [OccLoff Learning Optimized Feature Fusion for 3D Occupancy Prediction](#OccLoff-Learning-Optimized-Feature-Fusion-for-3D-Occupancy-Prediction)


## Low-depth quantum symmetrization

>Authors: Zhenning Liu, Andrew M. Childs, Daniel Gottesman

>2024-11-06

> http://arxiv.org/abs/2411.04019v1

Quantum symmetrization is the task of transforming a non-strictly increasing
list of $n$ integers into an equal superposition of all permutations of the
list (or more generally, performing this operation coherently on a
superposition of such lists). This task plays a key role in initial state
preparation for first-**quantize**d simulations. A weaker version of symmetrization
in which the input list is \emph{strictly} increasing has been extensively
studied due to its application to fermionic systems, but the general
symmetrization problem with repetitions in the input list has not been solved
previously. We present the first quantum algorithm for the general
symmetrization problem. Our algorithm, based on (quantum) sorting networks,
uses $O(\log n)$ depth and $O(n\log m)$ ancilla qubits where $m$ is the
greatest possible value of the list, enabling efficient simulation of bosonic
quantum systems in first **quantization**. Furthermore, our algorithm can prepare
(superpositions of) Dicke states of any Hamming weight in $O(\log n)$ depth
using $O(n\log n)$ ancilla qubits. We also propose an $O(\log^2 n)$-depth
quantum algorithm to transform second-**quantize**d states to first-**quantize**d
states. Using this algorithm, QFT-based quantum telescope arrays can image
brighter photon sources, extending quantum interferometric imaging systems to a
new regime.


## ParaGAN A Scalable Distributed Training Framework for Generative Adversarial Networks

>Authors: Ziji Shi, Jialin Li, Yang You

>2024-11-06

> http://arxiv.org/abs/2411.03999v1

Recent advances in Generative Artificial Intelligence have fueled numerous
applications, particularly those involving Generative Adversarial Networks
(GANs), which are essential for synthesizing realistic photos and videos.
However, efficiently training GANs remains a critical challenge due to their
computationally intensive and numerically unstable nature. Existing methods
often require days or even weeks for training, posing significant resource and
time constraints.
  In this work, we introduce ParaGAN, a scalable distributed GAN training
framework that leverages asynchronous training and an asymmetric optimization
policy to accelerate GAN training. ParaGAN employs a congestion-aware data
pipeline and hardware-aware layout transformation to enhance accelerator
utilization, resulting in over 30% improvements in throughput. With ParaGAN, we
reduce the training time of BigGAN from 15 days to 14 hours while achieving 91%
scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution
image generation using BigGAN.


## Interactions Across Blocks in Post-Training Quantization of Large Language Models

>Authors: Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil

>2024-11-06

> http://arxiv.org/abs/2411.03934v1

Post-training **quantization** is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are **quantize**d with the objective of minimizing
**quantization** errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only **quantization** of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple **quantize**d blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the **quantize**d block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.


## Polynomial Composition Activations Unleashing the Dynamics of Large Language Models

>Authors: Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma

>2024-11-06

> http://arxiv.org/abs/2411.03884v1

Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and **sparse** architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.


## Large Generative Model-assisted Talking-face Semantic Communication System

>Authors: Feibo Jiang, Siwei Tu, Li Dong, Cunhua Pan, Jiangzhou Wang, Xiaohu You

>2024-11-06

> http://arxiv.org/abs/2411.03876v1

The rapid development of generative Artificial Intelligence (AI) continually
unveils the potential of Semantic Communication (SemCom). However, current
talking-face SemCom systems still encounter challenges such as low bandwidth
utilization, semantic ambiguity, and diminished Quality of Experience (QoE).
This study introduces a Large Generative Model-assisted Talking-face Semantic
Communication (LGM-TSC) System tailored for the talking-face video
communication. Firstly, we introduce a Generative Semantic Extractor (GSE) at
the transmitter based on the FunASR model to convert semantically **sparse**
talking-face videos into texts with high information density. Secondly, we
establish a private Knowledge Base (KB) based on the Large Language Model (LLM)
for semantic disambiguation and correction, complemented by a joint knowledge
base-semantic-channel coding scheme. Finally, at the receiver, we propose a
Generative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker
models to transform text back into a high-QoE talking-face video matching the
user's timbre. Simulation results demonstrate the feasibility and effectiveness
of the proposed LGM-TSC system.


## An Edge Computing-Based Solution for Real-Time Leaf Disease Classification using Thermal Imaging

>Authors: PÃºblio Elon Correa da Silva, Jurandy Almeida

>2024-11-06

> http://arxiv.org/abs/2411.03835v1

Deep learning (DL) technologies can transform agriculture by improving crop
health monitoring and management, thus improving food safety. In this paper, we
explore the potential of edge computing for real-time classification of leaf
diseases using thermal imaging. We present a thermal image dataset for plant
disease classification and evaluate deep learning models, including
InceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained
devices like the Raspberry Pi 4B. Using **pruning** and **quantization**-aware
training, these models achieve inference times up to 1.48x faster on Edge TPU
Max for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2
for MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining
state-of-the-art accuracy.


## From Novice to Expert LLM Agent Policy Optimization via Step-wise Reinforcement Learning

>Authors: Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen

>2024-11-06

> http://arxiv.org/abs/2411.03817v1

The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
**sparse** reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.


## Automating Exploratory Proteomics Research via Language Models

>Authors: Ning Ding, Shang Qu, Linhai Xie, Yifei Li, Zaoqu Liu, Kaiyan Zhang, Yibai Xiong, Yuxin Zuo, Zhangren Chen, Ermo Hua, Xingtai Lv, Youbang Sun, Yang Li, Dong Li, Fuchu He, Bowen Zhou

>2024-11-06

> http://arxiv.org/abs/2411.03743v1

With the development of artificial intelligence, its contribution to science
is evolving from simulating a complex problem to automating entire research
processes and producing novel discoveries. Achieving this advancement requires
both specialized general models grounded in real-world scientific data and
iterative, exploratory frameworks that mirror human scientific methodologies.
In this paper, we present PROTEUS, a fully automated system for scientific
discovery from raw proteomics data. PROTEUS uses large language models (LLMs)
to perform hierarchical planning, execute specialized bioinformatics tools, and
iteratively refine analysis workflows to generate high-quality scientific
hypotheses. The system takes proteomics datasets as input and produces a
comprehensive set of research objectives, analysis results, and novel
biological hypotheses without human intervention. We evaluated PROTEUS on 12
proteomics datasets collected from various biological samples (e.g. immune
cells, tumors) and different sample types (single-cell and bulk), generating
191 scientific hypotheses. These were assessed using both automatic LLM-based
scoring on 5 metrics and detailed reviews from human experts. Results
demonstrate that PROTEUS consistently produces reliable, logically coherent
results that align well with existing literature while also proposing novel,
evaluable hypotheses. The system's flexible architecture facilitates seamless
integration of diverse analysis tools and adaptation to different proteomics
data types. By automating complex proteomics analysis workflows and hypothesis
generation, PROTEUS has the potential to considerably accelerate the pace of
scientific discovery in proteomics research, enabling researchers to
efficiently explore large-scale datasets and uncover biological insights.


## These Maps Are Made by Propagation Adapting Deep Stereo Networks to Road Scenarios with Decisive Disparity Diffusion

>Authors: Chuang-Wei Liu, Yikang Zhang, Qijun Chen, Ioannis Pitas, Rui Fan

>2024-11-06

> http://arxiv.org/abs/2411.03717v1

Stereo matching has emerged as a cost-effective solution for road surface 3D
reconstruction, garnering significant attention towards improving both
computational efficiency and accuracy. This article introduces decisive
disparity diffusion (D3Stereo), marking the first exploration of dense deep
feature matching that adapts pre-trained deep convolutional neural networks
(DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is
initially created using various levels of learned representations.
Subsequently, a novel recursive bilateral filtering algorithm is employed to
aggregate these costs. A key innovation of D3Stereo lies in its alternating
decisive disparity diffusion strategy, wherein intra-scale diffusion is
employed to complete **sparse** disparity images, while inter-scale inheritance
provides valuable prior information for higher resolutions. Extensive
experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets
underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs
and its superior performance compared to all other explicit programming-based
algorithms designed specifically for road surface 3D reconstruction. Additional
experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained
on the ImageNet database further validate the versatility of D3Stereo strategy
in tackling general stereo matching problems.


## TATAA Programmable Mixed-Precision Transformer Acceleration with a Transformable Arithmetic Architecture

>Authors: Jiajun Wu, Mo Song, Jingmin Zhao, Yizhao Gao, Jia Li, Hayden Kwok-Hay So

>2024-11-06

> http://arxiv.org/abs/2411.03697v1

Modern transformer-based deep neural networks present unique technical
challenges for effective **acceleration** in real-world applications. Apart from
the vast amount of linear operations needed due to their sizes, modern
transformer models are increasingly reliance on precise non-linear computations
that make traditional **low-bit**width **quantization** methods and fixed-dataflow
matrix accelerators ineffective for end-to-end **acceleration**. To address this
need to accelerate both linear and non-linear operations in a unified and
programmable framework, this paper introduces TATAA. TATAA employs 8-bit
integer (int8) arithmetic for **quantize**d linear layer operations through
post-training **quantization**, while it relies on bfloat16 floating-point
arithmetic to approximate non-linear layers of a transformer model. TATAA
hardware features a transformable arithmetic architecture that supports both
formats during runtime with minimal overhead, enabling it to switch between a
systolic array mode for int8 matrix multiplications and a SIMD mode for
vectorized bfloat16 operations. An end-to-end compiler is presented to enable
flexible mapping from emerging transformer models to the proposed hardware.
Experimental results indicate that our mixed-precision design incurs only 0.14%
to 1.16% accuracy drop when compared with the pre-trained single-precision
transformer models across a range of vision, language, and generative text
applications. Our prototype implementation on the Alveo U280 FPGA currently
achieves 2935.2 GOPS throughput on linear layers and a maximum of 189.5 GFLOPS
for non-linear operations, outperforming related works by up to 1.45x in
end-to-end throughput and 2.29x in DSP efficiency, while achieving 2.19x higher
power efficiency than modern NVIDIA RTX4090 GPU.


## OccLoff Learning Optimized Feature Fusion for 3D Occupancy Prediction

>Authors: Ji Zhang, Yiran Ding, Zixin Liu

>2024-11-06

> http://arxiv.org/abs/2411.03696v1

3D semantic occupancy prediction is crucial for finely representing the
surrounding environment, which is essential for ensuring the safety in
autonomous driving. Existing fusion-based occupancy methods typically involve
performing a 2D-to-3D view transformation on image features, followed by
computationally intensive 3D operations to fuse these with LiDAR features,
leading to high computational costs and reduced accuracy. Moreover, current
research on occupancy prediction predominantly focuses on designing specific
network architectures, often tailored to particular models, with limited
attention given to the more fundamental aspect of semantic feature learning.
This gap hinders the development of more transferable methods that could
enhance the performance of various occupancy models. To address these
challenges, we propose OccLoff, a framework that Learns to Optimize Feature
Fusion for 3D occupancy prediction. Specifically, we introduce a **sparse** fusion
encoder with entropy masks that directly fuses 3D and 2D features, improving
model accuracy while reducing computational overhead. Additionally, we propose
a transferable proxy-based loss function and an adaptive hard sample weighting
algorithm, which enhance the performance of several state-of-the-art methods.
Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate
the superiority of our framework, and ablation studies confirm the
effectiveness of each proposed module.

