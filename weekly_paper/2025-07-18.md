# 2025-07-18

# Table of Contents
* [CytoSAE Interpretable Cell Embeddings for Hematology](#CytoSAE-Interpretable-Cell-Embeddings-for-Hematology)
* [LLM-Based Config Synthesis requires Disambiguation](#LLM-Based-Config-Synthesis-requires-Disambiguation)
* [Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning](#Leveraging-Bi-Directional-Channel-Reciprocity-for-Robust-Ultra-Low-Rate-Implicit-CSI-Feedback-with-Deep-Learning)
* [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](#Toward-Efficient-SpMV-in-Sparse-LLMs-via-Block-Extraction-and-Compressed-Storage)
* [Sparse Autoencoders for Sequential Recommendation Models Interpretation and Flexible Control](#Sparse-Autoencoders-for-Sequential-Recommendation-Models-Interpretation-and-Flexible-Control)
* [Hybrid Ensemble Approaches Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](#Hybrid-Ensemble-Approaches-Optimal-Deep-Feature-Fusion-and-Hyperparameter-Tuned-Classifier-Ensembling-for-Enhanced-Brain-Tumor-Classification)
* [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](#Iterative-Augmentation-with-Summarization-Refinement-(IASR)-Evaluation-for-Unstructured-Survey-data-Modeling-and-Analysis)
* [DSSD Efficient Edge-Device LLM Deployment and Collaborative Inference via Distributed Split Speculative Decoding](#DSSD-Efficient-Edge-Device-LLM-Deployment-and-Collaborative-Inference-via-Distributed-Split-Speculative-Decoding)
* [Aime Towards Fully-Autonomous Multi-Agent Framework](#Aime-Towards-Fully-Autonomous-Multi-Agent-Framework)
* [A Review of Generative AI in Aquaculture Foundations, Applications, and Future Directions for Smart and Sustainable Farming](#A-Review-of-Generative-AI-in-Aquaculture-Foundations,-Applications,-and-Future-Directions-for-Smart-and-Sustainable-Farming)
* [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](#Toxicity-Aware-Few-Shot-Prompting-for-Low-Resource-Singlish-Translation)
* [IAM Efficient Inference through Attention Mapping between Different-scale LLMs](#IAM-Efficient-Inference-through-Attention-Mapping-between-Different-scale-LLMs)
* [Spatial Frequency Modulation for Semantic Segmentation](#Spatial-Frequency-Modulation-for-Semantic-Segmentation)
* [Protenix-Mini Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](#Protenix-Mini-Efficient-Structure-Predictor-via-Compact-Architecture,-Few-Step-Diffusion-and-Switchable-pLM)
* [Arctic Inference with Shift Parallelism Fast and Efficient Open Source Inference System for Enterprise AI](#Arctic-Inference-with-Shift-Parallelism-Fast-and-Efficient-Open-Source-Inference-System-for-Enterprise-AI)
* [AirLLM Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](#AirLLM-Diffusion-Policy-based-Adaptive-LoRA-for-Remote-Fine-Tuning-of-LLM-over-the-Air)
* [Real-World Summarization When Evaluation Reaches Its Limits](#Real-World-Summarization-When-Evaluation-Reaches-Its-Limits)
* [MIRAGE KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving](#MIRAGE-KV-Cache-Optimization-through-Parameter-Remapping-for-Multi-tenant-LLM-Serving)
* [KV-Latent Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](#KV-Latent-Dimensional-level-KV-Cache-Reduction-with-Frequency-aware-Rotary-Positional-Embedding)
* [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](#Sparse-Autoencoders-Can-Capture-Language-Specific-Concepts-Across-Diverse-Languages)
* [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](#Role-Playing-LLM-Based-Multi-Agent-Support-Framework-for-Detecting-and-Addressing-Family-Communication-Bias)
* [Mixture of Experts in Large Language Models](#Mixture-of-Experts-in-Large-Language-Models)
* [LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation](#LLM-Driven-Dual-Level-Multi-Interest-Modeling-for-Recommendation)
* [Sparse Fine-Tuning of Transformers for Generative Tasks](#Sparse-Fine-Tuning-of-Transformers-for-Generative-Tasks)
* [From Semantic Web and MAS to Agentic AI A Unified Narrative of the Web of Agents](#From-Semantic-Web-and-MAS-to-Agentic-AI-A-Unified-Narrative-of-the-Web-of-Agents)
* [Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads](#Towards-Emotion-Co-regulation-with-LLM-powered-Socially-Assistive-Robots-Integrating-LLM-Prompts-and-Robotic-Behaviors-to-Support-Parent-Neurodivergent-Child-Dyads)
* [Zorse Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](#Zorse-Optimizing-LLM-Training-Efficiency-on-Heterogeneous-GPU-Clusters)
* [Pimba A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](#Pimba-A-Processing-in-Memory-Acceleration-for-Post-Transformer-Large-Language-Model-Serving)
* [GeLaCo An Evolutionary Approach to Layer Compression](#GeLaCo-An-Evolutionary-Approach-to-Layer-Compression)
* [GHPO Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](#GHPO-Adaptive-Guidance-for-Stable-and-Efficient-LLM-Reinforcement-Learning)
* [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](#Differentially-Private-Federated-Low-Rank-Adaptation-Beyond-Fixed-Matrix)
* [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](#Mechanistic-Interpretability-of-LoRA-Adapted-Language-Models-for-Nuclear-Reactor-Safety-Applications)
* [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](#Hierarchical-Abstraction-Enables-Human-Like-3D-Object-Recognition-in-Deep-Learning-Models)
* [Token Compression Meets Compact Vision Transformers A Survey and Comparative Evaluation for Edge AI](#Token-Compression-Meets-Compact-Vision-Transformers-A-Survey-and-Comparative-Evaluation-for-Edge-AI)
* [QuarterMap Efficient Post-Training Token Pruning for Visual State Space Models](#QuarterMap-Efficient-Post-Training-Token-Pruning-for-Visual-State-Space-Models)
* [ViT-ProtoNet for Few-Shot Image Classification A Multi-Benchmark Evaluation](#ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation)
* [Psychology-Driven Enhancement of Humour Translation](#Psychology-Driven-Enhancement-of-Humour-Translation)
* [SLIM A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](#SLIM-A-Heterogeneous-Accelerator-for-Edge-Inference-of-Sparse-Large-Language-Model-via-Adaptive-Thresholding)
* [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](#Detecting-and-Pruning-Prominent-but-Detrimental-Neurons-in-Large-Language-Models)
* [Hide-and-Shill A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](#Hide-and-Shill-A-Reinforcement-Learning-Framework-for-Market-Manipulation-Detection-in-Symphony-a-Decentralized-Multi-Agent-System)
* [Lizard An Efficient Linearization Framework for Large Language Models](#Lizard-An-Efficient-Linearization-Framework-for-Large-Language-Models)
* [Bridging Literature and the Universe Via A Multi-Agent Large Language Model System](#Bridging-Literature-and-the-Universe-Via-A-Multi-Agent-Large-Language-Model-System)
* [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](#Optimizing-Sequential-Multi-Step-Tasks-with-Parallel-LLM-Agents)
* [BlockFFN Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](#BlockFFN-Towards-End-Side-Acceleration-Friendly-Mixture-of-Experts-with-Chunk-Level-Activation-Sparsity)
* [AgentsNet Coordination and Collaborative Reasoning in Multi-Agent LLMs](#AgentsNet-Coordination-and-Collaborative-Reasoning-in-Multi-Agent-LLMs)
* [Anthropomimetic Uncertainty What Verbalized Uncertainty in Language Models is Missing](#Anthropomimetic-Uncertainty-What-Verbalized-Uncertainty-in-Language-Models-is-Missing)
* [FreeAudio Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation](#FreeAudio-Training-Free-Timing-Planning-for-Controllable-Long-Form-Text-to-Audio-Generation)
* [Evaluating SAE interpretability without explanations](#Evaluating-SAE-interpretability-without-explanations)
* [Upsample What Matters Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](#Upsample-What-Matters-Region-Adaptive-Latent-Sampling-for-Accelerated-Diffusion-Transformers)
* [Invariant-based Robust Weights Watermark for Large Language Models](#Invariant-based-Robust-Weights-Watermark-for-Large-Language-Models)


## CytoSAE Interpretable Cell Embeddings for Hematology

>Authors: Muhammed Furkan Dasdelen, Hyesu Lim, Michele Buck, Katharina S. GÃ¶tze, Carsten Marr, Steffen Schneider

>2025-07-16

> http://arxiv.org/abs/2507.12464v1

Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of ![key](https://img.shields.io/badge/transformer-FF8C00)-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the ![key](https://img.shields.io/badge/transformer-FF8C00) model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a ![key](https://img.shields.io/badge/sparse-F08080) autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.


## LLM-Based Config Synthesis requires Disambiguation

>Authors: Rajdeep Mondal, Nikolaj Bjorner, Todd Millstein, Alan Tang, George Varghese

>2025-07-16

> http://arxiv.org/abs/2507.12443v1

Beyond hallucinations, another problem in program synthesis using ![key](https://img.shields.io/badge/LLM-FF8C00)s is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for ![key](https://img.shields.io/badge/LLM-FF8C00)-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently ![key](https://img.shields.io/badge/overlap-F08080) in header space, making the relative
priority of actions impossible for the ![key](https://img.shields.io/badge/LLM-FF8C00) to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of ![key](https://img.shields.io/badge/overlap-F08080)s,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an ![key](https://img.shields.io/badge/LLM-FF8C00) augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by ![key](https://img.shields.io/badge/LLM-FF8C00)s, but their integration is ambiguous and
can lead to different global behaviors.


## Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning

>Authors: Zhenyu Liu, Yi Ma, Rahim Tafazolli, Zhi Ding

>2025-07-16

> http://arxiv.org/abs/2507.12301v1

Deep learning-based implicit channel state information (CSI) feedback has
been introduced to enhance spectral efficiency in massive MIMO systems.
Existing methods often show performance degradation in ultra-low-rate scenarios
and inadaptability across diverse environments. In this paper, we propose
Dual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework
incorporating two novel plug-in preprocessing modules to achieve ultra-low
feedback rates while maintaining high environmental robustness. First, a novel
bi-directional correlation enhancement module is proposed to strengthen the
correlation between uplink and downlink CSI eigenvector matrices. This module
projects highly correlated uplink and downlink channel matrices into their
respective eigenspaces, effectively reducing redundancy for ultra-low-rate
feedback. Second, an innovative input format alignment module is designed to
maintain consistent data distributions at both encoder and decoder sides
without extra transmission overhead, thereby enhancing robustness against
environmental variations. Finally, we develop an efficient ![key](https://img.shields.io/badge/transformer-FF8C00)-based
implicit CSI feedback network to exploit angular-delay domain ![key](https://img.shields.io/badge/sparsity-F08080) and
bi-directional correlation for ultra-low-rate CSI compression. Simulation
results demonstrate successful reduction of the feedback overhead by 85%
compared with the state-of-the-art method and robustness against unseen
environments.


## Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage

>Authors: Junqing Lin, Jingwei Sun, Mingge Lu, Guangzhong Sun

>2025-07-16

> http://arxiv.org/abs/2507.12205v1

Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of ![key](https://img.shields.io/badge/sparse-F08080) Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and ![key](https://img.shields.io/badge/sparse-F08080) matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in ![key](https://img.shields.io/badge/sparse-F08080) ![key](https://img.shields.io/badge/LLM-FF8C00)s, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating ![key](https://img.shields.io/badge/sparse-F08080) ![key](https://img.shields.io/badge/LLM-FF8C00) inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within ![key](https://img.shields.io/badge/sparse-F08080) ![key](https://img.shields.io/badge/LLM-FF8C00)s, and (2) a novel compressed ![key](https://img.shields.io/badge/sparse-F08080) format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real ![key](https://img.shields.io/badge/sparse-F08080) weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.


## Sparse Autoencoders for Sequential Recommendation Models Interpretation and Flexible Control

>Authors: Anton Klenitskiy, Konstantin Polev, Daria Denisova, Alexey Vasilev, Dmitry Simakov, Gleb Gusev

>2025-07-16

> http://arxiv.org/abs/2507.12202v1

Many current state-of-the-art models for sequential recommendations are based
on ![key](https://img.shields.io/badge/transformer-FF8C00) architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently ![key](https://img.shields.io/badge/sparse-F08080)
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the ![key](https://img.shields.io/badge/transformer-FF8C00)'s internal layers from
![key](https://img.shields.io/badge/sparse-F08080) linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the ![key](https://img.shields.io/badge/transformer-FF8C00) trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.


## Hybrid Ensemble Approaches Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification

>Authors: Zahid Ullah, Dragan Pamucar, Jihie Kim

>2025-07-16

> http://arxiv.org/abs/2507.12177v1

Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or ![key](https://img.shields.io/badge/overlap-F08080) with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision ![key](https://img.shields.io/badge/transformer-FF8C00) networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.


## Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis

>Authors: Payal Bhattad, Sai Manoj Pudukotai Dinakarrao, Anju Gupta

>2025-07-16

> http://arxiv.org/abs/2507.12126v1

Text data augmentation is a widely used strategy for mitigating data ![key](https://img.shields.io/badge/sparsity-F08080)
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(![key](https://img.shields.io/badge/LLM-FF8C00)) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art ![key](https://img.shields.io/badge/LLM-FF8C00)s show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic ![key](https://img.shields.io/badge/overlap-F08080)s. These findings
validated the utility of the proposed frameworks for structured evaluation of
![key](https://img.shields.io/badge/LLM-FF8C00)-based augmentation in practical NLP pipelines.


## DSSD Efficient Edge-Device LLM Deployment and Collaborative Inference via Distributed Split Speculative Decoding

>Authors: Jiahong Ning, Ce Zheng, Tingting Yang

>2025-07-16

> http://arxiv.org/abs/2507.12000v2

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) have transformed natural language processing but
face critical deployment challenges in device-edge systems due to resource
limitations and ![key](https://img.shields.io/badge/communication-F08080) overhead. To address these issues, collaborative
frameworks have emerged that combine small language models (SLMs) on devices
with ![key](https://img.shields.io/badge/LLM-FF8C00)s at the edge, using speculative decoding (SD) to improve efficiency.
However, existing solutions often trade inference accuracy for latency or
suffer from high uplink transmission costs when verifying candidate tokens. In
this paper, we propose Distributed Split Speculative Decoding (DSSD), a novel
architecture that not only preserves the SLM-![key](https://img.shields.io/badge/LLM-FF8C00) split but also partitions the
verification phase between the device and edge. In this way, DSSD replaces the
uplink transmission of multiple vocabulary distributions with a single downlink
transmission, significantly reducing ![key](https://img.shields.io/badge/communication-F08080) latency while maintaining
inference quality. Experiments show that our solution outperforms current
methods, and codes are at:
https://github.com/JasonNing96/DSSD-Efficient-Edge-Computing


## Aime Towards Fully-Autonomous Multi-Agent Framework

>Authors: Yexuan Shi, Mingyu Wang, Yunxiang Cao, Hongjie Lai, Junjian Lan, Xin Han, Yu Wang, Jie Geng, Zhenan Li, Zihao Xia, Xiang Chen, Chen Li, Jian Xu, Wenbo Duan, Yuanshuo Zhu

>2025-07-16

> http://arxiv.org/abs/2507.11988v2

Multi-Agent Systems (MAS) powered by Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient ![key](https://img.shields.io/badge/communication-F08080). These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.


## A Review of Generative AI in Aquaculture Foundations, Applications, and Future Directions for Smart and Sustainable Farming

>Authors: Waseem Akram, Muhayy Ud Din, Lyes Saad Soud, Irfan Hussain

>2025-07-16

> http://arxiv.org/abs/2507.11974v1

Generative Artificial Intelligence (GAI) has rapidly emerged as a
transformative force in aquaculture, enabling intelligent synthesis of
multimodal data, including text, images, audio, and simulation outputs for
smarter, more adaptive decision-making. As the aquaculture industry shifts
toward data-driven, automation and digital integration operations under the
Aquaculture 4.0 paradigm, GAI models offer novel opportunities across
environmental monitoring, robotics, disease diagnostics, infrastructure
planning, reporting, and market analysis. This review presents the first
comprehensive synthesis of GAI applications in aquaculture, encompassing
foundational architectures (e.g., diffusion models, ![key](https://img.shields.io/badge/transformer-FF8C00)s, and retrieval
augmented generation), experimental systems, pilot deployments, and real-world
use cases. We highlight GAI's growing role in enabling underwater perception,
digital twin modeling, and autonomous planning for remotely operated vehicle
(ROV) missions. We also provide an updated application taxonomy that spans
sensing, control, optimization, ![key](https://img.shields.io/badge/communication-F08080), and regulatory compliance.
Beyond technical capabilities, we analyze key limitations, including limited
data availability, real-time performance constraints, trust and explainability,
environmental costs, and regulatory uncertainty. This review positions GAI not
merely as a tool but as a critical enabler of smart, resilient, and
environmentally aligned aquaculture systems.


## Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation

>Authors: Ziyu Ge, Gabriel Chua, Leanne Tan, Roy Ka-Wei Lee

>2025-07-16

> http://arxiv.org/abs/2507.11966v1

As online ![key](https://img.shields.io/badge/communication-F08080) increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
![key](https://img.shields.io/badge/LLM-FF8C00)s by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.


## IAM Efficient Inference through Attention Mapping between Different-scale LLMs

>Authors: Yi Zhao, Zuchao Li, Hai Zhao

>2025-07-16

> http://arxiv.org/abs/2507.11953v1

![key](https://img.shields.io/badge/LLM-FF8C00)s encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal ![key](https://img.shields.io/badge/sparsity-F08080) within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale ![key](https://img.shields.io/badge/LLM-FF8C00)s,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced ![key](https://img.shields.io/badge/KV-F08080) cache usage by performing attention
mapping between small and large ![key](https://img.shields.io/badge/LLM-FF8C00)s. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce ![key](https://img.shields.io/badge/KV-F08080) cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing ![key](https://img.shields.io/badge/KV-F08080) cache optimization methods, making it a versatile addition to the
current toolkit for enhancing ![key](https://img.shields.io/badge/LLM-FF8C00) efficiency.


## Spatial Frequency Modulation for Semantic Segmentation

>Authors: Linwei Chen, Ying Fu, Lin Gu, Dezhi Zheng, Jifeng Dai

>2025-07-16

> http://arxiv.org/abs/2507.11893v1

High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and ![key](https://img.shields.io/badge/sparse-F08080)ly resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to ![key](https://img.shields.io/badge/transformer-FF8C00)s. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.


## Protenix-Mini Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM

>Authors: Chengyue Gong, Xinshi Chen, Yuxuan Zhang, Yuxuan Song, Hao Zhou, Wenzhi Xiao

>2025-07-16

> http://arxiv.org/abs/2507.11839v1

Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion ![key](https://img.shields.io/badge/transformer-FF8C00) blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
![key](https://img.shields.io/badge/pruning-F08080) and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.


## Arctic Inference with Shift Parallelism Fast and Efficient Open Source Inference System for Enterprise AI

>Authors: Samyam Rajbhandari, Mert Hidayetoglu, Aurick Qiao, Ye Wang, Juncheng Yang, Jeff Rasley, Michael Wyatt, Yuxiong He

>2025-07-16

> http://arxiv.org/abs/2507.11830v1

Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source v![key](https://img.shields.io/badge/LLM-FF8C00) plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, Swift![key](https://img.shields.io/badge/KV-F08080) compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.


## AirLLM Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air

>Authors: Shiyi Yang, Xiaoxue Yu, Rongpeng Li, Jianhang Zhu, Zhifeng Zhao, Honggang Zhang

>2025-07-15

> http://arxiv.org/abs/2507.11515v1

Operating Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) on edge devices is increasingly
challenged by limited ![key](https://img.shields.io/badge/communication-F08080) bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop Air![key](https://img.shields.io/badge/LLM-FF8C00), a hierarchical diffusion policy framework
for ![key](https://img.shields.io/badge/communication-F08080)-aware LoRA adaptation. Specifically, Air![key](https://img.shields.io/badge/LLM-FF8C00) models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that Air![key](https://img.shields.io/badge/LLM-FF8C00)
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.


## Real-World Summarization When Evaluation Reaches Its Limits

>Authors: PatrÃ­cia SchmidtovÃ¡, OndÅej DuÅ¡ek, Saad Mahamood

>2025-07-15

> http://arxiv.org/abs/2507.11508v1

We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief ![key](https://img.shields.io/badge/LLM-FF8C00)-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and ![key](https://img.shields.io/badge/LLM-FF8C00)-as-a-judge approaches. Our findings reveal that simpler
metrics like word ![key](https://img.shields.io/badge/overlap-F08080) correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while ![key](https://img.shields.io/badge/LLM-FF8C00)s can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.


## MIRAGE KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving

>Authors: Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar

>2025-07-15

> http://arxiv.org/abs/2507.11507v1

![key](https://img.shields.io/badge/KV-F08080) cache accelerates ![key](https://img.shields.io/badge/LLM-FF8C00) inference by avoiding redundant computation, at the
expense of memory. To support larger ![key](https://img.shields.io/badge/KV-F08080) caches, prior work extends GPU memory
with CPU memory via CPU-offloading. This involves swapping ![key](https://img.shields.io/badge/KV-F08080) cache between GPU
and CPU memory. However, because the cache updates dynamically, such swapping
incurs high CPU memory traffic. We make a key observation that model parameters
remain constant during runtime, unlike the dynamically updated ![key](https://img.shields.io/badge/KV-F08080) cache.
Building on this, we introduce MIRAGE, which avoids ![key](https://img.shields.io/badge/KV-F08080) cache swapping by
remapping, and thereby repurposing, the memory allocated to model parameters
for ![key](https://img.shields.io/badge/KV-F08080) cache. This parameter remapping is especially beneficial in multi-tenant
environments, where the memory used for the parameters of the inactive models
can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth
offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we
show that MIRAGE significantly outperforms state-of-the-art solutions,
achieving a reduction of 44.8%-82.5% in tail time-between-token latency,
20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher
throughput compared to v![key](https://img.shields.io/badge/LLM-FF8C00).


## KV-Latent Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding

>Authors: Luohe Shi, Zuchao Li, Lefei Zhang, Guoming Liu, Baoyuan Qi, Hai Zhao

>2025-07-15

> http://arxiv.org/abs/2507.11273v1

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(![key](https://img.shields.io/badge/KV-F08080)) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called ![key](https://img.shields.io/badge/KV-F08080)-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the ![key](https://img.shields.io/badge/KV-F08080) Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on ![key](https://img.shields.io/badge/KV-F08080) Cache saving and efficient ![key](https://img.shields.io/badge/LLM-FF8C00)s. Our code is available at
https://github.com/ShiLuohe/![key](https://img.shields.io/badge/KV-F08080)-Latent.


## Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages

>Authors: Lyzander Marciano Andrylie, Inaya Rahmanisa, Mahardika Krisna Ihsani, Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, Alham Fikri Aji

>2025-07-15

> http://arxiv.org/abs/2507.11230v1

Understanding the multilingual mechanisms of large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore ![key](https://img.shields.io/badge/sparse-F08080) autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in ![key](https://img.shields.io/badge/LLM-FF8C00)s. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .


## Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias

>Authors: Rushia Harada, Yuken Kimura, Keito Inoshita

>2025-07-15

> http://arxiv.org/abs/2507.11210v1

Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven ![key](https://img.shields.io/badge/communication-F08080), which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (![key](https://img.shields.io/badge/LLM-FF8C00))-based support
for psychologically safe family ![key](https://img.shields.io/badge/communication-F08080). We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing ![key](https://img.shields.io/badge/LLM-FF8C00)-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.


## Mixture of Experts in Large Language Models

>Authors: Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao

>2025-07-15

> http://arxiv.org/abs/2507.11181v1

This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (![key](https://img.shields.io/badge/LLM-FF8C00)) applications, we examine
expert gating and routing mechanisms, hierarchical and ![key](https://img.shields.io/badge/sparse-F08080) MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.


## LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation

>Authors: Ziyan Wang, Yingpeng Du, Zhu Sun, Jieyi Bi, Haoyan Chua, Tianjun Wei, Jie Zhang

>2025-07-15

> http://arxiv.org/abs/2507.10917v2

Recently, much effort has been devoted to modeling users' multi-interests
based on their behaviors or auxiliary signals. However, existing methods often
rely on heuristic assumptions, e.g., co-occurring items indicate the same
interest of users, failing to capture user multi-interests aligning with
real-world scenarios. While large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) show significant
potential for multi-interest analysis due to their extensive knowledge and
powerful reasoning capabilities, two key challenges remain. First, the
granularity of ![key](https://img.shields.io/badge/LLM-FF8C00)-driven multi-interests is agnostic, possibly leading to
overly fine or coarse interest grouping. Second, individual user analysis
provides limited insights due to the data ![key](https://img.shields.io/badge/sparsity-F08080) issue. In this paper, we
propose an ![key](https://img.shields.io/badge/LLM-FF8C00)-driven dual-level multi-interest modeling framework for more
effective recommendation. At the user-individual level, we exploit ![key](https://img.shields.io/badge/LLM-FF8C00)s to
flexibly allocate items engaged by users into different semantic clusters,
indicating their diverse and distinct interests. To alleviate the agnostic
generation of ![key](https://img.shields.io/badge/LLM-FF8C00)s, we adaptively assign these semantic clusters to users'
collaborative multi-interests learned from global user-item interactions,
allowing the granularity to be automatically adjusted according to the user's
behaviors using an alignment module. To alleviate the limited insights derived
from individual users' behaviors, at the user-crowd level, we propose
aggregating user cliques into synthesized users with rich behaviors for more
comprehensive ![key](https://img.shields.io/badge/LLM-FF8C00)-driven multi-interest analysis. We formulate a max covering
problem to ensure the compactness and representativeness of synthesized users'
behaviors, and then conduct contrastive learning based on their ![key](https://img.shields.io/badge/LLM-FF8C00)-driven
multi-interests to disentangle item representations among different interests.
Experiments on real-world datasets show the superiority of our approach against
state-of-the-art methods.


## Sparse Fine-Tuning of Transformers for Generative Tasks

>Authors: Wei Chen, Jingxi Yu, Zichen Miao, Qiang Qiu

>2025-07-14

> http://arxiv.org/abs/2507.10855v1

Large pre-trained ![key](https://img.shields.io/badge/transformer-FF8C00)s have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by ![key](https://img.shields.io/badge/sparse-F08080)
coding, where fine-tuned features are represented as a ![key](https://img.shields.io/badge/sparse-F08080) combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
![key](https://img.shields.io/badge/sparse-F08080) coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a ![key](https://img.shields.io/badge/sparse-F08080)
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.


## From Semantic Web and MAS to Agentic AI A Unified Narrative of the Web of Agents

>Authors: Tatiana Petrova, Boris Bliznioukov, Aleksandr Puzikov, Radu State

>2025-07-14

> http://arxiv.org/abs/2507.10644v2

The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest ![key](https://img.shields.io/badge/LLM-FF8C00)-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, ![key](https://img.shields.io/badge/communication-F08080) paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(![key](https://img.shields.io/badge/LLM-FF8C00)). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.


## Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads

>Authors: Jing Li, Felix Schijve, Sheng Li, Yuye Yang, Jun Hu, Emilia Barakova

>2025-07-14

> http://arxiv.org/abs/2507.10427v1

Socially Assistive Robotics (SAR) has shown promise in supporting emotion
regulation for neurodivergent children. Recently, there has been increasing
interest in leveraging advanced technologies to assist parents in co-regulating
emotions with their children. However, limited research has explored the
integration of large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) with SAR to facilitate emotion
co-regulation between parents and children with neurodevelopmental disorders.
To address this gap, we developed an ![key](https://img.shields.io/badge/LLM-FF8C00)-powered social robot by deploying a
speech ![key](https://img.shields.io/badge/communication-F08080) module on the MiRo-E robotic platform. This supervised
autonomous system integrates ![key](https://img.shields.io/badge/LLM-FF8C00) prompts and robotic behaviors to deliver
tailored interventions for both parents and neurodivergent children. Pilot
tests were conducted with two parent-child dyads, followed by a qualitative
analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics
and its potential to facilitate emotion regulation, along with identified
design and technical challenges. Based on these insights, we provide design
implications to advance the future development of ![key](https://img.shields.io/badge/LLM-FF8C00)-powered SAR for mental
health applications.


## Zorse Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters

>Authors: Runsheng Benson Guo, Utkarsh Anand, Khuzaima Daudjee, Rathijit Sen

>2025-07-14

> http://arxiv.org/abs/2507.10392v1

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring ![key](https://img.shields.io/badge/communication-F08080)-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both ![key](https://img.shields.io/badge/communication-F08080)- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.


## Pimba A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving

>Authors: Wonung Kim, Yubin Lee, Yoonsung Kim, Jinwoo Hwang, Seongryong Oh, Jiyong Jung, Aziz Huseynov, Woong Gyu Park, Chang Hyun Park, Divya Mahajan, Jongse Park

>2025-07-14

> http://arxiv.org/abs/2507.10178v1

Transformers are the driving force behind today's Large Language Models
(![key](https://img.shields.io/badge/LLM-FF8C00)s), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-![key](https://img.shields.io/badge/transformer-FF8C00)s. This shift presents a key challenge: building a serving
system that efficiently supports both ![key](https://img.shields.io/badge/transformer-FF8C00) and post-![key](https://img.shields.io/badge/transformer-FF8C00) ![key](https://img.shields.io/badge/LLM-FF8C00)s
within a unified framework. To address this challenge, we analyze the
performance characteristics of ![key](https://img.shields.io/badge/transformer-FF8C00) and post-![key](https://img.shields.io/badge/transformer-FF8C00) ![key](https://img.shields.io/badge/LLM-FF8C00)s. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in ![key](https://img.shields.io/badge/transformer-FF8C00)s and state
updates in post-![key](https://img.shields.io/badge/transformer-FF8C00)s. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM ![key](https://img.shields.io/badge/acceleration-F08080) inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
![key](https://img.shields.io/badge/LLM-FF8C00)-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.


## GeLaCo An Evolutionary Approach to Layer Compression

>Authors: David Ponce, Thierry Etchegoyhen, Javier Del Ser

>2025-07-14

> http://arxiv.org/abs/2507.10059v1

Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured ![key](https://img.shields.io/badge/pruning-F08080), typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to ![key](https://img.shields.io/badge/LLM-FF8C00) compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.


## GHPO Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning

>Authors: Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, Dandan Tu

>2025-07-14

> http://arxiv.org/abs/2507.10628v2

Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (![key](https://img.shields.io/badge/LLM-FF8C00)s), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically ![key](https://img.shields.io/badge/sparse-F08080) reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient ![key](https://img.shields.io/badge/LLM-FF8C00)s. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.


## Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix

>Authors: Ming Wen, Jiaqi Zhu, Yuedong Xu, Yipeng Zhou, Dingding Han

>2025-07-14

> http://arxiv.org/abs/2507.09990v1

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also ![key](https://img.shields.io/badge/communication-F08080)-efficient for federated
![key](https://img.shields.io/badge/LLM-FF8C00)s when multiple users collaboratively fine-tune a global ![key](https://img.shields.io/badge/LLM-FF8C00) model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.


## Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications

>Authors: Yoon Pyo Lee

>2025-07-14

> http://arxiv.org/abs/2507.09931v1

The integration of Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an ![key](https://img.shields.io/badge/LLM-FF8C00) encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose ![key](https://img.shields.io/badge/LLM-FF8C00)
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
![key](https://img.shields.io/badge/sparse-F08080) set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.


## Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models

>Authors: Shuhao Fu, Philip J. Kellman, Hongjing Lu

>2025-07-13

> http://arxiv.org/abs/2507.09830v1

Both humans and deep learning models can recognize objects from 3D shapes
depicted with ![key](https://img.shields.io/badge/sparse-F08080) visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual ![key](https://img.shields.io/badge/transformer-FF8C00)s (point ![key](https://img.shields.io/badge/transformer-FF8C00)), with
human performance. We found that the point ![key](https://img.shields.io/badge/transformer-FF8C00) model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point ![key](https://img.shields.io/badge/transformer-FF8C00) model that supports
hierarchical abstraction of 3D shapes.


## Token Compression Meets Compact Vision Transformers A Survey and Comparative Evaluation for Edge AI

>Authors: Phat Nguyen, Ngai-Man Cheung

>2025-07-13

> http://arxiv.org/abs/2507.09702v1

Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
![key](https://img.shields.io/badge/pruning-F08080), merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed ![key](https://img.shields.io/badge/transformer-FF8C00)s, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact ![key](https://img.shields.io/badge/transformer-FF8C00)-based networks
for edge AI and AI agent applications.


## QuarterMap Efficient Post-Training Token Pruning for Visual State Space Models

>Authors: Tien-Yu Chi, Hung-Yueh Chiang, Diana Marculescu, Kai-Chiang Wu

>2025-07-13

> http://arxiv.org/abs/2507.09514v1

State space models (SSMs) reduce the quadratic complexity of ![key](https://img.shields.io/badge/transformer-FF8C00)s by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
![key](https://img.shields.io/badge/pruning-F08080) method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.


## ViT-ProtoNet for Few-Shot Image Classification A Multi-Benchmark Evaluation

>Authors: Abdulvahap Mutlu, ÅengÃ¼l DoÄan, TÃ¼rker Tuncer

>2025-07-12

> http://arxiv.org/abs/2507.09299v1

The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including ![key](https://img.shields.io/badge/overlap-F08080)ped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with ![key](https://img.shields.io/badge/transformer-FF8C00)-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of ![key](https://img.shields.io/badge/transformer-FF8C00) depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for ![key](https://img.shields.io/badge/transformer-FF8C00)-based meta-learners.


## Psychology-Driven Enhancement of Humour Translation

>Authors: Yuchen Su, Yonghua Zhu, Yang Chen, Diana Benavides-Prado, Michael Witbrock

>2025-07-12

> http://arxiv.org/abs/2507.09259v1

Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and ![key](https://img.shields.io/badge/communication-F08080). Although most existing Large
Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating ![key](https://img.shields.io/badge/LLM-FF8C00)s to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.


## SLIM A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding

>Authors: Weihong Xu, Haein Choi, Po-kai Hsu, Shimeng Yu, Tajana Rosing

>2025-07-12

> http://arxiv.org/abs/2507.09201v1

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload ![key](https://img.shields.io/badge/LLM-FF8C00)
inference to expensive heterogeneous computing systems, they fail to exploit
the significant ![key](https://img.shields.io/badge/sparsity-F08080) inherent in ![key](https://img.shields.io/badge/LLM-FF8C00) operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
![key](https://img.shields.io/badge/sparse-F08080) ![key](https://img.shields.io/badge/LLM-FF8C00) serving on edge devices. SLIM exploits ![key](https://img.shields.io/badge/LLM-FF8C00) ![key](https://img.shields.io/badge/sparsity-F08080) through an
adaptive thresholding algorithm that enables runtime-configurable ![key](https://img.shields.io/badge/sparsity-F08080) with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective ![key](https://img.shields.io/badge/LLM-FF8C00) deployment viable for edge computing environments.


## Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models

>Authors: Ameen Ali, Shahar Katz, Lior Wolf, Ivan Titov

>2025-07-12

> http://arxiv.org/abs/2507.09185v1

Large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and ![key](https://img.shields.io/badge/pruning-F08080) neurons associated with dataset-specific mechanisms in
![key](https://img.shields.io/badge/transformer-FF8C00)-based ![key](https://img.shields.io/badge/LLM-FF8C00)s. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively ![key](https://img.shields.io/badge/pruning-F08080) these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our ![key](https://img.shields.io/badge/pruning-F08080)-based fine-tuning significantly
enhances performance, surpassing prior (non-![key](https://img.shields.io/badge/pruning-F08080)) adaptation methods.


## Hide-and-Shill A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System

>Authors: Ronghua Shi, Yiou Liu, Xinyu Ying, Yang Tan, Yuchun Feng, Lynn Ai, Bill Shi, Xuhui Wang, Zhuang Liu

>2025-07-12

> http://arxiv.org/abs/2507.09179v1

Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in ![key](https://img.shields.io/badge/sparse-F08080)-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates ![key](https://img.shields.io/badge/LLM-FF8C00)-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.


## Lizard An Efficient Linearization Framework for Large Language Models

>Authors: Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen

>2025-07-11

> http://arxiv.org/abs/2507.09025v1

We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based ![key](https://img.shields.io/badge/LLM-FF8C00)s face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(![key](https://img.shields.io/badge/KV-F08080)) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.


## Bridging Literature and the Universe Via A Multi-Agent Large Language Model System

>Authors: Xiaowen Zhang, Zhenyu Bi, Patrick Lachance, Xuan Wang, Tiziana Di Matteo, Rupert A. C. Croft

>2025-07-11

> http://arxiv.org/abs/2507.08958v2

As cosmological simulations and their associated software become increasingly
complex, physicists face the challenge of searching through vast amounts of
literature and user manuals to extract simulation parameters from dense
academic papers, each using different models and formats. Translating these
parameters into executable scripts remains a time-consuming and error-prone
process. To improve efficiency in physics research and accelerate the
cosmological simulation process, we introduce SimAgents, a multi-agent system
designed to automate both parameter configuration from the literature and
preliminary analysis for cosmology research. SimAgents is powered by
specialized ![key](https://img.shields.io/badge/LLM-FF8C00) agents capable of physics reasoning, simulation software
validation, and tool execution. These agents collaborate through structured
![key](https://img.shields.io/badge/communication-F08080), ensuring that extracted parameters are physically meaningful,
internally consistent, and software-compliant. We also construct a cosmological
parameter extraction evaluation dataset by collecting over 40 simulations in
published papers from Arxiv and leading journals that cover diverse simulation
types. Experiments on the dataset demonstrate a strong performance of
SimAgents, highlighting its effectiveness and potential to accelerate
scientific research for physicists. Our demonstration video is available at:
https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly
available at https://github.com/xwzhang98/SimAgents.


## Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents

>Authors: Enhao Zhang, Erkang Zhu, Gagan Bansal, Adam Fourney, Hussein Mozannar, Jack Gerrits

>2025-07-11

> http://arxiv.org/abs/2507.08944v1

Large language model (![key](https://img.shields.io/badge/LLM-FF8C00))-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven ![key](https://img.shields.io/badge/communication-F08080) model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.


## BlockFFN Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity

>Authors: Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun

>2025-07-11

> http://arxiv.org/abs/2507.08771v1

To alleviate the computational burden of large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s),
architectures with activation ![key](https://img.shields.io/badge/sparsity-F08080), represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these ![key](https://img.shields.io/badge/sparse-F08080)ly-activated architectures
exhibit low chunk-level ![key](https://img.shields.io/badge/sparsity-F08080), indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a ![key](https://img.shields.io/badge/sparsity-F08080)
pattern is unfriendly for ![key](https://img.shields.io/badge/acceleration-F08080) under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream ![key](https://img.shields.io/badge/acceleration-F08080) techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level ![key](https://img.shields.io/badge/sparsity-F08080) (TLS) and chunk-level ![key](https://img.shields.io/badge/sparsity-F08080) (CLS), CLS-aware training
objectives are designed, making BlockFFN more ![key](https://img.shields.io/badge/acceleration-F08080)-friendly. Finally,
we implement efficient ![key](https://img.shields.io/badge/acceleration-F08080) kernels, combining activation ![key](https://img.shields.io/badge/sparsity-F08080) and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).


## AgentsNet Coordination and Collaborative Reasoning in Multi-Agent LLMs

>Authors: Florian GrÃ¶tschla, Luis MÃ¼ller, Jan TÃ¶nshoff, Mikhail Galkin, Bryan Perozzi

>2025-07-11

> http://arxiv.org/abs/2507.08616v1

Large-language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective ![key](https://img.shields.io/badge/communication-F08080) given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and ![key](https://img.shields.io/badge/communication-F08080). We find that some frontier ![key](https://img.shields.io/badge/LLM-FF8C00)s
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of ![key](https://img.shields.io/badge/LLM-FF8C00)s. As such, we also probe frontier
models in a setup with up to 100 agents.


## Anthropomimetic Uncertainty What Verbalized Uncertainty in Language Models is Missing

>Authors: Dennis Ulmer, Alexandra Lorson, Ivan Titov, Christian Hardmeier

>2025-07-11

> http://arxiv.org/abs/2507.10587v1

Human users increasingly rely on natural language interactions with large
language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of ![key](https://img.shields.io/badge/LLM-FF8C00)s is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty ![key](https://img.shields.io/badge/communication-F08080) and the data
biases that influence machine uncertainty ![key](https://img.shields.io/badge/communication-F08080). We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
![key](https://img.shields.io/badge/communication-F08080) requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human ![key](https://img.shields.io/badge/communication-F08080). We
present a thorough overview over the research in human uncertainty
![key](https://img.shields.io/badge/communication-F08080), survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine ![key](https://img.shields.io/badge/communication-F08080) of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.


## FreeAudio Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation

>Authors: Yuxuan Jiang, Zehua Chen, Zeqian Ju, Chang Li, Weibei Dou, Jun Zhu

>2025-07-11

> http://arxiv.org/abs/2507.08557v1

Text-to-audio (T2A) generation has achieved promising results with the recent
advances in generative models. However, because of the limited quality and
quantity of temporally-aligned audio-text pairs, existing T2A methods struggle
to handle the complex text prompts that contain precise timing control, e.g.,
"owl hooted at 2.4s-5.2s". Recent works have explored data augmentation
techniques or introduced timing conditions as model inputs to enable
timing-conditioned 10-second T2A generation, while their synthesis quality is
still limited. In this work, we propose a novel training-free timing-controlled
T2A framework, FreeAudio, making the first attempt to enable timing-controlled
long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping
at 0s-24s". Specifically, we first employ an ![key](https://img.shields.io/badge/LLM-FF8C00) to plan non-![key](https://img.shields.io/badge/overlap-F08080)ping time
windows and recaption each with a refined natural language description, based
on the input text and timing prompts. Then we introduce: 1) Decoupling and
Aggregating Attention Control for precise timing control; 2) Contextual Latent
Composition for local smoothness and Reference Guidance for global consistency.
Extensive experiments show that: 1) FreeAudio achieves state-of-the-art
timing-conditioned T2A synthesis quality among training-free methods and is
comparable to leading training-based methods; 2) FreeAudio demonstrates
comparable long-form generation quality with training-based Stable Audio and
paves the way for timing-controlled long-form T2A synthesis. Demo samples are
available at: https://freeaudio.github.io/FreeAudio/


## Evaluating SAE interpretability without explanations

>Authors: GonÃ§alo Paulo, Nora Belrose

>2025-07-11

> http://arxiv.org/abs/2507.08473v1

Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an ![key](https://img.shields.io/badge/LLM-FF8C00) to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
![key](https://img.shields.io/badge/sparse-F08080) coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.


## Upsample What Matters Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers

>Authors: Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun

>2025-07-11

> http://arxiv.org/abs/2507.08422v1

Diffusion ![key](https://img.shields.io/badge/transformer-FF8C00)s have emerged as an alternative to U-net-based
diffusion models for high-fidelity image and video generation, offering
superior scalability. However, their heavy computation remains a major obstacle
to real-world deployment. Existing ![key](https://img.shields.io/badge/acceleration-F08080) methods primarily exploit the
temporal dimension such as reusing cached features across diffusion timesteps.
Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free
framework that accelerates inference along spatial dimension. RALU performs
mixed-resolution sampling across three stages: 1) low-resolution denoising
latent diffusion to efficiently capture global semantic structure, 2)
region-adaptive upsampling on specific regions prone to artifacts at
full-resolution, and 3) all latent upsampling at full-resolution for detail
refinement. To stabilize generations across resolution transitions, we leverage
noise-timestep rescheduling to adapt the noise level across varying
resolutions. Our method significantly reduces computation while preserving
image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$
on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is
complementary to existing temporal ![key](https://img.shields.io/badge/acceleration-F08080)s such as caching methods, thus
can be seamlessly integrated to further reduce inference latency without
compromising generation quality.


## Invariant-based Robust Weights Watermark for Large Language Models

>Authors: Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, Xiaobing Guo

>2025-07-11

> http://arxiv.org/abs/2507.08288v1

Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for ![key](https://img.shields.io/badge/transformer-FF8C00) models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, ![key](https://img.shields.io/badge/pruning-F08080),
quantization, permutation, scaling, reversible matrix and collusion attacks).

