<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>2025-09-12 - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2025-09-12";
        var mkdocs_page_input_path = "weekly_paper/2025-09-12.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../2025-08-01/">2025-08-01</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-08-08/">2025-08-08</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-08-15/">2025-08-15</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-08-22/">2025-08-22</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-08-29/">2025-08-29</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-05/">2025-09-05</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">2025-09-12</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Lagency</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-07-25/">2025-07-25</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Weekly Paper</li>
      <li class="breadcrumb-item active">2025-09-12</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="2025-09-12">2025-09-12</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#butterflyquant-ultra-low-bit-llm-quantization-through-learnable-orthogonal-butterfly-transforms">ButterflyQuant Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</a></li>
<li><a href="#finite-scalar-quantization-enables-redundant-and-transmission-robust-neural-audio-compression-at-low-bit-rates">Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates</a></li>
<li><a href="#trenv-transparently-share-serverless-execution-environments-across-different-functions-and-nodes">TrEnv Transparently Share Serverless Execution Environments Across Different Functions and Nodes</a></li>
<li><a href="#combating-the-memory-walls-optimization-pathways-for-long-context-agentic-llm-inference">Combating the Memory Walls Optimization Pathways for Long-Context Agentic LLM Inference</a></li>
<li><a href="#ensi-efficient-non-interactive-secure-inference-for-large-language-models">ENSI Efficient Non-Interactive Secure Inference for Large Language Models</a></li>
<li><a href="#hd-moe-hybrid-and-dynamic-parallelism-for-mixture-of-expert-llms-with-3d-near-memory-processing">HD-MoE Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing</a></li>
<li><a href="#efficient-transformer-based-piano-transcription-with-sparse-attention-mechanisms">Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms</a></li>
<li><a href="#from-scratch-to-silver-creating-trustworthy-training-data-for-patent-sdg-classification-using-large-language-models">From scratch to silver Creating trustworthy training data for patent-SDG classification using Large Language Models</a></li>
<li><a href="#harnessing-uncertainty-entropy-modulated-policy-gradients-for-long-horizon-llm-agents">Harnessing Uncertainty Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents</a></li>
<li><a href="#Medverse-A-Universal-Model-for-Full-Resolution-3D-Medical-Image-Segmentation,-Transformation-and-Enhancement">Medverse A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement</a></li>
<li><a href="#gmslm--generative-marmoset-spoken-language-modeling">GmSLM  Generative Marmoset Spoken Language Modeling</a></li>
<li><a href="#ai-reasoning-for-wireless-communications-and-networking-a-survey-and-perspectives">AI Reasoning for Wireless Communications and Networking A Survey and Perspectives</a></li>
<li><a href="#adaptive-pareto-optimal-token-merging-for-edge-transformer-models-in-semantic-communication">Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication</a></li>
<li><a href="#dp-fedlora-privacy-enhanced-federated-fine-tuning-for-on-device-large-language-models">DP-FedLoRA Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models</a></li>
<li><a href="#towards-confidential-and-efficient-llm-inference-with-dual-privacy-protection">Towards Confidential and Efficient LLM Inference with Dual Privacy Protection</a></li>
<li><a href="#instructional-prompt-optimization-for-few-shot-llm-based-recommendations-on-cold-start-users">Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users</a></li>
<li><a href="#voxelformer-parameter-efficient-multi-subject-visual-decoding-from-fmri">VoxelFormer Parameter-Efficient Multi-Subject Visual Decoding from fMRI</a></li>
<li><a href="#crowdquery-density-guided-query-module-for-enhanced-2d-and-3d-detection-in-crowded-scenes">CrowdQuery Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</a></li>
<li><a href="#chembomas-accelerated-bo-in-chemistry-with-llm-enhanced-multi-agent-system">ChemBOMAS Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System</a></li>
<li><a href="#accelerating-diffusion-transformer-based-text-to-speech-with-transformer-layer-caching">Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer Layer Caching</a></li>
<li><a href="#BitROM-Weight-Reload-Free-CiROM-Architecture-Towards-Billion-Parameter-1.58-bit-LLM-Inference">BitROM Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference</a></li>
<li><a href="#two-sides-of-the-same-optimization-coin-model-degradation-and-representation-collapse-in-graph-foundation-models">Two Sides of the Same Optimization Coin Model Degradation and Representation Collapse in Graph Foundation Models</a></li>
<li><a href="#efficient-decoding-methods-for-language-models-on-encrypted-data">Efficient Decoding Methods for Language Models on Encrypted Data</a></li>
<li><a href="#bitrate-controlled-diffusion-for-disentangling-motion-and-content-in-video">Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</a></li>
<li><a href="#persistent-dpo-a-novel-loss-function-and-hybrid-learning-for-generative-quantum-eigensolver">Persistent-DPO A novel loss function and hybrid learning for generative quantum eigensolver</a></li>
<li><a href="#accelerating-mixture-of-expert-inference-with-adaptive-expert-split-mechanism">Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism</a></li>
<li><a href="#accelerating-reinforcement-learning-algorithms-convergence-using-pre-trained-large-language-models-as-tutors-with-advice-reusing">Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing</a></li>
<li><a href="#towards-knowledge-aware-document-systems-modeling-semantic-coverage-relations-via-answerability-detection">Towards Knowledge-Aware Document Systems Modeling Semantic Coverage Relations via Answerability Detection</a></li>
<li><a href="#rtr-a-transformer-based-lossless-crossover-with-perfect-phase-alignment">RTR A Transformer-Based Lossless Crossover with Perfect Phase Alignment</a></li>
<li><a href="#mitigating-catastrophic-forgetting-in-large-language-models-with-forgetting-aware-pruning">Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning</a></li>
<li><a href="#XML-Prompting-as-Grammar-Constrained-Interaction-Fixed-Point-Semantics,-Convergence-Guarantees,-and-Human-AI-Protocols">XML Prompting as Grammar-Constrained Interaction Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols</a></li>
<li><a href="#sca-llm-spectral-attentive-channel-prediction-with-large-language-models-in-mimo-ofdm">SCA-LLM Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM</a></li>
<li><a href="#feature-space-analysis-by-guided-diffusion-model">Feature Space Analysis by Guided Diffusion Model</a></li>
<li><a href="#Biased-Tales-Cultural-and-Topic-Bias-in-Generating-Children's-Stories">Biased Tales Cultural and Topic Bias in Generating Children's Stories</a></li>
<li><a href="#a-robot-that-listens-enhancing-self-disclosure-and-engagement-through-sentiment-based-backchannels-and-active-listening">A Robot That Listens Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening</a></li>
<li><a href="#Are-Humans-as-Brittle-as-Large-Language-Models?">Are Humans as Brittle as Large Language Models?</a></li>
<li><a href="#query-expansion-in-the-age-of-pre-trained-and-large-language-models-a-comprehensive-survey">Query Expansion in the Age of Pre-trained and Large Language Models A Comprehensive Survey</a></li>
<li><a href="#unleashing-the-true-potential-of-llms-a-feedback-triggered-self-correction-with-long-term-multipath-decoding">Unleashing the True Potential of LLMs A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding</a></li>
<li><a href="#malei-at-multiclinsum-summarisation-of-clinical-documents-using-perspective-aware-iterative-self-prompting-with-llms">MaLei at MultiClinSUM Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs</a></li>
<li><a href="#panolam-large-avatar-model-for-gaussian-full-head-synthesis-from-one-shot-unposed-image">PanoLAM Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</a></li>
<li><a href="#patchseeker-mapping-nvd-records-to-their-vulnerability-fixing-commits-with-llm-generated-commits-and-embeddings">PatchSeeker Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings</a></li>
<li><a href="#competitive-audio-language-models-with-data-efficient-single-stage-training-on-public-data">Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data</a></li>
<li><a href="#multi-view-guided-passage-reranking-with-large-language-models">Multi-view-guided Passage Reranking with Large Language Models</a></li>
<li><a href="#duoserve-moe-dual-phase-expert-prefetch-and-cache-scheduling-for-efficient-moe-llm-inference">DuoServe-MoE Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference</a></li>
<li><a href="#personafuse-a-personality-activation-driven-framework-for-enhancing-human-llm-interactions">PersonaFuse A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions</a></li>
</ul>
<h2 id="butterflyquant-ultra-low-bit-llm-quantization-through-learnable-orthogonal-butterfly-transforms">ButterflyQuant Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</h2>
<blockquote>
<p>Authors: Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09679v1">http://arxiv.org/abs/2509.09679v1</a></p>
</blockquote>
<p>Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, using computational invariance: <script type="math/tex">\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})</script> for orthogonal <script type="math/tex">\mathbf{Q}</script>. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence <script type="math/tex">\mu = 1/\sqrt{n}</script>--that cannot adapt to specific weight
distributions. We identify that different <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete <script type="math/tex">\{+1, -1\}</script>
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving <script type="math/tex">O(n \log
n)</script> computational complexity with only <script type="math/tex">\frac{n \log n}{2}</script> learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.</p>
<h2 id="finite-scalar-quantization-enables-redundant-and-transmission-robust-neural-audio-compression-at-low-bit-rates">Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates</h2>
<blockquote>
<p>Authors: Harry Julia, Rachel Beeson, Lohith Konathala, Johanna Ulin, Jiameng Gao</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09550v1">http://arxiv.org/abs/2509.09550v1</a></p>
</blockquote>
<p>Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>r and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.</p>
<h2 id="trenv-transparently-share-serverless-execution-environments-across-different-functions-and-nodes">TrEnv Transparently Share Serverless Execution Environments Across Different Functions and Nodes</h2>
<blockquote>
<p>Authors: Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09525v1">http://arxiv.org/abs/2509.09525v1</a></p>
</blockquote>
<p>Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.</p>
<h2 id="combating-the-memory-walls-optimization-pathways-for-long-context-agentic-llm-inference">Combating the Memory Walls Optimization Pathways for Long-Context Agentic LLM Inference</h2>
<blockquote>
<p>Authors: Haoran Wu, Can Xiao, Jiayi Nie, Xuan Guo, Binglei Lou, Jeffrey T. H. Wong, Zhiwen Mo, Cheng Zhang, Przemyslaw Forys, Wayne Luk, Hongxiang Fan, Jianyi Cheng, Timothy M. Jones, Rika Antonova, Robert Mullins, Aaron Zhao</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09505v1">http://arxiv.org/abs/2509.09505v1</a></p>
</blockquote>
<p><a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> for long-context <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.</p>
<h2 id="ensi-efficient-non-interactive-secure-inference-for-large-language-models">ENSI Efficient Non-Interactive Secure Inference for Large Language Models</h2>
<blockquote>
<p>Authors: Zhiyu He, Maojiang Wang, Xinwen Gao, Yuchuan Luo, Lin Liu, Shaojing Fu</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09424v1">http://arxiv.org/abs/2509.09424v1</a></p>
</blockquote>
<p>Secure inference enables privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) presents significant challenges, as the inherent
complexity of these protocols, together with <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s,
based on the principle of co-designing the cryptographic protocols and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.</p>
<h2 id="hd-moe-hybrid-and-dynamic-parallelism-for-mixture-of-expert-llms-with-3d-near-memory-processing">HD-MoE Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing</h2>
<blockquote>
<p>Authors: Haochen Huang, Shuzhang Zhong, Zhe Zhang, Shuangchen Li, Dimin Niu, Hongzhong Zheng, Runsheng Wang, Meng Li</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09420v1">http://arxiv.org/abs/2509.09420v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.</p>
<h2 id="efficient-transformer-based-piano-transcription-with-sparse-attention-mechanisms">Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms</h2>
<blockquote>
<p>Authors: Weixing Wei, Kazuyoshi Yoshii</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09318v1">http://arxiv.org/abs/2509.09318v1</a></p>
</blockquote>
<p>This paper investigates automatic piano transcription based on
computationally-efficient yet high-performant variants of the Transformer that
can capture longer-term dependency over the whole musical piece. Recently,
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based sequence-to-sequence models have demonstrated excellent
performance in piano transcription. These models, however, fail to deal with
the whole piece at once due to the quadratic complexity of the self-attention
mechanism, and music signals are thus typically processed in a sliding-window
manner in practice. To overcome this limitation, we propose an efficient
architecture with <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention mechanisms. Specifically, we introduce
sliding-window self-attention mechanisms for both the encoder and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r, and
a hybrid global-local cross-attention mechanism that attends to various spans
according to the MIDI token types. We also use a hierarchical pooling strategy
between the encoder and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r to further reduce computational load. Our
experiments on the MAESTRO dataset showed that the proposed model achieved a
significant reduction in computational cost and memory usage, accelerating
inference speed, while maintaining transcription performance comparable to the
full-attention baseline. This allows for training with longer audio contexts on
the same hardware, demonstrating the viability of <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention for building
efficient and high-performance piano transcription systems. The code is
available at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.</p>
<h2 id="from-scratch-to-silver-creating-trustworthy-training-data-for-patent-sdg-classification-using-large-language-models">From scratch to silver Creating trustworthy training data for patent-SDG classification using Large Language Models</h2>
<blockquote>
<p>Authors: Grazia Sveva Ascione, Nicol√≤ Tamagnone</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09303v1">http://arxiv.org/abs/2509.09303v1</a></p>
</blockquote>
<p>Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> and noise, we
develop a composite labeling function (LF) that uses large language models
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based models, and zero-shot
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.</p>
<h2 id="harnessing-uncertainty-entropy-modulated-policy-gradients-for-long-horizon-llm-agents">Harnessing Uncertainty Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents</h2>
<blockquote>
<p>Authors: Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09265v1">http://arxiv.org/abs/2509.09265v1</a></p>
</blockquote>
<p>In long-horizon tasks, recent agents based on Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s)
face a significant challenge that <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/</p>
<h2 id="medverse-a-universal-model-for-full-resolution-3d-medical-image-segmentation-transformation-and-enhancement">Medverse A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement</h2>
<blockquote>
<p>Authors: Jiesi Hu, Jianfeng Cao, Yanwu Yang, Chenfei Ye, Yixuan Zhang, Hanyang Peng, Ting Ma</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09232v1">http://arxiv.org/abs/2509.09232v1</a></p>
</blockquote>
<p>In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> computational efficiency through spatial <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.</p>
<h2 id="gmslm-generative-marmoset-spoken-language-modeling">GmSLM  Generative Marmoset Spoken Language Modeling</h2>
<blockquote>
<p>Authors: Talia Sternberg, Michael London, David Omer, Yossi Adi</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09198v1">http://arxiv.org/abs/2509.09198v1</a></p>
</blockquote>
<p>Marmoset monkeys exhibit complex vocal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, challenging the view
that nonhuman primates vocal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.</p>
<h2 id="ai-reasoning-for-wireless-communications-and-networking-a-survey-and-perspectives">AI Reasoning for Wireless Communications and Networking A Survey and Perspectives</h2>
<blockquote>
<p>Authors: Haoxiang Luo, Yu Yan, Yanhui Bian, Wenjiao Feng, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Gang Sun, Dusit Niyato, Hongfang Yu, Abbas Jamalipour, Shiwen Mao</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09193v1">http://arxiv.org/abs/2509.09193v1</a></p>
</blockquote>
<p>Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks, with a focus on Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) and other
advanced reasoning paradigms. In particular, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> performance. Finally, we discuss key research directions for AI
reasoning toward future wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks. By combining insights
from both <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.</p>
<h2 id="adaptive-pareto-optimal-token-merging-for-edge-transformer-models-in-semantic-communication">Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication</h2>
<blockquote>
<p>Authors: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09168v1">http://arxiv.org/abs/2509.09168v1</a></p>
</blockquote>
<p>Large-scale <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> models have emerged as a powerful tool for semantic
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based semantic
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> in future edge intelligence systems.</p>
<h2 id="dp-fedlora-privacy-enhanced-federated-fine-tuning-for-on-device-large-language-models">DP-FedLoRA Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models</h2>
<blockquote>
<p>Authors: Honghui Xu, Shiva Shrestha, Wei Chen, Zhiyuan Li, Zhipeng Cai</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09097v1">http://arxiv.org/abs/2509.09097v1</a></p>
</blockquote>
<p>As on-device large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy (<script type="math/tex">\epsilon</script>, <script type="math/tex">\delta</script>)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> deployment in on-device environments.</p>
<h2 id="towards-confidential-and-efficient-llm-inference-with-dual-privacy-protection">Towards Confidential and Efficient LLM Inference with Dual Privacy Protection</h2>
<blockquote>
<p>Authors: Honglan Yu, Yibin Wang, Feifei Dai, Dong Liu, Haihui Fan, Xiaoyan Gu</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09091v1">http://arxiv.org/abs/2509.09091v1</a></p>
</blockquote>
<p>CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) result in significant <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> user data privacy.</p>
<h2 id="instructional-prompt-optimization-for-few-shot-llm-based-recommendations-on-cold-start-users">Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users</h2>
<blockquote>
<p>Authors: Haowei Yang, Yushang Zhao, Sitao Min, Bo Su, Chao Yao, Wei Xu</p>
<p>2025-09-11</p>
<p><a href="http://arxiv.org/abs/2509.09066v1">http://arxiv.org/abs/2509.09066v1</a></p>
</blockquote>
<p>The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based autoregressive <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based pipelines.</p>
<h2 id="voxelformer-parameter-efficient-multi-subject-visual-decoding-from-fmri">VoxelFormer Parameter-Efficient Multi-Subject Visual Decoding from fMRI</h2>
<blockquote>
<p>Authors: Chenqian Le, Yilin Zhao, Nikasadat Emami, Kushagra Yadav, Xujin "Chris" Liu, Xupeng Chen, Yao Wang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.09015v1">http://arxiv.org/abs/2509.09015v1</a></p>
</blockquote>
<p>Recent advances in fMRI-based visual <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture that
enables multi-subject training for visual <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s as
promising strategies for parameter-efficient neural <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>.</p>
<h2 id="crowdquery-density-guided-query-module-for-enhanced-2d-and-3d-detection-in-crowded-scenes">CrowdQuery Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</h2>
<blockquote>
<p>Authors: Marius D√§hling, Sebastian Krebs, J. Marius Z√∂llner</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08738v1">http://arxiv.org/abs/2509.08738v1</a></p>
</blockquote>
<p>This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based
detectors. We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map. The embedded
density information is then systematically integrated into the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r.
Existing density map definitions typically depend on head positions or
object-based spatial statistics. Our method extends these definitions to
include individual bounding box dimensions. By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes. CQ is universally applicable to both 2D
and 3D detection without requiring additional data. Consequently, we are the
first to design a method that effectively bridges 2D and 3D detection in
crowded environments. We demonstrate the integration of CQ into both a general
2D and 3D <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based object detector, introducing the architectures CQ2D
and CQ3D. CQ is not limited to the specific <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> models we selected.
Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods. When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability. The code is released at
https://github.com/mdaehl/CrowdQuery.</p>
<h2 id="chembomas-accelerated-bo-in-chemistry-with-llm-enhanced-multi-agent-system">ChemBOMAS Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System</h2>
<blockquote>
<p>Authors: Dong Han, Zhehong Ai, Pengxiang Cai, Shuzhou Sun, Shanya Lu, Jianpeng Chen, Ben Gao, Lingli Ge, Weida Wang, Xiangxin Zhou, Xihui Liu, Mao Su, Wanli Ouyang, Lei Bai, Dongzhan Zhou, Tao XU, Yuqiang Li, Shufei Zhang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08736v1">http://arxiv.org/abs/2509.08736v1</a></p>
</blockquote>
<p>The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.</p>
<h2 id="accelerating-diffusion-transformer-based-text-to-speech-with-transformer-layer-caching">Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer Layer Caching</h2>
<blockquote>
<p>Authors: Siratish Sakpiboonchit</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08696v1">http://arxiv.org/abs/2509.08696v1</a></p>
</blockquote>
<p>This paper presents a method to accelerate the inference process of diffusion
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> (DiT)-based text-to-speech (TTS) models by applying a selective
caching mechanism to <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> layers. Specifically, I integrate SmoothCache
into the F5-TTS architecture, focusing on caching outputs of self-attention and
feed-forward network layers to reduce redundant computations during the
denoising process. A calibration phase is introduced to analyze L1 relative
errors between timesteps, guiding the selection of <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> schedules that
minimize quality degradation. To address the problem of inter-layer dependency,
a unified caching schedule is adopted, applying the <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> pattern derived from
self-attention layers to both layer types. Experiments on LibriSpeech-PC and
Seed-TTS datasets evaluate various <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> thresholds and denoising step
configurations. Results show that caching at higher denoising steps reduces
inference time without compromising output quality, whereas caching at lower
steps can negatively impact synthesis quality similarly to reducing the total
number of denoising steps. Objective and subjective metrics confirm the
effectiveness of SmoothCache in maintaining performance while improving
computational efficiency. Comparisons between <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>d inference and reduced-step
inference further highlight the benefits of selective caching, especially under
high-step configurations. This work demonstrates that <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> layer caching
is a practical solution for optimizing diffusion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based TTS models
without requiring architectural changes or retraining. Example inference
results can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .</p>
<h2 id="bitrom-weight-reload-free-cirom-architecture-towards-billion-parameter-158-bit-llm-inference">BitROM Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference</h2>
<blockquote>
<p>Authors: Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08542v1">http://arxiv.org/abs/2509.08542v1</a></p>
</blockquote>
<p>Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy
efficiency for CNNs by eliminating runtime weight updates. However, their
scalability to Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is fundamentally constrained by
their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA
series - demands more than 1,000 cm2 of silicon area even in advanced CMOS
nodes. This paper presents BitROM, the first CiROM-based accelerator that
overcomes this limitation through co-design with BitNet's 1.58-bit <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>
model, enabling practical and efficient <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference at the edge. BitROM
introduces three key innovations: 1) a novel Bidirectional ROM Array that
stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator
optimized for ternary-weight computations; and 3) an integrated Decode-Refresh
(DR) eDRAM that supports on-die <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> management, significantly reducing
external memory access during <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>. In addition, BitROM integrates
LoRA-based adapters to enable efficient transfer learning across various
downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit
density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over
prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%
reduction in external DRAM access, further enhancing deployment efficiency for
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in edge applications.</p>
<h2 id="two-sides-of-the-same-optimization-coin-model-degradation-and-representation-collapse-in-graph-foundation-models">Two Sides of the Same Optimization Coin Model Degradation and Representation Collapse in Graph Foundation Models</h2>
<blockquote>
<p>Authors: Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08401v2">http://arxiv.org/abs/2509.08401v2</a></p>
</blockquote>
<p>Graph foundation models, inspired by the success of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.</p>
<h2 id="efficient-decoding-methods-for-language-models-on-encrypted-data">Efficient Decoding Methods for Language Models on Encrypted Data</h2>
<blockquote>
<p>Authors: Matan Avitan, Moran Baruch, Nir Drucker, Itamar Zimerman, Yoav Goldberg</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08383v1">http://arxiv.org/abs/2509.08383v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a>
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.</p>
<h2 id="bitrate-controlled-diffusion-for-disentangling-motion-and-content-in-video">Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</h2>
<blockquote>
<p>Authors: Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08376v1">http://arxiv.org/abs/2509.08376v1</a></p>
</blockquote>
<p>We propose a novel and general framework to disentangle video data into its
dynamic motion and static content components. Our proposed method is a
self-supervised pipeline with less assumptions and inductive biases than
previous works: it utilizes a <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based architecture to jointly
generate flexible implicit features for frame-wise motion and clip-wise
content, and incorporates a <a class="glightbox" href="https://img.shields.io/badge/low-bit-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/low-bit-F08080" /></a>rate vector <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> as an information
bottleneck to promote disentanglement and form a meaningful discrete motion
space. The bitrate-controlled latent motion and content are used as conditional
inputs to a denoising diffusion model to facilitate self-supervised
representation learning. We validate our disentangled representation learning
framework on real-world talking head videos with motion transfer and
auto-regressive motion generation tasks. Furthermore, we also show that our
method can generalize to other types of video data, such as pixel sprites of 2D
cartoon characters. Our work presents a new perspective on self-supervised
learning of disentangled video representations, contributing to the broader
field of video analysis and generation.</p>
<h2 id="persistent-dpo-a-novel-loss-function-and-hybrid-learning-for-generative-quantum-eigensolver">Persistent-DPO A novel loss function and hybrid learning for generative quantum eigensolver</h2>
<blockquote>
<p>Authors: Junya Nakamura, Shinichiro Sanji</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08351v1">http://arxiv.org/abs/2509.08351v1</a></p>
</blockquote>
<p>We study the generative quantum eigensolver
(GQE)~\cite{nakaji2024generative}, which trains a classical generative model to
produce quantum circuits with desired properties such as describing molecular
ground states. We introduce two methods to improve GQE. First, we identify a
limitation of direct preference optimization (DPO) when used as the loss
function in GQE, and propose Persistent-DPO (P-DPO) as a solution to this
limitation. Second, as a method to improve the online learning during the
training phase of GQE, we introduce a hybrid approach that combines online and
offline learning. Using a <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r implementation of GQE, we
evaluate our methods through ground state search experiments on the
<script type="math/tex">\mathrm{BeH_2^{}}</script> molecule and observe that P-DPO achieves lower energies
than DPO. The hybrid approach further improves convergence and final energy
values, particularly with P-DPO.</p>
<h2 id="accelerating-mixture-of-expert-inference-with-adaptive-expert-split-mechanism">Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism</h2>
<blockquote>
<p>Authors: Jiaming Yan, Jianchun Liu, Hongli Xu, Liusheng Huang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08342v1">http://arxiv.org/abs/2509.08342v1</a></p>
</blockquote>
<p>Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>s the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>d experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>. Nevertheless, the
performance of MoEpic critically depends on the <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>
configuration. Extensive experiments on popular MoE <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.</p>
<h2 id="accelerating-reinforcement-learning-algorithms-convergence-using-pre-trained-large-language-models-as-tutors-with-advice-reusing">Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing</h2>
<blockquote>
<p>Authors: Lukas Toral, Teddy Lazebnik</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08329v1">http://arxiv.org/abs/2509.08329v1</a></p>
</blockquote>
<p>Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> model combination.</p>
<h2 id="towards-knowledge-aware-document-systems-modeling-semantic-coverage-relations-via-answerability-detection">Towards Knowledge-Aware Document Systems Modeling Semantic Coverage Relations via Answerability Detection</h2>
<blockquote>
<p>Authors: Yehudit Aperstein, Alon Gottlib, Gal Benita, Alexander Apartsin</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08304v1">http://arxiv.org/abs/2509.08304v1</a></p>
</blockquote>
<p>Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
<a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>, where each document presents partially <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>. This dataset allows us to benchmark
generative language models and train <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.</p>
<h2 id="rtr-a-transformer-based-lossless-crossover-with-perfect-phase-alignment">RTR A Transformer-Based Lossless Crossover with Perfect Phase Alignment</h2>
<blockquote>
<p>Authors: Xiangying Li, Jiankuan Li, Yong Tang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08272v1">http://arxiv.org/abs/2509.08272v1</a></p>
</blockquote>
<p>This paper proposes a <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based lossless crossover method, termed
Resonant Transformer Router (RTR), which achieves frequency separation while
ensuring perfect phase alignment between low-frequency (LF) and high-frequency
(HF) channels at the crossover frequency. The core property of RTR is that its
frequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so
that the original signal can be perfectly reconstructed by linear summation of
the two channels. Theoretical derivation and circuit simulations demonstrate
that RTR provides superior energy efficiency, phase consistency, and robustness
against component tolerances. Compared with conventional LC crossovers and
digital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted
filtering solution suitable for high-fidelity audio and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
front-ends.
  The core theory behind this paper's work, lossless crossover, is based on a
Chinese patent [CN116318117A] developed from the previous research of one of
the authors, Jianluan Li. We provide a comprehensive experimental validation of
this theory and propose a new extension.</p>
<h2 id="mitigating-catastrophic-forgetting-in-large-language-models-with-forgetting-aware-pruning">Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning</h2>
<blockquote>
<p>Authors: Wei Huang, Anda Cheng, Yinggui Wang</p>
<p>2025-09-10</p>
<p><a href="http://arxiv.org/abs/2509.08255v1">http://arxiv.org/abs/2509.08255v1</a></p>
</blockquote>
<p>Recent advancements in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&amp;A, Medical Q&amp;A, Math Q&amp;A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.</p>
<h2 id="xml-prompting-as-grammar-constrained-interaction-fixed-point-semantics-convergence-guarantees-and-human-ai-protocols">XML Prompting as Grammar-Constrained Interaction Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols</h2>
<blockquote>
<p>Authors: Faruk Alpay, Taylan Alpay</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.08182v1">http://arxiv.org/abs/2509.08182v1</a></p>
</blockquote>
<p>Structured prompting with XML tags has emerged as an effective way to steer
large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> guarantees
well-formedness while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan <script type="math/tex">\to</script> verify <script type="math/tex">\to</script> revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>, chain-of-verification, and programmatic
prompting.</p>
<h2 id="sca-llm-spectral-attentive-channel-prediction-with-large-language-models-in-mimo-ofdm">SCA-LLM Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM</h2>
<blockquote>
<p>Authors: Ke He, Le He, Lisheng Fan, Xianfu Lei, Thang X. Vu, George K. Karagiannidis, Symeon Chatzinotas</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.08139v1">http://arxiv.org/abs/2509.08139v1</a></p>
</blockquote>
<p>In recent years, the success of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) has inspired
growing interest in exploring their potential applications in wireless
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s, especially for channel prediction tasks. However, directly
applying <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to channel prediction faces a domain mismatch issue stemming from
their text-based pre-training. To mitigate this, the ``adapter + <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>" paradigm
has emerged, where an adapter is designed to bridge the domain gap between the
channel state information (CSI) data and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. While showing initial success,
existing adapters may not fully exploit the potential of this paradigm. To
address this limitation, this work provides a key insight that learning
representations from the spectral components of CSI features can more
effectively help bridge the domain gap. Accordingly, we propose a
spectral-attentive framework, named SCA-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, for channel prediction in
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral
details and better adapt the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> for channel prediction than previous methods.
Extensive simulations show that SCA-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> achieves state-of-the-art prediction
performance and strong generalization, yielding up to <script type="math/tex">-2.4~\text{dB}</script>
normalized mean squared error (NMSE) advantage over the previous <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> based
method. Ablation studies further confirm the superiority of SCA-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> in
mitigating domain mismatch.</p>
<h2 id="feature-space-analysis-by-guided-diffusion-model">Feature Space Analysis by Guided Diffusion Model</h2>
<blockquote>
<p>Authors: Kimiaki Shirahama, Miki Yanobu, Kaduki Yamashita, Miho Ohsaki</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07936v1">http://arxiv.org/abs/2509.07936v1</a></p>
</blockquote>
<p>One of the key issues in Deep Neural Networks (DNNs) is the black-box nature
of their internal feature extraction process. Targeting vision-related domains,
this paper focuses on analysing the feature space of a DNN by proposing a
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r that can generate images whose features are guaranteed to closely match
a user-specified feature. Owing to this guarantee that is missed in past
studies, our <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r allows us to evidence which of various attributes in an
image are encoded into a feature by the DNN, by generating images whose
features are in proximity to that feature. Our <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r is implemented as a
guided diffusion model that guides the reverse image generation of a
pre-trained diffusion model to minimise the Euclidean distance between the
feature of a clean image estimated at each step and the user-specified feature.
One practical advantage of our <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r is that it can analyse feature spaces of
different DNNs with no additional training and run on a single COTS GPU. The
experimental results targeting CLIP's image encoder, ResNet-50 and vision
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> demonstrate that images generated by our <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r have features
remarkably similar to the user-specified ones and reveal valuable insights into
these DNNs' feature spaces.</p>
<h2 id="biased-tales-cultural-and-topic-bias-in-generating-childrens-stories">Biased Tales Cultural and Topic Bias in Generating Children's Stories</h2>
<blockquote>
<p>Authors: Donya Rooein, Vil√©m Zouhar, Debora Nozza, Dirk Hovy</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07908v1">http://arxiv.org/abs/2509.07908v1</a></p>
</blockquote>
<p>Stories play a pivotal role in human <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, shaping beliefs and
morals, particularly in children. As parents increasingly rely on large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to craft bedtime stories, the presence of cultural and
gender stereotypes in these narratives raises significant concerns. To address
this issue, we present Biased Tales, a comprehensive dataset designed to
analyze how biases influence protagonists' attributes and story elements in
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-generated stories. Our analysis uncovers striking disparities. When the
protagonist is described as a girl (as compared to a boy), appearance-related
attributes increase by 55.26%. Stories featuring non-Western children
disproportionately emphasize cultural heritage, tradition, and family themes
far more than those for Western children. Our findings highlight the role of
sociocultural bias in making creative AI use more equitable and diverse.</p>
<h2 id="a-robot-that-listens-enhancing-self-disclosure-and-engagement-through-sentiment-based-backchannels-and-active-listening">A Robot That Listens Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening</h2>
<blockquote>
<p>Authors: Hieu Tran, Go-Eum Cha, Sooyeon Jeong</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07873v1">http://arxiv.org/abs/2509.07873v1</a></p>
</blockquote>
<p>As social robots get more deeply integrated intoour everyday lives, they will
be expected to engage in meaningful conversations and exhibit socio-emotionally
intelligent listening behaviors when interacting with people. Active listening
and backchanneling could be one way to enhance robots' communicative
capabilities and enhance their effectiveness in eliciting deeper
self-disclosure, providing a sense of empathy,and forming positive rapport and
relationships with people.Thus, we developed an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-powered social robot that
can exhibit contextually appropriate sentiment-based backchannelingand active
listening behaviors (active listening+backchanneling) and compared its efficacy
in eliciting people's self-disclosurein comparison to robots that do not
exhibit any of these listening behaviors (control) and a robot that only
exhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental
study with sixty-five participants, we found theparticipants who conversed with
the active listening robot per-ceived the interactions more positively, in
which they exhibited the highest self-disclosures, and reported the strongest
senseof being listened to. The results of our study suggest that the
implementation of active listening behaviors in social robotshas the potential
to improve human-robot <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> andcould further contribute to the
building of deeper human-robot relationships and rapport.</p>
<h2 id="are-humans-as-brittle-as-large-language-models">Are Humans as Brittle as Large Language Models?</h2>
<blockquote>
<p>Authors: Jiahui Li, Sean Papay, Roman Klinger</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07869v1">http://arxiv.org/abs/2509.07869v1</a></p>
</blockquote>
<p>The output of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) is unstable, due to both
non-determinism of the <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> process as well as to prompt brittleness. While
the intrinsic non-determinism of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="query-expansion-in-the-age-of-pre-trained-and-large-language-models-a-comprehensive-survey">Query Expansion in the Age of Pre-trained and Large Language Models A Comprehensive Survey</h2>
<blockquote>
<p>Authors: Minghan Li, Xinxuan Lv, Junjie Zou, Tongna Chen, Chao Zhang, Suchao An, Ercong Nie, Guodong Zhou</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07794v1">http://arxiv.org/abs/2509.07794v1</a></p>
</blockquote>
<p>Modern information retrieval (IR) must bridge short, ambiguous queries and
ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key
mechanism for mitigating vocabulary mismatch, but the design space has shifted
markedly with pre-trained language models (PLMs) and large language models
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). This survey synthesizes the field from three angles: (i) a
four-dimensional framework of query expansion - from the point of injection
(explicit vs. implicit QE), through grounding and interaction (knowledge bases,
model-internal capabilities, multi-turn retrieval) and learning alignment, to
knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning
encoder-only, encoder-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r, <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r-only, instruction-tuned, and
domain/multilingual variants, highlighting their characteristic affordances for
QE (contextual disambiguation, controllable generation, zero-/few-shot
reasoning); and (iii) practice-oriented guidance on where and how neural QE
helps in first-stage retrieval, multi-query fusion, re-ranking, and
retrieval-augmented generation (RAG). We compare traditional query expansion
with PLM/<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based methods across seven key aspects, and we map applications
across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational
and code search, and cross-lingual settings. The review distills design
grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG
constraints - as robust remedies to topic drift and hallucination. We conclude
with an agenda on quality control, cost-aware invocation, domain/temporal
adaptation, evaluation beyond end-task metrics, and fairness/privacy.
Collectively, these insights provide a principled blueprint for selecting and
combining QE techniques under real-world constraints.</p>
<h2 id="unleashing-the-true-potential-of-llms-a-feedback-triggered-self-correction-with-long-term-multipath-decoding">Unleashing the True Potential of LLMs A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding</h2>
<blockquote>
<p>Authors: Jipeng Li, Zeyu Gao, Yubin Qi, Hande Dong, Weijian Chen, Qiang Lin</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07676v1">http://arxiv.org/abs/2509.07676v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.</p>
<h2 id="malei-at-multiclinsum-summarisation-of-clinical-documents-using-perspective-aware-iterative-self-prompting-with-llms">MaLei at MultiClinSUM Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs</h2>
<blockquote>
<p>Authors: Libo Ren, Yee Man Ng, Lifeng Han</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07622v1">http://arxiv.org/abs/2509.07622v1</a></p>
</blockquote>
<p>Efficient <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> between patients and clinicians plays an important
role in shared decision-making. However, clinical reports are often lengthy and
filled with clinical jargon, making it difficult for domain experts to identify
important aspects in the document efficiently. This paper presents the
methodology we applied in the MultiClinSUM shared task for summarising clinical
case documents. We used an Iterative Self-Prompting technique on large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) by asking <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to generate task-specific prompts and refine them
via example-based few-shot learning. Furthermore, we used lexical and embedding
space metrics, ROUGE and BERT-score, to guide the model fine-tuning with
epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved
ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,
R, F1) from the official evaluation on 3,396 clinical case reports from various
specialties extracted from open journals. The high BERTscore indicates that the
model produced semantically equivalent output summaries compared to the
references, even though the <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> at the exact lexicon level is lower, as
reflected in the lower ROUGE scores. This work sheds some light on how
perspective-aware ISP (PA-ISP) can be deployed for clinical report
summarisation and support better <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> between patients and clinicians.</p>
<h2 id="panolam-large-avatar-model-for-gaussian-full-head-synthesis-from-one-shot-unposed-image">PanoLAM Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</h2>
<blockquote>
<p>Authors: Peng Li, Yisheng He, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Zilong Dong, Yike Guo</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07552v1">http://arxiv.org/abs/2509.07552v1</a></p>
</blockquote>
<p>We present a feed-forward framework for Gaussian full-head synthesis from a
single unposed image. Unlike previous work that relies on time-consuming GAN
inversion and test-time optimization, our framework can reconstruct the
Gaussian full-head model given a single unposed image in a single forward pass.
This enables fast reconstruction and rendering during inference. To mitigate
the lack of large-scale 3D head assets, we propose a large-scale synthetic
dataset from trained 3D GANs and train our framework using only synthetic data.
For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian
head generation pipeline, where <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> points from the FLAME model interact
with the image features by <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> blocks for feature extraction and coarse
shape reconstruction, which are then densified for high-fidelity
reconstruction. To fully leverage the prior knowledge residing in pretrained 3D
GANs for effective reconstruction, we propose a dual-branch framework that
effectively aggregates the structured spherical triplane feature and
unstructured point-based features for more effective Gaussian head
reconstruction. Experimental results show the effectiveness of our framework
towards existing work.</p>
<h2 id="patchseeker-mapping-nvd-records-to-their-vulnerability-fixing-commits-with-llm-generated-commits-and-embeddings">PatchSeeker Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings</h2>
<blockquote>
<p>Authors: Huu Hung Nguyen, Anh Tuan Nguyen, Thanh Le-Cong, Yikun Li, Han Wei Ang, Yide Yin, Frank Liauw, Shar Lwin Khin, Ouh Eng Lieh, Ting Zhang, David Lo</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07540v1">http://arxiv.org/abs/2509.07540v1</a></p>
</blockquote>
<p>Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.</p>
<h2 id="competitive-audio-language-models-with-data-efficient-single-stage-training-on-public-data">Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data</h2>
<blockquote>
<p>Authors: Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07526v1">http://arxiv.org/abs/2509.07526v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have transformed NLP, yet their integration with
audio remains underexplored -- despite audio's centrality to human
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. We introduce Falcon3-Audio, a family of Audio-Language Models
(ALMs) built on instruction-tuned <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and Whisper encoders. Using a remarkably
small amount of public audio data -- less than 30K hours (5K unique) --
Falcon3-Audio-7B matches the best reported performance among open-weight models
on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while
distinguishing itself through superior data and parameter efficiency,
single-stage training, and transparency. Notably, our smallest 1B model remains
competitive with larger open models ranging from 2B to 13B parameters. Through
extensive ablations, we find that common complexities -- such as curriculum
learning, multiple audio encoders, and intricate cross-attention connectors --
are not required for strong performance, even compared to models trained on
over 500K hours of data.</p>
<h2 id="multi-view-guided-passage-reranking-with-large-language-models">Multi-view-guided Passage Reranking with Large Language Models</h2>
<blockquote>
<p>Authors: Jeongwoo Na, Jun Kwon, Eunseong Choi, Jongwuk Lee</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07485v1">http://arxiv.org/abs/2509.07485v1</a></p>
</blockquote>
<p>Recent advances in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have shown impressive
performance in passage reranking tasks. Despite their success, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based
methods still face challenges in efficiency and sensitivity to external biases.
(1) Existing models rely mostly on autoregressive generation and sliding window
strategies to rank passages, which incur heavy computational overhead as the
number of passages increases. (2) External biases, such as position or
selection bias, hinder the model's ability to accurately represent passages and
increase input-order sensitivity. To address these limitations, we introduce a
novel passage reranking model, called Multi-View-guided Passage Reranking
(MVP). MVP is a non-generative <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based reranking method that encodes
query-passage information into diverse view embeddings without being influenced
by external biases. For each view, it combines query-aware passage embeddings
to produce a distinct anchor vector, which is then used to directly compute
relevance scores in a single <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> step. In addition, it employs an
orthogonal loss to make the views more distinctive. Extensive experiments
demonstrate that MVP, with just 220M parameters, matches the performance of
much larger 7B-scale fine-tuned models while achieving a 100x reduction in
inference latency. Notably, the 3B-parameter variant of MVP achieves
state-of-the-art performance on both in-domain and out-of-domain benchmarks.
The source code is available at: https://github.com/bulbna/MVP</p>
<h2 id="duoserve-moe-dual-phase-expert-prefetch-and-cache-scheduling-for-efficient-moe-llm-inference">DuoServe-MoE Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference</h2>
<blockquote>
<p>Authors: Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07379v1">http://arxiv.org/abs/2509.07379v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
<a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> stage where most experts are activated densely, and a <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> stage
where only a few experts are triggered <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> system that
explicitly separates <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> stages and applies tailored expert
scheduling strategies to each. In the <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> stage, DuoServe-MoE uses a
two-stream CUDA pipeline that <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>s expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.</p>
<h2 id="personafuse-a-personality-activation-driven-framework-for-enhancing-human-llm-interactions">PersonaFuse A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions</h2>
<blockquote>
<p>Authors: Yixuan Tang, Yi Yang, Ahmed Abbasi</p>
<p>2025-09-09</p>
<p><a href="http://arxiv.org/abs/2509.07370v2">http://arxiv.org/abs/2509.07370v2</a></p>
</blockquote>
<p>Recent advancements in Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) demonstrate remarkable
capabilities across various fields. These developments have led to more direct
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> between humans and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in various situations, such as social
companionship and psychological support. However, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s often exhibit
limitations in emotional perception and social competence during real-world
conversations. These limitations partly originate from their inability to adapt
their <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> style and emotional expression to different social and task
contexts. In this work, we introduce PersonaFuse, a novel <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> post-training
framework that enables <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to adapt and express different personalities for
varying situations. Inspired by Trait Activation Theory and the Big Five
personality model, PersonaFuse employs a Mixture-of-Expert architecture that
combines persona adapters with a dynamic routing network, enabling contextual
trait expression. Experimental results show that PersonaFuse substantially
outperforms baseline models across multiple dimensions of social-emotional
intelligence. Importantly, these gains are achieved without sacrificing general
reasoning ability or model safety, which remain common limitations of direct
prompting and supervised fine-tuning approaches. PersonaFuse also delivers
consistent improvements in downstream human-centered applications, such as
mental health counseling and review-based customer service. Finally, human
preference evaluations against leading <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, including GPT-4o and DeepSeek,
demonstrate that PersonaFuse achieves competitive response quality despite its
comparatively smaller model size. These findings demonstrate that PersonaFuse
offers a theoretically grounded and practical approach for developing
social-emotional enhanced <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, marking a significant advancement toward more
human-centric AI systems.</p>
              
  <!-- Giscus ËØÑËÆ∫Á≥ªÁªü - Âè™Âú® notes Êñá‰ª∂Â§π‰∏ãÊòæÁ§∫ -->
<script>
  // ‰ΩøÁî® JavaScript Êù•Âà§Êñ≠ URL Ë∑ØÂæÑ
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../2025-09-05/" class="btn btn-neutral float-left" title="2025-09-05"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../lagency/2025-07-25/" class="btn btn-neutral float-right" title="2025-07-25">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../2025-09-05/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../lagency/2025-07-25/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../js/prism-prototxt.js"></script>
      <script src="../../js/preview.js"></script>
      <script src="../../js/back-to-top.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
