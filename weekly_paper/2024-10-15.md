# 2024-10-15

# Table of Contents
* [DuoAttention Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](#DuoAttention-Efficient-Long-Context-LLM-Inference-with-Retrieval-and-Streaming-Heads)
* [Boosting Camera Motion Control for Video Diffusion Transformers](#Boosting-Camera-Motion-Control-for-Video-Diffusion-Transformers)
* [When Attention Sink Emerges in Language Models An Empirical View](#When-Attention-Sink-Emerges-in-Language-Models-An-Empirical-View)
* [A Counterexample in Image Registration](#A-Counterexample-in-Image-Registration)
* [AutoTurb Using Large Language Models for Automatic Algebraic Model Discovery of Turbulence Closure](#AutoTurb-Using-Large-Language-Models-for-Automatic-Algebraic-Model-Discovery-of-Turbulence-Closure)
* [Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](#Efficiently-Democratizing-Medical-LLMs-for-50-Languages-via-a-Mixture-of-Language-Family-Experts)
* [On the sparsity of binary numbers](#On-the-sparsity-of-binary-numbers)
* [SLaNC Static LayerNorm Calibration](#SLaNC-Static-LayerNorm-Calibration)
* [Comparison of deep learning and conventional methods for disease onset prediction](#Comparison-of-deep-learning-and-conventional-methods-for-disease-onset-prediction)
* [Moirai-MoE Empowering Time Series Foundation Models with Sparse Mixture of Experts](#Moirai-MoE-Empowering-Time-Series-Foundation-Models-with-Sparse-Mixture-of-Experts)
* [Pubic Symphysis-Fetal Head Segmentation Network Using BiFormer Attention Mechanism and Multipath Dilated Convolution](#Pubic-Symphysis-Fetal-Head-Segmentation-Network-Using-BiFormer-Attention-Mechanism-and-Multipath-Dilated-Convolution)
* [EasyRAG Efficient Retrieval-Augmented Generation Framework for Network Automated Operations](#EasyRAG-Efficient-Retrieval-Augmented-Generation-Framework-for-Network-Automated-Operations)
* [Enhancing Attributed Graph Networks with Alignment and Uniformity Constraints for Session-based Recommendation](#Enhancing-Attributed-Graph-Networks-with-Alignment-and-Uniformity-Constraints-for-Session-based-Recommendation)
* [A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration](#A-Consistency-Aware-Spot-Guided-Transformer-for-Versatile-and-Hierarchical-Point-Cloud-Registration)
* [Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies](#Large-Language-Model-Enhanced-Reinforcement-Learning-for-Generic-Bus-Holding-Control-Strategies)
* [HSR-Enhanced Sparse Attention Acceleration](#HSR-Enhanced-Sparse-Attention-Acceleration)
* [Mixture of Experts Made Personalized Federated Prompt Learning for Vision-Language Models](#Mixture-of-Experts-Made-Personalized-Federated-Prompt-Learning-for-Vision-Language-Models)
* [GALA Geometry-Aware Local Adaptive Grids for Detailed 3D Generation](#GALA-Geometry-Aware-Local-Adaptive-Grids-for-Detailed-3D-Generation)
* [Exploring Demonstration Retrievers in RAG for Coding Tasks Yeas and Nays!](#Exploring-Demonstration-Retrievers-in-RAG-for-Coding-Tasks-Yeas-and-Nays!)
* [SLiM One-shot Quantized Sparse Plus Low-rank Approximation of LLMs](#SLiM-One-shot-Quantized-Sparse-Plus-Low-rank-Approximation-of-LLMs)
* [MTL-LoRA Low-Rank Adaptation for Multi-Task Learning](#MTL-LoRA-Low-Rank-Adaptation-for-Multi-Task-Learning)
* [FlatQuant Flatness Matters for LLM Quantization](#FlatQuant-Flatness-Matters-for-LLM-Quantization)
* [Fine-grained Attention I/O Complexity Comprehensive Analysis for Backward Passes](#Fine-grained-Attention-I/O-Complexity-Comprehensive-Analysis-for-Backward-Passes)
* [Token Pruning using a Lightweight Background Aware Vision Transformer](#Token-Pruning-using-a-Lightweight-Background-Aware-Vision-Transformer)
* [Diffraction and pseudospectra in non-Hermitian quasiperiodic lattices](#Diffraction-and-pseudospectra-in-non-Hermitian-quasiperiodic-lattices)
* [Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures](#Extra-Global-Attention-Designation-Using-Keyword-Detection-in-Sparse-Transformer-Architectures)
* [Maximizing the Potential of Synthetic Data Insights from Random Matrix Theory](#Maximizing-the-Potential-of-Synthetic-Data-Insights-from-Random-Matrix-Theory)
* [Evolution of SAE Features Across Layers in LLMs](#Evolution-of-SAE-Features-Across-Layers-in-LLMs)
* [Unveiling Molecular Secrets An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction](#Unveiling-Molecular-Secrets-An-LLM-Augmented-Linear-Model-for-Explainable-and-Calibratable-Molecular-Property-Prediction)
* [AMPO Automatic Multi-Branched Prompt Optimization](#AMPO-Automatic-Multi-Branched-Prompt-Optimization)
* [QEFT Quantization for Efficient Fine-Tuning of LLMs](#QEFT-Quantization-for-Efficient-Fine-Tuning-of-LLMs)
* [Canonical Ramsey numbers of sparse graphs](#Canonical-Ramsey-numbers-of-sparse-graphs)
* [Words as Beacons Guiding RL Agents with High-Level Language Prompts](#Words-as-Beacons-Guiding-RL-Agents-with-High-Level-Language-Prompts)
* [Towards Cross-domain Few-shot Graph Anomaly Detection](#Towards-Cross-domain-Few-shot-Graph-Anomaly-Detection)
* [ZipVL Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression](#ZipVL-Efficient-Large-Vision-Language-Models-with-Dynamic-Token-Sparsification-and-KV-Cache-Compression)
* [DeBiFormer Vision Transformer with Deformable Agent Bi-level Routing Attention](#DeBiFormer-Vision-Transformer-with-Deformable-Agent-Bi-level-Routing-Attention)
* [A transformational approach to collective behavior](#A-transformational-approach-to-collective-behavior)
* [GIVE Structured Reasoning with Knowledge Graph Inspired Veracity Extrapolation](#GIVE-Structured-Reasoning-with-Knowledge-Graph-Inspired-Veracity-Extrapolation)
* [Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models](#Exploring-the-Role-of-Reasoning-Structures-for-Constructing-Proofs-in-Multi-Step-Natural-Language-Reasoning-with-Large-Language-Models)
* [Exact solution of the master equation for interacting quantized fields at finite temperature decay](#Exact-solution-of-the-master-equation-for-interacting-quantized-fields-at-finite-temperature-decay)


## DuoAttention Efficient Long-Context LLM Inference with Retrieval and Streaming Heads

>Authors: Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han

>2024-10-14

> http://arxiv.org/abs/2410.10819v1

Deploying long-context large language models (LLMs) is essential but poses
significant computational and memory challenges. Caching all Key and Value (KV)
states across all attention heads consumes substantial memory. Existing KV
cache pruning methods either damage the long-context capabilities of LLMs or
offer only limited efficiency improvements. In this paper, we identify that
only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for
processing long contexts and require full attention across all tokens. In
contrast, all other heads, which primarily focus on recent tokens and attention
sinks--referred to as Streaming Heads--do not require full attention. Based on
this insight, we introduce DuoAttention, a framework that only applies a full
KV cache to retrieval heads while using a light-weight, constant-length KV
cache for streaming heads, which reduces both LLM's decoding and pre-filling
memory and latency without compromising its long-context abilities.
DuoAttention uses a lightweight, optimization-based algorithm with synthetic
data to identify retrieval heads accurately. Our method significantly reduces
long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models
while speeding up decoding by up to 2.18x and 1.50x and accelerating
pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with
minimal accuracy loss compared to full attention. Notably, combined with
quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context
length on a single A100 GPU. Code is provided in
https://github.com/mit-han-lab/duo-attention.


## Boosting Camera Motion Control for Video Diffusion Transformers

>Authors: Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, Chun-Hao Paul Huang

>2024-10-14

> http://arxiv.org/abs/2410.10802v1

Recent advancements in diffusion models have significantly enhanced the
quality of video generation. However, fine-grained control over camera pose
remains a challenge. While U-Net-based models have shown promising results for
camera control, transformer-based diffusion models (DiT)-the preferred
architecture for large-scale video generation - suffer from severe degradation
in camera motion accuracy. In this paper, we investigate the underlying causes
of this issue and propose solutions tailored to DiT architectures. Our study
reveals that camera control performance depends heavily on the choice of
conditioning methods rather than camera pose representations that is commonly
believed. To address the persistent motion degradation in DiT, we introduce
Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts
camera control by over 400%. Additionally, we present a sparse camera control
pipeline, significantly simplifying the process of specifying camera poses for
long videos. Our method universally applies to both U-Net and DiT models,
offering improved camera control for video generation tasks.


## When Attention Sink Emerges in Language Models An Empirical View

>Authors: Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin

>2024-10-14

> http://arxiv.org/abs/2410.10781v1

Language Models (LMs) assign significant attention to the first token, even
if it is not semantically important, which is known as attention sink. This
phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in LMs with various inputs, even in small
models. Furthermore, attention sink is observed to emerge during the LM
pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its
emergence. We highlight that attention sink emerges after effective
optimization on sufficient training data. The sink position is highly
correlated with the loss function and data distribution. Most importantly, we
find that attention sink acts more like key biases, storing extra attention
scores, which could be non-informative and not contribute to the value
computation. We also observe that this phenomenon (at least partially) stems
from tokens' inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without
normalization, attention sinks do not emerge in LMs up to 1B parameters. The
code is available at https://github.com/sail-sg/Attention-Sink.


## A Counterexample in Image Registration

>Authors: Serap A. Savari

>2024-10-14

> http://arxiv.org/abs/2410.10725v1

Image registration is a widespread problem which applies models about image
transformation or image similarity to align discrete images of the same scene.
Nevertheless, the theoretical limits on its accuracy are not understood even in
the case of one-dimensional data. Just as Nyquist's sampling theorem states
conditions for the perfect reconstruction of signals from samples, there are
bounds to the quality of reproductions of quantized functions from sets of
ideal, noiseless samples in the absence of additional assumptions. In this work
we estimate spatially-limited piecewise constant signals from two or more sets
of noiseless sampling patterns. We mainly focus on the energy of the error
function and find that the uncertainties of the positions of the discontinuity
points of the function depend on the discontinuity point selected as the
reference point of the signal. As a consequence, the accuracy of the estimate
of the signal depends on the reference point of that signal.


## AutoTurb Using Large Language Models for Automatic Algebraic Model Discovery of Turbulence Closure

>Authors: Yu Zhang, Kefeng Zheng, Fei Liu, Qingfu Zhang, Zhenkun Wang

>2024-10-14

> http://arxiv.org/abs/2410.10657v1

Symbolic regression (SR) methods have been extensively investigated to
explore explicit algebraic Reynolds stress models (EARSM) for turbulence
closure of Reynolds-averaged Navier-Stokes (RANS) equations. The deduced EARSM
can be readily implemented in existing computational fluid dynamic (CFD) codes
and promotes the identification of physically interpretable turbulence models.
The existing SR methods, such as genetic programming, sparse regression, or
artificial neural networks, require user-defined functional operators, a
library of candidates, or complex optimization algorithms. In this work, a
novel framework using LLMs to automatically discover algebraic expressions for
correcting the RSM is proposed. The direct observation of Reynolds stress and
the indirect output of the CFD simulation are both involved in the training
process to guarantee data consistency and avoid numerical stiffness.
Constraints of functional complexity and convergence are supplementally imposed
in the objective function on account of the tremendous flexibility of LLMs. The
evolutionary search is employed for global optimization. The proposed method is
performed for separated flow over periodic hills at Re = 10,595. The
generalizability of the discovered model is verified on a set of 2D turbulent
separated flow configurations with different Reynolds numbers and geometries.
It is demonstrated that the corrective RANS can improve the prediction for both
the Reynolds stress and mean velocity fields. Compared with algebraic models
discovered by other works, the discovered model performs better in accuracy and
generalization capability. The proposed approach provides a promising paradigm
for using LLMs to improve turbulence modeling for a given class of flows.


## Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts

>Authors: Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, Benyou Wang

>2024-10-14

> http://arxiv.org/abs/2410.10626v1

Adapting medical Large Language Models to local languages can reduce barriers
to accessing healthcare services, but data scarcity remains a significant
challenge, particularly for low-resource languages. To address this, we first
construct a high-quality medical dataset and conduct analysis to ensure its
quality. In order to leverage the generalization capability of multilingual
LLMs to efficiently scale to more resource-constrained languages, we explore
the internal information flow of LLMs from a multilingual perspective using
Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE
routing method that employs language-specific experts and cross-lingual
routing. Inspired by circuit theory, our routing analysis revealed a Spread Out
in the End information flow mechanism: while earlier layers concentrate
cross-lingual information flow, the later layers exhibit language-specific
divergence. This insight directly led to the development of the Post-MoE
architecture, which applies sparse routing only in the later layers while
maintaining dense others. Experimental results demonstrate that this approach
enhances the generalization of multilingual models to other languages while
preserving interpretability. Finally, to efficiently scale the model to 50
languages, we introduce the concept of language family experts, drawing on
linguistic priors, which enables scaling the number of languages without adding
additional parameters.


## On the sparsity of binary numbers

>Authors: Meijun Zhu

>2024-10-14

> http://arxiv.org/abs/2410.10620v1

We introduce the concept of negative coefficients in various number-based
systems, with a focus on decimal and binary systems. We demonstrate that every
binary number can be transformed into a sparse form, significantly enhancing
computational speed by converting binary numbers into this form.


## SLaNC Static LayerNorm Calibration

>Authors: Mahsa Salmani, Nikita Trukhanov, Ilya Soloveychik

>2024-10-14

> http://arxiv.org/abs/2410.10553v1

The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of
billions of parameters have generated enormous pressure on the manufacturers of
dedicated hardware accelerators and made the innovative design of the latter
one of the most rapidly expanding fields of the AI industry. Various approaches
have been explored to enable efficient and accurate processing of LLMs on the
available accelerators given their computational and storage limitations. Among
these, various quantization techniques have become the main focus of the
community as a means of reducing the compute, communication and storage
requirements. Quantization to lower precision formats naturally poses a number
of challenges caused by the limited range of the available value
representations. When it comes to processing the popular Transformer models on
hardware, one of the main issues becomes calculation of the LayerNorm simply
because accumulation of the variance requires a much wider dynamic range than
the hardware enables. In this article, we address this matter and propose a
computationally-efficient scaling technique that can be easily applied to
Transformer models during inference. Our method suggests a straightforward way
of scaling the LayerNorm inputs based on the static weights of the immediately
preceding linear layers. The scaling factors are computed offline, based solely
on the linear layer weights, hence no latency or computational overhead is
added during inference. Most importantly, our technique ensures that no
numerical issues such as overflow or underflow could happen during the compute.
This approach offers smooth, accurate and resource-effective inference across a
wide range of hardware architectures. The article provides theoretical
justification as well as supporting numerical simulations.


## Comparison of deep learning and conventional methods for disease onset prediction

>Authors: Luis H. John, Chungsoo Kim, Jan A. Kors, Junhyuk Chang, Hannah Morgan-Cooper, Priya Desai, Chao Pang, Peter R. Rijnbeek, Jenna M. Reps, Egill A. Fridgeirsson

>2024-10-14

> http://arxiv.org/abs/2410.10505v1

Background: Conventional prediction methods such as logistic regression and
gradient boosting have been widely utilized for disease onset prediction for
their reliability and interpretability. Deep learning methods promise enhanced
prediction performance by extracting complex patterns from clinical data, but
face challenges like data sparsity and high dimensionality.
  Methods: This study compares conventional and deep learning approaches to
predict lung cancer, dementia, and bipolar disorder using observational data
from eleven databases from North America, Europe, and Asia. Models were
developed using logistic regression, gradient boosting, ResNet, and
Transformer, and validated both internally and externally across the data
sources. Discrimination performance was assessed using AUROC, and calibration
was evaluated using Eavg.
  Findings: Across 11 datasets, conventional methods generally outperformed
deep learning methods in terms of discrimination performance, particularly
during external validation, highlighting their better transportability.
Learning curves suggest that deep learning models require substantially larger
datasets to reach the same performance levels as conventional methods.
Calibration performance was also better for conventional methods, with ResNet
showing the poorest calibration.
  Interpretation: Despite the potential of deep learning models to capture
complex patterns in structured observational healthcare data, conventional
models remain highly competitive for disease onset prediction, especially in
scenarios involving smaller datasets and if lengthy training times need to be
avoided. The study underscores the need for future research focused on
optimizing deep learning models to handle the sparsity, high dimensionality,
and heterogeneity inherent in healthcare datasets, and find new strategies to
exploit the full capabilities of deep learning methods.


## Moirai-MoE Empowering Time Series Foundation Models with Sparse Mixture of Experts

>Authors: Xu Liu, Juncheng Liu, Gerald Woo, Taha Aksu, Yuxuan Liang, Roger Zimmermann, Chenghao Liu, Silvio Savarese, Caiming Xiong, Doyen Sahoo

>2024-10-14

> http://arxiv.org/abs/2410.10469v1

Time series foundation models have demonstrated impressive performance as
zero-shot forecasters. However, achieving effectively unified training on time
series remains an open challenge. Existing approaches introduce some level of
model specialization to account for the highly heterogeneous nature of time
series data. For instance, Moirai pursues unified training by employing
multiple input/output projection layers, each tailored to handle time series at
a specific frequency. Similarly, TimesFM maintains a frequency embedding
dictionary for this purpose. We identify two major drawbacks to this
human-imposed frequency-level model specialization: (1) Frequency is not a
reliable indicator of the underlying patterns in time series. For example, time
series with different frequencies can display similar patterns, while those
with the same frequency may exhibit varied patterns. (2) Non-stationarity is an
inherent property of real-world time series, leading to varied distributions
even within a short context window of a single time series. Frequency-level
specialization is too coarse-grained to capture this level of diversity. To
address these limitations, this paper introduces Moirai-MoE, using a single
input/output projection layer while delegating the modeling of diverse time
series patterns to the sparse mixture of experts (MoE) within Transformers.
With these designs, Moirai-MoE reduces reliance on human-defined heuristics and
enables automatic token-level specialization. Extensive experiments on 39
datasets demonstrate the superiority of Moirai-MoE over existing foundation
models in both in-distribution and zero-shot scenarios. Furthermore, this study
conducts comprehensive model analyses to explore the inner workings of time
series MoE foundation models and provides valuable insights for future
research.


## Pubic Symphysis-Fetal Head Segmentation Network Using BiFormer Attention Mechanism and Multipath Dilated Convolution

>Authors: Pengzhou Cai, Lu Jiang, Yanxin Li, Xiaojuan Liu, Libin Lan

>2024-10-14

> http://arxiv.org/abs/2410.10352v1

Pubic symphysis-fetal head segmentation in transperineal ultrasound images
plays a critical role for the assessment of fetal head descent and progression.
Existing transformer \iffalse-based\fi segmentation methods based on sparse
attention mechanism use handcrafted static patterns, which leads to great
differences \iffalse in \fi in terms of segmentation performance on specific
datasets. To address this issue, we introduce a dynamic, query-aware sparse
attention mechanism for ultrasound image segmentation. Specifically, we propose
a novel method, named BRAU-Net to solve the pubic symphysis-fetal head
segmentation task in this paper. The method adopts a U-Net-like encoder-decoder
architecture with bi-level routing attention and skip connections, which
effectively learns local-global semantic information. In addition, we propose
an inverted bottleneck patch expanding (IBPE) module to reduce information loss
while performing up-sampling operations. The proposed BRAU-Net is evaluated on
FH-PS-AoP and HC18 datasets. The results demonstrate that our method could
achieve excellent segmentation results. The code is available on GitHub.


## EasyRAG Efficient Retrieval-Augmented Generation Framework for Network Automated Operations

>Authors: Zhangchi Feng, Dongdong Kuang, Zhongyuan Wang, Zhijie Nie, Yaowei Zheng, Richong Zhang

>2024-10-14

> http://arxiv.org/abs/2410.10315v1

This paper presents EasyRAG, a simple, lightweight, and efficient
retrieval-augmented generation framework for network automated operations. The
advantages of our solution are: 1.Accurate Question Answering: We designed a
straightforward RAG scheme based on (1) a specific data processing workflow (2)
dual-route sparse retrieval for coarse ranking (3) LLM Reranker for reranking
(4) LLM answer generation and optimization. This approach achieved first place
in the GLM4 track in the preliminary round and second place in the GLM4 track
in the semifinals. 2.Simple Deployment: Our method primarily consists of BM25
retrieval and BGE-reranker reranking, requiring no fine-tuning of any models,
occupying minimal VRAM, easy to deploy, and highly scalable; we provide a
flexible code library with various search and generation strategies,
facilitating custom process implementation. 3.Efficient Inference: We designed
an efficient inference acceleration scheme for the entire coarse ranking,
reranking, and generation process that significantly reduces the inference
latency of RAG while maintaining a good level of accuracy; each acceleration
scheme can be plug-and-play into any component of the RAG process, consistently
enhancing the efficiency of the RAG system. Our code and data are released at
https://github.com/BUAADreamer/EasyRAG.


## Enhancing Attributed Graph Networks with Alignment and Uniformity Constraints for Session-based Recommendation

>Authors: Xinping Zhao, Chaochao Chen, Jiajie Su, Yizhao Zhang, Baotian Hu

>2024-10-14

> http://arxiv.org/abs/2410.10296v1

Session-based Recommendation (SBR), seeking to predict a user's next action
based on an anonymous session, has drawn increasing attention for its
practicability. Most SBR models only rely on the contextual transitions within
a short session to learn item representations while neglecting additional
valuable knowledge. As such, their model capacity is largely limited by the
data sparsity issue caused by short sessions. A few studies have exploited the
Modeling of Item Attributes (MIA) to enrich item representations. However, they
usually involve specific model designs that can hardly transfer to existing
attribute-agnostic SBR models and thus lack universality. In this paper, we
propose a model-agnostic framework, named AttrGAU (Attributed Graph Networks
with Alignment and Uniformity Constraints), to bring the MIA's superiority into
existing attribute-agnostic models, to improve their accuracy and robustness
for recommendation. Specifically, we first build a bipartite attributed graph
and design an attribute-aware graph convolution to exploit the rich attribute
semantics hidden in the heterogeneous item-attribute relationship. We then
decouple existing attribute-agnostic SBR models into the graph neural network
and attention readout sub-modules to satisfy the non-intrusive requirement.
Lastly, we design two representation constraints, i.e., alignment and
uniformity, to optimize distribution discrepancy in representation between the
attribute semantics and collaborative semantics. Extensive experiments on three
public benchmark datasets demonstrate that the proposed AttrGAU framework can
significantly enhance backbone models' recommendation performance and
robustness against data sparsity and data noise issues. Our implementation
codes will be available at https://github.com/ItsukiFujii/AttrGAU.


## A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration

>Authors: Renlang Huang, Yufan Tang, Jiming Chen, Liang Li

>2024-10-14

> http://arxiv.org/abs/2410.10295v1

Deep learning-based feature matching has shown great superiority for point
cloud registration in the absence of pose priors. Although coarse-to-fine
matching approaches are prevalent, the coarse matching of existing methods is
typically sparse and loose without consideration of geometric consistency,
which makes the subsequent fine matching rely on ineffective optimal transport
and hypothesis-and-selection methods for consistency. Therefore, these methods
are neither efficient nor scalable for real-time applications such as odometry
in robotics. To address these issues, we design a consistency-aware spot-guided
Transformer (CAST), which incorporates a spot-guided cross-attention module to
avoid interfering with irrelevant areas, and a consistency-aware self-attention
module to enhance matching capabilities with geometrically consistent
correspondences. Furthermore, a lightweight fine matching module for both
sparse keypoints and dense features can estimate the transformation accurately.
Extensive experiments on both outdoor LiDAR point cloud datasets and indoor
RGBD point cloud datasets demonstrate that our method achieves state-of-the-art
accuracy, efficiency, and robustness.


## Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies

>Authors: Jiajie Yu, Yuhong Wang, Wei Ma

>2024-10-14

> http://arxiv.org/abs/2410.10212v1

Bus holding control is a widely-adopted strategy for maintaining stability
and improving the operational efficiency of bus systems. Traditional
model-based methods often face challenges with the low accuracy of bus state
prediction and passenger demand estimation. In contrast, Reinforcement Learning
(RL), as a data-driven approach, has demonstrated great potential in
formulating bus holding strategies. RL determines the optimal control
strategies in order to maximize the cumulative reward, which reflects the
overall control goals. However, translating sparse and delayed control goals in
real-world tasks into dense and real-time rewards for RL is challenging,
normally requiring extensive manual trial-and-error. In view of this, this
study introduces an automatic reward generation paradigm by leveraging the
in-context learning and reasoning capabilities of Large Language Models (LLMs).
This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based
modules: reward initializer, reward modifier, performance analyzer, and reward
refiner. These modules cooperate to initialize and iteratively improve the
reward function according to the feedback from training and test results for
the specified RL-based task. Ineffective reward functions generated by the LLM
are filtered out to ensure the stable evolution of the RL agents' performance
over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL
paradigm, it is applied to various bus holding control scenarios, including a
synthetic single-line system and a real-world multi-line system. The results
demonstrate the superiority and robustness of the proposed paradigm compared to
vanilla RL strategies, the LLM-based controller, and conventional space
headway-based feedback control. This study sheds light on the great potential
of utilizing LLMs in various smart mobility applications.


## HSR-Enhanced Sparse Attention Acceleration

>Authors: Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song

>2024-10-14

> http://arxiv.org/abs/2410.10165v1

Large Language Models (LLMs) have demonstrated remarkable capabilities across
various applications, but their performance on long-context tasks is often
limited by the computational complexity of attention mechanisms. This paper
introduces a novel approach to accelerate attention computation in LLMs,
particularly for long-context scenarios. We leverage the inherent sparsity
within attention mechanisms, both in conventional Softmax attention and ReLU
attention (with $\mathsf{ReLU}^\alpha$ activation, $\alpha \in \mathbb{N}_+$),
to significantly reduce the running time complexity. Our method employs a
Half-Space Reporting (HSR) data structure to rapidly identify non-zero or
"massively activated" entries in the attention matrix. We present theoretical
analyses for two key scenarios: attention generation and full attention
computation with long input context. Our approach achieves a running time of
$O(mn^{4/5})$ significantly faster than the naive approach $O(mn)$ for
attention generation, where $n$ is the context length, $m$ is the query length,
and $d$ is the hidden dimension. We can also reduce the running time of full
attention computation from $O(mn)$ to $O(mn^{1 - 1 / \lfloor d/2\rfloor} +
mn^{4/5})$. Importantly, our method introduces no error for ReLU attention and
only provably negligible error for Softmax attention, where the latter is
supported by our empirical validation. This work represents a significant step
towards enabling efficient long-context processing in LLMs, potentially
broadening their applicability across various domains.


## Mixture of Experts Made Personalized Federated Prompt Learning for Vision-Language Models

>Authors: Jun Luo, Chen Chen, Shandong Wu

>2024-10-14

> http://arxiv.org/abs/2410.10114v1

Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has
demonstrated potent applicability across diverse downstream tasks. This
lightweight approach has quickly gained traction from federated learning (FL)
researchers who seek to efficiently adapt VLMs to heterogeneous scenarios.
However, current federated prompt learning methods are habitually restricted to
the traditional FL paradigm, where the participating clients are generally only
allowed to download a single globally aggregated model from the server. While
justifiable for training full-sized models under federated settings, in this
work, we argue that this paradigm is ill-suited for lightweight prompts. By
facilitating the clients to download multiple pre-aggregated prompts as fixed
non-local experts, we propose Personalized Federated Mixture of Adaptive
Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning
process through the lens of Mixture of Experts (MoE). pFedMoAP implements a
local attention-based gating network that learns to generate enhanced text
features for better alignment with local image data on the client, benefiting
from both local and downloaded non-local adaptive prompt experts. The non-local
experts are sparsely selected from a server-maintained pool, fostering
collaborative learning across clients. To evaluate the proposed algorithm, we
conduct extensive experiments across 9 datasets under various heterogeneous
federated settings. The results show that pFedMoAP consistently outperforms the
state-of-the-art alternatives, underscoring its efficacy in personalizing
prompt learning for CLIP within the federated learning paradigm.


## GALA Geometry-Aware Local Adaptive Grids for Detailed 3D Generation

>Authors: Dingdong Yang, Yizhi Wang, Konrad Schindler, Ali Mahdavi Amiri, Hao Zhang

>2024-10-13

> http://arxiv.org/abs/2410.10037v1

We propose GALA, a novel representation of 3D shapes that (i) excels at
capturing and reproducing complex geometry and surface details, (ii) is
computationally efficient, and (iii) lends itself to 3D generative modelling
with modern, diffusion-based schemes. The key idea of GALA is to exploit both
the global sparsity of surfaces within a 3D volume and their local surface
properties. Sparsity is promoted by covering only the 3D object boundaries, not
empty space, with an ensemble of tree root voxels. Each voxel contains an
octree to further limit storage and compute to regions that contain surfaces.
Adaptivity is achieved by fitting one local and geometry-aware coordinate frame
in each non-empty leaf node. Adjusting the orientation of the local grid, as
well as the anisotropic scales of its axes, to the local surface shape greatly
increases the amount of detail that can be stored in a given amount of memory,
which in turn allows for quantization without loss of quality. With our
optimized C++/CUDA implementation, GALA can be fitted to an object in less than
10 seconds. Moreover, the representation can efficiently be flattened and
manipulated with transformer networks. We provide a cascaded generation
pipeline capable of generating 3D shapes with great geometric detail.


## Exploring Demonstration Retrievers in RAG for Coding Tasks Yeas and Nays!

>Authors: Pengfei He, Shaowei Wang, Shaiful Chowdhury, Tse-Hsun Chen

>2024-10-12

> http://arxiv.org/abs/2410.09662v1

Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge bases, achieving state-of-the-art results in
various coding tasks. The core of RAG is retrieving demonstration examples,
which is essential to balance effectiveness (generation quality) and efficiency
(retrieval time) for optimal performance. However, the high-dimensional nature
of code representations and large knowledge bases often create efficiency
bottlenecks, which are overlooked in previous research. This paper
systematically evaluates the efficiency-effectiveness trade-off of retrievers
across three coding tasks: Program Synthesis, Commit Message Generation, and
Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)
and four dense retrievers, including one exhaustive dense retriever (SBERT's
Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).
Our findings show that while BM25 excels in effectiveness, it suffers in
efficiency as the knowledge base grows beyond 1000 entries. In large-scale
retrieval, efficiency differences become more pronounced, with approximate
dense retrievers offering the greatest gains. For instance, in Commit
Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in
RougeL compared with BM25. Our results also show that increasing the number of
demonstrations in the prompt doesn't always improve the effectiveness and can
increase latency and lead to incorrect outputs. Our findings provide valuable
insights for practitioners aiming to build efficient and effective RAG systems
for coding tasks.


## SLiM One-shot Quantized Sparse Plus Low-rank Approximation of LLMs

>Authors: Mohammad Mozaffari, Maryam Mehri Dehnavi

>2024-10-12

> http://arxiv.org/abs/2410.09615v1

Large Language Models (LLMs) have revolutionized natural language
understanding and generation tasks but suffer from high memory consumption and
slow inference times due to their large parameter sizes. Traditional model
compression techniques, such as quantization and pruning, mitigate these issues
but often require retraining to maintain accuracy, which is computationally
expensive. This paper introduces SLiM, a novel approach for compressing LLMs
using a one-shot Quantized Sparse Plus Low-rank Approximation. SLiM eliminates
the need for costly retraining by combining a symmetric quantization method
(SLiM-Quant) with a saliency-based low-rank approximation. Our method reduces
quantization error while leveraging sparse representations compatible with
accelerated hardware architectures. Additionally, we propose a
parameter-efficient fine-tuning recipe that significantly reduces overhead
compared to conventional quantization-aware training. SLiM achieves up to a
5.4% improvement in model accuracy for sparsity patterns like 2:4, and the
fine-tuning step further enhances accuracy by up to 5.8%, demonstrating
state-of-the-art performance. This work provides a pathway for efficiently
deploying large models in memory-constrained environments without compromising
accuracy.


## MTL-LoRA Low-Rank Adaptation for Multi-Task Learning

>Authors: Yaming Yang, Dilixat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Denvy Deng, Feng Sun, Qi Zhang, Weizhu Chen, Yunhai Tong

>2024-10-12

> http://arxiv.org/abs/2410.09437v1

Parameter-efficient fine-tuning (PEFT) has been widely employed for domain
adaptation, with LoRA being one of the most prominent methods due to its
simplicity and effectiveness. However, in multi-task learning (MTL) scenarios,
LoRA tends to obscure the distinction between tasks by projecting sparse
high-dimensional features from different tasks into the same dense
low-dimensional intrinsic space. This leads to task interference and suboptimal
performance for LoRA and its variants. To tackle this challenge, we propose
MTL-LoRA, which retains the advantages of low-rank adaptation while
significantly enhancing multi-task learning capabilities. MTL-LoRA augments
LoRA by incorporating additional task-adaptive parameters that differentiate
task-specific information and effectively capture shared knowledge across
various tasks within low-dimensional spaces. This approach enables large
language models (LLMs) pre-trained on general corpus to adapt to different
target task domains with a limited number of trainable parameters.
Comprehensive experimental results, including evaluations on public academic
benchmarks for natural language understanding, commonsense reasoning, and
image-text understanding, as well as real-world industrial text Ads relevance
datasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants
with comparable or even fewer learnable parameters in multitask learning.


## FlatQuant Flatness Matters for LLM Quantization

>Authors: Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, Wulong Liu, Jun Yao

>2024-10-12

> http://arxiv.org/abs/2410.09426v1

Recently, quantization has been widely used for the compression and
acceleration of large language models~(LLMs). Due to the outliers in LLMs, it
is crucial to flatten weights and activations to minimize quantization error
with the equally spaced quantization points. Prior research explores various
pre-quantization transformations to suppress outliers, such as per-channel
scaling and Hadamard transformation. However, we observe that these transformed
weights and activations can still remain steep and outspread. In this paper, we
propose FlatQuant (Fast and Learnable Affine Transformation), a new
post-training quantization approach to enhance flatness of weights and
activations. Our approach identifies optimal affine transformations tailored to
each linear layer, calibrated in hours via a lightweight objective. To reduce
runtime overhead, we apply Kronecker decomposition to the transformation
matrices, and fuse all operations in FlatQuant into a single kernel. Extensive
experiments show that FlatQuant sets up a new state-of-the-art quantization
benchmark. For instance, it achieves less than $\textbf{1}\%$ accuracy drop for
W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by
$\textbf{7.5}\%$. For inference latency, FlatQuant reduces the slowdown induced
by pre-quantization transformation from 0.26x of QuaRot to merely
$\textbf{0.07x}$, bringing up to $\textbf{2.3x}$ speedup for prefill and
$\textbf{1.7x}$ speedup for decoding, respectively. Code is available at:
\url{https://github.com/ruikangliu/FlatQuant}.


## Fine-grained Attention I/O Complexity Comprehensive Analysis for Backward Passes

>Authors: Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou

>2024-10-12

> http://arxiv.org/abs/2410.09397v1

Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing long-context information. However, the quadratic complexity of
attention computation with respect to sequence length poses significant
computational challenges, and I/O aware algorithms have been proposed. This
paper presents a comprehensive analysis of the I/O complexity for attention
mechanisms, focusing on backward passes by categorizing into small and large
cache scenarios. Using the red-blue pebble game framework, we establish tight
bounds on I/O complexity across all cache sizes. We confirm that the de facto
standard I/O aware algorithm FlashAttention is optimal for both forward and
backward passes for the large cache size scenario. For small cache sizes, we
provide an algorithm that improves over existing methods and achieves the tight
bounds. Additionally, we extend our analysis to sparse attention, a mainstream
speeding-up approach, deriving fine-grained lower bounds for both forward and
backward passes and both small and large caches. Our findings complete the
theoretical foundation for I/O complexity in attention mechanisms, offering
insights for designing efficient algorithms of LLM training and inference.


## Token Pruning using a Lightweight Background Aware Vision Transformer

>Authors: Sudhakar Sah, Ravish Kumar, Honnesh Rohmetra, Ehsan Saboori

>2024-10-12

> http://arxiv.org/abs/2410.09324v1

High runtime memory and high latency puts significant constraint on Vision
Transformer training and inference, especially on edge devices. Token pruning
reduces the number of input tokens to the ViT based on importance criteria of
each token. We present a Background Aware Vision Transformer (BAViT) model, a
pre-processing block to object detection models like DETR/YOLOS aimed to reduce
runtime memory and increase throughput by using a novel approach to identify
background tokens in the image. The background tokens can be pruned completely
or partially before feeding to a ViT based object detector. We use the semantic
information provided by segmentation map and/or bounding box annotation to
train a few layers of ViT to classify tokens to either foreground or
background. Using 2 layers and 10 layers of BAViT, background and foreground
tokens can be separated with 75% and 88% accuracy on VOC dataset and 71% and
80% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model
as pre-processor to YOLOS can increase the throughput by 30% - 40% with a mAP
drop of 3% without any sparse fine-tuning and 2% with sparse fine-tuning. Our
approach is specifically targeted for Edge AI use cases.


## Diffraction and pseudospectra in non-Hermitian quasiperiodic lattices

>Authors: Ananya Ghatak, Dimitrios H. Kaltsas, Manas Kulkarni, Konstantinos G. Makris

>2024-10-11

> http://arxiv.org/abs/2410.09185v1

Wave dynamics in disordered open media is an intriguing topic, and has lately
attracted a lot of attention in non-Hermitian physics, especially in photonics.
In fact, spatial distributions of gain and loss elements are physically
possible in the context of integrated photonic waveguide arrays. In particular,
in these type of lattices, counter-intuitive quantized jumps along the
propagation direction appear in the strong disorder limit (where all
eigenstates are localized) and they have also been recently experimentally
observed. We systematically study the non-Hermitian quasiperiodic
Aubry-Andr\'e-Harper model with on-site gain and loss distribution (NHAAH),
with an emphasis on the spectral sensitivity based on pseudospectra analysis.
Moreover, diffraction dynamics and the quantized jumps, as well as, the effect
of saturable nonlinearity, are investigated in detail. Our study reveals the
intricate relation between the nonlinearity and non-Hermiticity.


## Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures

>Authors: Evan Lucas, Dylan Kangas, Timothy C Havens

>2024-10-11

> http://arxiv.org/abs/2410.08971v1

In this paper, we propose an extension to Longformer Encoder-Decoder, a
popular sparse transformer architecture. One common challenge with sparse
transformers is that they can struggle with encoding of long range context,
such as connections between topics discussed at a beginning and end of a
document. A method to selectively increase global attention is proposed and
demonstrated for abstractive summarization tasks on several benchmark data
sets. By prefixing the transcript with additional keywords and encoding global
attention on these keywords, improvement in zero-shot, few-shot, and fine-tuned
cases is demonstrated for some benchmark data sets.


## Maximizing the Potential of Synthetic Data Insights from Random Matrix Theory

>Authors: Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid

>2024-10-11

> http://arxiv.org/abs/2410.08942v1

Synthetic data has gained attention for training large language models, but
poor-quality data can harm performance (see, e.g., Shumailov et al. (2023);
Seddik et al. (2024)). A potential solution is data pruning, which retains only
high-quality data based on a score function (human or machine feedback).
Previous work Feng et al. (2024) analyzed models trained on synthetic data as
sample size increases. We extend this by using random matrix theory to derive
the performance of a binary classifier trained on a mix of real and pruned
synthetic data in a high dimensional setting. Our findings identify conditions
where synthetic data could improve performance, focusing on the quality of the
generative model and verification strategy. We also show a smooth phase
transition in synthetic label noise, contrasting with prior sharp behavior in
infinite sample limits. Experiments with toy models and large language models
validate our theoretical results.


## Evolution of SAE Features Across Layers in LLMs

>Authors: Daniel Balcells, Benjamin Lerner, Michael Oesterle, Ediz Ucar, Stefan Heimersheim

>2024-10-11

> http://arxiv.org/abs/2410.08869v1

Sparse Autoencoders for transformer-based language models are typically
defined independently per layer. In this work we analyze statistical
relationships between features in adjacent layers to understand how features
evolve through a forward pass. We provide a graph visualization interface for
features and their most similar next-layer neighbors, and build communities of
related features across layers. We find that a considerable amount of features
are passed through from a previous layer, some features can be expressed as
quasi-boolean combinations of previous features, and some features become more
specialized in later layers.


## Unveiling Molecular Secrets An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction

>Authors: Zhuoran Li, Xu Sun, Wanyu Lin, Jiannong Cao

>2024-10-11

> http://arxiv.org/abs/2410.08829v1

Explainable molecular property prediction is essential for various scientific
fields, such as drug discovery and material science. Despite delivering
intrinsic explainability, linear models struggle with capturing complex,
non-linear patterns. Large language models (LLMs), on the other hand, yield
accurate predictions through powerful inference capabilities yet fail to
provide chemically meaningful explanations for their predictions. This work
proposes a novel framework, called MoleX, which leverages LLM knowledge to
build a simple yet powerful linear model for accurate molecular property
prediction with faithful explanations. The core of MoleX is to model
complicated molecular structure-property relationships using a simple linear
model, augmented by LLM knowledge and a crafted calibration strategy.
Specifically, to extract the maximum amount of task-relevant knowledge from LLM
embeddings, we employ information bottleneck-inspired fine-tuning and
sparsity-inducing dimensionality reduction. These informative embeddings are
then used to fit a linear model for explainable inference. Moreover, we
introduce residual calibration to address prediction errors stemming from
linear models' insufficient expressiveness of complex LLM embeddings, thus
recovering the LLM's predictive power and boosting overall accuracy.
Theoretically, we provide a mathematical foundation to justify MoleX's
explainability. Extensive experiments demonstrate that MoleX outperforms
existing methods in molecular property prediction, establishing a new milestone
in predictive performance, explainability, and efficiency. In particular, MoleX
enables CPU inference and accelerates large-scale dataset processing, achieving
comparable performance 300x faster with 100,000 fewer parameters than LLMs.
Additionally, the calibration improves model performance by up to 12.7% without
compromising explainability.


## AMPO Automatic Multi-Branched Prompt Optimization

>Authors: Sheng Yang, Yurong Wu, Yan Gao, Zineng Zhou, Bin Benjamin Zhu, Xiaodi Sun, Jian-Guang Lou, Zhiming Ding, Anbang Hu, Yuan Fang, Yunsong Li, Junyan Chen, Linjun Yang

>2024-10-11

> http://arxiv.org/abs/2410.08696v1

Prompt engineering is very important to enhance the performance of large
language models (LLMs). When dealing with complex issues, prompt engineers tend
to distill multiple patterns from examples and inject relevant solutions to
optimize the prompts, achieving satisfying results. However, existing automatic
prompt optimization techniques are only limited to producing single flow
instructions, struggling with handling diverse patterns. In this paper, we
present AMPO, an automatic prompt optimization method that can iteratively
develop a multi-branched prompt using failure cases as feedback. Our goal is to
explore a novel way of structuring prompts with multi-branches to better handle
multiple patterns in complex tasks, for which we introduce three modules:
Pattern Recognition, Branch Adjustment, and Branch Pruning. In experiments
across five tasks, AMPO consistently achieves the best results. Additionally,
our approach demonstrates significant optimization efficiency due to our
adoption of a minimal search strategy.


## QEFT Quantization for Efficient Fine-Tuning of LLMs

>Authors: Changhun Lee, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park

>2024-10-11

> http://arxiv.org/abs/2410.08661v1

With the rapid growth in the use of fine-tuning for large language models
(LLMs), optimizing fine-tuning while keeping inference efficient has become
highly important. However, this is a challenging task as it requires
improvements in all aspects, including inference speed, fine-tuning speed,
memory consumption, and, most importantly, model quality. Previous studies have
attempted to achieve this by combining quantization with fine-tuning, but they
have failed to enhance all four aspects simultaneously. In this study, we
propose a new lightweight technique called Quantization for Efficient
Fine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is
supported by robust theoretical foundations, offers high flexibility, and
maintains good hardware compatibility. Our extensive experiments demonstrate
that QEFT matches the quality and versatility of full-precision
parameter-efficient fine-tuning, while using fewer resources. Our code is
available at https://github.com/xvyaward/qeft.


## Canonical Ramsey numbers of sparse graphs

>Authors: Lior Gishboliner, Aleksa Milojevi, Benny Sudakov, Yuval Wigderson

>2024-10-11

> http://arxiv.org/abs/2410.08644v1

The canonical Ramsey theorem of Erd\H{o}s and Rado implies that for any graph
$H$, any edge-coloring (with an arbitrary number of colors) of a sufficiently
large complete graph $K_N$ contains a monochromatic, lexicographic, or rainbow
copy of $H$. The least such $N$ is called the Erd\H{o}s-Rado number of $H$,
denoted by $ER(H)$. Erd\H{o}s-Rado numbers of cliques have received
considerable attention, and in this paper we extend this line of research by
studying Erd\H{o}s-Rado numbers of sparse graphs. For example, we prove that if
$H$ has bounded degree, then $ER(H)$ is polynomial in $|V(H)|$ if $H$ is
bipartite, but exponential in general.
  We also study the closely-related problem of constrained Ramsey numbers. For
a given tree $S$ and given path $P_t$, we study the minimum $N$ such that every
edge-coloring of $K_N$ contains a monochromatic copy of $S$ or a rainbow copy
of $P_t$. We prove a nearly optimal upper bound for this problem, which differs
from the best known lower bound by a function of inverse-Ackermann type.


## Words as Beacons Guiding RL Agents with High-Level Language Prompts

>Authors: Unai Ruiz-Gonzalez, Alain Andres, Pedro G. Bascoy, Javier Del Ser

>2024-10-11

> http://arxiv.org/abs/2410.08632v1

Sparse reward environments in reinforcement learning (RL) pose significant
challenges for exploration, often leading to inefficient or incomplete learning
processes. To tackle this issue, this work proposes a teacher-student RL
framework that leverages Large Language Models (LLMs) as "teachers" to guide
the agent's learning process by decomposing complex tasks into subgoals. Due to
their inherent capability to understand RL environments based on a textual
description of structure and purpose, LLMs can provide subgoals to accomplish
the task defined for the environment in a similar fashion to how a human would
do. In doing so, three types of subgoals are proposed: positional targets
relative to the agent, object representations, and language-based instructions
generated directly by the LLM. More importantly, we show that it is possible to
query the LLM only during the training phase, enabling agents to operate within
the environment without any LLM intervention. We assess the performance of this
proposed framework by evaluating three state-of-the-art open-source LLMs
(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally
generated environment of the MiniGrid benchmark. Experimental results
demonstrate that this curriculum-based approach accelerates learning and
enhances exploration in complex tasks, achieving up to 30 to 200 times faster
convergence in training steps compared to recent baselines designed for sparse
reward environments.


## Towards Cross-domain Few-shot Graph Anomaly Detection

>Authors: Jiazhen Chen, Sichao Fu, Zhibin Zhang, Zheng Ma, Mingbin Feng, Tony S. Wirjanto, Qinmu Peng

>2024-10-11

> http://arxiv.org/abs/2410.08629v1

Few-shot graph anomaly detection (GAD) has recently garnered increasing
attention, which aims to discern anomalous patterns among abundant unlabeled
test nodes under the guidance of a limited number of labeled training nodes.
Existing few-shot GAD approaches typically adopt meta-training methods trained
on richly labeled auxiliary networks to facilitate rapid adaptation to target
networks that possess sparse labels. However, these proposed methods often
assume that the auxiliary and target networks exist in the same data
distributions-an assumption rarely holds in practical settings. This paper
explores a more prevalent and complex scenario of cross-domain few-shot GAD,
where the goal is to identify anomalies within sparsely labeled target graphs
using auxiliary graphs from a related, yet distinct domain. The challenge here
is nontrivial owing to inherent data distribution discrepancies between the
source and target domains, compounded by the uncertainties of sparse labeling
in the target domain. In this paper, we propose a simple and effective
framework, termed CDFS-GAD, specifically designed to tackle the aforementioned
challenges. CDFS-GAD first introduces a domain-adaptive graph contrastive
learning module, which is aimed at enhancing cross-domain feature alignment.
Then, a prompt tuning module is further designed to extract domain-specific
features tailored to each domain. Moreover, a domain-adaptive hypersphere
classification loss is proposed to enhance the discrimination between normal
and anomalous instances under minimal supervision, utilizing domain-sensitive
norms. Lastly, a self-training strategy is introduced to further refine the
predicted scores, enhancing its reliability in few-shot settings. Extensive
experiments on twelve real-world cross-domain data pairs demonstrate the
effectiveness of the proposed CDFS-GAD framework in comparison to various
existing GAD methods.


## ZipVL Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression

>Authors: Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang

>2024-10-11

> http://arxiv.org/abs/2410.08584v1

The efficiency of large vision-language models (LVLMs) is constrained by the
computational bottleneck of the attention mechanism during the prefill phase
and the memory bottleneck of fetching the key-value (KV) cache in the decoding
phase, particularly in scenarios involving high-resolution images or videos.
Visual content often exhibits substantial redundancy, resulting in highly
sparse attention maps within LVLMs. This sparsity can be leveraged to
accelerate attention computation or compress the KV cache through various
approaches. However, most studies focus on addressing only one of these
bottlenecks and do not adequately support dynamic adjustment of sparsity
concerning distinct layers or tasks. In this paper, we present ZipVL, an
efficient inference framework designed for LVLMs that resolves both computation
and memory bottlenecks through a dynamic ratio allocation strategy of important
tokens. This ratio is adaptively determined based on the layer-specific
distribution of attention scores, rather than fixed hyper-parameters, thereby
improving efficiency for less complex tasks while maintaining high performance
for more challenging ones. Then we select important tokens based on their
normalized attention scores and perform attention mechanism solely on those
important tokens to accelerate the prefill phase. To mitigate the memory
bottleneck in the decoding phase, we employ mixed-precision quantization to the
KV cache, where high-bit quantization is used for caches of important tokens,
while low-bit quantization is applied to those of less importance. Our
experiments demonstrate that ZipVL can accelerate the prefill phase by
2.6$\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy
reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively
enhancing the generation efficiency of LVLMs.


## DeBiFormer Vision Transformer with Deformable Agent Bi-level Routing Attention

>Authors: Nguyen Huu Bao Long, Chenyu Zhang, Yuzhi Shi, Tsubasa Hirakawa, Takayoshi Yamashita, Tohgoroh Matsui, Hironobu Fujiyoshi

>2024-10-11

> http://arxiv.org/abs/2410.08582v1

Vision Transformers with various attention modules have demonstrated superior
performance on vision tasks. While using sparsity-adaptive attention, such as
in DAT, has yielded strong results in image classification, the key-value pairs
selected by deformable points lack semantic relevance when fine-tuning for
semantic segmentation tasks. The query-aware sparsity attention in BiFormer
seeks to focus each query on top-k routed regions. However, during attention
calculation, the selected key-value pairs are influenced by too many irrelevant
queries, reducing attention on the more important ones. To address these
issues, we propose the Deformable Bi-level Routing Attention (DBRA) module,
which optimizes the selection of key-value pairs using agent queries and
enhances the interpretability of queries in attention maps. Based on this, we
introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a
novel general-purpose vision transformer built with the DBRA module. DeBiFormer
has been validated on various computer vision tasks, including image
classification, object detection, and semantic segmentation, providing strong
evidence of its effectiveness.Code is available at
{https://github.com/maclong01/DeBiFormer}


## A transformational approach to collective behavior

>Authors: Michael E. Glinsky

>2024-10-11

> http://arxiv.org/abs/2410.08558v1

This paper presents a revolutionary approach to the characterization,
forecast, and control of collective systems. Collective systems are an ensemble
of conservatively interacting entities. The evolution of the entities are
determined by symmetries of the interaction. Collective systems take many
different forms. A plasma is a collective of charged particles, a fluid is a
collective of molecules, a elementary field is a collective of elementary
particles, and a cosmos is a collective of celestial bodies. Our new theory
builds on the canonical transformation approach to dynamics. This approach
recognizes that the symmetry leads to the conservation of a real function, that
is the infinitesimal generator of a Lie group. The finite generator of the
canonical transformation is derived from the infinitesimal generator by the
solution of the Hamilton-Jacobi equation. This generating function is also
known as the action, the entropy, and the logarithmic likelihood. The new
theory generalizes this generating function to the generating functional of the
collective field. Finally, this paper derives the formula for the Mayer Cluster
Expansion, or the S-matrix expansion of the generating functional. We call it
the Heisenberg Scattering Transformation (HST). Practically, this is a
localized Fourier Transformation, whose principal components give the
singularity spectrums, that is the solution to the Renormalization Group
Equations. Limitations on the measurement of the system (that is the Born Rule
and the Heisenberg Uncertainty Principle) lead to quantization of the
stochastic probabilities of the collective field. How different collective
systems couple together to form systems-of-systems is formalized. The details
of a practical implementation of the HST will be presented.


## GIVE Structured Reasoning with Knowledge Graph Inspired Veracity Extrapolation

>Authors: Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro

>2024-10-11

> http://arxiv.org/abs/2410.08475v1

Existing retrieval-based reasoning approaches for large language models
(LLMs) heavily rely on the density and quality of the non-parametric knowledge
source to provide domain knowledge and explicit reasoning chain. However,
inclusive knowledge sources are expensive and sometimes infeasible to build for
scientific or corner domains. To tackle the challenges, we introduce Graph
Inspired Veracity Extrapolation (GIVE), a novel reasoning framework that
integrates the parametric and non-parametric memories to enhance both knowledge
retrieval and faithful reasoning processes on very sparse knowledge graphs. By
leveraging the external structured knowledge to inspire LLM to model the
interconnections among relevant concepts, our method facilitates a more logical
and step-wise reasoning approach akin to experts' problem-solving, rather than
gold answer retrieval. Specifically, the framework prompts LLMs to decompose
the query into crucial concepts and attributes, construct entity groups with
relevant entities, and build an augmented reasoning chain by probing potential
relationships among node pairs across these entity groups. Our method
incorporates both factual and extrapolated linkages to enable comprehensive
understanding and response generation. Extensive experiments on
reasoning-intense benchmarks on biomedical and commonsense QA demonstrate the
effectiveness of our proposed method. Specifically, GIVE enables GPT3.5-turbo
to outperform advanced models like GPT4 without any additional training cost,
thereby underscoring the efficacy of integrating structured information and
internal reasoning ability of LLMs for tackling specialized tasks with limited
external resources.


## Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models

>Authors: Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu

>2024-10-11

> http://arxiv.org/abs/2410.08436v1

When performing complex multi-step reasoning tasks, the ability of Large
Language Models (LLMs) to derive structured intermediate proof steps is
important for ensuring that the models truly perform the desired reasoning and
for improving models' explainability. This paper is centred around a focused
study: whether the current state-of-the-art generalist LLMs can leverage the
structures in a few examples to better construct the proof structures with
\textit{in-context learning}. Our study specifically focuses on structure-aware
demonstration and structure-aware pruning. We demonstrate that they both help
improve performance. A detailed analysis is provided to help understand the
results.


## Exact solution of the master equation for interacting quantized fields at finite temperature decay

>Authors: L. Hernndez-Snchez, I. A. Bocanegra-Garay, I. Ramos-Prieto, F. Soto-Eguibar, H. M. Moya-Cessa

>2024-10-11

> http://arxiv.org/abs/2410.08428v1

We analyze the Markovian dynamics of a quantum system involving the
interaction of two quantized fields at finite temperature decay. Utilizing
superoperator techniques and applying two non-unitary transformations, we
reformulate the Lindblad master equation into a von Neumann-like equation with
an effective non-Hermitian Hamiltonian. Furthermore, an additional non-unitary
transformation is employed to diagonalize this Hamiltonian, enabling us to
derive an exact solution to the Lindblad master equation. This method provides
a framework to calculate the evolution of any initial state in a fully quantum
regime. As a specific example, we present the photon coincidence rates for two
indistinguishable photons initially interacting within a cavity.

