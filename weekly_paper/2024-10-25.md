# 2024-10-25

# Table of Contents
* [Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration](#Leveraging-Skills-from-Unlabeled-Prior-Data-for-Efficient-Online-Exploration)
* [ExpertFlow Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference](#ExpertFlow-Optimized-Expert-Activation-and-Token-Allocation-for-Efficient-Mixture-of-Experts-Inference)
* [Quantizable Ghost-ridden theories using Kinetic Positivity Constraints](#Quantizable-Ghost-ridden-theories-using-Kinetic-Positivity-Constraints)
* [Identifiable Representation and Model Learning for Latent Dynamic Systems](#Identifiable-Representation-and-Model-Learning-for-Latent-Dynamic-Systems)
* [Att2CPC Attention-Guided Lossy Attribute Compression of Point Clouds](#Att2CPC-Attention-Guided-Lossy-Attribute-Compression-of-Point-Clouds)
* [Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network](#Anomaly-Resilient-Temporal-QoS-Prediction-using-Hypergraph-Convoluted-Transformer-Network)
* [Escaping the Forest Sparse Interpretable Neural Networks for Tabular Data](#Escaping-the-Forest-Sparse-Interpretable-Neural-Networks-for-Tabular-Data)
* [Beware of Calibration Data for Pruning Large Language Models](#Beware-of-Calibration-Data-for-Pruning-Large-Language-Models)
* [PETAH Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context](#PETAH-Parameter-Efficient-Task-Adaptation-for-Hybrid-Transformers-in-a-resource-limited-Context)
* [AutoRNet Automatically Optimizing Heuristics for Robust Network Design via Large Language Models](#AutoRNet-Automatically-Optimizing-Heuristics-for-Robust-Network-Design-via-Large-Language-Models)
* [Process Supervision-Guided Policy Optimization for Code Generation](#Process-Supervision-Guided-Policy-Optimization-for-Code-Generation)
* [Adaptive Wireless Image Semantic Transmission Design, Simulation, and Prototype Validation](#Adaptive-Wireless-Image-Semantic-Transmission-Design,-Simulation,-and-Prototype-Validation)
* [Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised Learning using Sparse Passenger Reports](#Congestion-Forecast-for-Trains-with-Railroad-Graph-based-Semi-Supervised-Learning-using-Sparse-Passenger-Reports)
* [PtychoFormer A Transformer-based Model for Ptychographic Phase Retrieval](#PtychoFormer-A-Transformer-based-Model-for-Ptychographic-Phase-Retrieval)
* [LVSM A Large View Synthesis Model with Minimal 3D Inductive Bias](#LVSM-A-Large-View-Synthesis-Model-with-Minimal-3D-Inductive-Bias)
* [Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?](#Can-General-Purpose-Large-Language-Models-Generalize-to-English-Thai-Machine-Translation-?)
* [Math Neurosurgery Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](#Math-Neurosurgery-Isolating-Language-Models'-Math-Reasoning-Abilities-Using-Only-Forward-Passes)
* [Pyramid Vector Quantization for LLMs](#Pyramid-Vector-Quantization-for-LLMs)
* [Semantic-guided Search for Efficient Program Repair with Large Language Models](#Semantic-guided-Search-for-Efficient-Program-Repair-with-Large-Language-Models)
* [Bayesian High-dimensional Linear Regression with Sparse Projection-posterior](#Bayesian-High-dimensional-Linear-Regression-with-Sparse-Projection-posterior)
* [Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge](#Does-your-LLM-truly-unlearn?-An-embarrassingly-simple-approach-to-recover-unlearned-knowledge)
* [Improving Neuron-level Interpretability with White-box Language Models](#Improving-Neuron-level-Interpretability-with-White-box-Language-Models)
* [MagicPIG LSH Sampling for Efficient LLM Generation](#MagicPIG-LSH-Sampling-for-Efficient-LLM-Generation)
* [PODTILE Facilitating Podcast Episode Browsing with Auto-generated Chapters](#PODTILE-Facilitating-Podcast-Episode-Browsing-with-Auto-generated-Chapters)
* [Beyond 24 exploring VNM sparsity for efficient transformer inference on GPUs](#Beyond-24-exploring-VNM-sparsity-for-efficient-transformer-inference-on-GPUs)
* [Continuous Speech Synthesis using per-token Latent Diffusion](#Continuous-Speech-Synthesis-using-per-token-Latent-Diffusion)
* [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](#Steering-Knowledge-Selection-Behaviours-in-LLMs-via-SAE-Based-Representation-Engineering)
* [Residual vector quantization for KV cache compression in large language model](#Residual-vector-quantization-for-KV-cache-compression-in-large-language-model)
* [A quantum anchor for higher Koszul brackets](#A-quantum-anchor-for-higher-Koszul-brackets)
* [Pruning Foundation Models for High Accuracy without Retraining](#Pruning-Foundation-Models-for-High-Accuracy-without-Retraining)
* [Bayesian Concept Bottleneck Models with LLM Priors](#Bayesian-Concept-Bottleneck-Models-with-LLM-Priors)


## Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration

>Authors: Max Wilcoxson, Qiyang Li, Kevin Frans, Sergey Levine

>2024-10-23

> http://arxiv.org/abs/2410.18076v1

Unsupervised pretraining has been transformative in many supervised domains.
However, applying such ideas to reinforcement learning (RL) presents a unique
challenge in that fine-tuning does not involve mimicking task-specific data,
but rather exploring and locating the solution through iterative
self-improvement. In this work, we study how unlabeled prior trajectory data
can be leveraged to learn efficient exploration strategies. While prior data
can be used to pretrain a set of low-level skills, or as additional off-policy
data for online RL, it has been unclear how to combine these ideas effectively
for online exploration. Our method SUPE (Skills from Unlabeled Prior data for
Exploration) demonstrates that a careful combination of these ideas compounds
their benefits. Our method first extracts low-level skills using a variational
autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an
optimistic reward model, transforming prior data into high-level, task-relevant
examples. Finally, SUPE uses these transformed examples as additional
off-policy data for online RL to learn a high-level policy that composes
pretrained low-level skills to explore efficiently. We empirically show that
SUPE reliably outperforms prior strategies, successfully solving a suite of
long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.


## ExpertFlow Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference

>Authors: Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon

>2024-10-23

> http://arxiv.org/abs/2410.17954v1

Sparse Mixture of Experts (MoE) models, while outperforming dense Large
Language Models (LLMs) in terms of performance, face significant deployment
challenges during inference due to their high memory demands. Existing
offloading techniques, which involve swapping activated and idle experts
between the GPU and CPU, often suffer from rigid expert caching mechanisms.
These mechanisms fail to adapt to dynamic routing, leading to inefficient cache
utilization, or incur prohibitive costs for prediction training. To tackle
these inference-specific challenges, we introduce ExpertFlow, a comprehensive
system specifically designed to enhance inference efficiency by accommodating
flexible routing and enabling efficient expert scheduling between CPU and GPU.
This reduces overhead and boosts system performance. Central to our approach is
a predictive routing path-based offloading mechanism that utilizes a
lightweight predictor to accurately forecast routing paths before computation
begins. This proactive strategy allows for real-time error correction in expert
caching, significantly increasing cache hit ratios and reducing the frequency
of expert transfers, thereby minimizing I/O overhead. Additionally, we
implement a dynamic token scheduling strategy that optimizes MoE inference by
rearranging input tokens across different batches. This method not only reduces
the number of activated experts per batch but also improves computational
efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up
to 93.72\% GPU memory savings and enhances inference speed by 2 to 10 times
compared to baseline methods, highlighting its effectiveness and utility as a
robust solution for resource-constrained inference scenarios.


## Quantizable Ghost-ridden theories using Kinetic Positivity Constraints

>Authors: Sukanta Panda, Archit Vidyarthi

>2024-10-23

> http://arxiv.org/abs/2410.17924v1

We present a novel way to constrain the ghost field with respect to other
physical fields present in a given theory such that the theory becomes
quantizable. This is achieved by imposing positivity of the total kinetic
energy of the system and performing Lorentz transformations in the field space
manifold to arrive at an effective Lagrangian containing only physical degrees
of freedom. Since models containing ghost fields such as quintom models are
relevant in the cosmological context, this method can help ensure that such
theories don't violate unitarity and can be treated as realistic candidates
without the need to completely eliminate ghost(s).


## Identifiable Representation and Model Learning for Latent Dynamic Systems

>Authors: Congxi Zhang, Yongchun Xie

>2024-10-23

> http://arxiv.org/abs/2410.17882v1

Learning identifiable representations and models from low-level observations
is useful for an intelligent spacecraft to reliability finish downstream tasks.
For temporal observations, to ensure that the data generating process is
provably inverted, most existing works either assume the noise variables in the
dynamic mechanisms are (conditionally) independent, or require interventions
which can directly affect each latent variable. However, in practice, the
relationship between the exogenous inputs/interventions and the latent
variables may follow some complex deterministic mechanisms. In this work, we
study the problem of identifiable representation and model learning for latent
dynamic systems. The key idea is that we use an inductive bias inspired by
controllable canonical forms, which is invariant, sparse, and input dependent
by definition. We prove that, for linear or affine nonlinear latent dynamic
systems, it is possible to identify the representations up to scaling and
determine the models up to some simple transformations. The results have
potential to provide some theoretical guarantees for developing more
trustworthy decision-making and control methods for intelligent spacecrafts.


## Att2CPC Attention-Guided Lossy Attribute Compression of Point Clouds

>Authors: Kai Liu, Kang You, Pan Gao, Manoranjan Paul

>2024-10-23

> http://arxiv.org/abs/2410.17823v1

With the great progress of 3D sensing and acquisition technology, the volume
of point cloud data has grown dramatically, which urges the development of
efficient point cloud compression methods. In this paper, we focus on the task
of learned lossy point cloud attribute compression (PCAC). We propose an
efficient attention-based method for lossy compression of point cloud
attributes leveraging on an autoencoder architecture. Specifically, at the
encoding side, we conduct multiple downsampling to best exploit the local
attribute patterns, in which effective External Cross Attention (ECA) is
devised to hierarchically aggregate features by intergrating attributes and
geometry contexts. At the decoding side, the attributes of the point cloud are
progressively reconstructed based on the multi-scale representation and the
zero-padding upsampling tactic. To the best of our knowledge, this is the first
approach to introduce attention mechanism to point-based lossy PCAC task. We
verify the compression efficiency of our model on various sequences, including
human body frames, sparse objects, and large-scale point cloud scenes.
Experiments show that our method achieves an average improvement of 1.15 dB and
2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing
with the state-of-the-art point-based method Deep-PCAC. Codes of this paper are
available at https://github.com/I2-Multimedia-Lab/Att2CPC.


## Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network

>Authors: Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak

>2024-10-23

> http://arxiv.org/abs/2410.17762v1

Quality-of-Service (QoS) prediction is a critical task in the service
lifecycle, enabling precise and adaptive service recommendations by
anticipating performance variations over time in response to evolving network
uncertainties and user preferences. However, contemporary QoS prediction
methods frequently encounter data sparsity and cold-start issues, which hinder
accurate QoS predictions and limit the ability to capture diverse user
preferences. Additionally, these methods often assume QoS data reliability,
neglecting potential credibility issues such as outliers and the presence of
greysheep users and services with atypical invocation patterns. Furthermore,
traditional approaches fail to leverage diverse features, including
domain-specific knowledge and complex higher-order patterns, essential for
accurate QoS predictions. In this paper, we introduce a real-time, trust-aware
framework for temporal QoS prediction to address the aforementioned challenges,
featuring an end-to-end deep architecture called the Hypergraph Convoluted
Transformer Network (HCTN). HCTN combines a hypergraph structure with graph
convolution over hyper-edges to effectively address high-sparsity issues by
capturing complex, high-order correlations. Complementing this, the transformer
network utilizes multi-head attention along with parallel 1D convolutional
layers and fully connected dense blocks to capture both fine-grained and
coarse-grained dynamic patterns. Additionally, our approach includes a
sparsity-resilient solution for detecting greysheep users and services,
incorporating their unique characteristics to improve prediction accuracy.
Trained with a robust loss function resistant to outliers, HCTN demonstrated
state-of-the-art performance on the large-scale WSDREAM-2 datasets for response
time and throughput.


## Escaping the Forest Sparse Interpretable Neural Networks for Tabular Data

>Authors: Salvatore Raieli, Abdulrahman Altahhan, Nathalie Jeanray, StÃ©phane Gerart, Sebastien Vachenc

>2024-10-23

> http://arxiv.org/abs/2410.17758v1

Tabular datasets are widely used in scientific disciplines such as biology.
While these disciplines have already adopted AI methods to enhance their
findings and analysis, they mainly use tree-based methods due to their
interpretability. At the same time, artificial neural networks have been shown
to offer superior flexibility and depth for rich and complex non-tabular
problems, but they are falling behind tree-based models for tabular data in
terms of performance and interpretability. Although sparsity has been shown to
improve the interpretability and performance of ANN models for complex
non-tabular datasets, enforcing sparsity structurally and formatively for
tabular data before training the model, remains an open question. To address
this question, we establish a method that infuses sparsity in neural networks
by utilising attention mechanisms to capture the features' importance in
tabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with
attention mechanisms, are more effective than tree-based models, reaching the
state-of-the-art on biological datasets. They further permit the extraction of
insights from these datasets and achieve better performance than post-hoc
methods like SHAP.


## Beware of Calibration Data for Pruning Large Language Models

>Authors: Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang

>2024-10-23

> http://arxiv.org/abs/2410.17711v1

As large language models (LLMs) are widely applied across various fields,
model compression has become increasingly crucial for reducing costs and
improving inference efficiency. Post-training pruning is a promising method
that does not require resource-intensive iterative training and only needs a
small amount of calibration data to assess the importance of parameters.
Previous research has primarily focused on designing advanced pruning methods,
while different calibration data's impact on pruning performance still lacks
systematical exploration. We fill this blank and surprisingly observe that the
effects of calibration data even value more than designing advanced pruning
strategies, especially for high sparsity. Our preliminary exploration also
discloses that using calibration data similar to the training data can yield
better performance. As pre-training data is usually inaccessible for advanced
LLMs, we further provide a self-generating calibration data synthesis strategy
to construct feasible calibration data. We conduct experiments on the recent
strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that
the proposed method outperforms commonly used calibration data and can
effectively enhance strong pruning methods (e.g., Wanda, OWL).


## PETAH Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context

>Authors: Maximilian Augustin, Syed Shakib Sarwar, Mostafa Elhoushi, Sai Qian Zhang, Yuecheng Li, Barbara De Salvo

>2024-10-23

> http://arxiv.org/abs/2410.17661v1

Following their success in natural language processing (NLP), there has been
a shift towards transformer models in computer vision. While transformers
perform well and offer promising multi-tasking performance, due to their high
compute requirements, many resource-constrained applications still rely on
convolutional or hybrid models that combine the benefits of convolution and
attention layers and achieve the best results in the sub 100M parameter range.
Simultaneously, task adaptation techniques that allow for the use of one shared
transformer backbone for multiple downstream tasks, resulting in great storage
savings at negligible cost in performance, have not yet been adopted for hybrid
transformers. In this work, we investigate how to achieve the best
task-adaptation performance and introduce PETAH: Parameter Efficient Task
Adaptation for Hybrid Transformers. We further combine PETAH adaptation with
pruning to achieve highly performant and storage friendly models for
multi-tasking. In our extensive evaluation on classification and other vision
tasks, we demonstrate that our PETAH-adapted hybrid models outperform
established task-adaptation techniques for ViTs while requiring fewer
parameters and being more efficient on mobile hardware.


## AutoRNet Automatically Optimizing Heuristics for Robust Network Design via Large Language Models

>Authors: He Yu, Jing Liu

>2024-10-23

> http://arxiv.org/abs/2410.17656v1

Achieving robust networks is a challenging problem due to its NP-hard nature
and complex solution space. Current methods, from handcrafted feature
extraction to deep learning, have made progress but remain rigid, requiring
manual design and large labeled datasets. To address these issues, we propose
AutoRNet, a framework that integrates large language models (LLMs) with
evolutionary algorithms to generate heuristics for robust network design. We
design network optimization strategies to provide domain-specific prompts for
LLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,
we introduce an adaptive fitness function to balance convergence and diversity
while maintaining degree distributions. AutoRNet is evaluated on sparse and
dense scale-free networks, outperforming current methods by reducing the need
for manual design and large datasets.


## Process Supervision-Guided Policy Optimization for Code Generation

>Authors: Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, Lin Yan

>2024-10-23

> http://arxiv.org/abs/2410.17621v1

Reinforcement Learning (RL) with unit test feedback has enhanced large
language models (LLMs) code generation, but relies on sparse rewards provided
only after complete code evaluation, limiting learning efficiency and
incremental improvements. When generated code fails all unit tests, no learning
signal is received, hindering progress on complex tasks. To address this, we
propose a Process Reward Model (PRM) that delivers dense, line-level feedback
on code correctness during generation, mimicking human code refinement and
providing immediate guidance. We explore various strategies for training PRMs
and integrating them into the RL framework, finding that using PRMs both as
dense rewards and for value function initialization significantly boosts
performance. Our approach increases our in-house LLM's pass rate from 28.2% to
29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our
experimental results highlight the effectiveness of PRMs in enhancing RL-driven
code generation, especially for long-horizon scenarios.


## Adaptive Wireless Image Semantic Transmission Design, Simulation, and Prototype Validation

>Authors: Jiarun Ding, Peiwen Jiang, Chao-Kai Wen, Shi Jin

>2024-10-23

> http://arxiv.org/abs/2410.17536v1

The rapid development of artificial intelligence has significantly advanced
semantic communications, particularly in wireless image transmission. However,
most existing approaches struggle to precisely distinguish and prioritize image
content, and they do not sufficiently incorporate semantic priorities into
system design. In this study, we propose an adaptive wireless image semantic
transmission scheme called ASCViT-JSCC, which utilizes vision transformer-based
joint source-channel coding (JSCC). This scheme prioritizes different image
regions based on their importance, identified through object and feature point
detection. Unimportant background sections are masked, enabling them to be
recovered at the receiver, while the freed resources are allocated to enhance
object protection via the JSCC network. We also integrate quantization modules
to enable compatibility with quadrature amplitude modulation, commonly used in
modern wireless communications. To address frequency-selective fading channels,
we introduce CSIPA-Net, which allocates power based on channel information,
further improving performance. Notably, we conduct over-the-air testing on a
prototype platform composed of a software-defined radio and embedded graphics
processing unit systems, validating our methods. Both simulations and
real-world measurements demonstrate that ASCViT-JSCC effectively prioritizes
object protection according to channel conditions, significantly enhancing
image reconstruction quality, especially in challenging channel environments.


## Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised Learning using Sparse Passenger Reports

>Authors: Soto Anno, Kota Tsubouchi, Masamichi Shimosaka

>2024-10-23

> http://arxiv.org/abs/2410.17510v1

Forecasting rail congestion is crucial for efficient mobility in transport
systems. We present rail congestion forecasting using reports from passengers
collected through a transit application. Although reports from passengers have
received attention from researchers, ensuring a sufficient volume of reports is
challenging due to passenger's reluctance. The limited number of reports
results in the sparsity of the congestion label, which can be an issue in
building a stable prediction model. To address this issue, we propose a
semi-supervised method for congestion forecasting for trains, or SURCONFORT.
Our key idea is twofold: firstly, we adopt semi-supervised learning to leverage
sparsely labeled data and many unlabeled data. Secondly, in order to complement
the unlabeled data from nearby stations, we design a railway network-oriented
graph and apply the graph to semi-supervised graph regularization. Empirical
experiments with actual reporting data show that SURCONFORT improved the
forecasting performance by 14.9% over state-of-the-art methods under the label
sparsity.


## PtychoFormer A Transformer-based Model for Ptychographic Phase Retrieval

>Authors: Ryuma Nakahata, Shehtab Zaman, Mingyuan Zhang, Fake Lu, Kenneth Chiu

>2024-10-22

> http://arxiv.org/abs/2410.17377v1

Ptychography is a computational method of microscopy that recovers
high-resolution transmission images of samples from a series of diffraction
patterns. While conventional phase retrieval algorithms can iteratively recover
the images, they require oversampled diffraction patterns, incur significant
computational costs, and struggle to recover the absolute phase of the sample's
transmission function. Deep learning algorithms for ptychography are a
promising approach to resolving the limitations of iterative algorithms. We
present PtychoFormer, a hierarchical transformer-based model for data-driven
single-shot ptychographic phase retrieval. PtychoFormer processes subsets of
diffraction patterns, generating local inferences that are seamlessly stitched
together to produce a high-quality reconstruction. Our model exhibits tolerance
to sparsely scanned diffraction patterns and achieves up to 3600 times faster
imaging speed than the extended ptychographic iterative engine (ePIE). We also
propose the extended-PtychoFormer (ePF), a hybrid approach that combines the
benefits of PtychoFormer with the ePIE. ePF minimizes global phase shifts and
significantly enhances reconstruction quality, achieving state-of-the-art phase
retrieval in ptychography.


## LVSM A Large View Synthesis Model with Minimal 3D Inductive Bias

>Authors: Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu

>2024-10-22

> http://arxiv.org/abs/2410.17242v1

We propose the Large View Synthesis Model (LVSM), a novel transformer-based
approach for scalable and generalizable novel view synthesis from sparse-view
inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which
encodes input image tokens into a fixed number of 1D latent tokens, functioning
as a fully learned scene representation, and decodes novel-view images from
them; and (2) a decoder-only LVSM, which directly maps input images to
novel-view outputs, completely eliminating intermediate scene representations.
Both models bypass the 3D inductive biases used in previous methods -- from 3D
representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar
projections, plane sweeps) -- addressing novel view synthesis with a fully
data-driven approach. While the encoder-decoder model offers faster inference
due to its independent latent representation, the decoder-only LVSM achieves
superior quality, scalability, and zero-shot generalization, outperforming
previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive
evaluations across multiple datasets demonstrate that both LVSM variants
achieve state-of-the-art novel view synthesis quality. Notably, our models
surpass all previous methods even with reduced computational resources (1-2
GPUs). Please see our website for more details:
https://haian-jin.github.io/projects/LVSM/ .


## Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?

>Authors: Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat

>2024-10-22

> http://arxiv.org/abs/2410.17145v1

Large language models (LLMs) perform well on common tasks but struggle with
generalization in low-resource and low-computation settings. We examine this
limitation by testing various LLMs and specialized translation models on
English-Thai machine translation and code-switching datasets. Our findings
reveal that under more strict computational constraints, such as 4-bit
quantization, LLMs fail to translate effectively. In contrast, specialized
models, with comparable or lower computational requirements, consistently
outperform LLMs. This underscores the importance of specialized models for
maintaining performance under resource constraints.


## Math Neurosurgery Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes

>Authors: Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen

>2024-10-22

> http://arxiv.org/abs/2410.16930v1

Math reasoning is a highly active area of Large Language Model (LLM) research
because it is a hallmark of artificial intelligence. However, few works have
explored how math reasoning is encoded within LLM parameters and if it is a
skill that can be isolated within a model. Doing so could allow targeted
intervention to improve math performance without altering non-math behavior and
foster understanding of how models encode math reasoning. We introduce Math
Neurosurgery (MathNeuro), a method for isolating math-specific parameters in
LLMs using only forward passes. MathNeuro builds on existing work by using
weights and activations to calculate parameter importance, but isolates
math-specific parameters by removing those important for general language
tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning
ability without destroying its general language ability. Scaling these
parameters by a small constant improves a pretrained or instruction-tuned LLM's
performance by 4-17% on GSM8K while leaving non-math behavior unaltered.
MathNeuro is also data efficient: most of its effectiveness holds when
identifying math-specific parameters using a single sample. MathNeuro
highlights the potential for future work to intervene on math-specific
parameters.


## Pyramid Vector Quantization for LLMs

>Authors: Tycho F. A. van der Ouderaa, Maximilian L. Croci, Agrin Hilmkil, James Hensman

>2024-10-22

> http://arxiv.org/abs/2410.16926v1

Recent works on compression of large language models (LLM) using quantization
considered reparameterizing the architecture such that weights are distributed
on the sphere. This demonstratively improves the ability to quantize by
increasing the mathematical notion of coherence, resulting in fewer weight
outliers without affecting the network output. In this work, we aim to further
exploit this spherical geometry of the weights when performing quantization by
considering Pyramid Vector Quantization (PVQ) for large language models.
Arranging points evenly on the sphere is notoriously difficult, especially in
high dimensions, and in case approximate solutions exists, representing points
explicitly in a codebook is typically not feasible due to its additional memory
cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting
points onto the 1-sphere, which allows for efficient encoding and decoding
without requiring an explicit codebook in memory. To obtain a practical
algorithm, we propose to combine PVQ with scale quantization for which we
derive theoretically optimal quantizations, under empirically verified
assumptions. Further, we extend pyramid vector quantization to use Hessian
information to minimize quantization error under expected feature activations,
instead of only relying on weight magnitudes. Experimentally, we achieves
state-of-the-art quantization performance with pareto-optimal trade-off between
performance and bits per weight and bits per activation, compared to compared
methods. On weight-only, we find that we can quantize a Llama-3 70B model to
3.25 bits per weight and retain 98\% accuracy on downstream tasks.


## Semantic-guided Search for Efficient Program Repair with Large Language Models

>Authors: Thanh Le-Cong, Bach Le, Toby Murray

>2024-10-22

> http://arxiv.org/abs/2410.16655v1

In this paper, we first show that increases in beam size of even just
small-sized LLM (1B-7B parameters) require an extensive GPU resource
consumption, leading to up to 80% of recurring crashes due to memory overloads
in LLM-based APR. Seemingly simple solutions to reduce memory consumption are
(1) to quantize LLM models, i.e., converting the weights of a LLM from
high-precision values to lower-precision ones. and (2) to make beam search
sequential, i.e., forwarding each beam through the model sequentially and then
concatenate them back into a single model output. However, we show that these
approaches still do not work via both theoretical analysis and experiments. To
address this, we introduce FLAMES, a novel LLM-based APR technique that employs
semantic-guided patch generation to enhance repair effectiveness and memory
efficiency. Unlike conventional methods that rely on beam search, FLAMES
utilizes greedy decoding to enhance memory efficiency while steering the search
to more potentially good repair candidates via a semantic-guided best-first
search algorithm. At each decoding step, FLAMES uses semantic feedback from
test validation such as the number of passing and failing test cases to select
the most promising token to explore further. Our empirical evaluation on the
Defects4J and HumanEval-Java datasets shows that FLAMES not only substantially
reduces memory consumption by up to 83% compared to conventional LLM-based APR,
but also accelerates the repair process. Remarkably, FLAMES successfully
generated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and
HumanEval-Java datasets, respectively. This suggests that FLAMES is not only
more efficient but also outperforms state-of-the-art techniques, fixing at
least 10 and 11 more bugs than SOTA baselines in the Defects4J and
HumanEval-Java datasets, respectively.


## Bayesian High-dimensional Linear Regression with Sparse Projection-posterior

>Authors: Samhita Pal, Subhashis Ghoshal

>2024-10-21

> http://arxiv.org/abs/2410.16577v2

We consider a novel Bayesian approach to estimation, uncertainty
quantification, and variable selection for a high-dimensional linear regression
model under sparsity. The number of predictors can be nearly exponentially
large relative to the sample size. We put a conjugate normal prior initially
disregarding sparsity, but for making an inference, instead of the original
multivariate normal posterior, we use the posterior distribution induced by a
map transforming the vector of regression coefficients to a sparse vector
obtained by minimizing the sum of squares of deviations plus a suitably scaled
$\ell_1$-penalty on the vector. We show that the resulting sparse
projection-posterior distribution contracts around the true value of the
parameter at the optimal rate adapted to the sparsity of the vector. We show
that the true sparsity structure gets a large sparse projection-posterior
probability. We further show that an appropriately recentred credible ball has
the correct asymptotic frequentist coverage. Finally, we describe how the
computational burden can be distributed to many machines, each dealing with
only a small fraction of the whole dataset. We conduct a comprehensive
simulation study under a variety of settings and found that the proposed method
performs well for finite sample sizes. We also apply the method to several real
datasets, including the ADNI data, and compare its performance with the
state-of-the-art methods. We implemented the method in the \texttt{R} package
called \texttt{sparseProj}, and all computations have been carried out using
this package.


## Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge

>Authors: Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang

>2024-10-21

> http://arxiv.org/abs/2410.16454v1

Large language models (LLMs) have shown remarkable proficiency in generating
text, benefiting from extensive training on vast textual corpora. However, LLMs
may also acquire unwanted behaviors from the diverse and sensitive nature of
their training data, which can include copyrighted and private content. Machine
unlearning has been introduced as a viable solution to remove the influence of
such problematic content without the need for costly and time-consuming
retraining. This process aims to erase specific knowledge from LLMs while
preserving as much model utility as possible. Despite the effectiveness of
current unlearning methods, little attention has been given to whether existing
unlearning methods for LLMs truly achieve forgetting or merely hide the
knowledge, which current unlearning benchmarks fail to detect. This paper
reveals that applying quantization to models that have undergone unlearning can
restore the "forgotten" information. To thoroughly evaluate this phenomenon, we
conduct comprehensive experiments using various quantization techniques across
multiple precision levels. We find that for unlearning methods with utility
constraints, the unlearned model retains an average of 21\% of the intended
forgotten knowledge in full precision, which significantly increases to 83\%
after 4-bit quantization. Based on our empirical findings, we provide a
theoretical explanation for the observed phenomenon and propose a
quantization-robust unlearning strategy to mitigate this intricate issue...


## Improving Neuron-level Interpretability with White-box Language Models

>Authors: Hao Bai, Yi Ma

>2024-10-21

> http://arxiv.org/abs/2410.16443v1

Neurons in auto-regressive language models like GPT-2 can be interpreted by
analyzing their activation patterns. Recent studies have shown that techniques
such as dictionary learning, a form of post-hoc sparse coding, enhance this
neuron-level interpretability. In our research, we are driven by the goal to
fundamentally improve neural network interpretability by embedding sparse
coding directly within the model architecture, rather than applying it as an
afterthought. In our study, we introduce a white-box transformer-like
architecture named Coding RAte TransformEr (CRATE), explicitly engineered to
capture sparse, low-dimensional structures within data distributions. Our
comprehensive experiments showcase significant improvements (up to 103%
relative improvement) in neuron-level interpretability across a variety of
evaluation metrics. Detailed investigations confirm that this enhanced
interpretability is steady across different layers irrespective of the model
size, underlining CRATE's robust performance in enhancing neural network
interpretability. Further analysis shows that CRATE's increased
interpretability comes from its enhanced ability to consistently and
distinctively activate on relevant tokens. These findings point towards a
promising direction for creating white-box foundation models that excel in
neuron-level interpretation.


## MagicPIG LSH Sampling for Efficient LLM Generation

>Authors: Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen

>2024-10-21

> http://arxiv.org/abs/2410.16179v1

Large language models (LLMs) with long context windows have gained
significant attention. However, the KV cache, stored to avoid re-computation,
becomes a bottleneck. Various dynamic sparse or TopK-based attention
approximation methods have been proposed to leverage the common insight that
attention is sparse. In this paper, we first show that TopK attention itself
suffers from quality degradation in certain downstream tasks because attention
is not always as sparse as expected. Rather than selecting the keys and values
with the highest attention scores, sampling with theoretical guarantees can
provide a better estimation for attention output. To make the sampling-based
approximation practical in LLM generation, we propose MagicPIG, a heterogeneous
system based on Locality Sensitive Hashing (LSH). MagicPIG significantly
reduces the workload of attention computation while preserving high accuracy
for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention
computation on the CPU, which allows it to serve longer contexts and larger
batch sizes with high approximation accuracy. MagicPIG can improve decoding
throughput by $1.9\sim3.9\times$ across various GPU hardware and achieve 110ms
decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a
context of 96k tokens. The code is available at
\url{https://github.com/Infini-AI-Lab/MagicPIG}.


## PODTILE Facilitating Podcast Episode Browsing with Auto-generated Chapters

>Authors: Azin Ghazimatin, Ekaterina Garmash, Gustavo Penha, Kristen Sheets, Martin Achenbach, Oguz Semerci, Remi Galvez, Marcus Tannenberg, Sahitya Mantravadi, Divya Narayanan, Ofeliya Kalaydzhyan, Douglas Cole, Ben Carterette, Ann Clifton, Paul N. Bennett, Claudia Hauff, Mounia Lalmas

>2024-10-21

> http://arxiv.org/abs/2410.16148v1

Listeners of long-form talk-audio content, such as podcast episodes, often
find it challenging to understand the overall structure and locate relevant
sections. A practical solution is to divide episodes into
chapters--semantically coherent segments labeled with titles and timestamps.
Since most episodes on our platform at Spotify currently lack creator-provided
chapters, automating the creation of chapters is essential. Scaling the
chapterization of podcast episodes presents unique challenges. First, episodes
tend to be less structured than written texts, featuring spontaneous
discussions with nuanced transitions. Second, the transcripts are usually
lengthy, averaging about 16,000 tokens, which necessitates efficient processing
that can preserve context. To address these challenges, we introduce PODTILE, a
fine-tuned encoder-decoder transformer to segment conversational data. The
model simultaneously generates chapter transitions and titles for the input
transcript. To preserve context, each input text is augmented with global
context, including the episode's title, description, and previous chapter
titles. In our intrinsic evaluation, PODTILE achieved an 11% improvement in
ROUGE score over the strongest baseline. Additionally, we provide insights into
the practical benefits of auto-generated chapters for listeners navigating
episode content. Our findings indicate that auto-generated chapters serve as a
useful tool for engaging with less popular podcasts. Finally, we present
empirical evidence that using chapter titles can enhance effectiveness of
sparse retrieval in search tasks.


## Beyond 24 exploring VNM sparsity for efficient transformer inference on GPUs

>Authors: Kang Zhao, Tao Yuan, Han Bao, Zhenfeng Su, Chang Gao, Zhaofeng Sun, Zichen Liang, Liping Jing, Jianfei Chen

>2024-10-21

> http://arxiv.org/abs/2410.16135v1

To date, 2:4 sparsity has stood as the only sparse pattern that can be
accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often
possesses low actual speedups ($\leq 1.3$) and requires fixed sparse ratios,
meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity,
do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity
is promising in addressing these limitations of 2:4 sparsity. However,
regarding accuracy, the effects of V:N:M sparsity on broader Transformer
models, such as vision Transformers and large language models (LLMs), are
largely unexamined. Moreover, Some specific issues related to V:N:M sparsity,
such as how to select appropriate V and M values, remain unresolved. In this
study, we thoroughly investigate the application of V:N:M sparsity in vision
models and LLMs across multiple tasks, from pertaining to downstream tasks. We
propose three key approaches to enhance the applicability and accuracy of
V:N:M-sparse Transformers, including heuristic V and M selection,
V:N:M-specific channel permutation, and three-staged LoRA training techniques.
Experimental results show that, with our methods, the DeiT-small achieves
lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy
even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5
sparsity performs comparably or better than training-free 2:4 sparse
alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers
offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity.
Overall, our exploration largely facilitates the V:N:M sparsity to act as a
truly effective acceleration solution for Transformers in cost-sensitive
inference scenarios.


## Continuous Speech Synthesis using per-token Latent Diffusion

>Authors: Arnon Turetzky, Nimrod Shabtay, Slava Shechtman, Hagai Aronowitz, David Haws, Ron Hoory, Avihu Dekel

>2024-10-21

> http://arxiv.org/abs/2410.16048v1

The success of autoregressive transformer models with discrete tokens has
inspired quantization-based approaches for continuous modalities, though these
often limit reconstruction quality. We therefore introduce SALAD, a per-token
latent diffusion model for zero-shot text-to-speech, that operates on
continuous representations. SALAD builds upon the recently proposed expressive
diffusion head for image generation, and extends it to generate variable-length
outputs. Our approach utilizes semantic tokens for providing contextual
information and determining the stopping condition. We suggest three continuous
variants for our method, extending popular discrete speech synthesis
techniques. Additionally, we implement discrete baselines for each variant and
conduct a comparative analysis of discrete versus continuous speech modeling
techniques. Our results demonstrate that both continuous and discrete
approaches are highly competent, and that SALAD achieves a superior
intelligibility score while obtaining speech quality and speaker similarity on
par with the ground-truth audio.


## Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering

>Authors: Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Kam-Fai Wong, Pasquale Minervini

>2024-10-21

> http://arxiv.org/abs/2410.15999v1

Large language models (LLMs) can store a significant amount of factual
knowledge in their parameters. However, their parametric knowledge may conflict
with the information provided in the context -- this phenomenon, known as
\emph{context-memory knowledge conflicts}, can lead to undesirable model
behaviour, such as reliance on outdated or incorrect information. Analysing the
internal activations of LLMs, we find that they can internally register the
signals of knowledge conflict at mid-layers. Such signals allow us to detect
whether a knowledge conflict occurs and use \emph{inference-time} intervention
strategies to resolve it. In this work, we propose \textsc{SpARE}, a
\emph{training-free} representation engineering method that uses pre-trained
sparse auto-encoders (SAEs) to control the knowledge selection behaviour of
LLMs. \textsc{SpARE} identifies the functional features that control the
knowledge selection behaviours and applies them to edit the internal
activations of LLMs at inference time. Our experimental results show that
\textsc{SpARE} can effectively control the usage of either knowledge source to
resolve knowledge conflict in open-domain question-answering tasks, surpassing
existing representation engineering methods ($+10\%$) as well as contrastive
decoding methods ($+15\%$).


## Residual vector quantization for KV cache compression in large language model

>Authors: Ankur Kumar

>2024-10-21

> http://arxiv.org/abs/2410.15704v1

KV cache compression methods have mainly relied on scalar quantization
techniques to reduce the memory requirements during decoding. In this work, we
apply residual vector quantization, which has been widely used for high
fidelity audio compression, to compress KV cache in large language models
(LLM). We adapt the standard recipe with minimal changes to compress the output
of any key or value projection matrix in a pretrained LLM: we scale the vector
by its standard deviation, divide channels into groups and then quantize each
group with the same residual vector quantizer. We learn the codebook using
exponential moving average and there are no other learnable parameters
including the input and output projections normally used in a vector
quantization set up. We find that a residual depth of 8 recovers most of the
performance of the unquantized model. We also find that grouping non-contiguous
channels together works better than grouping contiguous channels for
compressing key matrix and the method further benefits from a light weight
finetuning of LLM together with the quantization. Overall, the proposed
technique is competitive with existing quantization methods while being much
simpler and results in 5.5x compression compared to half precision.


## A quantum anchor for higher Koszul brackets

>Authors: Ekaterina Shemyakova, Yagmur Yilmaz

>2024-10-21

> http://arxiv.org/abs/2410.15664v1

It is well known that the chain map between the de Rham and Poisson complexes
on a Poisson manifold also maps the Koszul bracket of differential forms into
the Schouten bracket of multivector fields.
  In the generalized case of a $P_\infty$-structure, where a Poisson bivector
$P$ is replaced by an arbitrary even multivector obeying $[[P,P]]=0$, an analog
of the chain map and an $L_\infty$-morphism from the higher Koszul brackets
into the Schouten bracket are also known; however, they differ significantly in
nature.
  In the present paper, we address the problem of quantizing this picture. In
particular, we show that the $L_\infty$-morphism is quantized into a single
linear operator, which is a formal Fourier integral operator.
  This paper employs Voronov's thick morphism technique and quantum
Mackenzie-Xu transformations in the framework of $L_\infty$-algebroids.


## Pruning Foundation Models for High Accuracy without Retraining

>Authors: Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin

>2024-10-21

> http://arxiv.org/abs/2410.15567v1

Despite the superior performance, it is challenging to deploy foundation
models or large language models (LLMs) due to their massive parameters and
computations. While pruning is a promising technique to reduce model size and
accelerate the inference, the traditional pruning techniques can hardly be
applied for LLMs as they need to finetune the model on the full dataset with
multiple epochs consuming massive data and hardware resources. To deal with
this problem, post-training pruning methods are proposed to prune LLMs in
one-shot without retraining. However, their accuracy after pruning may suffer
from certain performance degradation due to the lack of retraining with massive
data. To address this issue, in this paper, we first formulate the
post-training problem for layer-wise LLM compression to simultaneously prune
multiple weights in LLMs. Next, we provide an optimal solution for this problem
and design our post-training pruning algorithm for both unstructured and
semi-structured sparsity. Our extensive experiments demonstrate the superior
performance of the proposed methods in comparison to SOTA baselines across
various LLM families including transformer-based LLMs and Mamba-based LLMs.
Code link: https://github.com/piuzha/APT


## Bayesian Concept Bottleneck Models with LLM Priors

>Authors: Jean Feng, Avni Kothari, Luke Zier, Chandan Singh, Yan Shuo Tan

>2024-10-21

> http://arxiv.org/abs/2410.15555v1

Concept Bottleneck Models (CBMs) have been proposed as a compromise between
white-box and black-box models, aiming to achieve interpretability without
sacrificing accuracy. The standard training procedure for CBMs is to predefine
a candidate set of human-interpretable concepts, extract their values from the
training data, and identify a sparse subset as inputs to a transparent
prediction model. However, such approaches are often hampered by the tradeoff
between enumerating a sufficiently large set of concepts to include those that
are truly relevant versus controlling the cost of obtaining concept
extractions. This work investigates a novel approach that sidesteps these
challenges: BC-LLM iteratively searches over a potentially infinite set of
concepts within a Bayesian framework, in which Large Language Models (LLMs)
serve as both a concept extraction mechanism and prior. BC-LLM is broadly
applicable and multi-modal. Despite imperfections in LLMs, we prove that BC-LLM
can provide rigorous statistical inference and uncertainty quantification. In
experiments, it outperforms comparator methods including black-box models,
converges more rapidly towards relevant concepts and away from spuriously
correlated ones, and is more robust to out-of-distribution samples.

