# 2024-10-21

# Table of Contents
* [$γ-$MoD Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](#$γ-$MoD-Exploring-Mixture-of-Depth-Adaptation-for-Multimodal-Large-Language-Models)
* [SimLayerKV A Simple Framework for Layer-Level KV Cache Reduction](#SimLayerKV-A-Simple-Framework-for-Layer-Level-KV-Cache-Reduction)
* [Active-Dormant Attention Heads Mechanistically Demystifying Extreme-Token Phenomena in LLMs](#Active-Dormant-Attention-Heads-Mechanistically-Demystifying-Extreme-Token-Phenomena-in-LLMs)
* [Learning Graph Quantized Tokenizers for Transformers](#Learning-Graph-Quantized-Tokenizers-for-Transformers)
* [Variational Quantum Framework for Nonlinear PDE Constrained Optimization Using Carleman Linearization](#Variational-Quantum-Framework-for-Nonlinear-PDE-Constrained-Optimization-Using-Carleman-Linearization)
* [Transformer-Based Approaches for Sensor-Based Human Activity Recognition Opportunities and Challenges](#Transformer-Based-Approaches-for-Sensor-Based-Human-Activity-Recognition-Opportunities-and-Challenges)
* [Progressive Mixed-Precision Decoding for Efficient LLM Inference](#Progressive-Mixed-Precision-Decoding-for-Efficient-LLM-Inference)
* [Linguistically Grounded Analysis of Language Models using Shapley Head Values](#Linguistically-Grounded-Analysis-of-Language-Models-using-Shapley-Head-Values)
* [Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and Surface Representation](#Self-Supervised-Scene-Flow-Estimation-with-Point-Voxel-Fusion-and-Surface-Representation)
* [LLM-Rank A Graph Theoretical Approach to Pruning Large Language Models](#LLM-Rank-A-Graph-Theoretical-Approach-to-Pruning-Large-Language-Models)
* [SeerAttention Learning Intrinsic Sparse Attention in Your LLMs](#SeerAttention-Learning-Intrinsic-Sparse-Attention-in-Your-LLMs)
* [Quamba A Post-Training Quantization Recipe for Selective State Space Models](#Quamba-A-Post-Training-Quantization-Recipe-for-Selective-State-Space-Models)
* [AsymKV Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations](#AsymKV-Enabling-1-Bit-Quantization-of-KV-Cache-with-Layer-Wise-Asymmetric-Quantization-Configurations)
* [UniG Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction](#UniG-Modelling-Unitary-3D-Gaussians-for-View-consistent-3D-Reconstruction)


## $γ-$MoD Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models

>Authors: Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji

>2024-10-17

> http://arxiv.org/abs/2410.13859v1

Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.


## SimLayerKV A Simple Framework for Layer-Level KV Cache Reduction

>Authors: Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin

>2024-10-17

> http://arxiv.org/abs/2410.13846v1

Recent advancements in large language models (LLMs) have extended their
capabilities to handle long contexts. However, increasing the number of model
layers and the length of input sequences significantly escalates the memory
required to store key-value (KV) cache, posing challenges for efficient
inference. To mitigate this issue, we present SimLayerKV, a simple yet
effective method that reduces inter-layer KV cache redundancies by selectively
dropping cache in identified lazy layers. Our approach is based on the
observation that certain layers in long-context LLMs exhibit "lazy" behavior,
contributing less to modeling long-range dependencies compared to non-lazy
layers. By analyzing attention weight patterns, we find that the behavior of
these lazy layers is consistent across tokens during generation for a given
input. This insight motivates our SimLayerKV, which identifies lazy layers and
reduces their KV cache accordingly. SimLayerKV is training-free, generalizable,
and can be implemented with only seven lines of code. We conduct extensive
experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and
Mistral-7B across 16 tasks from the LongBench benchmark. The results
demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$
with only a 1.2% performance drop when combined with 4-bit quantization. Our
code is available at https://github.com/sail-sg/SimLayerKV.


## Active-Dormant Attention Heads Mechanistically Demystifying Extreme-Token Phenomena in LLMs

>Authors: Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei

>2024-10-17

> http://arxiv.org/abs/2410.13835v1

Practitioners have consistently observed three puzzling phenomena in
transformer-based large language models (LLMs): attention sinks, value-state
drains, and residual-state peaks, collectively referred to as extreme-token
phenomena. These phenomena are characterized by certain so-called "sink tokens"
receiving disproportionately high attention weights, exhibiting significantly
smaller value states, and having much larger residual-state norms than those of
other tokens. These extreme tokens give rise to various challenges in LLM
inference, quantization, and interpretability.
  We elucidate the mechanisms behind extreme-token phenomena. First, we show
that these phenomena arise in very simple architectures -- transformers with
one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.
In this setting, we identify an active-dormant mechanism, where attention heads
become sinks for specific input domains while remaining non-sinks for others.
Our theoretical analysis of the training dynamics reveals that these phenomena
are driven by a mutual reinforcement mechanism. Building on these insights, we
propose strategies to mitigate extreme-token phenomena during pretraining,
including replacing softmax with ReLU and Adam with SGD. Next, we extend our
analysis to pretrained LLMs, including Llama and OLMo, showing that many
attention heads exhibit a similar active-dormant mechanism as in the BB task,
and that the mutual reinforcement mechanism also governs the emergence of
extreme-token phenomena during LLM pretraining. Our results reveal that many of
the static and dynamic properties of extreme-token phenomena predicted by the
BB task align with observations in pretrained LLMs.


## Learning Graph Quantized Tokenizers for Transformers

>Authors: Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long

>2024-10-17

> http://arxiv.org/abs/2410.13798v1

Transformers serve as the backbone architectures of Foundational Models,
where a domain-specific tokenizer helps them adapt to various domains. Graph
Transformers (GTs) have recently emerged as a leading model in geometric deep
learning, outperforming Graph Neural Networks (GNNs) in various graph learning
tasks. However, the development of tokenizers for graphs has lagged behind
other modalities, with existing approaches relying on heuristics or GNNs
co-trained with Transformers. To address this, we introduce GQT (\textbf{G}raph
\textbf{Q}uantized \textbf{T}okenizer), which decouples tokenizer training from
Transformer training by leveraging multi-task graph self-supervised learning,
yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes
Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens,
resulting in significantly reduced memory requirements and improved
generalization capabilities. By combining the GQT with token modulation, a
Transformer encoder achieves state-of-the-art performance on 16 out of 18
benchmarks, including large-scale homophilic and heterophilic datasets. The
code is available at: https://github.com/limei0307/graph-tokenizer


## Variational Quantum Framework for Nonlinear PDE Constrained Optimization Using Carleman Linearization

>Authors: Abeynaya Gnanasekaran, Amit Surana, Hongyu Zhu

>2024-10-17

> http://arxiv.org/abs/2410.13688v1

We present a novel variational quantum framework for nonlinear partial
differential equation (PDE) constrained optimization problems. The proposed
work extends the recently introduced bi-level variational quantum PDE
constrained optimization (BVQPCO) framework for linear PDE to a nonlinear
setting by leveraging Carleman linearization (CL). CL framework allows one to
transform a system of polynomial ordinary differential equations (ODE), i,e.
ODE with polynomial vector field, into an system of infinite but linear system
of ODE. For instance, such polynomial ODEs naturally arise when the PDE are
semi-discretized in the spatial dimensions. By truncating the CL system to a
finite order, one obtains a finite system of linear ODE to which the linear
BVQPCO framework can be applied. In particular, the finite system of linear ODE
is discretized in time and embedded as a system of linear equations. The
variational quantum linear solver (VQLS) is used to solve the linear system for
given optimization parameters, and evaluate the design cost/objective function,
and a classical black box optimizer is used to select next set of parameter
values based on this evaluated cost. We present detailed computational error
and complexity analysis and prove that under suitable assumptions, our proposed
framework can provide potential advantage over classical techniques. We
implement our framework using the PennyLane library and apply it to solve
inverse Burgers' problem. We also explore an alternative tensor product
decomposition which exploits the sparsity/structure of linear system arising
from PDE discretization to facilitate the computation of VQLS cost functions.


## Transformer-Based Approaches for Sensor-Based Human Activity Recognition Opportunities and Challenges

>Authors: Clayton Souza Leite, Henry Mauranen, Aziza Zhanabatyrova, Yu Xiao

>2024-10-17

> http://arxiv.org/abs/2410.13605v1

Transformers have excelled in natural language processing and computer
vision, paving their way to sensor-based Human Activity Recognition (HAR).
Previous studies show that transformers outperform their counterparts
exclusively when they harness abundant data or employ compute-intensive
optimization algorithms. However, neither of these scenarios is viable in
sensor-based HAR due to the scarcity of data in this field and the frequent
need to perform training and inference on resource-constrained devices. Our
extensive investigation into various implementations of transformer-based
versus non-transformer-based HAR using wearable sensors, encompassing more than
500 experiments, corroborates these concerns. We observe that transformer-based
solutions pose higher computational demands, consistently yield inferior
performance, and experience significant performance degradation when quantized
to accommodate resource-constrained devices. Additionally, transformers
demonstrate lower robustness to adversarial attacks, posing a potential threat
to user trust in HAR.


## Progressive Mixed-Precision Decoding for Efficient LLM Inference

>Authors: Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris

>2024-10-17

> http://arxiv.org/abs/2410.13461v1

In spite of the great potential of large language models (LLMs) across
various tasks, their deployment on resource-constrained devices remains
challenging due to their excessive computational and memory demands.
Quantization has emerged as an effective solution by storing weights in reduced
precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially
alleviate the memory-boundedness of LLM decoding, still suffers from
prohibitive performance drop. In this work, we argue that existing approaches
fail to explore the diversity in computational patterns, redundancy, and
sensitivity to approximations of the different phases of LLM inference,
resorting to a uniform quantization policy throughout. Instead, we propose a
novel phase-aware method that selectively allocates precision during different
phases of LLM inference, achieving both strong context extraction during
prefill and efficient memory bandwidth utilization during decoding. To further
address the memory-boundedness of the decoding phase, we introduce Progressive
Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering
of precision deeper in the generated sequence, together with a spectrum of
precision-switching schedulers that dynamically drive the precision-lowering
decisions in either task-adaptive or prompt-adaptive manner. Extensive
evaluation across diverse language tasks shows that when targeting Nvidia GPUs,
PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over
fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a
throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$
over uniform quantization approaches while preserving the output quality.


## Linguistically Grounded Analysis of Language Models using Shapley Head Values

>Authors: Marcell Fekete, Johannes Bjerva

>2024-10-17

> http://arxiv.org/abs/2410.13396v1

Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).


## Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and Surface Representation

>Authors: Xuezhi Xiang, Xi Wang, Lei Zhang, Denis Ombati, Himaloy Himu, Xiantong Zhen

>2024-10-17

> http://arxiv.org/abs/2410.13355v1

Scene flow estimation aims to generate the 3D motion field of points between
two consecutive frames of point clouds, which has wide applications in various
fields. Existing point-based methods ignore the irregularity of point clouds
and have difficulty capturing long-range dependencies due to the inefficiency
of point-level computation. Voxel-based methods suffer from the loss of detail
information. In this paper, we propose a point-voxel fusion method, where we
utilize a voxel branch based on sparse grid attention and the shifted window
strategy to capture long-range dependencies and a point branch to capture
fine-grained features to compensate for the information loss in the voxel
branch. In addition, since xyz coordinates are difficult to describe the
geometric structure of complex 3D objects in the scene, we explicitly encode
the local surface information of the point cloud through the umbrella surface
feature extraction (USFE) module. We verify the effectiveness of our method by
conducting experiments on the Flyingthings3D and KITTI datasets. Our method
outperforms all other self-supervised methods and achieves highly competitive
results compared to fully supervised methods. We achieve improvements in all
metrics, especially EPE, which is reduced by 8.51% and 10.52% on the KITTIo and
KITTIs datasets, respectively.


## LLM-Rank A Graph Theoretical Approach to Pruning Large Language Models

>Authors: David Hoffmann, Kailash Budhathoki, Matthaeus Kleindessner

>2024-10-17

> http://arxiv.org/abs/2410.13299v1

The evolving capabilities of large language models are accompanied by growing
sizes and deployment costs, necessitating effective inference optimisation
techniques. We propose a novel pruning method utilising centrality measures
from graph theory, reducing both the computational requirements and the memory
footprint of these models. Specifically, we devise a method for creating a
weighted directed acyclical graph representation of multilayer perceptrons to
which we apply a modified version of the weighted PageRank centrality measure
to compute node importance scores. In combination with uniform pruning this
leads to structured sparsity. We call this pruning method MLPRank. Furthermore
we introduce an extension to decoder-only transformer models and call it
LLMRank. For both variants we demonstrate a strong performance. With MLPRank on
average leading to 6.09 % higher accuracy retention than three popular
baselines and 13.42 % with LLMRank compared to two popular baselines.


## SeerAttention Learning Intrinsic Sparse Attention in Your LLMs

>Authors: Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang

>2024-10-17

> http://arxiv.org/abs/2410.13276v2

Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity limits the efficiency and scalability of LLMs, especially
for those with a long-context window. A promising approach addressing this
limitation is to leverage the sparsity in attention. However, existing
sparsity-based solutions predominantly rely on predefined patterns or
heuristics to approximate sparsity. This practice falls short to fully capture
the dynamic nature of attention sparsity in language-based tasks. This paper
argues that attention sparsity should be learned rather than predefined. To
this end, we design SeerAttention, a new Attention mechanism that augments the
conventional attention with a learnable gate that adaptively selects
significant blocks in an attention map and deems the rest blocks sparse. Such
block-level sparsity effectively balances accuracy and speedup. To enable
efficient learning of the gating network, we develop a customized
FlashAttention implementation that extracts the block-level ground truth of
attention map with minimum overhead. SeerAttention not only applies to
post-training, but also excels in long-context fine-tuning. Our results show
that at post-training stages, SeerAttention significantly outperforms
state-of-the-art static or heuristic-based sparse attention methods, while also
being more versatile and flexible to adapt to varying context lengths and
sparsity ratios. When applied to long-context fine-tuning with YaRN,
SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context
length with minimal perplexity loss, offering a 5.67x speedup over
FlashAttention-2.


## Quamba A Post-Training Quantization Recipe for Selective State Space Models

>Authors: Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Diana Marculescu

>2024-10-17

> http://arxiv.org/abs/2410.13229v1

State Space Models (SSMs) have emerged as an appealing alternative to
Transformers for large language models, achieving state-of-the-art accuracy
with constant memory complexity which allows for holding longer context lengths
than attention-based networks. The superior computational efficiency of SSMs in
long sequence modeling positions them favorably over Transformers in many
scenarios. However, improving the efficiency of SSMs on request-intensive
cloud-serving and resource-limited edge applications is still a formidable
task. SSM quantization is a possible solution to this problem, making SSMs more
suitable for wide deployment, while still maintaining their accuracy.
Quantization is a common technique to reduce the model size and to utilize the
low bit-width acceleration features on modern computing units, yet existing
quantization techniques are poorly suited for SSMs. Most notably, SSMs have
highly sensitive feature maps within the selective scan mechanism (i.e., linear
recurrence) and massive outliers in the output activations which are not
present in the output of token-mixing in the self-attention modules. To address
this issue, we propose a static 8-bit per-tensor SSM quantization method which
suppresses the maximum values of the input activations to the selective SSM for
finer quantization precision and quantizes the output activations in an
outlier-free space with Hadamard transform. Our 8-bit weight-activation
quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a
1.72x lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9% drop
in average accuracy on zero-shot tasks. The experiments demonstrate the
effectiveness and practical applicability of our approach for deploying
SSM-based models of all sizes on both cloud and edge platforms.


## AsymKV Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations

>Authors: Qian Tao, Wenyuan Yu, Jingren Zhou

>2024-10-17

> http://arxiv.org/abs/2410.13212v1

Large language models have shown exceptional capabilities in a wide range of
tasks, such as text generation and video generation, among others. However, due
to their massive parameter count, these models often require substantial
storage space, imposing significant constraints on the machines deploying LLMs.
To overcome this limitation, one research direction proposes to compress the
models using integer replacements for floating-point numbers, in a process
known as Quantization. Some recent studies suggest quantizing the key and value
cache (KV Cache) of LLMs, and designing quantization techniques that treat the
key and value matrices equivalently.
  This work delves deeper into the asymmetric structural roles of KV Cache, a
phenomenon where the transformer's output loss is more sensitive to the
quantization of key matrices. We conduct a systematic examination of the
attention output error resulting from key and value quantization. The
phenomenon inspires us to propose an asymmetric quantization strategy. Our
approach allows for 1-bit quantization of the KV cache by implementing distinct
configurations for key and value matrices. We carry out experiments across a
variety of datasets, demonstrating that our proposed model allows for the
quantization of up to 75% decoder layers with 1 bit, while simultaneously
maintaining performance levels comparable to those of the models with floating
parameters.


## UniG Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction

>Authors: Jiamin Wu, Kenkun Liu, Yukai Shi, Xiaoke Jiang, Yuan Yao, Lei Zhang

>2024-10-17

> http://arxiv.org/abs/2410.13195v2

In this work, we present UniG, a view-consistent 3D reconstruction and novel
view synthesis model that generates a high-fidelity representation of 3D
Gaussians from sparse images. Existing 3D Gaussians-based methods usually
regress Gaussians per-pixel of each view, create 3D Gaussians per view
separately, and merge them through point concatenation. Such a view-independent
reconstruction approach often results in a view inconsistency issue, where the
predicted positions of the same 3D point from different views may have
discrepancies. To address this problem, we develop a DETR (DEtection
TRansformer)-like framework, which treats 3D Gaussians as decoder queries and
updates their parameters layer by layer by performing multi-view
cross-attention (MVDFA) over multiple input images. In this way, multiple views
naturally contribute to modeling a unitary representation of 3D Gaussians,
thereby making 3D reconstruction more view-consistent. Moreover, as the number
of 3D Gaussians used as decoder queries is irrespective of the number of input
views, allow an arbitrary number of input images without causing memory
explosion. Extensive experiments validate the advantages of our approach,
showcasing superior performance over existing methods quantitatively (improving
PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and
qualitatively. The code will be released at https://github.com/jwubz123/UNIG.

