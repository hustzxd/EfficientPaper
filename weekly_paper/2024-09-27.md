# 2024-09-27

# Table of Contents
* [Role-RL Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles](#Role-RL-Online-Long-Context-Processing-with-Role-Reinforcement-Learning-for-Distinct-LLMs-in-Their-Optimal-Roles)
* [BEATS Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search](#BEATS-Optimizing-LLM-Mathematical-Capabilities-with-BackVerify-and-Adaptive-Disambiguate-based-Efficient-Tree-Search)
* [Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores](#Efficient-Arbitrary-Precision-Acceleration-for-Large-Language-Models-on-GPU-Tensor-Cores)
* [Language Models as Zero-shot Lossless Gradient Compressors Towards General Neural Parameter Prior Models](#Language-Models-as-Zero-shot-Lossless-Gradient-Compressors-Towards-General-Neural-Parameter-Prior-Models)
* [MoGenTS Motion Generation based on Spatial-Temporal Joint Modeling](#MoGenTS-Motion-Generation-based-on-Spatial-Temporal-Joint-Modeling)
* [Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking](#Dynamic-Subframe-Splitting-and-Spatio-Temporal-Motion-Entangled-Sparse-Attention-for-RGB-E-Tracking)
* [MaskLLM Learnable Semi-Structured Sparsity for Large Language Models](#MaskLLM-Learnable-Semi-Structured-Sparsity-for-Large-Language-Models)
* [AgRegNet A Deep Regression Network for Flower and Fruit Density Estimation, Localization, and Counting in Orchards](#AgRegNet-A-Deep-Regression-Network-for-Flower-and-Fruit-Density-Estimation,-Localization,-and-Counting-in-Orchards)
* [Search for Efficient Large Language Models](#Search-for-Efficient-Large-Language-Models)
* [Riemannian conjugate Sobolev gradients and their application to compute ground states of BECs](#Riemannian-conjugate-Sobolev-gradients-and-their-application-to-compute-ground-states-of-BECs)
* [Sparsity, Regularization and Causality in Agricultural Yield The Case of Paddy Rice in Peru](#Sparsity,-Regularization-and-Causality-in-Agricultural-Yield-The-Case-of-Paddy-Rice-in-Peru)
* [Accumulator-Aware Post-Training Quantization](#Accumulator-Aware-Post-Training-Quantization)
* [VPTQ Extreme Low-bit Vector Post-Training Quantization for Large Language Models](#VPTQ-Extreme-Low-bit-Vector-Post-Training-Quantization-for-Large-Language-Models)
* [INT-FlashAttention Enabling Flash Attention for INT8 Quantization](#INT-FlashAttention-Enabling-Flash-Attention-for-INT8-Quantization)
* [LLaMa-SciQ An Educational Chatbot for Answering Science MCQ](#LLaMa-SciQ-An-Educational-Chatbot-for-Answering-Science-MCQ)
* [A Novel Framework for Analyzing Structural Transformation in Data-Constrained Economies Using Bayesian Modeling and Machine Learning](#A-Novel-Framework-for-Analyzing-Structural-Transformation-in-Data-Constrained-Economies-Using-Bayesian-Modeling-and-Machine-Learning)
* [RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems](#RoleBreak-Character-Hallucination-as-a-Jailbreak-Attack-in-Role-Playing-Systems)
* [A Survey of Low-bit Large Language Models Basics, Systems, and Algorithms](#A-Survey-of-Low-bit-Large-Language-Models-Basics,-Systems,-and-Algorithms)
* [Pre-trained Graphformer-based Ranking at Web-scale Search (Extended Abstract)](#Pre-trained-Graphformer-based-Ranking-at-Web-scale-Search-(Extended-Abstract))
* [AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization](#AlignedKV-Reducing-Memory-Access-of-KV-Cache-with-Precision-Aligned-Quantization)
* [Underground Mapping and Localization Based on Ground-Penetrating Radar](#Underground-Mapping-and-Localization-Based-on-Ground-Penetrating-Radar)
* [MaskBit Embedding-free Image Generation via Bit Tokens](#MaskBit-Embedding-free-Image-Generation-via-Bit-Tokens)
* [Time-MoE Billion-Scale Time Series Foundation Models with Mixture of Experts](#Time-MoE-Billion-Scale-Time-Series-Foundation-Models-with-Mixture-of-Experts)
* [GraphGIA GNN Explanation Method using Game Interaction](#GraphGIA-GNN-Explanation-Method-using-Game-Interaction)
* [Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI](#Data-Augmentation-for-Sparse-Multidimensional-Learning-Performance-Data-Using-Generative-AI)
* [CSPS A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts](#CSPS-A-Communication-Efficient-Sequence-Parallelism-based-Serving-System-for-Transformer-based-Models-with-Long-Prompts)
* [Efficiently Dispatching Flash Attention For Partially Filled Attention Masks](#Efficiently-Dispatching-Flash-Attention-For-Partially-Filled-Attention-Masks)
* [Cucheb A GPU implementation of the filtered Lanczos procedure](#Cucheb-A-GPU-implementation-of-the-filtered-Lanczos-procedure)
* [Deep Cost Ray Fusion for Sparse Depth Video Completion](#Deep-Cost-Ray-Fusion-for-Sparse-Depth-Video-Completion)
* [Kriformer A Novel Spatiotemporal Kriging Approach Based on Graph Transformers](#Kriformer-A-Novel-Spatiotemporal-Kriging-Approach-Based-on-Graph-Transformers)
* [MICSim A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator](#MICSim-A-Modular-Simulator-for-Mixed-signal-Compute-in-Memory-based-AI-Accelerator)
* [Parse Trees Guided LLM Prompt Compression](#Parse-Trees-Guided-LLM-Prompt-Compression)
* [ERPoT Effective and Reliable Pose Tracking for Mobile Robots Based on Lightweight and Compact Polygon Maps](#ERPoT-Effective-and-Reliable-Pose-Tracking-for-Mobile-Robots-Based-on-Lightweight-and-Compact-Polygon-Maps)
* [Harmonising the Clinical Melody Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding](#Harmonising-the-Clinical-Melody-Tuning-Large-Language-Models-for-Hospital-Course-Summarisation-in-Clinical-Coding)
* [EQ-CBM A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors](#EQ-CBM-A-Probabilistic-Concept-Bottleneck-with-Energy-based-Models-and-Quantized-Vectors)
* [Patch Ranking Efficient CLIP by Learning to Rank Local Patches](#Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches)
* [A is for Absorption Studying Feature Splitting and Absorption in Sparse Autoencoders](#A-is-for-Absorption-Studying-Feature-Splitting-and-Absorption-in-Sparse-Autoencoders)
* [Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers](#Sparse-Low-Ranked-Self-Attention-Transformer-for-Remaining-Useful-Lifetime-Prediction-of-Optical-Fiber-Amplifiers)
* [Thinking in Granularity Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues](#Thinking-in-Granularity-Dynamic-Quantization-for-Image-Super-Resolution-by-Intriguing-Multi-Granularity-Clues)
* [Enhanced DOA Estimation via Hybrid Massive MIMO Receive Array with Switches-based Sparse Architecture](#Enhanced-DOA-Estimation-via-Hybrid-Massive-MIMO-Receive-Array-with-Switches-based-Sparse-Architecture)
* [On Importance of Pruning and Distillation for Efficient Low Resource NLP](#On-Importance-of-Pruning-and-Distillation-for-Efficient-Low-Resource-NLP)
* [Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis](#Interpreting-Arithmetic-Mechanism-in-Large-Language-Models-through-Comparative-Neuron-Analysis)
* [ProTEA Programmable Transformer Encoder Acceleration on FPGA](#ProTEA-Programmable-Transformer-Encoder-Acceleration-on-FPGA)
* [Periodic micromagnetic finite element method](#Periodic-micromagnetic-finite-element-method)
* [TalkMosaic Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions](#TalkMosaic-Interactive-PhotoMosaic-with-Multi-modal-LLM-Q&A-Interactions)
* [OATS Outlier-Aware Pruning Through Sparse and Low Rank Decomposition](#OATS-Outlier-Aware-Pruning-Through-Sparse-and-Low-Rank-Decomposition)
* [Tackling fluffy clouds field boundaries detection using time series of S2 and/or S1 imagery](#Tackling-fluffy-clouds-field-boundaries-detection-using-time-series-of-S2-and/or-S1-imagery)
* [Data Augmentation for Sequential Recommendation A Survey](#Data-Augmentation-for-Sequential-Recommendation-A-Survey)
* [Generalizing Deep Learning-Based CSI Feedback in Massive MIMO via ID-Photo-Inspired Preprocessing](#Generalizing-Deep-Learning-Based-CSI-Feedback-in-Massive-MIMO-via-ID-Photo-Inspired-Preprocessing)
* [Selective Exploration and Information Gathering in Search and Rescue Using Hierarchical Learning Guided by Natural Language Input](#Selective-Exploration-and-Information-Gathering-in-Search-and-Rescue-Using-Hierarchical-Learning-Guided-by-Natural-Language-Input)
* [T2M-X Learning Expressive Text-to-Motion Generation from Partially Annotated Data](#T2M-X-Learning-Expressive-Text-to-Motion-Generation-from-Partially-Annotated-Data)
* [Neural-Symbolic Collaborative Distillation Advancing Small Language Models for Complex Reasoning Tasks](#Neural-Symbolic-Collaborative-Distillation-Advancing-Small-Language-Models-for-Complex-Reasoning-Tasks)
* [CFSP An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information](#CFSP-An-Efficient-Structured-Pruning-Framework-for-LLMs-with-Coarse-to-Fine-Activation-Information)
* [Learning to Compare Hardware Designs for High-Level Synthesis](#Learning-to-Compare-Hardware-Designs-for-High-Level-Synthesis)


## Role-RL Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles

>Authors: Lewei He, Tianyu Shi, Pengran Huang, Bingzhi Chen, Qianglong Chen, Jiahui Pan

>2024-09-26

> http://arxiv.org/abs/2409.18014v1

Large language models (LLMs) with long-context processing are still
challenging because of their implementation complexity, training efficiency and
data sparsity. To address this issue, a new paradigm named Online Long-context
Processing (OLP) is proposed when we process a document of unlimited length,
which typically occurs in the information reception and organization of diverse
streaming media such as automated news reporting, live e-commerce, and viral
short videos. Moreover, a dilemma was often encountered when we tried to select
the most suitable LLM from a large number of LLMs amidst explosive growth
aiming for outstanding performance, affordable prices, and short response
delays. In view of this, we also develop Role Reinforcement Learning (Role-RL)
to automatically deploy different LLMs in their respective roles within the OLP
pipeline according to their actual performance. Extensive experiments are
conducted on our OLP-MINI dataset and it is found that OLP with Role-RL
framework achieves OLP benchmark with an average recall rate of 93.2% and the
LLM cost saved by 79.4%. The code and dataset are publicly available at:
https://anonymous.4open.science/r/Role-RL.


## BEATS Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search

>Authors: Linzhuang Sun, Hao Liang, Wentao Zhang

>2024-09-26

> http://arxiv.org/abs/2409.17972v1

Large Language Models (LLMs) have exhibited exceptional performance across a
broad range of tasks and domains. However, they still encounter difficulties in
solving mathematical problems due to the rigorous and logical nature of
mathematics. Previous studies have employed techniques such as supervised
fine-tuning (SFT), prompt engineering, and search-based methods to improve the
mathematical problem-solving abilities of LLMs. Despite these efforts, their
performance remains suboptimal and demands substantial computational resources.
To address this issue, we propose a novel approach, BEATS, to enhance
mathematical problem-solving abilities. Our method leverages newly designed
prompts that guide the model to iteratively rewrite, advance by one step, and
generate answers based on previous steps. Additionally, we introduce a new
back-verification technique that uses LLMs to validate the correctness of the
generated answers. Furthermore, we employ a pruning tree search to optimize
search time while achieving strong performance. Notably, our method improves
Qwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the
MATH benchmark.


## Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores

>Authors: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang

>2024-09-26

> http://arxiv.org/abs/2409.17870v1

Large language models (LLMs) have been widely applied but face challenges in
efficient inference. While quantization methods reduce computational demands,
ultra-low bit quantization with arbitrary precision is hindered by limited GPU
Tensor Core support and inefficient memory management, leading to suboptimal
acceleration. To address these challenges, we propose a comprehensive
acceleration scheme for arbitrary precision LLMs. At its core, we introduce a
novel bipolar-INT data format that facilitates parallel computing and supports
symmetric quantization, effectively reducing data redundancy. Building on this,
we implement an arbitrary precision matrix multiplication scheme that
decomposes and recovers matrices at the bit level, enabling flexible precision
while maximizing GPU Tensor Core utilization. Furthermore, we develop an
efficient matrix preprocessing method that optimizes data layout for subsequent
computations. Finally, we design a data recovery-oriented memory management
system that strategically utilizes fast shared memory, significantly enhancing
kernel execution speed and minimizing memory access latency. Experimental
results demonstrate our approach's effectiveness, with up to 13\times speedup
in matrix multiplication compared to NVIDIA's CUTLASS. When integrated into
LLMs, we achieve up to 6.7\times inference acceleration. These improvements
significantly enhance LLM inference efficiency, enabling broader and more
responsive applications of LLMs.


## Language Models as Zero-shot Lossless Gradient Compressors Towards General Neural Parameter Prior Models

>Authors: Hui-Po Wang, Mario Fritz

>2024-09-26

> http://arxiv.org/abs/2409.17836v1

Despite the widespread use of statistical prior models in various fields,
such models for neural network gradients have long been overlooked. The
inherent challenge stems from their high-dimensional structures and complex
interdependencies, which complicate effective modeling. In this work, we
demonstrate the potential of large language models (LLMs) to act as gradient
priors in a zero-shot setting. We examine the property by considering lossless
gradient compression -- a critical application in distributed learning -- that
depends heavily on precise probability modeling. To achieve this, we introduce
LM-GC, a novel method that integrates LLMs with arithmetic coding. Our
technique converts plain gradients into text-like formats, enhancing token
efficiency by up to 38 times compared to their plain representations. We ensure
that this data conversion maintains a close alignment with the structure of
plain gradients and the symbols commonly recognized by LLMs. Our experiments
indicate that LM-GC surpasses existing state-of-the-art lossless compression
methods, improving compression rates by 10\% up to 17.2\% across various
datasets and architectures. Additionally, our approach shows promising
compatibility with lossy compression techniques such as quantization and
sparsification. These findings highlight the significant potential of LLMs as a
model for effectively handling gradients. We will release the source code upon
publication.


## MoGenTS Motion Generation based on Spatial-Temporal Joint Modeling

>Authors: Weihao Yuan, Weichao Shen, Yisheng He, Yuan Dong, Xiaodong Gu, Zilong Dong, Liefeng Bo, Qixing Huang

>2024-09-26

> http://arxiv.org/abs/2409.17686v1

Motion generation from discrete quantization offers many advantages over
continuous regression, but at the cost of inevitable approximation errors.
Previous methods usually quantize the entire body pose into one code, which not
only faces the difficulty in encoding all joints within one vector but also
loses the spatial relationship between different joints. Differently, in this
work we quantize each individual joint into one vector, which i) simplifies the
quantization process as the complexity associated with a single joint is
markedly lower than that of the entire pose; ii) maintains a spatial-temporal
structure that preserves both the spatial relationships among joints and the
temporal movement patterns; iii) yields a 2D token map, which enables the
application of various 2D operations widely used in 2D images. Grounded in the
2D motion quantization, we build a spatial-temporal modeling framework, where
2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D
attention are proposed to take advantage of spatial-temporal signals among the
2D tokens. Extensive experiments demonstrate that our method significantly
outperforms previous methods across different datasets, with a $26.6\%$
decrease of FID on HumanML3D and a $29.9\%$ decrease on KIT-ML.


## Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking

>Authors: Pengcheng Shao, Tianyang Xu, Xuefeng Zhu, Xiaojun Wu, Josef Kittler

>2024-09-26

> http://arxiv.org/abs/2409.17560v1

Event-based bionic camera asynchronously captures dynamic scenes with high
temporal resolution and high dynamic range, offering potential for the
integration of events and RGB under conditions of illumination degradation and
fast motion. Existing RGB-E tracking methods model event characteristics
utilising attention mechanism of Transformer before integrating both
modalities. Nevertheless, these methods involve aggregating the event stream
into a single event frame, lacking the utilisation of the temporal information
inherent in the event stream.Moreover, the traditional attention mechanism is
well-suited for dense semantic features, while the attention mechanism for
sparse event features require revolution. In this paper, we propose a dynamic
event subframe splitting strategy to split the event stream into more
fine-grained event clusters, aiming to capture spatio-temporal features that
contain motion cues. Based on this, we design an event-based sparse attention
mechanism to enhance the interaction of event features in temporal and spatial
dimensions. The experimental results indicate that our method outperforms
existing state-of-the-art methods on the FE240 and COESOT datasets, providing
an effective processing manner for the event data.


## MaskLLM Learnable Semi-Structured Sparsity for Large Language Models

>Authors: Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang

>2024-09-26

> http://arxiv.org/abs/2409.17481v1

Large Language Models (LLMs) are distinguished by their massive parameter
counts, which typically result in significant redundancy. This work introduces
MaskLLM, a learnable pruning method that establishes Semi-structured (or
``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during
inference. Instead of developing a new importance criterion, MaskLLM explicitly
models N:M patterns as a learnable distribution through Gumbel Softmax
sampling. This approach facilitates end-to-end training on large-scale datasets
and offers two notable advantages: 1) High-quality Masks - our method
effectively scales to large datasets and learns accurate masks; 2)
Transferability - the probabilistic modeling of mask distribution enables the
transfer learning of sparsity across domains or tasks. We assessed MaskLLM
using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,
with sizes ranging from 843M to 15B parameters, and our empirical results show
substantial improvements over state-of-the-art methods. For instance, leading
approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to
the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL
solely by learning the masks with frozen weights. Furthermore, MaskLLM's
learnable nature allows customized masks for lossless application of 2:4
sparsity to downstream tasks or domains. Code is available at
\url{https://github.com/NVlabs/MaskLLM}.


## AgRegNet A Deep Regression Network for Flower and Fruit Density Estimation, Localization, and Counting in Orchards

>Authors: Uddhav Bhattarai, Santosh Bhusal, Qin Zhang, Manoj Karkee

>2024-09-25

> http://arxiv.org/abs/2409.17400v1

One of the major challenges for the agricultural industry today is the
uncertainty in manual labor availability and the associated cost. Automated
flower and fruit density estimation, localization, and counting could help
streamline harvesting, yield estimation, and crop-load management strategies
such as flower and fruitlet thinning. This article proposes a deep
regression-based network, AgRegNet, to estimate density, count, and location of
flower and fruit in tree fruit canopies without explicit object detection or
polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a
U-shaped network with an encoder-to-decoder skip connection and modified
ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on
information from point annotation and leverages segmentation information and
attention modules (spatial and channel) to highlight relevant flower and fruit
features while suppressing non-relevant background features. Experimental
evaluation in apple flower and fruit canopy images under an unstructured
orchard environment showed that AgRegNet achieved promising accuracy as
measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error
(pMAE) and mean Average Precision (mAP) to estimate flower and fruit density,
count, and centroid location, respectively. Specifically, the SSIM, pMAE, and
mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For
fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the
proposed approach relies on information from point annotation, it is suitable
for sparsely and densely located objects. This simplified technique will be
highly applicable for growers to accurately estimate yields and decide on
optimal chemical and mechanical flower thinning practices.


## Search for Efficient Large Language Models

>Authors: Xuan Shen, Pu Zhao, Yifan Gong, Zhenglun Kong, Zheng Zhan, Yushu Wu, Ming Lin, Chao Wu, Xue Lin, Yanzhi Wang

>2024-09-25

> http://arxiv.org/abs/2409.17372v1

Large Language Models (LLMs) have long held sway in the realms of artificial
intelligence research. Numerous efficient techniques, including weight pruning,
quantization, and distillation, have been embraced to compress LLMs, targeting
memory reduction and inference acceleration, which underscore the redundancy in
LLMs. However, most model compression techniques concentrate on weight
optimization, overlooking the exploration of optimal architectures. Besides,
traditional architecture search methods, limited by the elevated complexity
with extensive parameters, struggle to demonstrate their effectiveness on LLMs.
In this paper, we propose a training-free architecture search framework to
identify optimal subnets that preserve the fundamental strengths of the
original LLMs while achieving inference acceleration. Furthermore, after
generating subnets that inherit specific weights from the original LLMs, we
introduce a reformation algorithm that utilizes the omitted weights to rectify
the inherited weights with a small amount of calibration data. Compared with
SOTA training-free structured pruning works that can generate smaller networks,
our method demonstrates superior performance across standard benchmarks.
Furthermore, our generated subnets can directly reduce the usage of GPU memory
and achieve inference acceleration.


## Riemannian conjugate Sobolev gradients and their application to compute ground states of BECs

>Authors: Yueshan Ai, Patrick Henning, Mahima Yadav, Sitong Yuan

>2024-09-25

> http://arxiv.org/abs/2409.17302v1

This work considers the numerical computation of ground states of rotating
Bose-Einstein condensates (BECs) which can exhibit a multiscale lattice of
quantized vortices. This problem involves the minimization of an energy
functional on a Riemannian manifold. For this we apply the framework of
nonlinear conjugate gradient methods in combination with the paradigm of
Sobolev gradients to investigate different metrics. Here we build on previous
work that proposed to enhance the convergence of regular Riemannian gradients
methods by an adaptively changing metric that is based on the current energy.
In this work, we extend this approach to the branch of Riemannian conjugate
gradient (CG) methods and investigate the arising schemes numerically. Special
attention is given to the selection of the momentum parameter in search
direction and how this affects the performance of the resulting schemes. As
known from similar applications, we find that the choice of the momentum
parameter plays a critical role, with certain parameters reducing the number of
iterations required to achieve a specified tolerance by a significant factor.
Besides the influence of the momentum parameters, we also investigate how the
methods with adaptive metric compare to the corresponding realizations with a
standard $H^1_0$-metric. As one of our main findings, the results of the
numerical experiments show that the Riemannian CG method with the proposed
adaptive metric along with a Polak-Ribi\'ere or Hestenes-Stiefel-type momentum
parameter show the best performance and highest robustness compared to the
other CG methods that were part of our numerical study.


## Sparsity, Regularization and Causality in Agricultural Yield The Case of Paddy Rice in Peru

>Authors: Rita Rocio Guzman-Lopez, Luis Huamanchumo, Kevin Fernandez, Oscar Cutipa-Luque, Yhon Tiahuallpa, Helder Rojas

>2024-09-25

> http://arxiv.org/abs/2409.17298v1

This study introduces a novel approach that integrates agricultural census
data with remotely sensed time series to develop precise predictive models for
paddy rice yield across various regions of Peru. By utilizing sparse regression
and Elastic-Net regularization techniques, the study identifies causal
relationships between key remotely sensed variables-such as NDVI,
precipitation, and temperature-and agricultural yield. To further enhance
prediction accuracy, the first- and second-order dynamic transformations
(velocity and acceleration) of these variables are applied, capturing
non-linear patterns and delayed effects on yield. The findings highlight the
improved predictive performance when combining regularization techniques with
climatic and geospatial variables, enabling more precise forecasts of yield
variability. The results confirm the existence of causal relationships in the
Granger sense, emphasizing the value of this methodology for strategic
agricultural management. This contributes to more efficient and sustainable
production in paddy rice cultivation.


## Accumulator-Aware Post-Training Quantization

>Authors: Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab

>2024-09-25

> http://arxiv.org/abs/2409.17092v1

Several recent studies have investigated low-precision accumulation,
reporting improvements in throughput, power, and area across various platforms.
However, the accompanying proposals have only considered the quantization-aware
training (QAT) paradigm, in which models are fine-tuned or trained from scratch
with quantization in the loop. As models continue to grow in size, QAT
techniques become increasingly more expensive, which has motivated the recent
surge in post-training quantization (PTQ) research. To the best of our
knowledge, ours marks the first formal study of accumulator-aware quantization
in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework
of accumulator-aware extensions designed to endow overflow avoidance guarantees
to existing layer-wise PTQ algorithms. We theoretically motivate AXE and
demonstrate its flexibility by implementing it on top of two state-of-the-art
PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage
accumulation for the first time, opening the door for full datapath
optimization and scaling to large language models (LLMs). We evaluate AXE
across image classification and language generation models, and observe
significant improvements in the trade-off between accumulator bit width and
model accuracy over baseline methods.


## VPTQ Extreme Low-bit Vector Post-Training Quantization for Large Language Models

>Authors: Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang

>2024-09-25

> http://arxiv.org/abs/2409.17066v1

Scaling model size significantly challenges the deployment and inference of
Large Language Models (LLMs). Due to the redundancy in LLM weights, recent
research has focused on pushing weight-only quantization to extremely low-bit
(even down to 2 bits). It reduces memory requirements, optimizes storage costs,
and decreases memory bandwidth needs during inference. However, due to
numerical representation limitations, traditional scalar-based weight
quantization struggles to achieve such extreme low-bit. Recent research on
Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely
low-bit model quantization by compressing vectors into indices using lookup
tables.
  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for
extremely low-bit quantization of LLMs. We use Second-Order Optimization to
formulate the LLM VQ problem and guide our quantization algorithm design by
solving the optimization. We further refine the weights using
Channel-Independent Second-Order Optimization for a granular VQ. In addition,
by decomposing the optimization problem, we propose a brief and effective
codebook initialization algorithm. We also extend VPTQ to support residual and
outlier quantization, which enhances model accuracy and further compresses the
model. Our experimental results show that VPTQ reduces model quantization
perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,
$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy
improvement of $0.79$-$1.5\%$ on LLaMA-2, $1\%$ on Mistral-7B, $11$-$22\%$ on
LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\%$ of the
quantization algorithm execution time, resulting in a $1.6$-$1.8\times$
increase in inference throughput compared to SOTA.


## INT-FlashAttention Enabling Flash Attention for INT8 Quantization

>Authors: Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Yuhan Wu, Lei Su, Tong Yang

>2024-09-25

> http://arxiv.org/abs/2409.16997v2

As the foundation of large language models (LLMs), self-attention module
faces the challenge of quadratic time and memory complexity with respect to
sequence length. FlashAttention accelerates attention computation and reduces
its memory usage by leveraging the GPU memory hierarchy. A promising research
direction is to integrate FlashAttention with quantization methods. This paper
introduces INT-FlashAttention, the first INT8 quantization architecture
compatible with the forward workflow of FlashAttention, which significantly
improves the inference speed of FlashAttention on Ampere GPUs. We implement our
INT-FlashAttention prototype with fully INT8 activations and general
matrix-multiplication (GEMM) kernels, making it the first attention operator
with fully INT8 input. As a general token-level post-training quantization
framework, INT-FlashAttention is also compatible with other data formats like
INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster
inference speed and 82% smaller quantization error compared to standard
FlashAttention with FP16 and FP8 data format.


## LLaMa-SciQ An Educational Chatbot for Answering Science MCQ

>Authors: Marc-Antoine Allard, Matin Ansaripour, Maria Yuffa, Paul Teiletche

>2024-09-25

> http://arxiv.org/abs/2409.16779v1

Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements.


## A Novel Framework for Analyzing Structural Transformation in Data-Constrained Economies Using Bayesian Modeling and Machine Learning

>Authors: Ronald Katende

>2024-09-25

> http://arxiv.org/abs/2409.16738v1

Structural transformation, the shift from agrarian economies to more
diversified industrial and service-based systems, is a key driver of economic
development. However, in low- and middle-income countries (LMICs), data
scarcity and unreliability hinder accurate assessments of this process. This
paper presents a novel statistical framework designed to address these
challenges by integrating Bayesian hierarchical modeling, machine
learning-based data imputation, and factor analysis. The framework is
specifically tailored for conditions of data sparsity and is capable of
providing robust insights into sectoral productivity and employment shifts
across diverse economies. By utilizing Bayesian models, uncertainties in data
are effectively managed, while machine learning techniques impute missing data
points, ensuring the integrity of the analysis. Factor analysis reduces the
dimensionality of complex datasets, distilling them into core economic
structures. The proposed framework has been validated through extensive
simulations, demonstrating its ability to predict structural changes even when
up to 60\% of data is missing. This approach offers policymakers and
researchers a valuable tool for making informed decisions in environments where
data quality is limited, contributing to the broader understanding of economic
development in LMICs.


## RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems

>Authors: Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou

>2024-09-25

> http://arxiv.org/abs/2409.16727v1

Role-playing systems powered by large language models (LLMs) have become
increasingly influential in emotional communication applications. However,
these systems are susceptible to character hallucinations, where the model
deviates from predefined character roles and generates responses that are
inconsistent with the intended persona. This paper presents the first
systematic analysis of character hallucination from an attack perspective,
introducing the RoleBreak framework. Our framework identifies two core
mechanisms-query sparsity and role-query conflict-as key factors driving
character hallucination. Leveraging these insights, we construct a novel
dataset, RoleBreakEval, to evaluate existing hallucination mitigation
techniques. Our experiments reveal that even enhanced models trained to
minimize hallucination remain vulnerable to attacks. To address these
vulnerabilities, we propose a novel defence strategy, the Narrator Mode, which
generates supplemental context through narration to mitigate role-query
conflicts and improve query generalization. Experimental results demonstrate
that Narrator Mode significantly outperforms traditional refusal-based
strategies by reducing hallucinations, enhancing fidelity to character roles
and queries, and improving overall narrative coherence.


## A Survey of Low-bit Large Language Models Basics, Systems, and Algorithms

>Authors: Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu

>2024-09-25

> http://arxiv.org/abs/2409.16694v1

Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.


## Pre-trained Graphformer-based Ranking at Web-scale Search (Extended Abstract)

>Authors: Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, Dawei Yin

>2024-09-25

> http://arxiv.org/abs/2409.16590v1

Both Transformer and Graph Neural Networks (GNNs) have been employed in the
domain of learning to rank (LTR). However, these approaches adhere to two
distinct yet complementary problem formulations: ranking score regression based
on query-webpage pairs, and link prediction within query-webpage bipartite
graphs, respectively. While it is possible to pre-train GNNs or Transformers on
source datasets and subsequently fine-tune them on sparsely annotated LTR
datasets, the distributional shifts between the pair-based and bipartite graph
domains present significant challenges in integrating these heterogeneous
models into a unified LTR framework at web scale. To address this, we introduce
the novel MPGraf model, which leverages a modular and capsule-based
pre-training strategy, aiming to cohesively integrate the regression
capabilities of Transformers with the link prediction strengths of GNNs. We
conduct extensive offline and online experiments to rigorously evaluate the
performance of MPGraf.


## AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization

>Authors: Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng

>2024-09-25

> http://arxiv.org/abs/2409.16546v1

Model quantization has become a crucial technique to address the issues of
large memory consumption and long inference times associated with LLMs.
Mixed-precision quantization, which distinguishes between important and
unimportant parameters, stands out among numerous quantization schemes as it
achieves a balance between precision and compression rate. However, existing
approaches can only identify important parameters through qualitative analysis
and manual experiments without quantitatively analyzing how their importance is
determined. We propose a new criterion, so-called 'precision alignment', to
build a quantitative framework to holistically evaluate the importance of
parameters in mixed-precision quantization. Our observations on floating point
addition under various real-world scenarios suggest that two addends should
have identical precision, otherwise the information in the higher-precision
number will be wasted. Such an observation offers an essential principle to
determine the precision of each parameter in matrix multiplication operation.
As the first step towards applying the above discovery to large model
inference, we develop a dynamic KV-Cache quantization technique to effectively
reduce memory access latency. Different from existing quantization approaches
that focus on memory saving, this work directly aims to accelerate LLM
inference through quantifying floating numbers. The proposed technique attains
a 25% saving of memory access and delivers up to 1.3x speedup in the
computation of attention in the decoding phase of LLM, with almost no loss of
precision.


## Underground Mapping and Localization Based on Ground-Penetrating Radar

>Authors: Jinchang Zhang, Guoyu Lu

>2024-09-24

> http://arxiv.org/abs/2409.16446v1

3D object reconstruction based on deep neural networks has gained increasing
attention in recent years. However, 3D reconstruction of underground objects to
generate point cloud maps remains a challenge. Ground Penetrating Radar (GPR)
is one of the most powerful and extensively used tools for detecting and
locating underground objects such as plant root systems and pipelines, with its
cost-effectiveness and continuously evolving technology. This paper introduces
a parabolic signal detection network based on deep convolutional neural
networks, utilizing B-scan images from GPR sensors. The detected keypoints can
aid in accurately fitting parabolic curves used to interpret the original GPR
B-scan images as cross-sections of the object model. Additionally, a multi-task
point cloud network was designed to perform both point cloud segmentation and
completion simultaneously, filling in sparse point cloud maps. For unknown
locations, GPR A-scan data can be used to match corresponding A-scan data in
the constructed map, pinpointing the position to verify the accuracy of the map
construction by the model. Experimental results demonstrate the effectiveness
of our method.


## MaskBit Embedding-free Image Generation via Bit Tokens

>Authors: Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen

>2024-09-24

> http://arxiv.org/abs/2409.16211v1

Masked transformer models for class-conditional image generation have become
a compelling alternative to diffusion models. Typically comprising two stages -
an initial VQGAN model for transitioning between latent space and image space,
and a subsequent Transformer model for image generation within latent space -
these frameworks offer promising avenues for image synthesis. In this study, we
present two primary contributions: Firstly, an empirical and systematic
examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel
embedding-free generation network operating directly on bit tokens - a binary
quantized representation of tokens with rich semantics. The first contribution
furnishes a transparent, reproducible, and high-performing VQGAN model,
enhancing accessibility and matching the performance of current
state-of-the-art methods while revealing previously undisclosed details. The
second contribution demonstrates that embedding-free image generation using bit
tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256
benchmark, with a compact generator model of mere 305M parameters.


## Time-MoE Billion-Scale Time Series Foundation Models with Mixture of Experts

>Authors: Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin

>2024-09-24

> http://arxiv.org/abs/2409.16040v1

Deep learning for time series forecasting has seen significant advancements
over the past decades. However, despite the success of large-scale pre-training
in language and vision domains, pre-trained time series models remain limited
in scale and operate at a high cost, hindering the development of larger
capable forecasting models in real-world applications. In response, we
introduce Time-MoE, a scalable and unified architecture designed to pre-train
larger, more capable forecasting foundation models while reducing inference
costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE
enhances computational efficiency by activating only a subset of networks for
each prediction, reducing computational load while maintaining high model
capacity. This allows Time-MoE to scale effectively without a corresponding
increase in inference costs. Time-MoE comprises a family of decoder-only
transformer models that operate in an auto-regressive manner and support
flexible forecasting horizons with varying input context lengths. We
pre-trained these models on our newly introduced large-scale data Time-300B,
which spans over 9 domains and encompassing over 300 billion time points. For
the first time, we scaled a time series foundation model up to 2.4 billion
parameters, achieving significantly improved forecasting precision. Our results
validate the applicability of scaling laws for training tokens and model size
in the context of time series forecasting. Compared to dense models with the
same number of activated parameters or equivalent computation budgets, our
models consistently outperform them by large margin. These advancements
position Time-MoE as a state-of-the-art solution for tackling real-world time
series forecasting challenges with superior capability, efficiency, and
flexibility.


## GraphGIA GNN Explanation Method using Game Interaction

>Authors: Xingping Xian, Jianlu Liu, Tao Wu, Lin Yuan, Chao Wang, Baiyun Chen

>2024-09-24

> http://arxiv.org/abs/2409.15698v1

Graph Neural Networks (GNNs) have garnered significant attention and have
been extensively utilized across various domains. However, similar to other
deep learning models, GNNs are often viewed as black-box models, making it
challenging to interpret their prediction mechanisms. Current graph explanation
techniques focus on identifying key nodes or edges, attributing the critical
data features that drive model predictions. Nevertheless, these features do not
independently influence the model's outcomes; rather, they interact with one
another to collectively affect predictions. In this work, we propose a novel
explanatory method GraphGI, which identifies the coalition with the highest
interaction strength and presents it as an explanatory subgraph. Given a
trained model and an input graph, our method explains predictions by gradually
incorporating significant edges into the selected subgraph. We utilize
game-theoretic interaction values to assess the interaction strength after edge
additions, ensuring that the newly added edges confer maximum interaction
strength to the explanatory subgraph. To enhance computational efficiency, we
adopt effective approximation techniques for calculating Shapley values and
game-theoretic interaction values. Empirical evaluations demonstrate that our
method achieves superior fidelity and sparsity, maintaining the
interpretability of the results at a comprehensible level.


## Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI

>Authors: Liang Zhang, Jionghao Lin, John Sabatini, Conrad Borchers, Daniel Weitekamp, Meng Cao, John Hollander, Xiangen Hu, Arthur C. Graesser

>2024-09-24

> http://arxiv.org/abs/2409.15631v1

Learning performance data describe correct and incorrect answers or
problem-solving attempts in adaptive learning, such as in intelligent tutoring
systems (ITSs). Learning performance data tend to be highly sparse
(80\%\(\sim\)90\% missing observations) in most real-world applications due to
adaptive item selection. This data sparsity presents challenges to using
learner models to effectively predict future performance explore new hypotheses
about learning. This article proposes a systematic framework for augmenting
learner data to address data sparsity in learning performance data. First,
learning performance is represented as a three-dimensional tensor of learners'
questions, answers, and attempts, capturing longitudinal knowledge states
during learning. Second, a tensor factorization method is used to impute
missing values in sparse tensors of collected learner data, thereby grounding
the imputation on knowledge tracing tasks that predict missing performance
values based on real observations. Third, a module for generating patterns of
learning is used. This study contrasts two forms of generative Artificial
Intelligence (AI), including Generative Adversarial Networks (GANs) and
Generate Pre-Trained Transformers (GPT) to generate data associated with
different clusters of learner data. We tested this approach on an adult
literacy dataset from AutoTutor lessons developed for Adult Reading
Comprehension (ARC). We found that: (1) tensor factorization improved the
performance in tracing and predicting knowledge mastery compared with other
knowledge tracing techniques without data augmentation, showing higher relative
fidelity for this imputation method, and (2) the GAN-based simulation showed
greater overall stability and less statistical bias based on a divergence
evaluation with varying simulation sample sizes compared to GPT.


## CSPS A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts

>Authors: Zeyu Zhang, Haiying Shen

>2024-09-23

> http://arxiv.org/abs/2409.15104v1

Long-sequence generative large-language model (LLM) applications have become
increasingly popular. In this paper, through trace-based experiments, we found
that the existing method for long sequences results in a high
Time-To-First-Token (TTFT) due to sequential chunk processing, long
Time-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and
low throughput due to constrained key-value cache (KVC) for long sequences. To
address these issues, we propose two Sequence-Parallelism (SP) architectures
for both tensor parallelism (TP) and non-TP. However, SP introduces two
challenges: 1) network communication and computation become performance
bottlenecks; 2) the latter two issues above are mitigated but not resolved, and
SP's resultant KV value distribution across GPUs still requires communication
for decode, increasing TBT. Hence, we propose a Communication-efficient Sparse
Attention (CSA) and communication-computation-communication three-phase
pipelining. We also propose SP-based decode that processes decode separately
from prefill, distributes KV values of a request across different GPUs, and
novelly moves Query (Q) values instead of KV values to reduce communication
overhead. These methods constitute a communication-efficient
Sequence-Parallelism based LLM Serving System (SPS2). Our trace-driven
evaluation demonstrates that SPS2 improves the average TTFT, TBT, and response
time by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode
throughput by 8.2x and 5.2x while maintaining the accuracy compared to
Sarathi-Serve. We distributed our source code.


## Efficiently Dispatching Flash Attention For Partially Filled Attention Masks

>Authors: Agniv Sharma, Jonas Geiping

>2024-09-23

> http://arxiv.org/abs/2409.15097v2

Transformers are widely used across various applications, many of which yield
sparse or partially filled attention matrices. Examples include attention masks
designed to reduce the quadratic complexity of attention, sequence packing
techniques, and recent innovations like tree masking for fast validation in
MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art
algorithm Flash Attention still processes them with quadratic complexity as
though they were dense. In this paper, we introduce Binary Block Masking, a
highly efficient modification that enhances Flash Attention by making it
mask-aware. We further propose two optimizations: one tailored for masks with
contiguous non-zero patterns and another for extremely sparse masks. Our
experiments on attention masks derived from real-world scenarios demonstrate up
to a 9x runtime improvement. The implementation will be publicly released to
foster further research and application.


## Cucheb A GPU implementation of the filtered Lanczos procedure

>Authors: Jared L. Aurentz, Vassilis Kalantzis, Yousef Saad

>2024-09-23

> http://arxiv.org/abs/2409.15053v1

This paper describes the software package Cucheb, a GPU implementation of the
filtered Lanczos procedure for the solution of large sparse symmetric
eigenvalue problems. The filtered Lanczos procedure uses a carefully chosen
polynomial spectral transformation to accelerate convergence of the Lanczos
method when computing eigenvalues within a desired interval. This method has
proven particularly effective for eigenvalue problems that arise in electronic
structure calculations and density functional theory. We compare our
implementation against an equivalent CPU implementation and show that using the
GPU can reduce the computation time by more than a factor of 10.


## Deep Cost Ray Fusion for Sparse Depth Video Completion

>Authors: Jungeon Kim, Soongjin Kim, Jaesik Park, Seungyong Lee

>2024-09-23

> http://arxiv.org/abs/2409.14935v1

In this paper, we present a learning-based framework for sparse depth video
completion. Given a sparse depth map and a color image at a certain viewpoint,
our approach makes a cost volume that is constructed on depth hypothesis
planes. To effectively fuse sequential cost volumes of the multiple viewpoints
for improved depth completion, we introduce a learning-based cost volume fusion
framework, namely RayFusion, that effectively leverages the attention mechanism
for each pair of overlapped rays in adjacent cost volumes. As a result of
leveraging feature statistics accumulated over time, our proposed framework
consistently outperforms or rivals state-of-the-art approaches on diverse
indoor and outdoor datasets, including the KITTI Depth Completion benchmark,
VOID Depth Completion benchmark, and ScanNetV2 dataset, using much fewer
network parameters.


## Kriformer A Novel Spatiotemporal Kriging Approach Based on Graph Transformers

>Authors: Renbin Pan, Feng Xiao, Hegui Zhang, Minyu Shen

>2024-09-23

> http://arxiv.org/abs/2409.14906v1

Accurately estimating data in sensor-less areas is crucial for understanding
system dynamics, such as traffic state estimation and environmental monitoring.
This study addresses challenges posed by sparse sensor deployment and
unreliable data by framing the problem as a spatiotemporal kriging task and
proposing a novel graph transformer model, Kriformer. This model estimates data
at locations without sensors by mining spatial and temporal correlations, even
with limited resources. Kriformer utilizes transformer architecture to enhance
the model's perceptual range and solve edge information aggregation challenges,
capturing spatiotemporal information effectively. A carefully constructed
positional encoding module embeds the spatiotemporal features of nodes, while a
sophisticated spatiotemporal attention mechanism enhances estimation accuracy.
The multi-head spatial interaction attention module captures subtle spatial
relationships between observed and unobserved locations. During training, a
random masking strategy prompts the model to learn with partial information
loss, allowing the spatiotemporal embedding and multi-head attention mechanisms
to synergistically capture correlations among locations. Experimental results
show that Kriformer excels in representation learning for unobserved locations,
validated on two real-world traffic speed datasets, demonstrating its
effectiveness in spatiotemporal kriging tasks.


## MICSim A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator

>Authors: Cong Wang, Zeming Chen, Shanshi Huang

>2024-09-23

> http://arxiv.org/abs/2409.14838v1

This work introduces MICSim, an open-source, pre-circuit simulator designed
for early-stage evaluation of chip-level software performance and hardware
overhead of mixed-signal compute-in-memory (CIM) accelerators. MICSim features
a modular design, allowing easy multi-level co-design and design space
exploration. Modularized from the state-of-the-art CIM simulator NeuroSim,
MICSim provides a highly configurable simulation framework supporting multiple
quantization algorithms, diverse circuit/architecture designs, and different
memory devices. This modular approach also allows MICSim to be effectively
extended to accommodate new designs.
  MICSim natively supports evaluating accelerators' software and hardware
performance for CNNs and Transformers in Python, leveraging the popular PyTorch
and HuggingFace Transformers frameworks. These capabilities make MICSim highly
adaptive when simulating different networks and user-friendly. This work
demonstrates that MICSim can easily be combined with optimization strategies to
perform design space exploration and used for chip-level Transformers CIM
accelerators evaluation. Also, MICSim can achieve a 9x - 32x speedup of
NeuroSim through a statistic-based average mode proposed by this work.


## Parse Trees Guided LLM Prompt Compression

>Authors: Wenhao Mao, Chengbin Hou, Tianyu Zhang, Xinyu Lin, Ke Tang, Hairong Lv

>2024-09-23

> http://arxiv.org/abs/2409.15395v1

Offering rich contexts to Large Language Models (LLMs) has shown to boost the
performance in various tasks, but the resulting longer prompt would increase
the computational cost and might exceed the input limit of LLMs. Recently, some
prompt compression methods have been suggested to shorten the length of prompts
by using language models to generate shorter prompts or by developing
computational models to select important parts of original prompt. The
generative compression methods would suffer from issues like hallucination,
while the selective compression methods have not involved linguistic rules and
overlook the global structure of prompt. To this end, we propose a novel
selective compression method called PartPrompt. It first obtains a parse tree
for each sentence based on linguistic rules, and calculates local information
entropy for each node in a parse tree. These local parse trees are then
organized into a global tree according to the hierarchical structure such as
the dependency of sentences, paragraphs, and sections. After that, the
root-ward propagation and leaf-ward propagation are proposed to adjust node
values over the global tree. Finally, a recursive algorithm is developed to
prune the global tree based on the adjusted node values. The experiments show
that PartPrompt receives the state-of-the-art performance across various
datasets, metrics, compression ratios, and target LLMs for inference. The
in-depth ablation studies confirm the effectiveness of designs in PartPrompt,
and other additional experiments also demonstrate its superiority in terms of
the coherence of compressed prompts and in the extreme long prompt scenario.


## ERPoT Effective and Reliable Pose Tracking for Mobile Robots Based on Lightweight and Compact Polygon Maps

>Authors: Haiming Gao, Qibo Qiu, Hongyan Liu, Dingkun Liang, Chaoqun Wang, Xuebo Zhang

>2024-09-23

> http://arxiv.org/abs/2409.14723v1

This paper presents an effective and reliable pose tracking solution termed
ERPoT for mobile robots operating in large-scale outdoor environments,
underpinned by an innovative prior polygon map. Especially, to overcome the
challenge that arises as the map size grows with the expansion of the
environment, the novel form of a prior map composed of multiple polygons is
proposed. Benefiting from the use of polygons to concisely and accurately
depict environmental occupancy, the prior polygon map achieves long-term
reliable pose tracking while ensuring a compact form. More importantly, pose
tracking is carried out under pure LiDAR mode, and the dense 3D point cloud is
transformed into a sparse 2D scan through ground removal and obstacle
selection. On this basis, a novel cost function for pose estimation through
point-polygon matching is introduced, encompassing two distinct constraint
forms: point-to-vertex and point-to-edge. In this study, our primary focus lies
on two crucial aspects: lightweight and compact prior map construction, as well
as effective and reliable robot pose tracking. Both aspects serve as the
foundational pillars for future navigation across different mobile platforms
equipped with different LiDAR sensors in different environments. Comparative
experiments based on the publicly available datasets and our self-recorded
datasets are conducted, and evaluation results show the superior performance of
ERPoT on reliability, prior map size, pose estimation error, and runtime over
the other five approaches. The corresponding code can be accessed at
https://github.com/ghm0819/ERPoT, and the supplementary video is at
https://youtu.be/cseml5FrW1Q.


## Harmonising the Clinical Melody Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding

>Authors: Bokang Bi, Leibo Liu, Sanja Lujic, Louisa Jorm, Oscar Perez-Concha

>2024-09-23

> http://arxiv.org/abs/2409.14638v2

The increasing volume and complexity of clinical documentation in Electronic
Medical Records systems pose significant challenges for clinical coders, who
must mentally process and summarise vast amounts of clinical text to extract
essential information needed for coding tasks. While large language models have
been successfully applied to shorter summarisation tasks in recent years, the
challenge of summarising a hospital course remains an open area for further
research and development. In this study, we adapted three pre trained LLMs,
Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course
summarisation task, using Quantized Low Rank Adaptation fine tuning. We created
a free text clinical dataset from MIMIC III data by concatenating various
clinical notes as the input clinical text, paired with ground truth Brief
Hospital Course sections extracted from the discharge summaries for model
training. The fine tuned models were evaluated using BERTScore and ROUGE
metrics to assess the effectiveness of clinical domain fine tuning.
Additionally, we validated their practical utility using a novel hospital
course summary assessment metric specifically tailored for clinical coding. Our
findings indicate that fine tuning pre trained LLMs for the clinical domain can
significantly enhance their performance in hospital course summarisation and
suggest their potential as assistive tools for clinical coding. Future work
should focus on refining data curation methods to create higher quality
clinical datasets tailored for hospital course summary tasks and adapting more
advanced open source LLMs comparable to proprietary models to further advance
this research.


## EQ-CBM A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors

>Authors: Sangwon Kim, Dasom Ahn, Byoung Chul Ko, In-su Jang, Kwang-Ju Kim

>2024-09-22

> http://arxiv.org/abs/2409.14630v1

The demand for reliable AI systems has intensified the need for interpretable
deep neural networks. Concept bottleneck models (CBMs) have gained attention as
an effective approach by leveraging human-understandable concepts to enhance
interpretability. However, existing CBMs face challenges due to deterministic
concept encoding and reliance on inconsistent concepts, leading to
inaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through
probabilistic concept encoding using energy-based models (EBMs) with quantized
concept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties,
thereby improving prediction reliability and accuracy. By employing qCAVs, our
method selects homogeneous vectors during concept encoding, enabling more
decisive task performance and facilitating higher levels of human intervention.
Empirical results using benchmark datasets demonstrate that our approach
outperforms the state-of-the-art in both concept and task accuracy.


## Patch Ranking Efficient CLIP by Learning to Rank Local Patches

>Authors: Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado

>2024-09-22

> http://arxiv.org/abs/2409.14607v1

Contrastive image-text pre-trained models such as CLIP have shown remarkable
adaptability to downstream tasks. However, they face challenges due to the high
computational requirements of the Vision Transformer (ViT) backbone. Current
strategies to boost ViT efficiency focus on pruning patch tokens but fall short
in addressing the multimodal nature of CLIP and identifying the optimal subset
of tokens for maximum performance. To address this, we propose greedy search
methods to establish a "Golden Ranking" and introduce a lightweight predictor
specifically trained to approximate this Ranking. To compensate for any
performance degradation resulting from token pruning, we incorporate learnable
visual tokens that aid in restoring and potentially enhancing the model's
performance. Our work presents a comprehensive and systematic investigation of
pruning tokens within the ViT backbone of CLIP models. Through our framework,
we successfully reduced 40% of patch tokens in CLIP's ViT while only suffering
a minimal average accuracy loss of 0.3 across seven datasets. Our study lays
the groundwork for building more computationally efficient multimodal models
without sacrificing their performance, addressing a key challenge in the
application of advanced vision-language models.


## A is for Absorption Studying Feature Splitting and Absorption in Sparse Autoencoders

>Authors: David Chanin, James Wilken-Smith, Tom Dulka, Hardik Bhatnagar, Joseph Bloom

>2024-09-22

> http://arxiv.org/abs/2409.14507v3

Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose
the activations of Large Language Models (LLMs) into human-interpretable
latents. In this paper, we pose two questions. First, to what extent do SAEs
extract monosemantic and interpretable latents? Second, to what extent does
varying the sparsity or the size of the SAE affect monosemanticity /
interpretability? By investigating these questions in the context of a simple
first-letter identification task where we have complete access to ground truth
labels for all tokens in the vocabulary, we are able to provide more detail
than prior investigations. Critically, we identify a problematic form of
feature-splitting we call feature absorption where seemingly monosemantic
latents fail to fire in cases where they clearly should. Our investigation
suggests that varying SAE size or sparsity is insufficient to solve this issue,
and that there are deeper conceptual issues in need of resolution.


## Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers

>Authors: Dominic Schneider, Lutz Rapp

>2024-09-22

> http://arxiv.org/abs/2409.14378v1

Optical fiber amplifiers are key elements in present optical networks.
Failures of these components result in high financial loss of income of the
network operator as the communication traffic over an affected link is
interrupted. Applying Remaining useful lifetime (RUL) prediction in the context
of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming
system failures at an early stage, so that network outages can be minimized
through planning of targeted maintenance actions, ensures reliability and
safety. Optical fiber amplifier are complex systems, that work under various
operating conditions, which makes correct forecasting a difficult task.
Increased monitoring capabilities of systems results in datasets that
facilitate the application of data-driven RUL prediction methods. Deep learning
models in particular have shown good performance, but generalization based on
comparatively small datasets for RUL prediction is difficult. In this paper, we
propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL
prediction method. SLAT is based on an encoder-decoder architecture, wherein
two parallel working encoders extract features for sensors and time steps. By
utilizing the self-attention mechanism, long-term dependencies can be learned
from long sequences. The implementation of sparsity in the attention matrix and
a low-rank parametrization reduce overfitting and increase generalization.
Experimental application to optical fiber amplifiers exemplified on EDFA, as
well as a reference dataset from turbofan engines, shows that SLAT outperforms
the state-of-the-art methods.


## Thinking in Granularity Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues

>Authors: Mingshen Wang, Zhao Zhang, Feng Li, Ke Xu, Kang Miao, Meng Wang

>2024-09-22

> http://arxiv.org/abs/2409.14330v1

Dynamic quantization has attracted rising attention in image super-resolution
(SR) as it expands the potential of heavy SR models onto mobile devices while
preserving competitive performance. Existing methods explore layer-to-bit
configuration upon varying local regions, adaptively allocating the bit to each
layer and patch. Despite the benefits, they still fall short in the trade-off
of SR accuracy and quantization efficiency. Apart from this, adapting the
quantization level for each layer individually can disturb the original
inter-layer relationships, thus diminishing the representation capability of
quantized models. In this work, we propose Granular-DQ, which capitalizes on
the intrinsic characteristics of images while dispensing with the previous
consideration for layer sensitivity in quantization. Granular-DQ conducts a
multi-granularity analysis of local patches with further exploration of their
information densities, achieving a distinctive patch-wise and layer-invariant
dynamic quantization paradigm. Specifically, Granular-DQ initiates by
developing a granularity-bit controller (GBC) to apprehend the coarse-to-fine
granular representations of different patches, matching their proportional
contribution to the entire image to determine the proper bit-width allocation.
On this premise, we investigate the relation between bit-width and information
density, devising an entropy-to-bit (E2B) mechanism that enables further
fine-grained dynamic bit adaption of high-bit patches. Extensive experiments
validate the superiority and generalization ability of Granular-DQ over recent
state-of-the-art methods on various SR models. Code will be available at
\url{https://github.com/MmmingS/Granular-DQ.git}.


## Enhanced DOA Estimation via Hybrid Massive MIMO Receive Array with Switches-based Sparse Architecture

>Authors: Yifan Li, Feng Shu, Yaoliang Song, Jiangzhou Wang

>2024-09-22

> http://arxiv.org/abs/2409.14297v1

Hybrid massive arrays have been widely used in direction of arrival (DOA)
estimation for it can provide larger aperture with lower hardware complexity.
However, as the signals received by a hybrid array are compressed by the phase
shifter network or the switch network, the degree of freedom (DOF) or spatial
resolution of hybrid array is lower than fully-digital (FD) array with same
number of antennas. Therefore, we develop a novel sparse hybrid array called
switches-based sparse hybrid array (SW-SHA) which by combining nested array and
switches-based hybrid array to achieve a huge improvement on DOF over
traditional hybrid arrays. Simulations of the spatial spectrums verify that
SW-SHA can accurately solve the problem of DOA estimation with the number of
signal sources much larger than the number of RF chains. Finally, to further
improve the accuracy of DOA estimation for SW-SHA, MMV-SW-SHA is proposed by
transforming the single-snapshot co-array signal into MMV form. The simulation
results also show that MMV-SW-SHA has better performance than SW-SHA when
signal-to-noise ratio (SNR) is low.


## On Importance of Pruning and Distillation for Efficient Low Resource NLP

>Authors: Aishwarya Mirashi, Purva Lingayat, Srushti Sonavane, Tejas Padhiyar, Raviraj Joshi, Geetanjali Kale

>2024-09-21

> http://arxiv.org/abs/2409.14162v1

The rise of large transformer models has revolutionized Natural Language
Processing, leading to significant advances in tasks like text classification.
However, this progress demands substantial computational resources, escalating
training duration, and expenses with larger model sizes. Efforts have been made
to downsize and accelerate English models (e.g., Distilbert, MobileBert). Yet,
research in this area is scarce for low-resource languages.
  In this study, we explore the case of the low-resource Indic language
Marathi. Leveraging the marathi-topic-all-doc-v2 model as our baseline, we
implement optimization techniques to reduce computation time and memory usage.
Our focus is on enhancing the efficiency of Marathi transformer models while
maintaining top-tier accuracy and reducing computational demands. Using the
MahaNews document classification dataset and the marathi-topic-all-doc-v2 model
from L3Cube, we apply Block Movement Pruning, Knowledge Distillation, and Mixed
Precision methods individually and in combination to boost efficiency. We
demonstrate the importance of strategic pruning levels in achieving desired
efficiency gains. Furthermore, we analyze the balance between efficiency
improvements and environmental impact, highlighting how optimized model
architectures can contribute to a more sustainable computational ecosystem.
Implementing these techniques on a single GPU system, we determine that the
optimal configuration is 25\% pruning + knowledge distillation. This approach
yielded a 2.56x speedup in computation time while maintaining baseline accuracy
levels.


## Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis

>Authors: Zeping Yu, Sophia Ananiadou

>2024-09-21

> http://arxiv.org/abs/2409.14144v1

We find arithmetic ability resides within a limited number of attention
heads, with each head specializing in distinct operations. To delve into the
reason, we introduce the Comparative Neuron Analysis (CNA) method, which
identifies an internal logic chain consisting of four distinct stages from
input to prediction: feature enhancing with shallow FFN neurons, feature
transferring by shallow attention layers, feature predicting by arithmetic
heads, and prediction enhancing among deep FFN neurons. Moreover, we identify
the human-interpretable FFN neurons within both feature-enhancing and
feature-predicting stages. These findings lead us to investigate the mechanism
of LoRA, revealing that it enhances prediction probabilities by amplifying the
coefficient scores of FFN neurons related to predictions. Finally, we apply our
method in model pruning for arithmetic tasks and model editing for reducing
gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.


## ProTEA Programmable Transformer Encoder Acceleration on FPGA

>Authors: Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang

>2024-09-21

> http://arxiv.org/abs/2409.13975v1

Transformer neural networks (TNN) have been widely utilized on a diverse
range of applications, including natural language processing (NLP), machine
translation, and computer vision (CV). Their widespread adoption has been
primarily driven by the exceptional performance of their multi-head
self-attention block used to extract key features from sequential data. The
multi-head self-attention block is followed by feedforward neural networks,
which play a crucial role in introducing non-linearity to assist the model in
learning complex patterns. Despite the popularity of TNNs, there has been
limited numbers of hardware accelerators targeting these two critical blocks.
Most prior works have concentrated on sparse architectures that are not
flexible for popular TNN variants. This paper introduces \textit{ProTEA}, a
runtime programmable accelerator tailored for the dense computations of most of
state-of-the-art transformer encoders. \textit{ProTEA} is designed to reduce
latency by maximizing parallelism. We introduce an efficient tiling of large
matrices that can distribute memory and computing resources across different
hardware components within the FPGA. We provide run time evaluations of
\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator
card. Experimental results demonstrate that \textit{ProTEA} can host a wide
range of popular transformer networks and achieve near optimal performance with
a tile size of 64 in the multi-head self-attention block and 6 in the
feedforward networks block when configured with 8 parallel attention heads, 12
layers, and an embedding dimension of 768 on the U55C. Comparative results are
provided showing \textit{ProTEA} is 2.5$\times$ faster than an NVIDIA Titan XP
GPU. Results also show that it achieves 1.3 -- 2.8$\times$ speed up compared
with current state-of-the-art custom designed FPGA accelerators.


## Periodic micromagnetic finite element method

>Authors: Fangzhou Ai, Jiawei Duan, Vitaliy Lomakin

>2024-09-21

> http://arxiv.org/abs/2409.13958v1

Periodic micromagnetic finite element method (PM-FEM) is introduced to solve
periodic unit cell problems using the Landau-Lifshitz-Gilbert equation. PM-FEM
is applicable to general problems with 1D, 2D, and 3D periodicities. PM-FEM is
based on a non-periodic FEM-based micromagnetic solver and extends it in
several aspects to account for periodicities, including the computation of
exchange and magnetostatic fields. For the exchange field, PM-FEM modifies the
sparse matrix construction for computing the Laplace operator to include
additional elements arising due to the periodicities. For the magnetostatic
field, the periodic extensions include modifications in the local operators,
such as gradient, divergence, and surface magnetic charges as well as the
long-range superposition operator for computing the periodic scalar potential.
The local operators are extended to account for the periodicities similar to
handling the Laplace operator. For the long-range superposition operator,
PM-FEM utilizes a periodic Green's function (PGF) and fast spatial
convolutions. The PGF is computed rapidly via exponentially rapidly convergent
sums. The spatial convolutions are accomplished via a modified fast Fourier
transform based adaptive integral method that allows calculating spatial
convolutions with non-uniform meshes in $O(N\log N)$ numerical operations.
PM-FEM is implemented on CPU and GPU based computer architectures. PM-FEM
allows efficiently handling cases of structures contained withing the periodic
unit cell touching or not touching its boundaries as well as structures that
protrude beyond the unit cell boundaries. PM-FEM is demonstrated to have about
the same or even higher performance than its parent non-periodic code. The
demonstrated numerical examples show the efficiency of PM-FEM for highly
complex structures with 1D, 2D, and 3D periodicities.


## TalkMosaic Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions

>Authors: Kevin Li, Fulu Li

>2024-09-20

> http://arxiv.org/abs/2409.13941v1

We use images of cars of a wide range of varieties to compose an image of an
animal such as a bird or a lion for the theme of environmental protection to
maximize the information about cars in a single composed image and to raise the
awareness about environmental challenges. We present a novel way of image
interaction with an artistically-composed photomosaic image, in which a simple
operation of "click and display" is used to demonstrate the interactive switch
between a tile image in a photomosaic image and the corresponding original car
image, which will be automatically saved on the Desktop. We build a multimodal
custom GPT named TalkMosaic by incorporating car images information and the
related knowledge to ChatGPT. By uploading the original car image to
TalkMosaic, we can ask questions about the given car image and get the
corresponding answers efficiently and effectively such as where to buy the tire
in the car image that satisfies high environmental standards. We give an
in-depth analysis on how to speed up the inference of multimodal LLM using
sparse attention and quantization techniques with presented probabilistic
FlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)
methods. The implemented prototype demonstrates the feasibility and
effectiveness of the presented approach.


## OATS Outlier-Aware Pruning Through Sparse and Low Rank Decomposition

>Authors: Stephen Zhang, Vardan Papyan

>2024-09-20

> http://arxiv.org/abs/2409.13652v1

The recent paradigm shift to large-scale foundation models has brought about
a new era for deep learning that, while has found great success in practice,
has also been plagued by prohibitively expensive costs in terms of high memory
consumption and compute. To mitigate these issues, there has been a concerted
effort in post-hoc neural network pruning techniques that do not require costly
retraining. Despite the considerable progress being made, existing methods
often exhibit a steady drop in model performance as the compression increases.
In this paper, we present a novel approach to compressing large transformers,
coined OATS, that utilizes the second moment information in the input
embeddings to decompose the model weights into a sum of sparse and low-rank
matrices. Without any retraining, OATS achieves state-of-the-art performance
when compressing models by up to $60\%$ on large language models such as
Llama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while
delivering up to $1.37\times$ the CPU acceleration versus a model that was
comparably pruned.


## Tackling fluffy clouds field boundaries detection using time series of S2 and/or S1 imagery

>Authors: Foivos I. Diakogiannis, Zheng-Shu Zhou, Jeff Wang, Gonzalo Mata, Dave Henry, Roger Lawes, Amy Parker, Peter Caccetta, Rodrigo Ibata, Ondrej Hlinka, Jonathan Richetti, Kathryn Batchelor, Chris Herrmann, Andrew Toovey, John Taylor

>2024-09-20

> http://arxiv.org/abs/2409.13568v1

Accurate field boundary delineation is a critical challenge in digital
agriculture, impacting everything from crop monitoring to resource management.
Existing methods often struggle with noise and fail to generalize across varied
landscapes, particularly when dealing with cloud cover in optical remote
sensing. In response, this study presents a new approach that leverages time
series data from Sentinel-2 (S2) and Sentinel-1 (S1) imagery to improve
performance under diverse cloud conditions, without the need for manual cloud
filtering. We introduce a 3D Vision Transformer architecture specifically
designed for satellite image time series, incorporating a memory-efficient
attention mechanism. Two models are proposed: PTAViT3D, which handles either S2
or S1 data independently, and PTAViT3D-CA, which fuses both datasets to enhance
accuracy. Both models are evaluated under sparse and dense cloud coverage by
exploiting spatio-temporal correlations. Our results demonstrate that the
models can effectively delineate field boundaries, even with partial (S2 or S2
and S1 data fusion) or dense cloud cover (S1), with the S1-based model
providing performance comparable to S2 imagery in terms of spatial resolution.
A key strength of this approach lies in its capacity to directly process
cloud-contaminated imagery by leveraging spatio-temporal correlations in a
memory-efficient manner. This methodology, used in the ePaddocks product to map
Australia's national field boundaries, offers a robust, scalable solution
adaptable to varying agricultural environments, delivering precision and
reliability where existing methods falter. Our code is available at
https://github.com/feevos/tfcl.


## Data Augmentation for Sequential Recommendation A Survey

>Authors: Yizhou Dang, Enneng Yang, Yuting Liu, Guibing Guo, Linying Jiang, Jianzhe Zhao, Xingwei Wang

>2024-09-20

> http://arxiv.org/abs/2409.13545v1

As an essential branch of recommender systems, sequential recommendation (SR)
has received much attention due to its well-consistency with real-world
situations. However, the widespread data sparsity issue limits the SR model's
performance. Therefore, researchers have proposed many data augmentation (DA)
methods to mitigate this phenomenon and have achieved impressive progress. In
this survey, we provide a comprehensive review of DA methods for SR. We start
by introducing the research background and motivation. Then, we categorize
existing methodologies regarding their augmentation principles, objects, and
purposes. Next, we present a comparative discussion of their advantages and
disadvantages, followed by the exhibition and analysis of representative
experimental results. Finally, we outline directions for future research and
summarize this survey. We also maintain a repository with a paper list at
\url{https://github.com/KingGugu/DA-CL-4Rec}.


## Generalizing Deep Learning-Based CSI Feedback in Massive MIMO via ID-Photo-Inspired Preprocessing

>Authors: Zhenyu Liu, Yi Ma, Rahim Tafazolli

>2024-09-20

> http://arxiv.org/abs/2409.13494v1

Deep learning (DL)-based channel state information (CSI) feedback has shown
great potential in improving spectrum efficiency in massive MIMO systems.
However, DL models optimized for specific environments often experience
performance degradation in others due to model mismatch. To overcome this
barrier in the practical deployment, we propose UniversalNet, an
ID-photo-inspired universal CSI feedback framework that enhances model
generalizability by standardizing the input format across diverse data
distributions. Specifically, UniversalNet employs a standardized input format
to mitigate the influence of environmental variability, coupled with a
lightweight sparsity-aligning operation in the transformed sparse domain and
marginal control bits for original format recovery. This enables seamless
integration with existing CSI feedback models, requiring minimal modifications
in preprocessing and postprocessing without updating neural network weights.
Furthermore, we propose a simple yet effective eigenvector joint phase
optimization technique to enhance the sparsity of the precoding matrix by
reducing phase randomness, thus improving the implicit CSI compression
efficiency. Test results demonstrate that UniversalNet effectively improves
generalization and ensures precise CSI feedback, even in scenarios with limited
training diversity and previously unseen CSI environments.


## Selective Exploration and Information Gathering in Search and Rescue Using Hierarchical Learning Guided by Natural Language Input

>Authors: Dimitrios Panagopoulos, Adolfo Perrusquia, Weisi Guo

>2024-09-20

> http://arxiv.org/abs/2409.13445v1

In recent years, robots and autonomous systems have become increasingly
integral to our daily lives, offering solutions to complex problems across
various domains. Their application in search and rescue (SAR) operations,
however, presents unique challenges. Comprehensively exploring the
disaster-stricken area is often infeasible due to the vastness of the terrain,
transformed environment, and the time constraints involved. Traditional robotic
systems typically operate on predefined search patterns and lack the ability to
incorporate and exploit ground truths provided by human stakeholders, which can
be the key to speeding up the learning process and enhancing triage. Addressing
this gap, we introduce a system that integrates social interaction via large
language models (LLMs) with a hierarchical reinforcement learning (HRL)
framework. The proposed system is designed to translate verbal inputs from
human stakeholders into actionable RL insights and adjust its search strategy.
By leveraging human-provided information through LLMs and structuring task
execution through HRL, our approach not only bridges the gap between autonomous
capabilities and human intelligence but also significantly improves the agent's
learning efficiency and decision-making process in environments characterised
by long horizons and sparse rewards.


## T2M-X Learning Expressive Text-to-Motion Generation from Partially Annotated Data

>Authors: Mingdian Liu, Yilin Liu, Gurunandan Krishnan, Karl S Bayer, Bing Zhou

>2024-09-20

> http://arxiv.org/abs/2409.13251v1

The generation of humanoid animation from text prompts can profoundly impact
animation production and AR/VR experiences. However, existing methods only
generate body motion data, excluding facial expressions and hand movements.
This limitation, primarily due to a lack of a comprehensive whole-body motion
dataset, inhibits their readiness for production use. Recent attempts to create
such a dataset have resulted in either motion inconsistency among different
body parts in the artificially augmented data or lower quality in the data
extracted from RGB videos. In this work, we propose T2M-X, a two-stage method
that learns expressive text-to-motion generation from partially annotated data.
T2M-X trains three separate Vector Quantized Variational AutoEncoders (VQ-VAEs)
for body, hand, and face on respective high-quality data sources to ensure
high-quality motion outputs, and a Multi-indexing Generative Pretrained
Transformer (GPT) model with motion consistency loss for motion generation and
coordination among different body parts. Our results show significant
improvements over the baselines both quantitatively and qualitatively,
demonstrating its robustness against the dataset limitations.


## Neural-Symbolic Collaborative Distillation Advancing Small Language Models for Complex Reasoning Tasks

>Authors: Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Jun Zhao

>2024-09-20

> http://arxiv.org/abs/2409.13203v1

In this paper, we propose $\textbf{Ne}$ural-$\textbf{Sy}$mbolic
$\textbf{C}$ollaborative $\textbf{D}$istillation ($\textbf{NesyCD}$), a novel
knowledge distillation method for learning the complex reasoning abilities of
Large Language Models (LLMs, e.g., \textgreater 13B). We argue that complex
reasoning tasks are difficult for Small Language Models (SLMs, e.g., $\leq$
7B), as these tasks demand not only general cognitive abilities but also
specialized knowledge, which is often sparse and difficult for these
neural-based SLMs to effectively capture. Therefore, NesyCD distills the
general capabilities and specialized knowledge in LLMs using different manners.
On the one hand, we distill only general abilities from teacher LLMs into the
student SLMs of parameterized neural networks. On the other hand, for the
specialized abilities and uncommon knowledge of a complex reasoning task, we
employ a symbolic knowledge distillation approach to obtain and store the
specialized knowledge within a symbolic knowledge base (KB). By decoupling
general and specialized capabilities, the proposed NesyCD can achieve superior
performance cost-effectively, utilizing smaller models and blending
parameterized neural networks with symbolic KB. Moreover, the specialized KB
generalizes well and is comprehended and manipulated by humans. Our experiments
show that NesyCD significantly boosts SLMs' complex reasoning performance on
in-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our
approach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in
performance and come close to matching LLaMA3-70B, despite the latter having
nine times more parameters. Our code will be available at
https://github.com/Xnhyacinth/NesyCD.


## CFSP An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information

>Authors: Yuxin Wang, Minghua Ma, Zekun Wang, Jingchang Chen, Huiming Fan, Liping Shan, Qing Yang, Dongliang Xu, Ming Liu, Bing Qin

>2024-09-20

> http://arxiv.org/abs/2409.13199v1

The colossal parameters and computational overhead of Large Language Models
(LLMs) challenge their real-world applications. Network pruning, which targets
unstructured or structured sparsity by removing redundant parameters, has
recently been explored for LLM acceleration. Existing LLM pruning works focus
on unstructured pruning, which typically requires special hardware support for
a practical speed-up. In contrast, structured pruning can reduce latency on
general devices. However, it remains a challenge to perform structured pruning
efficiently and maintain performance, especially at high sparsity ratios. To
this end, we introduce an efficient structured pruning framework named CFSP,
which leverages both Coarse (interblock) and Fine-grained (intrablock)
activation information as an importance criterion to guide pruning. The pruning
is highly efficient, as it only requires one forward pass to compute feature
activations. Specifically, we first allocate the sparsity budget across blocks
based on their importance and then retain important weights within each block.
In addition, we introduce a recovery fine-tuning strategy that adaptively
allocates training overhead based on coarse-grained importance to further
improve performance. Experimental results demonstrate that CFSP outperforms
existing methods on diverse models across various sparsity budgets. Our code
will be available at https://github.com/wyxscir/CFSP.


## Learning to Compare Hardware Designs for High-Level Synthesis

>Authors: Yunsheng Bai, Atefeh Sohrabizadeh, Zijian Ding, Rongjian Liang, Weikai Li, Ding Wang, Haoxing Ren, Yizhou Sun, Jason Cong

>2024-09-20

> http://arxiv.org/abs/2409.13138v1

High-level synthesis (HLS) is an automated design process that transforms
high-level code into hardware designs, enabling the rapid development of
hardware accelerators. HLS relies on pragmas, which are directives inserted
into the source code to guide the synthesis process, and pragmas have various
settings and values that significantly impact the resulting hardware design.
State-of-the-art ML-based HLS methods, such as HARP, first train a deep
learning model, typically based on graph neural networks (GNNs) applied to
graph-based representations of the source code and pragmas. They then perform
design space exploration (DSE) to explore the pragma design space, rank
candidate designs using the model, and return the top designs. However,
traditional DSE methods face challenges due to the highly nonlinear
relationship between pragma settings and performance metrics, along with
complex interactions between pragmas that affect performance in non-obvious
ways.
  To address these challenges, we propose compareXplore, a novel approach that
learns to compare hardware designs for effective HLS optimization.
CompareXplore introduces a hybrid loss function that combines pairwise
preference learning with pointwise performance prediction, enabling the model
to capture both relative preferences and absolute performance. Moreover, we
introduce a novel node difference attention module that focuses on the most
informative differences between designs, enabling the model to identify
critical pragmas impacting performance. CompareXplore adopts a two-stage DSE,
where a pointwise prediction model is used for the initial design pruning,
followed by a pairwise comparison stage for precise performance verification.
In extensive experiments, compareXplore achieves significant improvements in
ranking metrics and generates high-quality HLS results for the selected
designs, outperforming the existing SOTA method.

