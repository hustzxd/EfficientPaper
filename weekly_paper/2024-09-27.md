# 2024-09-27

# Table of Contents
* [Accumulator-Aware Post-Training Quantization](#Accumulator-Aware-Post-Training-Quantization)
* [VPTQ Extreme Low-bit Vector Post-Training Quantization for Large Language Models](#VPTQ-Extreme-Low-bit-Vector-Post-Training-Quantization-for-Large-Language-Models)
* [INT-FlashAttention Enabling Flash Attention for INT8 Quantization](#INT-FlashAttention-Enabling-Flash-Attention-for-INT8-Quantization)
* [LLaMa-SciQ An Educational Chatbot for Answering Science MCQ](#LLaMa-SciQ-An-Educational-Chatbot-for-Answering-Science-MCQ)
* [A Novel Framework for Analyzing Structural Transformation in Data-Constrained Economies Using Bayesian Modeling and Machine Learning](#A-Novel-Framework-for-Analyzing-Structural-Transformation-in-Data-Constrained-Economies-Using-Bayesian-Modeling-and-Machine-Learning)
* [RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems](#RoleBreak-Character-Hallucination-as-a-Jailbreak-Attack-in-Role-Playing-Systems)
* [A Survey of Low-bit Large Language Models Basics, Systems, and Algorithms](#A-Survey-of-Low-bit-Large-Language-Models-Basics,-Systems,-and-Algorithms)
* [Pre-trained Graphformer-based Ranking at Web-scale Search (Extended Abstract)](#Pre-trained-Graphformer-based-Ranking-at-Web-scale-Search-(Extended-Abstract))
* [AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization](#AlignedKV-Reducing-Memory-Access-of-KV-Cache-with-Precision-Aligned-Quantization)
* [Underground Mapping and Localization Based on Ground-Penetrating Radar](#Underground-Mapping-and-Localization-Based-on-Ground-Penetrating-Radar)
* [MaskBit Embedding-free Image Generation via Bit Tokens](#MaskBit-Embedding-free-Image-Generation-via-Bit-Tokens)
* [Time-MoE Billion-Scale Time Series Foundation Models with Mixture of Experts](#Time-MoE-Billion-Scale-Time-Series-Foundation-Models-with-Mixture-of-Experts)
* [GraphGIA GNN Explanation Method using Game Interaction](#GraphGIA-GNN-Explanation-Method-using-Game-Interaction)
* [Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI](#Data-Augmentation-for-Sparse-Multidimensional-Learning-Performance-Data-Using-Generative-AI)
* [CSPS A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts](#CSPS-A-Communication-Efficient-Sequence-Parallelism-based-Serving-System-for-Transformer-based-Models-with-Long-Prompts)
* [Efficiently Dispatching Flash Attention For Partially Filled Attention Masks](#Efficiently-Dispatching-Flash-Attention-For-Partially-Filled-Attention-Masks)
* [Cucheb A GPU implementation of the filtered Lanczos procedure](#Cucheb-A-GPU-implementation-of-the-filtered-Lanczos-procedure)
* [Deep Cost Ray Fusion for Sparse Depth Video Completion](#Deep-Cost-Ray-Fusion-for-Sparse-Depth-Video-Completion)
* [Kriformer A Novel Spatiotemporal Kriging Approach Based on Graph Transformers](#Kriformer-A-Novel-Spatiotemporal-Kriging-Approach-Based-on-Graph-Transformers)
* [MICSim A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator](#MICSim-A-Modular-Simulator-for-Mixed-signal-Compute-in-Memory-based-AI-Accelerator)
* [Parse Trees Guided LLM Prompt Compression](#Parse-Trees-Guided-LLM-Prompt-Compression)
* [ERPoT Effective and Reliable Pose Tracking for Mobile Robots Based on Lightweight and Compact Polygon Maps](#ERPoT-Effective-and-Reliable-Pose-Tracking-for-Mobile-Robots-Based-on-Lightweight-and-Compact-Polygon-Maps)
* [Harmonising the Clinical Melody Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding](#Harmonising-the-Clinical-Melody-Tuning-Large-Language-Models-for-Hospital-Course-Summarisation-in-Clinical-Coding)
* [EQ-CBM A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors](#EQ-CBM-A-Probabilistic-Concept-Bottleneck-with-Energy-based-Models-and-Quantized-Vectors)
* [Patch Ranking Efficient CLIP by Learning to Rank Local Patches](#Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches)
* [A is for Absorption Studying Feature Splitting and Absorption in Sparse Autoencoders](#A-is-for-Absorption-Studying-Feature-Splitting-and-Absorption-in-Sparse-Autoencoders)
* [Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers](#Sparse-Low-Ranked-Self-Attention-Transformer-for-Remaining-Useful-Lifetime-Prediction-of-Optical-Fiber-Amplifiers)
* [Thinking in Granularity Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues](#Thinking-in-Granularity-Dynamic-Quantization-for-Image-Super-Resolution-by-Intriguing-Multi-Granularity-Clues)
* [Enhanced DOA Estimation via Hybrid Massive MIMO Receive Array with Switches-based Sparse Architecture](#Enhanced-DOA-Estimation-via-Hybrid-Massive-MIMO-Receive-Array-with-Switches-based-Sparse-Architecture)
* [On Importance of Pruning and Distillation for Efficient Low Resource NLP](#On-Importance-of-Pruning-and-Distillation-for-Efficient-Low-Resource-NLP)
* [Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis](#Interpreting-Arithmetic-Mechanism-in-Large-Language-Models-through-Comparative-Neuron-Analysis)
* [ProTEA Programmable Transformer Encoder Acceleration on FPGA](#ProTEA-Programmable-Transformer-Encoder-Acceleration-on-FPGA)
* [Periodic micromagnetic finite element method](#Periodic-micromagnetic-finite-element-method)


## Accumulator-Aware Post-Training Quantization

>Authors: Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab

>2024-09-25

> http://arxiv.org/abs/2409.17092v1

Several recent studies have investigated low-precision accumulation,
reporting improvements in throughput, power, and area across various platforms.
However, the accompanying proposals have only considered the quantization-aware
training (QAT) paradigm, in which models are fine-tuned or trained from scratch
with quantization in the loop. As models continue to grow in size, QAT
techniques become increasingly more expensive, which has motivated the recent
surge in post-training quantization (PTQ) research. To the best of our
knowledge, ours marks the first formal study of accumulator-aware quantization
in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework
of accumulator-aware extensions designed to endow overflow avoidance guarantees
to existing layer-wise PTQ algorithms. We theoretically motivate AXE and
demonstrate its flexibility by implementing it on top of two state-of-the-art
PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage
accumulation for the first time, opening the door for full datapath
optimization and scaling to large language models (LLMs). We evaluate AXE
across image classification and language generation models, and observe
significant improvements in the trade-off between accumulator bit width and
model accuracy over baseline methods.


## VPTQ Extreme Low-bit Vector Post-Training Quantization for Large Language Models

>Authors: Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang

>2024-09-25

> http://arxiv.org/abs/2409.17066v1

Scaling model size significantly challenges the deployment and inference of
Large Language Models (LLMs). Due to the redundancy in LLM weights, recent
research has focused on pushing weight-only quantization to extremely low-bit
(even down to 2 bits). It reduces memory requirements, optimizes storage costs,
and decreases memory bandwidth needs during inference. However, due to
numerical representation limitations, traditional scalar-based weight
quantization struggles to achieve such extreme low-bit. Recent research on
Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely
low-bit model quantization by compressing vectors into indices using lookup
tables.
  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for
extremely low-bit quantization of LLMs. We use Second-Order Optimization to
formulate the LLM VQ problem and guide our quantization algorithm design by
solving the optimization. We further refine the weights using
Channel-Independent Second-Order Optimization for a granular VQ. In addition,
by decomposing the optimization problem, we propose a brief and effective
codebook initialization algorithm. We also extend VPTQ to support residual and
outlier quantization, which enhances model accuracy and further compresses the
model. Our experimental results show that VPTQ reduces model quantization
perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,
$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy
improvement of $0.79$-$1.5\%$ on LLaMA-2, $1\%$ on Mistral-7B, $11$-$22\%$ on
LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\%$ of the
quantization algorithm execution time, resulting in a $1.6$-$1.8\times$
increase in inference throughput compared to SOTA.


## INT-FlashAttention Enabling Flash Attention for INT8 Quantization

>Authors: Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Yuhan Wu, Lei Su, Tong Yang

>2024-09-25

> http://arxiv.org/abs/2409.16997v2

As the foundation of large language models (LLMs), self-attention module
faces the challenge of quadratic time and memory complexity with respect to
sequence length. FlashAttention accelerates attention computation and reduces
its memory usage by leveraging the GPU memory hierarchy. A promising research
direction is to integrate FlashAttention with quantization methods. This paper
introduces INT-FlashAttention, the first INT8 quantization architecture
compatible with the forward workflow of FlashAttention, which significantly
improves the inference speed of FlashAttention on Ampere GPUs. We implement our
INT-FlashAttention prototype with fully INT8 activations and general
matrix-multiplication (GEMM) kernels, making it the first attention operator
with fully INT8 input. As a general token-level post-training quantization
framework, INT-FlashAttention is also compatible with other data formats like
INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster
inference speed and 82% smaller quantization error compared to standard
FlashAttention with FP16 and FP8 data format.


## LLaMa-SciQ An Educational Chatbot for Answering Science MCQ

>Authors: Marc-Antoine Allard, Matin Ansaripour, Maria Yuffa, Paul Teiletche

>2024-09-25

> http://arxiv.org/abs/2409.16779v1

Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements.


## A Novel Framework for Analyzing Structural Transformation in Data-Constrained Economies Using Bayesian Modeling and Machine Learning

>Authors: Ronald Katende

>2024-09-25

> http://arxiv.org/abs/2409.16738v1

Structural transformation, the shift from agrarian economies to more
diversified industrial and service-based systems, is a key driver of economic
development. However, in low- and middle-income countries (LMICs), data
scarcity and unreliability hinder accurate assessments of this process. This
paper presents a novel statistical framework designed to address these
challenges by integrating Bayesian hierarchical modeling, machine
learning-based data imputation, and factor analysis. The framework is
specifically tailored for conditions of data sparsity and is capable of
providing robust insights into sectoral productivity and employment shifts
across diverse economies. By utilizing Bayesian models, uncertainties in data
are effectively managed, while machine learning techniques impute missing data
points, ensuring the integrity of the analysis. Factor analysis reduces the
dimensionality of complex datasets, distilling them into core economic
structures. The proposed framework has been validated through extensive
simulations, demonstrating its ability to predict structural changes even when
up to 60\% of data is missing. This approach offers policymakers and
researchers a valuable tool for making informed decisions in environments where
data quality is limited, contributing to the broader understanding of economic
development in LMICs.


## RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems

>Authors: Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou

>2024-09-25

> http://arxiv.org/abs/2409.16727v1

Role-playing systems powered by large language models (LLMs) have become
increasingly influential in emotional communication applications. However,
these systems are susceptible to character hallucinations, where the model
deviates from predefined character roles and generates responses that are
inconsistent with the intended persona. This paper presents the first
systematic analysis of character hallucination from an attack perspective,
introducing the RoleBreak framework. Our framework identifies two core
mechanisms-query sparsity and role-query conflict-as key factors driving
character hallucination. Leveraging these insights, we construct a novel
dataset, RoleBreakEval, to evaluate existing hallucination mitigation
techniques. Our experiments reveal that even enhanced models trained to
minimize hallucination remain vulnerable to attacks. To address these
vulnerabilities, we propose a novel defence strategy, the Narrator Mode, which
generates supplemental context through narration to mitigate role-query
conflicts and improve query generalization. Experimental results demonstrate
that Narrator Mode significantly outperforms traditional refusal-based
strategies by reducing hallucinations, enhancing fidelity to character roles
and queries, and improving overall narrative coherence.


## A Survey of Low-bit Large Language Models Basics, Systems, and Algorithms

>Authors: Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu

>2024-09-25

> http://arxiv.org/abs/2409.16694v1

Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.


## Pre-trained Graphformer-based Ranking at Web-scale Search (Extended Abstract)

>Authors: Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, Dawei Yin

>2024-09-25

> http://arxiv.org/abs/2409.16590v1

Both Transformer and Graph Neural Networks (GNNs) have been employed in the
domain of learning to rank (LTR). However, these approaches adhere to two
distinct yet complementary problem formulations: ranking score regression based
on query-webpage pairs, and link prediction within query-webpage bipartite
graphs, respectively. While it is possible to pre-train GNNs or Transformers on
source datasets and subsequently fine-tune them on sparsely annotated LTR
datasets, the distributional shifts between the pair-based and bipartite graph
domains present significant challenges in integrating these heterogeneous
models into a unified LTR framework at web scale. To address this, we introduce
the novel MPGraf model, which leverages a modular and capsule-based
pre-training strategy, aiming to cohesively integrate the regression
capabilities of Transformers with the link prediction strengths of GNNs. We
conduct extensive offline and online experiments to rigorously evaluate the
performance of MPGraf.


## AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization

>Authors: Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng

>2024-09-25

> http://arxiv.org/abs/2409.16546v1

Model quantization has become a crucial technique to address the issues of
large memory consumption and long inference times associated with LLMs.
Mixed-precision quantization, which distinguishes between important and
unimportant parameters, stands out among numerous quantization schemes as it
achieves a balance between precision and compression rate. However, existing
approaches can only identify important parameters through qualitative analysis
and manual experiments without quantitatively analyzing how their importance is
determined. We propose a new criterion, so-called 'precision alignment', to
build a quantitative framework to holistically evaluate the importance of
parameters in mixed-precision quantization. Our observations on floating point
addition under various real-world scenarios suggest that two addends should
have identical precision, otherwise the information in the higher-precision
number will be wasted. Such an observation offers an essential principle to
determine the precision of each parameter in matrix multiplication operation.
As the first step towards applying the above discovery to large model
inference, we develop a dynamic KV-Cache quantization technique to effectively
reduce memory access latency. Different from existing quantization approaches
that focus on memory saving, this work directly aims to accelerate LLM
inference through quantifying floating numbers. The proposed technique attains
a 25% saving of memory access and delivers up to 1.3x speedup in the
computation of attention in the decoding phase of LLM, with almost no loss of
precision.


## Underground Mapping and Localization Based on Ground-Penetrating Radar

>Authors: Jinchang Zhang, Guoyu Lu

>2024-09-24

> http://arxiv.org/abs/2409.16446v1

3D object reconstruction based on deep neural networks has gained increasing
attention in recent years. However, 3D reconstruction of underground objects to
generate point cloud maps remains a challenge. Ground Penetrating Radar (GPR)
is one of the most powerful and extensively used tools for detecting and
locating underground objects such as plant root systems and pipelines, with its
cost-effectiveness and continuously evolving technology. This paper introduces
a parabolic signal detection network based on deep convolutional neural
networks, utilizing B-scan images from GPR sensors. The detected keypoints can
aid in accurately fitting parabolic curves used to interpret the original GPR
B-scan images as cross-sections of the object model. Additionally, a multi-task
point cloud network was designed to perform both point cloud segmentation and
completion simultaneously, filling in sparse point cloud maps. For unknown
locations, GPR A-scan data can be used to match corresponding A-scan data in
the constructed map, pinpointing the position to verify the accuracy of the map
construction by the model. Experimental results demonstrate the effectiveness
of our method.


## MaskBit Embedding-free Image Generation via Bit Tokens

>Authors: Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen

>2024-09-24

> http://arxiv.org/abs/2409.16211v1

Masked transformer models for class-conditional image generation have become
a compelling alternative to diffusion models. Typically comprising two stages -
an initial VQGAN model for transitioning between latent space and image space,
and a subsequent Transformer model for image generation within latent space -
these frameworks offer promising avenues for image synthesis. In this study, we
present two primary contributions: Firstly, an empirical and systematic
examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel
embedding-free generation network operating directly on bit tokens - a binary
quantized representation of tokens with rich semantics. The first contribution
furnishes a transparent, reproducible, and high-performing VQGAN model,
enhancing accessibility and matching the performance of current
state-of-the-art methods while revealing previously undisclosed details. The
second contribution demonstrates that embedding-free image generation using bit
tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256
benchmark, with a compact generator model of mere 305M parameters.


## Time-MoE Billion-Scale Time Series Foundation Models with Mixture of Experts

>Authors: Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin

>2024-09-24

> http://arxiv.org/abs/2409.16040v1

Deep learning for time series forecasting has seen significant advancements
over the past decades. However, despite the success of large-scale pre-training
in language and vision domains, pre-trained time series models remain limited
in scale and operate at a high cost, hindering the development of larger
capable forecasting models in real-world applications. In response, we
introduce Time-MoE, a scalable and unified architecture designed to pre-train
larger, more capable forecasting foundation models while reducing inference
costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE
enhances computational efficiency by activating only a subset of networks for
each prediction, reducing computational load while maintaining high model
capacity. This allows Time-MoE to scale effectively without a corresponding
increase in inference costs. Time-MoE comprises a family of decoder-only
transformer models that operate in an auto-regressive manner and support
flexible forecasting horizons with varying input context lengths. We
pre-trained these models on our newly introduced large-scale data Time-300B,
which spans over 9 domains and encompassing over 300 billion time points. For
the first time, we scaled a time series foundation model up to 2.4 billion
parameters, achieving significantly improved forecasting precision. Our results
validate the applicability of scaling laws for training tokens and model size
in the context of time series forecasting. Compared to dense models with the
same number of activated parameters or equivalent computation budgets, our
models consistently outperform them by large margin. These advancements
position Time-MoE as a state-of-the-art solution for tackling real-world time
series forecasting challenges with superior capability, efficiency, and
flexibility.


## GraphGIA GNN Explanation Method using Game Interaction

>Authors: Xingping Xian, Jianlu Liu, Tao Wu, Lin Yuan, Chao Wang, Baiyun Chen

>2024-09-24

> http://arxiv.org/abs/2409.15698v1

Graph Neural Networks (GNNs) have garnered significant attention and have
been extensively utilized across various domains. However, similar to other
deep learning models, GNNs are often viewed as black-box models, making it
challenging to interpret their prediction mechanisms. Current graph explanation
techniques focus on identifying key nodes or edges, attributing the critical
data features that drive model predictions. Nevertheless, these features do not
independently influence the model's outcomes; rather, they interact with one
another to collectively affect predictions. In this work, we propose a novel
explanatory method GraphGI, which identifies the coalition with the highest
interaction strength and presents it as an explanatory subgraph. Given a
trained model and an input graph, our method explains predictions by gradually
incorporating significant edges into the selected subgraph. We utilize
game-theoretic interaction values to assess the interaction strength after edge
additions, ensuring that the newly added edges confer maximum interaction
strength to the explanatory subgraph. To enhance computational efficiency, we
adopt effective approximation techniques for calculating Shapley values and
game-theoretic interaction values. Empirical evaluations demonstrate that our
method achieves superior fidelity and sparsity, maintaining the
interpretability of the results at a comprehensible level.


## Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI

>Authors: Liang Zhang, Jionghao Lin, John Sabatini, Conrad Borchers, Daniel Weitekamp, Meng Cao, John Hollander, Xiangen Hu, Arthur C. Graesser

>2024-09-24

> http://arxiv.org/abs/2409.15631v1

Learning performance data describe correct and incorrect answers or
problem-solving attempts in adaptive learning, such as in intelligent tutoring
systems (ITSs). Learning performance data tend to be highly sparse
(80\%\(\sim\)90\% missing observations) in most real-world applications due to
adaptive item selection. This data sparsity presents challenges to using
learner models to effectively predict future performance explore new hypotheses
about learning. This article proposes a systematic framework for augmenting
learner data to address data sparsity in learning performance data. First,
learning performance is represented as a three-dimensional tensor of learners'
questions, answers, and attempts, capturing longitudinal knowledge states
during learning. Second, a tensor factorization method is used to impute
missing values in sparse tensors of collected learner data, thereby grounding
the imputation on knowledge tracing tasks that predict missing performance
values based on real observations. Third, a module for generating patterns of
learning is used. This study contrasts two forms of generative Artificial
Intelligence (AI), including Generative Adversarial Networks (GANs) and
Generate Pre-Trained Transformers (GPT) to generate data associated with
different clusters of learner data. We tested this approach on an adult
literacy dataset from AutoTutor lessons developed for Adult Reading
Comprehension (ARC). We found that: (1) tensor factorization improved the
performance in tracing and predicting knowledge mastery compared with other
knowledge tracing techniques without data augmentation, showing higher relative
fidelity for this imputation method, and (2) the GAN-based simulation showed
greater overall stability and less statistical bias based on a divergence
evaluation with varying simulation sample sizes compared to GPT.


## CSPS A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts

>Authors: Zeyu Zhang, Haiying Shen

>2024-09-23

> http://arxiv.org/abs/2409.15104v1

Long-sequence generative large-language model (LLM) applications have become
increasingly popular. In this paper, through trace-based experiments, we found
that the existing method for long sequences results in a high
Time-To-First-Token (TTFT) due to sequential chunk processing, long
Time-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and
low throughput due to constrained key-value cache (KVC) for long sequences. To
address these issues, we propose two Sequence-Parallelism (SP) architectures
for both tensor parallelism (TP) and non-TP. However, SP introduces two
challenges: 1) network communication and computation become performance
bottlenecks; 2) the latter two issues above are mitigated but not resolved, and
SP's resultant KV value distribution across GPUs still requires communication
for decode, increasing TBT. Hence, we propose a Communication-efficient Sparse
Attention (CSA) and communication-computation-communication three-phase
pipelining. We also propose SP-based decode that processes decode separately
from prefill, distributes KV values of a request across different GPUs, and
novelly moves Query (Q) values instead of KV values to reduce communication
overhead. These methods constitute a communication-efficient
Sequence-Parallelism based LLM Serving System (SPS2). Our trace-driven
evaluation demonstrates that SPS2 improves the average TTFT, TBT, and response
time by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode
throughput by 8.2x and 5.2x while maintaining the accuracy compared to
Sarathi-Serve. We distributed our source code.


## Efficiently Dispatching Flash Attention For Partially Filled Attention Masks

>Authors: Agniv Sharma, Jonas Geiping

>2024-09-23

> http://arxiv.org/abs/2409.15097v2

Transformers are widely used across various applications, many of which yield
sparse or partially filled attention matrices. Examples include attention masks
designed to reduce the quadratic complexity of attention, sequence packing
techniques, and recent innovations like tree masking for fast validation in
MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art
algorithm Flash Attention still processes them with quadratic complexity as
though they were dense. In this paper, we introduce Binary Block Masking, a
highly efficient modification that enhances Flash Attention by making it
mask-aware. We further propose two optimizations: one tailored for masks with
contiguous non-zero patterns and another for extremely sparse masks. Our
experiments on attention masks derived from real-world scenarios demonstrate up
to a 9x runtime improvement. The implementation will be publicly released to
foster further research and application.


## Cucheb A GPU implementation of the filtered Lanczos procedure

>Authors: Jared L. Aurentz, Vassilis Kalantzis, Yousef Saad

>2024-09-23

> http://arxiv.org/abs/2409.15053v1

This paper describes the software package Cucheb, a GPU implementation of the
filtered Lanczos procedure for the solution of large sparse symmetric
eigenvalue problems. The filtered Lanczos procedure uses a carefully chosen
polynomial spectral transformation to accelerate convergence of the Lanczos
method when computing eigenvalues within a desired interval. This method has
proven particularly effective for eigenvalue problems that arise in electronic
structure calculations and density functional theory. We compare our
implementation against an equivalent CPU implementation and show that using the
GPU can reduce the computation time by more than a factor of 10.


## Deep Cost Ray Fusion for Sparse Depth Video Completion

>Authors: Jungeon Kim, Soongjin Kim, Jaesik Park, Seungyong Lee

>2024-09-23

> http://arxiv.org/abs/2409.14935v1

In this paper, we present a learning-based framework for sparse depth video
completion. Given a sparse depth map and a color image at a certain viewpoint,
our approach makes a cost volume that is constructed on depth hypothesis
planes. To effectively fuse sequential cost volumes of the multiple viewpoints
for improved depth completion, we introduce a learning-based cost volume fusion
framework, namely RayFusion, that effectively leverages the attention mechanism
for each pair of overlapped rays in adjacent cost volumes. As a result of
leveraging feature statistics accumulated over time, our proposed framework
consistently outperforms or rivals state-of-the-art approaches on diverse
indoor and outdoor datasets, including the KITTI Depth Completion benchmark,
VOID Depth Completion benchmark, and ScanNetV2 dataset, using much fewer
network parameters.


## Kriformer A Novel Spatiotemporal Kriging Approach Based on Graph Transformers

>Authors: Renbin Pan, Feng Xiao, Hegui Zhang, Minyu Shen

>2024-09-23

> http://arxiv.org/abs/2409.14906v1

Accurately estimating data in sensor-less areas is crucial for understanding
system dynamics, such as traffic state estimation and environmental monitoring.
This study addresses challenges posed by sparse sensor deployment and
unreliable data by framing the problem as a spatiotemporal kriging task and
proposing a novel graph transformer model, Kriformer. This model estimates data
at locations without sensors by mining spatial and temporal correlations, even
with limited resources. Kriformer utilizes transformer architecture to enhance
the model's perceptual range and solve edge information aggregation challenges,
capturing spatiotemporal information effectively. A carefully constructed
positional encoding module embeds the spatiotemporal features of nodes, while a
sophisticated spatiotemporal attention mechanism enhances estimation accuracy.
The multi-head spatial interaction attention module captures subtle spatial
relationships between observed and unobserved locations. During training, a
random masking strategy prompts the model to learn with partial information
loss, allowing the spatiotemporal embedding and multi-head attention mechanisms
to synergistically capture correlations among locations. Experimental results
show that Kriformer excels in representation learning for unobserved locations,
validated on two real-world traffic speed datasets, demonstrating its
effectiveness in spatiotemporal kriging tasks.


## MICSim A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator

>Authors: Cong Wang, Zeming Chen, Shanshi Huang

>2024-09-23

> http://arxiv.org/abs/2409.14838v1

This work introduces MICSim, an open-source, pre-circuit simulator designed
for early-stage evaluation of chip-level software performance and hardware
overhead of mixed-signal compute-in-memory (CIM) accelerators. MICSim features
a modular design, allowing easy multi-level co-design and design space
exploration. Modularized from the state-of-the-art CIM simulator NeuroSim,
MICSim provides a highly configurable simulation framework supporting multiple
quantization algorithms, diverse circuit/architecture designs, and different
memory devices. This modular approach also allows MICSim to be effectively
extended to accommodate new designs.
  MICSim natively supports evaluating accelerators' software and hardware
performance for CNNs and Transformers in Python, leveraging the popular PyTorch
and HuggingFace Transformers frameworks. These capabilities make MICSim highly
adaptive when simulating different networks and user-friendly. This work
demonstrates that MICSim can easily be combined with optimization strategies to
perform design space exploration and used for chip-level Transformers CIM
accelerators evaluation. Also, MICSim can achieve a 9x - 32x speedup of
NeuroSim through a statistic-based average mode proposed by this work.


## Parse Trees Guided LLM Prompt Compression

>Authors: Wenhao Mao, Chengbin Hou, Tianyu Zhang, Xinyu Lin, Ke Tang, Hairong Lv

>2024-09-23

> http://arxiv.org/abs/2409.15395v1

Offering rich contexts to Large Language Models (LLMs) has shown to boost the
performance in various tasks, but the resulting longer prompt would increase
the computational cost and might exceed the input limit of LLMs. Recently, some
prompt compression methods have been suggested to shorten the length of prompts
by using language models to generate shorter prompts or by developing
computational models to select important parts of original prompt. The
generative compression methods would suffer from issues like hallucination,
while the selective compression methods have not involved linguistic rules and
overlook the global structure of prompt. To this end, we propose a novel
selective compression method called PartPrompt. It first obtains a parse tree
for each sentence based on linguistic rules, and calculates local information
entropy for each node in a parse tree. These local parse trees are then
organized into a global tree according to the hierarchical structure such as
the dependency of sentences, paragraphs, and sections. After that, the
root-ward propagation and leaf-ward propagation are proposed to adjust node
values over the global tree. Finally, a recursive algorithm is developed to
prune the global tree based on the adjusted node values. The experiments show
that PartPrompt receives the state-of-the-art performance across various
datasets, metrics, compression ratios, and target LLMs for inference. The
in-depth ablation studies confirm the effectiveness of designs in PartPrompt,
and other additional experiments also demonstrate its superiority in terms of
the coherence of compressed prompts and in the extreme long prompt scenario.


## ERPoT Effective and Reliable Pose Tracking for Mobile Robots Based on Lightweight and Compact Polygon Maps

>Authors: Haiming Gao, Qibo Qiu, Hongyan Liu, Dingkun Liang, Chaoqun Wang, Xuebo Zhang

>2024-09-23

> http://arxiv.org/abs/2409.14723v1

This paper presents an effective and reliable pose tracking solution termed
ERPoT for mobile robots operating in large-scale outdoor environments,
underpinned by an innovative prior polygon map. Especially, to overcome the
challenge that arises as the map size grows with the expansion of the
environment, the novel form of a prior map composed of multiple polygons is
proposed. Benefiting from the use of polygons to concisely and accurately
depict environmental occupancy, the prior polygon map achieves long-term
reliable pose tracking while ensuring a compact form. More importantly, pose
tracking is carried out under pure LiDAR mode, and the dense 3D point cloud is
transformed into a sparse 2D scan through ground removal and obstacle
selection. On this basis, a novel cost function for pose estimation through
point-polygon matching is introduced, encompassing two distinct constraint
forms: point-to-vertex and point-to-edge. In this study, our primary focus lies
on two crucial aspects: lightweight and compact prior map construction, as well
as effective and reliable robot pose tracking. Both aspects serve as the
foundational pillars for future navigation across different mobile platforms
equipped with different LiDAR sensors in different environments. Comparative
experiments based on the publicly available datasets and our self-recorded
datasets are conducted, and evaluation results show the superior performance of
ERPoT on reliability, prior map size, pose estimation error, and runtime over
the other five approaches. The corresponding code can be accessed at
https://github.com/ghm0819/ERPoT, and the supplementary video is at
https://youtu.be/cseml5FrW1Q.


## Harmonising the Clinical Melody Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding

>Authors: Bokang Bi, Leibo Liu, Sanja Lujic, Louisa Jorm, Oscar Perez-Concha

>2024-09-23

> http://arxiv.org/abs/2409.14638v2

The increasing volume and complexity of clinical documentation in Electronic
Medical Records systems pose significant challenges for clinical coders, who
must mentally process and summarise vast amounts of clinical text to extract
essential information needed for coding tasks. While large language models have
been successfully applied to shorter summarisation tasks in recent years, the
challenge of summarising a hospital course remains an open area for further
research and development. In this study, we adapted three pre trained LLMs,
Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course
summarisation task, using Quantized Low Rank Adaptation fine tuning. We created
a free text clinical dataset from MIMIC III data by concatenating various
clinical notes as the input clinical text, paired with ground truth Brief
Hospital Course sections extracted from the discharge summaries for model
training. The fine tuned models were evaluated using BERTScore and ROUGE
metrics to assess the effectiveness of clinical domain fine tuning.
Additionally, we validated their practical utility using a novel hospital
course summary assessment metric specifically tailored for clinical coding. Our
findings indicate that fine tuning pre trained LLMs for the clinical domain can
significantly enhance their performance in hospital course summarisation and
suggest their potential as assistive tools for clinical coding. Future work
should focus on refining data curation methods to create higher quality
clinical datasets tailored for hospital course summary tasks and adapting more
advanced open source LLMs comparable to proprietary models to further advance
this research.


## EQ-CBM A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors

>Authors: Sangwon Kim, Dasom Ahn, Byoung Chul Ko, In-su Jang, Kwang-Ju Kim

>2024-09-22

> http://arxiv.org/abs/2409.14630v1

The demand for reliable AI systems has intensified the need for interpretable
deep neural networks. Concept bottleneck models (CBMs) have gained attention as
an effective approach by leveraging human-understandable concepts to enhance
interpretability. However, existing CBMs face challenges due to deterministic
concept encoding and reliance on inconsistent concepts, leading to
inaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through
probabilistic concept encoding using energy-based models (EBMs) with quantized
concept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties,
thereby improving prediction reliability and accuracy. By employing qCAVs, our
method selects homogeneous vectors during concept encoding, enabling more
decisive task performance and facilitating higher levels of human intervention.
Empirical results using benchmark datasets demonstrate that our approach
outperforms the state-of-the-art in both concept and task accuracy.


## Patch Ranking Efficient CLIP by Learning to Rank Local Patches

>Authors: Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado

>2024-09-22

> http://arxiv.org/abs/2409.14607v1

Contrastive image-text pre-trained models such as CLIP have shown remarkable
adaptability to downstream tasks. However, they face challenges due to the high
computational requirements of the Vision Transformer (ViT) backbone. Current
strategies to boost ViT efficiency focus on pruning patch tokens but fall short
in addressing the multimodal nature of CLIP and identifying the optimal subset
of tokens for maximum performance. To address this, we propose greedy search
methods to establish a "Golden Ranking" and introduce a lightweight predictor
specifically trained to approximate this Ranking. To compensate for any
performance degradation resulting from token pruning, we incorporate learnable
visual tokens that aid in restoring and potentially enhancing the model's
performance. Our work presents a comprehensive and systematic investigation of
pruning tokens within the ViT backbone of CLIP models. Through our framework,
we successfully reduced 40% of patch tokens in CLIP's ViT while only suffering
a minimal average accuracy loss of 0.3 across seven datasets. Our study lays
the groundwork for building more computationally efficient multimodal models
without sacrificing their performance, addressing a key challenge in the
application of advanced vision-language models.


## A is for Absorption Studying Feature Splitting and Absorption in Sparse Autoencoders

>Authors: David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Joseph Bloom

>2024-09-22

> http://arxiv.org/abs/2409.14507v3

Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose
the activations of Large Language Models (LLMs) into human-interpretable
latents. In this paper, we pose two questions. First, to what extent do SAEs
extract monosemantic and interpretable latents? Second, to what extent does
varying the sparsity or the size of the SAE affect monosemanticity /
interpretability? By investigating these questions in the context of a simple
first-letter identification task where we have complete access to ground truth
labels for all tokens in the vocabulary, we are able to provide more detail
than prior investigations. Critically, we identify a problematic form of
feature-splitting we call feature absorption where seemingly monosemantic
latents fail to fire in cases where they clearly should. Our investigation
suggests that varying SAE size or sparsity is insufficient to solve this issue,
and that there are deeper conceptual issues in need of resolution.


## Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers

>Authors: Dominic Schneider, Lutz Rapp

>2024-09-22

> http://arxiv.org/abs/2409.14378v1

Optical fiber amplifiers are key elements in present optical networks.
Failures of these components result in high financial loss of income of the
network operator as the communication traffic over an affected link is
interrupted. Applying Remaining useful lifetime (RUL) prediction in the context
of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming
system failures at an early stage, so that network outages can be minimized
through planning of targeted maintenance actions, ensures reliability and
safety. Optical fiber amplifier are complex systems, that work under various
operating conditions, which makes correct forecasting a difficult task.
Increased monitoring capabilities of systems results in datasets that
facilitate the application of data-driven RUL prediction methods. Deep learning
models in particular have shown good performance, but generalization based on
comparatively small datasets for RUL prediction is difficult. In this paper, we
propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL
prediction method. SLAT is based on an encoder-decoder architecture, wherein
two parallel working encoders extract features for sensors and time steps. By
utilizing the self-attention mechanism, long-term dependencies can be learned
from long sequences. The implementation of sparsity in the attention matrix and
a low-rank parametrization reduce overfitting and increase generalization.
Experimental application to optical fiber amplifiers exemplified on EDFA, as
well as a reference dataset from turbofan engines, shows that SLAT outperforms
the state-of-the-art methods.


## Thinking in Granularity Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues

>Authors: Mingshen Wang, Zhao Zhang, Feng Li, Ke Xu, Kang Miao, Meng Wang

>2024-09-22

> http://arxiv.org/abs/2409.14330v1

Dynamic quantization has attracted rising attention in image super-resolution
(SR) as it expands the potential of heavy SR models onto mobile devices while
preserving competitive performance. Existing methods explore layer-to-bit
configuration upon varying local regions, adaptively allocating the bit to each
layer and patch. Despite the benefits, they still fall short in the trade-off
of SR accuracy and quantization efficiency. Apart from this, adapting the
quantization level for each layer individually can disturb the original
inter-layer relationships, thus diminishing the representation capability of
quantized models. In this work, we propose Granular-DQ, which capitalizes on
the intrinsic characteristics of images while dispensing with the previous
consideration for layer sensitivity in quantization. Granular-DQ conducts a
multi-granularity analysis of local patches with further exploration of their
information densities, achieving a distinctive patch-wise and layer-invariant
dynamic quantization paradigm. Specifically, Granular-DQ initiates by
developing a granularity-bit controller (GBC) to apprehend the coarse-to-fine
granular representations of different patches, matching their proportional
contribution to the entire image to determine the proper bit-width allocation.
On this premise, we investigate the relation between bit-width and information
density, devising an entropy-to-bit (E2B) mechanism that enables further
fine-grained dynamic bit adaption of high-bit patches. Extensive experiments
validate the superiority and generalization ability of Granular-DQ over recent
state-of-the-art methods on various SR models. Code will be available at
\url{https://github.com/MmmingS/Granular-DQ.git}.


## Enhanced DOA Estimation via Hybrid Massive MIMO Receive Array with Switches-based Sparse Architecture

>Authors: Yifan Li, Feng Shu, Yaoliang Song, Jiangzhou Wang

>2024-09-22

> http://arxiv.org/abs/2409.14297v1

Hybrid massive arrays have been widely used in direction of arrival (DOA)
estimation for it can provide larger aperture with lower hardware complexity.
However, as the signals received by a hybrid array are compressed by the phase
shifter network or the switch network, the degree of freedom (DOF) or spatial
resolution of hybrid array is lower than fully-digital (FD) array with same
number of antennas. Therefore, we develop a novel sparse hybrid array called
switches-based sparse hybrid array (SW-SHA) which by combining nested array and
switches-based hybrid array to achieve a huge improvement on DOF over
traditional hybrid arrays. Simulations of the spatial spectrums verify that
SW-SHA can accurately solve the problem of DOA estimation with the number of
signal sources much larger than the number of RF chains. Finally, to further
improve the accuracy of DOA estimation for SW-SHA, MMV-SW-SHA is proposed by
transforming the single-snapshot co-array signal into MMV form. The simulation
results also show that MMV-SW-SHA has better performance than SW-SHA when
signal-to-noise ratio (SNR) is low.


## On Importance of Pruning and Distillation for Efficient Low Resource NLP

>Authors: Aishwarya Mirashi, Purva Lingayat, Srushti Sonavane, Tejas Padhiyar, Raviraj Joshi, Geetanjali Kale

>2024-09-21

> http://arxiv.org/abs/2409.14162v1

The rise of large transformer models has revolutionized Natural Language
Processing, leading to significant advances in tasks like text classification.
However, this progress demands substantial computational resources, escalating
training duration, and expenses with larger model sizes. Efforts have been made
to downsize and accelerate English models (e.g., Distilbert, MobileBert). Yet,
research in this area is scarce for low-resource languages.
  In this study, we explore the case of the low-resource Indic language
Marathi. Leveraging the marathi-topic-all-doc-v2 model as our baseline, we
implement optimization techniques to reduce computation time and memory usage.
Our focus is on enhancing the efficiency of Marathi transformer models while
maintaining top-tier accuracy and reducing computational demands. Using the
MahaNews document classification dataset and the marathi-topic-all-doc-v2 model
from L3Cube, we apply Block Movement Pruning, Knowledge Distillation, and Mixed
Precision methods individually and in combination to boost efficiency. We
demonstrate the importance of strategic pruning levels in achieving desired
efficiency gains. Furthermore, we analyze the balance between efficiency
improvements and environmental impact, highlighting how optimized model
architectures can contribute to a more sustainable computational ecosystem.
Implementing these techniques on a single GPU system, we determine that the
optimal configuration is 25\% pruning + knowledge distillation. This approach
yielded a 2.56x speedup in computation time while maintaining baseline accuracy
levels.


## Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis

>Authors: Zeping Yu, Sophia Ananiadou

>2024-09-21

> http://arxiv.org/abs/2409.14144v1

We find arithmetic ability resides within a limited number of attention
heads, with each head specializing in distinct operations. To delve into the
reason, we introduce the Comparative Neuron Analysis (CNA) method, which
identifies an internal logic chain consisting of four distinct stages from
input to prediction: feature enhancing with shallow FFN neurons, feature
transferring by shallow attention layers, feature predicting by arithmetic
heads, and prediction enhancing among deep FFN neurons. Moreover, we identify
the human-interpretable FFN neurons within both feature-enhancing and
feature-predicting stages. These findings lead us to investigate the mechanism
of LoRA, revealing that it enhances prediction probabilities by amplifying the
coefficient scores of FFN neurons related to predictions. Finally, we apply our
method in model pruning for arithmetic tasks and model editing for reducing
gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.


## ProTEA Programmable Transformer Encoder Acceleration on FPGA

>Authors: Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang

>2024-09-21

> http://arxiv.org/abs/2409.13975v1

Transformer neural networks (TNN) have been widely utilized on a diverse
range of applications, including natural language processing (NLP), machine
translation, and computer vision (CV). Their widespread adoption has been
primarily driven by the exceptional performance of their multi-head
self-attention block used to extract key features from sequential data. The
multi-head self-attention block is followed by feedforward neural networks,
which play a crucial role in introducing non-linearity to assist the model in
learning complex patterns. Despite the popularity of TNNs, there has been
limited numbers of hardware accelerators targeting these two critical blocks.
Most prior works have concentrated on sparse architectures that are not
flexible for popular TNN variants. This paper introduces \textit{ProTEA}, a
runtime programmable accelerator tailored for the dense computations of most of
state-of-the-art transformer encoders. \textit{ProTEA} is designed to reduce
latency by maximizing parallelism. We introduce an efficient tiling of large
matrices that can distribute memory and computing resources across different
hardware components within the FPGA. We provide run time evaluations of
\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator
card. Experimental results demonstrate that \textit{ProTEA} can host a wide
range of popular transformer networks and achieve near optimal performance with
a tile size of 64 in the multi-head self-attention block and 6 in the
feedforward networks block when configured with 8 parallel attention heads, 12
layers, and an embedding dimension of 768 on the U55C. Comparative results are
provided showing \textit{ProTEA} is 2.5$\times$ faster than an NVIDIA Titan XP
GPU. Results also show that it achieves 1.3 -- 2.8$\times$ speed up compared
with current state-of-the-art custom designed FPGA accelerators.


## Periodic micromagnetic finite element method

>Authors: Fangzhou Ai, Jiawei Duan, Vitaliy Lomakin

>2024-09-21

> http://arxiv.org/abs/2409.13958v1

Periodic micromagnetic finite element method (PM-FEM) is introduced to solve
periodic unit cell problems using the Landau-Lifshitz-Gilbert equation. PM-FEM
is applicable to general problems with 1D, 2D, and 3D periodicities. PM-FEM is
based on a non-periodic FEM-based micromagnetic solver and extends it in
several aspects to account for periodicities, including the computation of
exchange and magnetostatic fields. For the exchange field, PM-FEM modifies the
sparse matrix construction for computing the Laplace operator to include
additional elements arising due to the periodicities. For the magnetostatic
field, the periodic extensions include modifications in the local operators,
such as gradient, divergence, and surface magnetic charges as well as the
long-range superposition operator for computing the periodic scalar potential.
The local operators are extended to account for the periodicities similar to
handling the Laplace operator. For the long-range superposition operator,
PM-FEM utilizes a periodic Green's function (PGF) and fast spatial
convolutions. The PGF is computed rapidly via exponentially rapidly convergent
sums. The spatial convolutions are accomplished via a modified fast Fourier
transform based adaptive integral method that allows calculating spatial
convolutions with non-uniform meshes in $O(N\log N)$ numerical operations.
PM-FEM is implemented on CPU and GPU based computer architectures. PM-FEM
allows efficiently handling cases of structures contained withing the periodic
unit cell touching or not touching its boundaries as well as structures that
protrude beyond the unit cell boundaries. PM-FEM is demonstrated to have about
the same or even higher performance than its parent non-periodic code. The
demonstrated numerical examples show the efficiency of PM-FEM for highly
complex structures with 1D, 2D, and 3D periodicities.

