# 2024-11-04

# Table of Contents
* [Learning Video Representations without Natural Videos](#Learning-Video-Representations-without-Natural-Videos)
* [DELTA Dense Efficient Long-range 3D Tracking for any video](#DELTA-Dense-Efficient-Long-range-3D-Tracking-for-any-video)
* [No Pose, No Problem Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images](#No-Pose,-No-Problem-Surprisingly-Simple-3D-Gaussian-Splats-from-Sparse-Unposed-Images)
* [Group Crosscoders for Mechanistic Analysis of Symmetry](#Group-Crosscoders-for-Mechanistic-Analysis-of-Symmetry)
* [Breaking Determinism Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model](#Breaking-Determinism-Fuzzy-Modeling-of-Sequential-Recommendation-Using-Discrete-State-Space-Diffusion-Model)
* [BitStack Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments](#BitStack-Fine-Grained-Size-Control-for-Compressed-Large-Language-Models-in-Variable-Memory-Environments)
* [ECDQC Efficient Compilation for Distributed Quantum Computing with Linear Layout](#ECDQC-Efficient-Compilation-for-Distributed-Quantum-Computing-with-Linear-Layout)
* [Kernel Looping Eliminating Synchronization Boundaries for Peak Inference Performance](#Kernel-Looping-Eliminating-Synchronization-Boundaries-for-Peak-Inference-Performance)
* [Context-Aware Token Selection and Packing for Enhanced Vision Transformer](#Context-Aware-Token-Selection-and-Packing-for-Enhanced-Vision-Transformer)
* [ALISE Accelerating Large Language Model Serving with Speculative Scheduling](#ALISE-Accelerating-Large-Language-Model-Serving-with-Speculative-Scheduling)
* [Collage Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs](#Collage-Decomposable-Rapid-Prototyping-for-Information-Extraction-on-Scientific-PDFs)
* [Learning and Transferring Sparse Contextual Bigrams with Linear Transformers](#Learning-and-Transferring-Sparse-Contextual-Bigrams-with-Linear-Transformers)
* [Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning](#Real-Time-Personalization-for-LLM-based-Recommendation-with-Customized-In-Context-Learning)
* [BUZZ Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference](#BUZZ-Beehive-structured-Sparse-KV-Cache-with-Segmented-Heavy-Hitters-for-Efficient-LLM-Inference)
* [Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback](#Online-Intrinsic-Rewards-for-Decision-Making-Agents-from-Large-Language-Model-Feedback)
* [DisenTS Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting](#DisenTS-Disentangled-Channel-Evolving-Pattern-Modeling-for-Multivariate-Time-Series-Forecasting)
* [Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis](#Epipolar-Free-3D-Gaussian-Splatting-for-Generalizable-Novel-View-Synthesis)
* [Wavelet Burst Accumulation for turbulence mitigation](#Wavelet-Burst-Accumulation-for-turbulence-mitigation)
* [MALoRA Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning](#MALoRA-Mixture-of-Asymmetric-Low-Rank-Adaptation-for-Enhanced-Multi-Task-Learning)
* [ETOEfficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses](#ETOEfficient-Transformer-based-Local-Feature-Matching-by-Organizing-Multiple-Homography-Hypotheses)
* [NeFF-BioNet Crop Biomass Prediction from Point Cloud to Drone Imagery](#NeFF-BioNet-Crop-Biomass-Prediction-from-Point-Cloud-to-Drone-Imagery)
* [Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem](#Automatic-programming-via-large-language-models-with-population-self-evolution-for-dynamic-job-shop-scheduling-problem)
* [WaveRoRA Wavelet Rotary Route Attention for Multivariate Time Series Forecasting](#WaveRoRA-Wavelet-Rotary-Route-Attention-for-Multivariate-Time-Series-Forecasting)
* [Fast and reliable atom transport by optical tweezers](#Fast-and-reliable-atom-transport-by-optical-tweezers)


## Learning Video Representations without Natural Videos

>Authors: Xueyang Yu, Xinlei Chen, Yossi Gandelsman

>2024-10-31

> http://arxiv.org/abs/2410.24213v1

In this paper, we show that useful video representations can be learned from
synthetic videos and natural images, without incorporating natural videos in
the training. We propose a progression of video datasets synthesized by simple
generative processes, that model a growing set of natural video properties
(e.g. motion, **acceleration**, and shape transformations). The downstream
performance of video models pre-trained on these generated datasets gradually
increases with the dataset progression. A VideoMAE model pre-trained on our
synthetic videos closes 97.2% of the performance gap on UCF101 action
classification between training from scratch and self-supervised pre-training
from natural videos, and outperforms the pre-trained model on HMDB51.
Introducing crops of static images to the pre-training stage results in similar
performance to UCF101 pre-training and outperforms the UCF101 pre-trained model
on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the
low-level properties of the datasets, we identify correlations between frame
diversity, frame similarity to natural data, and downstream performance. Our
approach provides a more controllable and transparent alternative to video data
curation processes for pre-training.


## DELTA Dense Efficient Long-range 3D Tracking for any video

>Authors: Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, Chaoyang Wang

>2024-10-31

> http://arxiv.org/abs/2410.24211v2

Tracking dense 3D motion from monocular videos remains challenging,
particularly when aiming for pixel-level precision over long sequences. We
introduce DELTA, a novel method that efficiently tracks every pixel in 3D
space, enabling accurate motion estimation across entire videos. Our approach
leverages a joint global-local attention mechanism for reduced-resolution
tracking, followed by a transformer-based upsampler to achieve high-resolution
predictions. Unlike existing methods, which are limited by computational
inefficiency or **sparse** tracking, DELTA delivers dense 3D tracking at scale,
running over 8x faster than previous methods while achieving state-of-the-art
accuracy. Furthermore, we explore the impact of depth representation on
tracking performance and identify log-depth as the optimal choice. Extensive
experiments demonstrate the superiority of DELTA on multiple benchmarks,
achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.
Our method provides a robust solution for applications requiring fine-grained,
long-term motion tracking in 3D space.


## No Pose, No Problem Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images

>Authors: Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng

>2024-10-31

> http://arxiv.org/abs/2410.24207v1

We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D
scenes parameterized by 3D Gaussians from \textit{unposed} **sparse** multi-view
images. Our model, trained exclusively with photometric loss, achieves
real-time 3D Gaussian reconstruction during inference. To eliminate the need
for accurate pose input during reconstruction, we anchor one input view's local
camera coordinates as the canonical space and train the network to predict
Gaussian primitives for all views within this space. This approach obviates the
need to transform Gaussian primitives from local coordinates into a global
coordinate system, thus avoiding errors associated with per-frame Gaussians and
pose estimation. To resolve scale ambiguity, we design and compare various
intrinsic embedding methods, ultimately opting to convert camera intrinsics
into a token embedding and concatenate it with image tokens as input to the
model, enabling accurate scene scale prediction. We utilize the reconstructed
3D Gaussians for novel view synthesis and pose estimation tasks and propose a
two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental
results demonstrate that our pose-free approach can achieve superior novel view
synthesis quality compared to pose-required methods, particularly in scenarios
with limited input image overlap. For pose estimation, our method, trained
without ground truth depth or explicit matching loss, significantly outperforms
the state-of-the-art methods with substantial improvements. This work makes
significant advances in pose-free generalizable 3D reconstruction and
demonstrates its applicability to real-world scenarios. Code and trained models
are available at https://noposplat.github.io/.


## Group Crosscoders for Mechanistic Analysis of Symmetry

>Authors: Liv Gorton

>2024-10-31

> http://arxiv.org/abs/2410.24184v2

We introduce group crosscoders, an extension of crosscoders that
systematically discover and analyse symmetrical features in neural networks.
While neural networks often develop equivariant representations without
explicit architectural constraints, understanding these emergent symmetries has
traditionally relied on manual analysis. Group crosscoders automate this
process by performing dictionary learning across transformed versions of inputs
under a symmetry group. Applied to InceptionV1's mixed3b layer using the
dihedral group $\mathrm{D}_{32}$, our method reveals several key insights:
First, it naturally clusters features into interpretable families that
correspond to previously hypothesised feature types, providing more precise
separation than standard **sparse** autoencoders. Second, our transform block
analysis enables the automatic characterisation of feature symmetries,
revealing how different geometric features (such as curves versus lines)
exhibit distinct patterns of invariance and equivariance. These results
demonstrate that group crosscoders can provide systematic insights into how
neural networks represent symmetry, offering a promising new tool for
mechanistic interpretability.


## Breaking Determinism Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model

>Authors: Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, Enhong Chen

>2024-10-31

> http://arxiv.org/abs/2410.23994v2

Sequential recommendation (SR) aims to predict items that users may be
interested in based on their historical behavior sequences. We revisit SR from
a novel information-theoretic perspective and find that conventional sequential
modeling methods fail to adequately capture the randomness and unpredictability
of user behavior. Inspired by fuzzy information processing theory, this paper
introduces the DDSR model, which uses fuzzy sets of interaction sequences to
overcome the limitations and better capture the evolution of users' real
interests. Formally based on diffusion transition processes in discrete state
spaces, which is unlike common diffusion models such as DDPM that operate in
continuous domains. It is better suited for discrete data, using structured
transitions instead of arbitrary noise introduction to avoid information loss.
Additionally, to address the inefficiency of matrix transformations due to the
vast discrete space, we use semantic labels derived from **quantization** or RQ-VAE
to replace item IDs, enhancing efficiency and improving cold start issues.
Testing on three public benchmark datasets shows that DDSR outperforms existing
state-of-the-art methods in various settings, demonstrating its potential and
effectiveness in handling SR tasks.


## BitStack Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments

>Authors: Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu

>2024-10-31

> http://arxiv.org/abs/2410.23918v1

Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
**quantization**, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong **quantization** baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like **quantization**. Code is available at
https://github.com/xinghaow99/BitStack.


## ECDQC Efficient Compilation for Distributed Quantum Computing with Linear Layout

>Authors: Kecheng Liu, Yidong Zhou, Haochen Luo, Lingjun Xiong, Yuchen Zhu, Eilis Casey, Jinglei Cheng, Samuel Yen-Chi Chen, Zhiding Liang

>2024-10-31

> http://arxiv.org/abs/2410.23857v2

In this paper, we propose an efficient compilation method for distributed
quantum computing (DQC) using the Linear Nearest Neighbor (LNN) architecture.
By exploiting the LNN topology's symmetry, we optimize quantum circuit
compilation for High Local Connectivity, Sparse Full Connectivity (HLC-SFC)
algorithms like Quantum Approximate Optimization Algorithm (QAOA) and Quantum
Fourier Transform (QFT). We also utilize dangling qubits to minimize non-local
interactions and reduce SWAP gates. Our approach significantly decreases
compilation time, gate count, and circuit depth, improving scalability and
robustness for large-scale quantum computations.


## Kernel Looping Eliminating Synchronization Boundaries for Peak Inference Performance

>Authors: David Koeplinger, Darshan Gandhi, Pushkar Nandkar, Nathan Sheeley, Matheen Musaddiq, Leon Zhang, Reid Goodbar, Matthew Shaffer, Han Wang, Angela Wang, Mingran Wang, Raghu Prabhakar

>2024-10-31

> http://arxiv.org/abs/2410.23668v1

Token generation speed is critical to power the next wave of AI inference
applications. GPUs significantly underperform during token generation due to
synchronization overheads at kernel boundaries, utilizing only 21% of their
peak memory bandwidth. While recent dataflow architectures mitigate these
overheads by enabling aggressive fusion of decoder layers into a single kernel,
they too leave performance on the table due to synchronization penalties at
layer boundaries.
  This paper presents kernel looping, a specialized global optimization
technique which exploits an optimization opportunity brought by combining the
unique layer-level fusion possible in modern dataflow architectures with the
repeated layer structure found in language models. Kernel looping eliminates
synchronization costs between consecutive calls to the same kernel by
transforming these calls into a single call to a modified kernel containing a
pipelined outer loop. We evaluate kernel looping on the SambaNova SN40L
Reconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.
Experiments demonstrate that kernel looping speeds up the decode phase of a
wide array of powerful open-source models by up to 2.2$\times$ on SN40L. Kernel
looping allows scaling of decode performance over multiple SN40L sockets,
achieving speedups of up to 2.5$\times$. Finally, kernel looping enables SN40L
to achieve over 90% of peak performance on 8 and 16 sockets and achieve a
speedup of up to 3.7$\times$ over DGX H100. Kernel looping, as well as the
models evaluated in this paper, are deployed in production in a commercial AI
inference cloud.


## Context-Aware Token Selection and Packing for Enhanced Vision Transformer

>Authors: Tianyi Zhang, Baoxin Li, Jae-sun Seo, Yu Cap

>2024-10-31

> http://arxiv.org/abs/2410.23608v1

In recent years, the long-range attention mechanism of vision transformers
has driven significant performance breakthroughs across various computer vision
tasks. However, the traditional self-attention mechanism, which processes both
informative and non-informative tokens, suffers from inefficiency and
inaccuracies. While **sparse** attention mechanisms have been introduced to
mitigate these issues by **pruning** tokens involved in attention, they often lack
context-awareness and intelligence. These mechanisms frequently apply a uniform
token selection strategy across different inputs for batch training or optimize
efficiency only for the inference stage. To overcome these challenges, we
propose a novel algorithm: Select and Pack Attention (SPA). SPA dynamically
selects informative tokens using a low-cost gating layer supervised by
selection labels and packs these tokens into new batches, enabling a variable
number of tokens to be used in parallelized GPU batch training and inference.
Extensive experiments across diverse datasets and computer vision tasks
demonstrate that SPA delivers superior performance and efficiency, including a
0.6 mAP improvement in object detection and a 16.4% reduction in computational
costs.


## ALISE Accelerating Large Language Model Serving with Speculative Scheduling

>Authors: Youpeng Zhao, Jun Wang

>2024-10-31

> http://arxiv.org/abs/2410.23537v1

Large Language Models (LLMs) represent a revolutionary advancement in the
contemporary landscape of artificial general intelligence (AGI). As exemplified
by ChatGPT, LLM-based applications necessitate minimal response latency and
maximal throughput for inference serving. However, due to the unpredictability
of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed
by current LLM serving systems suffers from head-of-line (HoL) blocking issues
and long job response times.
  In this paper, we propose a new efficient LLM inference serving framework,
named ALISE. The key design paradigm of ALISE is to leverage a novel
speculative scheduler by estimating the execution time for each job and
exploiting such prior knowledge to assign appropriate job priority orders, thus
minimizing potential queuing delays for heterogeneous workloads. Furthermore,
to mitigate the memory overhead of the intermediate key-value (**KV**) cache, we
employ a priority-based adaptive memory management protocol and
**quantization**-based compression techniques. Evaluations demonstrate that in
comparison to the state-of-the-art solution vLLM, ALISE improves the throughput
of inference serving by up to 1.8x and 2.1x under the same latency constraint
on the Alpaca and ShareGPT datasets, respectively.


## Collage Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs

>Authors: Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell

>2024-10-30

> http://arxiv.org/abs/2410.23478v1

Recent years in NLP have seen the continued development of domain-specific
information extraction tools for scientific documents, alongside the release of
increasingly multimodal pretrained transformer models. While the opportunity
for scientists outside of NLP to evaluate and apply such systems to their own
domains has never been clearer, these models are difficult to compare: they
accept different input formats, are often black-box and give little insight
into processing failures, and rarely handle PDF documents, the most common
format of scientific publication. In this work, we present Collage, a tool
designed for rapid prototyping, visualization, and evaluation of different
information extraction models on scientific PDFs. Collage allows the use and
evaluation of any HuggingFace token classifier, several LLMs, and multiple
other task-specific models out of the box, and provides extensible software
interfaces to accelerate experimentation with new models. Further, we enable
both developers and users of NLP-based tools to inspect, debug, and better
understand modeling pipelines by providing granular views of intermediate
states of processing. We demonstrate our system in the context of information
extraction to assist with literature review in materials science.


## Learning and Transferring Sparse Contextual Bigrams with Linear Transformers

>Authors: Yunwei Ren, Zixuan Wang, Jason D. Lee

>2024-10-30

> http://arxiv.org/abs/2410.23438v1

Transformers have excelled in natural language modeling and one reason behind
this success is their exceptional ability to combine contextual informal and
global knowledge. However, the theoretical basis remains unclear. In this
paper, first we introduce the Sparse Contextual Bigram (SCB), a natural
extension of the classical bigram model, where the next token's generation
depends on a **sparse** set of earlier positions determined by the last token. We
then analyze the training dynamics and sample complexity of learning SCB using
a one-layer linear transformer with a gradient-based algorithm. We show that
when trained from scratch, the training process can be split into an initial
sample-intensive stage where the correlation is boosted from zero to a
nontrivial value, followed by a more sample-efficient stage of further
improvement. Additionally, we prove that, provided a nontrivial correlation
between the downstream and pretraining tasks, finetuning from a pretrained
model allows us to bypass the initial sample-intensive stage. We also
empirically demonstrate that our algorithm can outperform SGD in this setting
and discuss its relationship with the usual softmax-based transformers.


## Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning

>Authors: Keqin Bao, Ming Yan, Yang Zhang, Jizhi Zhang, Wenjie Wang, Fuli Feng, Xiangnan He

>2024-10-30

> http://arxiv.org/abs/2410.23136v1

Frequently updating Large Language Model (LLM)-based recommender systems to
adapt to new user interests -- as done for traditional ones -- is impractical
due to high training costs, even with **acceleration** methods. This work explores
adapting to dynamic user interests without any model updates by leveraging
In-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot
examples provided in the input. Using new-interest examples as the ICL few-shot
examples, LLMs may learn real-time interest directly, avoiding the need for
model updates. However, existing LLM-based recommenders often lose the
in-context learning ability during recommendation tuning, while the original
LLM's in-context learning lacks recommendation-specific focus. To address this,
we propose RecICL, which customizes recommendation-specific in-context learning
for real-time recommendations. RecICL organizes training examples in an
in-context learning format, ensuring that in-context learning ability is
preserved and aligned with the recommendation task during tuning.
  Extensive experiments demonstrate RecICL's effectiveness in delivering
real-time recommendations without requiring model updates. Our code is
available at https://github.com/ym689/rec_icl.


## BUZZ Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference

>Authors: Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He

>2024-10-30

> http://arxiv.org/abs/2410.23079v1

Large language models (LLMs) are essential in natural language processing but
often struggle with inference speed and computational efficiency, limiting
real-time deployment. The key-value (**KV**) cache mechanism reduces computational
overhead in transformer models, but challenges in maintaining contextual
understanding remain. In this paper, we propose BUZZ, a novel **KV** caching
algorithm that leverages structured contextual information to minimize cache
memory usage while enhancing inference speed. BUZZ employs a beehive-structured
**sparse** cache, incorporating a sliding window to capture recent information and
dynamically segmenting historical tokens into chunks to prioritize important
tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:
CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ
(1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while
maintaining over 99% accuracy in long-text summarization, and (2) surpasses
state-of-the-art performance in multi-document question answering by
$\textbf{7.69%}$ under the same memory limit, where full cache methods
encounter out-of-memory issues. Additionally, BUZZ achieves significant
inference speedup with a $\log{n}$ time complexity. The code is available at
https://github.com/JunqiZhao888/buzz-llm.


## Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback

>Authors: Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos

>2024-10-30

> http://arxiv.org/abs/2410.23022v1

Automatically synthesizing dense rewards from natural language descriptions
is a promising paradigm in reinforcement learning (RL), with applications to
**sparse** reward problems, open-ended exploration, and hierarchical skill design.
Recent works have made promising steps by exploiting the prior knowledge of
large language models (LLMs). However, these approaches suffer from important
limitations: they are either not scalable to problems requiring billions of
environment samples; or are limited to reward functions expressible by compact
code, which may require source code and have difficulty capturing nuanced
semantics; or require a diverse offline dataset, which may not exist or be
impossible to collect. In this work, we address these limitations through a
combination of algorithmic and systems-level contributions. We propose ONI, a
distributed architecture that simultaneously learns an RL policy and an
intrinsic reward function using LLM feedback. Our approach annotates the
agent's collected experience via an asynchronous LLM server, which is then
distilled into an intrinsic reward model. We explore a range of algorithmic
choices for reward modeling with varying complexity, including hashing,
classification, and ranking models. By studying their relative tradeoffs, we
shed light on questions regarding intrinsic reward design for **sparse** reward
problems. Our approach achieves state-of-the-art performance across a range of
challenging, **sparse** reward tasks from the NetHack Learning Environment in a
simple unified process, solely using the agent's gathered experience, without
requiring external datasets nor source code. We make our code available at
\url{URL} (coming soon).


## DisenTS Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting

>Authors: Zhiding Liu, Jiqian Yang, Qingyang Mao, Yuze Zhao, Mingyue Cheng, Zhi Li, Qi Liu, Enhong Chen

>2024-10-30

> http://arxiv.org/abs/2410.22981v1

Multivariate time series forecasting plays a crucial role in various
real-world applications. Significant efforts have been made to integrate
advanced network architectures and training strategies that enhance the capture
of temporal dependencies, thereby improving forecasting accuracy. On the other
hand, mainstream approaches typically utilize a single unified model with
simplistic channel-mixing embedding or cross-channel attention operations to
account for the critical intricate inter-channel dependencies. Moreover, some
methods even trade capacity for robust prediction based on the
channel-independent assumption. Nonetheless, as time series data may display
distinct evolving patterns due to the unique characteristics of each channel
(including multiple strong seasonalities and trend changes), the unified
modeling methods could yield suboptimal results. To this end, we propose
DisenTS, a tailored framework for modeling disentangled channel evolving
patterns in general multivariate time series forecasting. The central idea of
DisenTS is to model the potential diverse patterns within the multivariate time
series data in a decoupled manner. Technically, the framework employs multiple
distinct forecasting models, each tasked with uncovering a unique evolving
pattern. To guide the learning process without supervision of pattern
partition, we introduce a novel Forecaster Aware Gate (FAG) module that
generates the routing signals adaptively according to both the forecasters'
states and input series' characteristics. The forecasters' states are derived
from the Linear Weight Approximation (LWA) strategy, which **quantize**s the
complex deep neural networks into compact matrices. Additionally, the
Similarity Constraint (SC) is further proposed to guide each model to
specialize in an underlying pattern by minimizing the mutual information
between the representations.


## Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis

>Authors: Zhiyuan Min, Yawei Luo, Jianwen Sun, Yi Yang

>2024-10-30

> http://arxiv.org/abs/2410.22817v2

Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from
**sparse**-view observations in a feed-forward inference manner, eliminating the
need for scene-specific retraining required in conventional 3DGS. However,
existing methods rely heavily on epipolar priors, which can be unreliable in
complex realworld scenes, particularly in non-overlapping and occluded regions.
In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based
model for generalizable novel view synthesis that operates independently of
epipolar line constraints. To enhance multiview feature extraction with 3D
perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view
completion pre-training on large-scale datasets. Additionally, we introduce an
Iterative Cross-view Gaussians Alignment method to ensure consistent depth
scales across different views. Our eFreeSplat represents an innovative approach
for generalizable novel view synthesis. Different from the existing pure
geometry-free methods, eFreeSplat focuses more on achieving epipolar-free
feature matching and encoding by providing 3D priors through cross-view
pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks
using the RealEstate10K and ACID datasets. Extensive experiments demonstrate
that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar
priors, achieving superior geometry reconstruction and novel view synthesis
quality. Project page: https://tatakai1.github.io/efreesplat/.


## Wavelet Burst Accumulation for turbulence mitigation

>Authors: Jerome Gilles, Stanley Osher

>2024-10-30

> http://arxiv.org/abs/2410.22802v1

In this paper, we investigate the extension of the recently proposed weighted
Fourier burst accumulation (FBA) method into the wavelet domain. The purpose of
FBA is to reconstruct a clean and sharp image from a sequence of blurred
frames. This concept lies in the construction of weights to amplify dominant
frequencies in the Fourier spectrum of each frame. The reconstructed image is
then obtained by taking the inverse Fourier transform of the average of all
processed spectra. In this paper, we first suggest to replace the rigid
registration step used in the original algorithm by a non-rigid registration in
order to be able to process sequences acquired through atmospheric turbulence.
Second, we propose to work in a wavelet domain instead of the Fourier one. This
leads us to the construction of two types of algorithms. Finally, we propose an
alternative approach to replace the weighting idea by an approach promoting the
**sparsity** in the used space. Several experiments are provided to illustrate the
efficiency of the proposed methods.


## MALoRA Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning

>Authors: Xujia Wang, Haiyan Zhao, Shuo Wang, Hanqing Wang, Zhiyuan Liu

>2024-10-30

> http://arxiv.org/abs/2410.22782v1

Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly
improved the adaptation of LLMs to downstream tasks in a resource-efficient
manner. However, in multi-task scenarios, challenges such as training imbalance
and the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which
combines LoRA with **sparse** Mixture-of-Experts, mitigates some of these issues by
promoting task-specific learning across experts. Despite this, MoLoRA remains
inefficient in terms of training speed, parameter utilization, and overall
multi-task performance. In this paper, we propose Mixture of Asymmetric
Low-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages
asymmetric optimization across LoRA experts. MALoRA reduces the number of
trainable parameters by 30% to 48%, increases training speed by 1.2x, and
matches the computational efficiency of single-task LoRA models. Additionally,
MALoRA addresses overfitting issues commonly seen in high-rank configurations,
enhancing performance stability. Extensive experiments across diverse
multi-task learning scenarios demonstrate that MALoRA consistently outperforms
all baseline methods in both inter-domain and intra-domain tasks.


## ETOEfficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses

>Authors: Junjie Ni, Guofeng Zhang, Guanglin Li, Yijin Li, Xinyang Liu, Zhaoyang Huang, Hujun Bao

>2024-10-30

> http://arxiv.org/abs/2410.22733v2

We tackle the efficiency problem of learning local feature matching. Recent
advancements have given rise to purely CNN-based and transformer-based
approaches, each augmented with deep learning techniques. While CNN-based
methods often excel in matching speed, transformer-based methods tend to
provide more accurate matches. We propose an efficient transformer-based
network architecture for local feature matching. This technique is built on
constructing multiple homography hypotheses to approximate the continuous
correspondence in the real world and uni-directional cross-attention to
accelerate the refinement. On the YFCC100M dataset, our matching accuracy is
competitive with LoFTR, a state-of-the-art transformer-based architecture,
while the inference speed is boosted to 4 times, even outperforming the
CNN-based methods. Comprehensive evaluations on other open datasets such as
Megadepth, ScanNet, and HPatches demonstrate our method's efficacy,
highlighting its potential to significantly enhance a wide array of downstream
applications.


## NeFF-BioNet Crop Biomass Prediction from Point Cloud to Drone Imagery

>Authors: Xuesong Li, Zeeshan Hayder, Ali Zia, Connor Cassidy, Shiming Liu, Warwick Stiller, Eric Stone, Warren Conaty, Lars Petersson, Vivien Rolland

>2024-10-30

> http://arxiv.org/abs/2410.23901v1

Crop biomass offers crucial insights into plant health and yield, making it
essential for crop science, farming systems, and agricultural research.
However, current measurement methods, which are labor-intensive, destructive,
and imprecise, hinder large-scale quantification of this trait. To address this
limitation, we present a biomass prediction network (BioNet), designed for
adaptation across different data modalities, including point clouds and drone
imagery. Our BioNet, utilizing a **sparse** 3D convolutional neural network (CNN)
and a transformer-based prediction module, processes point clouds and other 3D
data representations to predict biomass. To further extend BioNet for drone
imagery, we integrate a neural feature field (NeFF) module, enabling 3D
structure reconstruction and the transformation of 2D semantic features from
vision foundation models into the corresponding 3D surfaces. For the point
cloud modality, BioNet demonstrates superior performance on two public
datasets, with an approximate 6.1% relative improvement (RI) over the
state-of-the-art. In the RGB image modality, the combination of BioNet and NeFF
achieves a 7.9% RI. Additionally, the NeFF-based approach utilizes inexpensive,
portable drone-mounted cameras, providing a scalable solution for large field
applications.


## Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem

>Authors: Jin Huang, Xinyu Li, Liang Gao, Qihao Liu, Yue Teng

>2024-10-30

> http://arxiv.org/abs/2410.22657v1

Heuristic dispatching rules (HDRs) are widely regarded as effective methods
for solving dynamic job shop scheduling problems (DJSSP) in real-world
production environments. However, their performance is highly
scenario-dependent, often requiring expert customization. To address this,
genetic programming (GP) and gene expression programming (GEP) have been
extensively used for automatic algorithm design. Nevertheless, these approaches
often face challenges due to high randomness in the search process and limited
generalization ability, hindering the application of trained dispatching rules
to new scenarios or dynamic environments. Recently, the integration of large
language models (LLMs) with evolutionary algorithms has opened new avenues for
prompt engineering and automatic algorithm design. To enhance the capabilities
of LLMs in automatic HDRs design, this paper proposes a novel population
self-evolutionary (SeEvo) method, a general search framework inspired by the
self-reflective design strategies of human experts. The SeEvo method
accelerates the search process and enhances exploration capabilities.
Experimental results show that the proposed SeEvo method outperforms GP, GEP,
end-to-end deep reinforcement learning methods, and more than 10 common HDRs
from the literature, particularly in unseen and dynamic scenarios.


## WaveRoRA Wavelet Rotary Route Attention for Multivariate Time Series Forecasting

>Authors: Aobo Liang, Yan Sun

>2024-10-30

> http://arxiv.org/abs/2410.22649v1

In recent years, Transformer-based models (Transformers) have achieved
significant success in multivariate time series forecasting (MTSF). However,
previous works focus on extracting features either from the time domain or the
frequency domain, which inadequately captures the trends and periodic
characteristics. To address this issue, we propose a wavelet learning framework
to model complex temporal dependencies of the time series data. The wavelet
domain integrates both time and frequency information, allowing for the
analysis of local characteristics of signals at different scales. Additionally,
the Softmax self-attention mechanism used by Transformers has quadratic
complexity, which leads to excessive computational costs when capturing
long-term dependencies. Therefore, we propose a novel attention mechanism:
Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary
position embeddings to inject relative positional information to sequence
tokens and introduces a small number of routing tokens $r$ to aggregate
information from the $**KV**$ matrices and redistribute it to the $Q$ matrix,
offering linear complexity. We further propose WaveRoRA, which leverages RoRA
to capture inter-series dependencies in the wavelet domain. We conduct
extensive experiments on eight real-world datasets. The results indicate that
WaveRoRA outperforms existing state-of-the-art models while maintaining lower
computational costs.


## Fast and reliable atom transport by optical tweezers

>Authors: Sunhwa Hwang, Hansub Hwang, Kangjin Kim, Andrew Byun, Seokho Jeong, Maynardo Pratama Soegianto, Jaewook Ahn

>2024-10-30

> http://arxiv.org/abs/2410.22627v1

Movable single atoms have drawn significant attention for their potentials as
flying quantum memory in non-local, dynamic quantum computing architectures.
However, when dynamic optical tweezers are employed to control atoms
opto-mechanically, conventional methods such as adiabatic controls and constant
jerk controls are either inherently slow or induce mechanical heating, leading
to atom loss over long distances or at high speeds. To address these
challenges, we explore the method known as shortcuts to adiabaticity (STA) as
an efficient alternative for fast and reliable atom transport control. We
present a series of proof-of-concept experiments demonstrating that STA-based
optical tweezer trajectories can achieve both rapid and reliable single-atom
transport. These experiments include moving atoms between two locations,
adjusting speeds en route, and navigating curved trajectories. Our results
indicate that atoms can be transported with a constant **acceleration** on average
over distances that is only limited by trap lifetime, while effectively
suppressing vibrational heating. This makes STA methods particularly
well-suited for long-distance atom transport, potentially spanning distances
over centimeter scales, such as between quantum information devices.

