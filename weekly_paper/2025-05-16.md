# 2025-05-16

# Table of Contents
* [MASSV Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](#MASSV-Multimodal-Adaptation-and-Self-Data-Distillation-for-Speculative-Decoding-of-Vision-Language-Models)
* [Quantized Approximate Signal Processing (QASP) Towards Homomorphic Encryption for audio](#Quantized-Approximate-Signal-Processing-(QASP)-Towards-Homomorphic-Encryption-for-audio)
* [Are Sparse Autoencoders Useful for Java Function Bug Detection?](#Are-Sparse-Autoencoders-Useful-for-Java-Function-Bug-Detection?)
* [SpecOffload Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](#SpecOffload-Unlocking-Latent-GPU-Capacity-for-LLM-Inference-on-Resource-Constrained-Devices)
* [MTVCrafter 4D Motion Tokenization for Open-World Human Image Animation](#MTVCrafter-4D-Motion-Tokenization-for-Open-World-Human-Image-Animation)
* [VQ-Logits Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](#VQ-Logits-Compressing-the-Output-Bottleneck-of-Large-Language-Models-via-Vector-Quantized-Logits)
* [QuXAI Explainers for Hybrid Quantum Machine Learning Models](#QuXAI-Explainers-for-Hybrid-Quantum-Machine-Learning-Models)
* [All You Need Is Synthetic Task Augmentation](#All-You-Need-Is-Synthetic-Task-Augmentation)
* [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](#A-Scalable-Gradient-Based-Optimization-Framework-for-Sparse-Minimum-Variance-Portfolio-Selection)
* [Rethinking Circuit Completeness in Language Models AND, OR, and ADDER Gates](#Rethinking-Circuit-Completeness-in-Language-Models-AND,-OR,-and-ADDER-Gates)
* [Quantum Computing and AI Perspectives on Advanced Automation in Science and Engineering](#Quantum-Computing-and-AI-Perspectives-on-Advanced-Automation-in-Science-and-Engineering)
* [Optimal Control of Parabolic Differential Equations Using Radau Collocation](#Optimal-Control-of-Parabolic-Differential-Equations-Using-Radau-Collocation)
* [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](#Contextual-Phenotyping-of-Pediatric-Sepsis-Cohort-Using-Large-Language-Models)
* [Accelerating Fast Ewald Summation with Prolates for Molecular Dynamics Simulations](#Accelerating-Fast-Ewald-Summation-with-Prolates-for-Molecular-Dynamics-Simulations)
* [Analog Foundation Models](#Analog-Foundation-Models)
* [MUST Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](#MUST-Multi-Scale-Structural-Temporal-Link-Prediction-Model-for-UAV-Ad-Hoc-Networks)
* [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](#Optimal-Transport-Based-Domain-Adaptation-for-Rotated-Linear-Regression)
* [Manifestly Covariant Canonical Formalism of Quadratic Gravity](#Manifestly-Covariant-Canonical-Formalism-of-Quadratic-Gravity)
* [Dataflow & Tiling Strategies in Edge-AI FPGA Accelerators A Comprehensive Literature Review](#Dataflow-&-Tiling-Strategies-in-Edge-AI-FPGA-Accelerators-A-Comprehensive-Literature-Review)
* [A Formalism for the Transport and Matching of Coupled Beams in Accelerators](#A-Formalism-for-the-Transport-and-Matching-of-Coupled-Beams-in-Accelerators)
* [ITERA-LLM Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition](#ITERA-LLM-Boosting-Sub-8-Bit-Large-Language-Model-Inference-via-Iterative-Tensor-Decomposition)
* [ForeCite Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](#ForeCite-Adapting-Pre-Trained-Language-Models-to-Predict-Future-Citation-Rates-of-Academic-Papers)
* [Comparing Parallel Functional Array Languages Programming and Performance](#Comparing-Parallel-Functional-Array-Languages-Programming-and-Performance)
* [SPAT Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models](#SPAT-Sensitivity-based-Multihead-attention-Pruning-on-Time-Series-Forecasting-Models)
* [PWC-MoE Privacy-Aware Wireless Collaborative Mixture of Experts](#PWC-MoE-Privacy-Aware-Wireless-Collaborative-Mixture-of-Experts)
* [AC-PKAN Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks](#AC-PKAN-Attention-Enhanced-and-Chebyshev-Polynomial-Based-Physics-Informed-Kolmogorov-Arnold-Networks)
* [TRAIL Trace Reasoning and Agentic Issue Localization](#TRAIL-Trace-Reasoning-and-Agentic-Issue-Localization)
* [Resource-Efficient Language Models Quantization for Fast and Accessible Inference](#Resource-Efficient-Language-Models-Quantization-for-Fast-and-Accessible-Inference)
* [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](#Automatic-Task-Detection-and-Heterogeneous-LLM-Speculative-Decoding)
* [SPP-SBL Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery](#SPP-SBL-Space-Power-Prior-Sparse-Bayesian-Learning-for-Block-Sparse-Recovery)
* [IterKey Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](#IterKey-Iterative-Keyword-Generation-with-LLMs-for-Enhanced-Retrieval-Augmented-Generation)
* [Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments](#Efficient-Unstructured-Pruning-of-Mamba-State-Space-Models-for-Resource-Constrained-Environments)
* [Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles](#Scaling-Multi-Agent-Reinforcement-Learning-for-Underwater-Acoustic-Tracking-via-Autonomous-Vehicles)
* [Aitomia Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations](#Aitomia-Your-Intelligent-Assistant-for-AI-Driven-Atomistic-and-Quantum-Chemical-Simulations)
* [GDNTT an Area-Efficient Parallel NTT Accelerator Using Glitch-Driven Near-Memory Computing and Reconfigurable 10T SRAM](#GDNTT-an-Area-Efficient-Parallel-NTT-Accelerator-Using-Glitch-Driven-Near-Memory-Computing-and-Reconfigurable-10T-SRAM)
* [Self Rewarding Self Improving](#Self-Rewarding-Self-Improving)
* [Fused3S Fast Sparse Attention on Tensor Cores](#Fused3S-Fast-Sparse-Attention-on-Tensor-Cores)
* [Beyond Input Activations Identifying Influential Latents by Gradient Sparse Autoencoders](#Beyond-Input-Activations-Identifying-Influential-Latents-by-Gradient-Sparse-Autoencoders)
* [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](#An-Extra-RMSNorm-is-All-You-Need-for-Fine-Tuning-to-1.58-Bits)
* [RDD Robust Feature Detector and Descriptor using Deformable Transformer](#RDD-Robust-Feature-Detector-and-Descriptor-using-Deformable-Transformer)
* [SpecRouter Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models](#SpecRouter-Adaptive-Routing-for-Multi-Level-Speculative-Decoding-in-Large-Language-Models)
* [OnPrem.LLM A Privacy-Conscious Document Intelligence Toolkit](#OnPrem.LLM-A-Privacy-Conscious-Document-Intelligence-Toolkit)
* [Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using Edge AI](#Privacy-Preserving-Real-Time-Vietnamese-English-Translation-on-iOS-using-Edge-AI)
* [GIFStream 4D Gaussian-based Immersive Video with Feature Stream](#GIFStream-4D-Gaussian-based-Immersive-Video-with-Feature-Stream)
* [QuantX A Framework for Hardware-Aware Quantization of Generative AI Workloads](#QuantX-A-Framework-for-Hardware-Aware-Quantization-of-Generative-AI-Workloads)
* [Integrating Machine Learning with Triboelectric Nanogenerators Optimizing Electrode Materials and Doping Strategies for Intelligent Energy Harves](#Integrating-Machine-Learning-with-Triboelectric-Nanogenerators-Optimizing-Electrode-Materials-and-Doping-Strategies-for-Intelligent-Energy-Harves)
* [Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption](#Private-LoRA-Fine-tuning-of-Open-Source-LLMs-with-Homomorphic-Encryption)
* [Autonomous Robotic Pruning in Orchards and Vineyards a Review](#Autonomous-Robotic-Pruning-in-Orchards-and-Vineyards-a-Review)
* [Semantic Retention and Extreme Compression in LLMs Can We Have Both?](#Semantic-Retention-and-Extreme-Compression-in-LLMs-Can-We-Have-Both?)
* [UMoE Unifying Attention and FFN with Shared Experts](#UMoE-Unifying-Attention-and-FFN-with-Shared-Experts)
* [Comet Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity](#Comet-Accelerating-Private-Inference-for-Large-Language-Model-by-Predicting-Activation-Sparsity)
* [Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding](#Multi-band-Frequency-Reconstruction-for-Neural-Psychoacoustic-Coding)
* [PrefillOnly An Inference Engine for Prefill-only Workloads in Large Language Model Applications](#PrefillOnly-An-Inference-Engine-for-Prefill-only-Workloads-in-Large-Language-Model-Applications)
* [High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution](#High-Frequency-Prior-Driven-Adaptive-Masking-for-Accelerating-Image-Super-Resolution)
* [Ecco Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware Cache Compression](#Ecco-Improving-Memory-Bandwidth-and-Capacity-for-LLMs-via-Entropy-aware-Cache-Compression)
* [Learning Soft Sparse Shapes for Efficient Time-Series Classification](#Learning-Soft-Sparse-Shapes-for-Efficient-Time-Series-Classification)
* [FreqMoE Dynamic Frequency Enhancement for Neural PDE Solvers](#FreqMoE-Dynamic-Frequency-Enhancement-for-Neural-PDE-Solvers)
* [Image processing Application Development on Software Configurable Processor Array](#Image-processing-Application-Development-on-Software-Configurable-Processor-Array)
* [Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers](#Symbolic-Rule-Extraction-from-Attention-Guided-Sparse-Representations-in-Vision-Transformers)
* [Gated Attention for Large Language Models Non-linearity, Sparsity, and Attention-Sink-Free](#Gated-Attention-for-Large-Language-Models-Non-linearity,-Sparsity,-and-Attention-Sink-Free)
* [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4) Analysis and Variations](#Improving-Block-Wise-LLM-Quantization-by-4-bit-Block-Wise-Optimal-Float-(BOF4)-Analysis-and-Variations)
* [Efficient Telecom Specific LLM TSLAM-Mini with QLoRA and Digital Twin Data](#Efficient-Telecom-Specific-LLM-TSLAM-Mini-with-QLoRA-and-Digital-Twin-Data)
* [Challenging GPU Dominance When CPUs Outperform for On-Device LLM Inference](#Challenging-GPU-Dominance-When-CPUs-Outperform-for-On-Device-LLM-Inference)
* [Decoding Algorithms for Two-dimensional Constacyclic Codes over $\mathbb{F}_q$](#Decoding-Algorithms-for-Two-dimensional-Constacyclic-Codes-over-$\mathbb{F}_q$)
* [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](#Remote-Rowhammer-Attack-using-Adversarial-Observations-on-Federated-Learning-Clients)
* [New Advances in Phonons From Band Topology to Quasiparticle Chirality](#New-Advances-in-Phonons-From-Band-Topology-to-Quasiparticle-Chirality)
* [Turbo-ICL In-Context Learning-Based Turbo Equalization](#Turbo-ICL-In-Context-Learning-Based-Turbo-Equalization)
* [Fast recovery of parametric eigenvalues depending on several parameters and location of high order exceptional points](#Fast-recovery-of-parametric-eigenvalues-depending-on-several-parameters-and-location-of-high-order-exceptional-points)
* [Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities](#Assessing-Tenstorrent's-RISC-V-MatMul-Acceleration-Capabilities)
* [Functoriality of Enriched Data Types](#Functoriality-of-Enriched-Data-Types)
* [Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition](#Accelerating-Diffusion-Transformer-via-Increment-Calibrated-Caching-with-Channel-Aware-Singular-Value-Decomposition)
* [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](#Sparse-Attention-Remapping-with-Clustering-for-Efficient-LLM-Decoding-on-PIM)
* [QoSBERT An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](#QoSBERT-An-Uncertainty-Aware-Approach-based-on-Pre-trained-Language-Models-for-Service-Quality-Prediction)
* [Dome-DETR DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection](#Dome-DETR-DETR-with-Density-Oriented-Feature-Query-Manipulation-for-Efficient-Tiny-Object-Detection)
* [All-to-All Communication with Mobile Edge Adversary Almost Linearly More Faults, For Free](#All-to-All-Communication-with-Mobile-Edge-Adversary-Almost-Linearly-More-Faults,-For-Free)
* [A framework for learning symbolic turbulence models from indirect observation data via neural networks and feature importance analysis](#A-framework-for-learning-symbolic-turbulence-models-from-indirect-observation-data-via-neural-networks-and-feature-importance-analysis)


## MASSV Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models

>Authors: Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa

>2025-05-15

> http://arxiv.org/abs/2505.10526v1

Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.


## Quantized Approximate Signal Processing (QASP) Towards Homomorphic Encryption for audio

>Authors: Tu Duyen Nguyen, Adrien Lesage, Clotilde Cantini, Rachid Riad

>2025-05-15

> http://arxiv.org/abs/2505.10500v1

Audio and speech data are increasingly used in machine learning applications
such as speech recognition, speaker identification, and mental health
monitoring. However, the passive collection of this data by audio listening
devices raises significant privacy concerns. Fully homomorphic encryption (FHE)
offers a promising solution by enabling computations on encrypted data and
preserving user privacy. Despite its potential, prior attempts to apply FHE to
audio processing have faced challenges, particularly in securely computing time
frequency representations, a critical step in many audio tasks.
  Here, we addressed this gap by introducing a fully secure pipeline that
computes, with FHE and **quantize**d neural network operations, four fundamental
time-frequency representations: Short-Time Fourier Transform (STFT), Mel
filterbanks, Mel-frequency cepstral coefficients (MFCCs), and gammatone
filters. Our methods also support the private computation of audio descriptors
and convolutional neural network (CNN) classifiers. Besides, we proposed
approximate STFT algorithms that lighten computation and bit use for
statistical and machine learning analyses.
  We ran experiments on the VocalSet and OxVoc datasets demonstrating the fully
private computation of our approach. We showed significant performance
improvements with STFT approximation in private statistical analysis of audio
markers, and for vocal exercise classification with CNNs. Our results reveal
that our approximations substantially reduce error rates compared to
conventional STFT implementations in FHE. We also demonstrated a fully private
classification based on the raw audio for gender and vocal exercise
classification. Finally, we provided a practical heuristic for parameter
selection, making **quantize**d approximate signal processing accessible to
researchers and practitioners aiming to protect sensitive audio data.


## Are Sparse Autoencoders Useful for Java Function Bug Detection?

>Authors: Rui Melo, Claudia Mamede, Andre Catarino, Rui Abreu, Henrique Lopes Cardoso

>2025-05-15

> http://arxiv.org/abs/2505.10375v1

Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.


## SpecOffload Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices

>Authors: Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang

>2025-05-15

> http://arxiv.org/abs/2505.10259v1

Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .


## MTVCrafter 4D Motion Tokenization for Open-World Human Image Animation

>Authors: Yanbo Ding

>2025-05-15

> http://arxiv.org/abs/2505.10238v1

Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to **quantize** 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.


## VQ-Logits Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits

>Authors: Jintian Shao, Hongyi Huang, Jiayi Wu, YiMing Cheng, ZhiYu Wu, You Shan, MingKai Zheng

>2025-05-15

> http://arxiv.org/abs/2505.10202v1

Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.


## QuXAI Explainers for Hybrid Quantum Machine Learning Models

>Authors: Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique

>2025-05-15

> http://arxiv.org/abs/2505.10167v1

The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
**quantize**d feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.


## All You Need Is Synthetic Task Augmentation

>Authors: Guillaume Godin

>2025-05-15

> http://arxiv.org/abs/2505.10120v1

Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both **sparse**
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.


## A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection

>Authors: Sarat Moka, Matias Quiroz, Vali Asimit, Samuel Muller

>2025-05-15

> http://arxiv.org/abs/2505.10099v1

Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial **sparse** selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a **sparse** binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.


## Rethinking Circuit Completeness in Language Models AND, OR, and ADDER Gates

>Authors: Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang

>2025-05-15

> http://arxiv.org/abs/2505.10039v1

Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and **sparsity** of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.


## Quantum Computing and AI Perspectives on Advanced Automation in Science and Engineering

>Authors: Tadashi Kadowaki

>2025-05-15

> http://arxiv.org/abs/2505.10012v1

Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.


## Optimal Control of Parabolic Differential Equations Using Radau Collocation

>Authors: Alexander M. Davies, Sara Pollock, Miriam E. Dennis, Anil V. Rao

>2025-05-14

> http://arxiv.org/abs/2505.09815v1

A method is presented for the numerical solution of optimal boundary control
problems governed by parabolic partial differential equations. The continuous
space-time optimal control problem is transcribed into a **sparse** nonlinear
programming problem through state and control parameterization. In particular,
a multi-interval flipped Legendre-Gauss-Radau collocation method is implemented
for temporal discretization alongside a Galerkin finite element spatial
discretization. The finite element discretization allows for a reduction in
problem size and avoids the redefinition of constraints required under a
previous method. Further, a generalization of a Kirchoff transformation is
performed to handle variational form nonlinearities in the context of numerical
optimization. Due to the correspondence between the collocation points and the
applied boundary conditions, the multi-interval flipped Legendre-Gauss-Radau
collocation method is demonstrated to be preferable over the standard
Legendre-Gauss-Radau collocation method for optimal control problems governed
by parabolic partial differential equations. The details of the resulting
transcription of the optimal control problem into a nonlinear programming
problem are provided. Lastly, numerical examples demonstrate that the use of a
multi-interval flipped Legendre-Gauss-Radau temporal discretization can lead to
a reduction in the required number of collocation points to compute accurate
values of the optimal objective in comparison to other methods.


## Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models

>Authors: Aditya Nagori, Ayush Gautam, Matthew O. Wiens, Vuong Nguyen, Nathan Kenya Mugisha, Jerome Kabakyenga, Niranjan Kissoon, John Mark Ansermino, Rishikesan Kamaleswaran

>2025-05-14

> http://arxiv.org/abs/2505.09805v1

Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using **quantize**d LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.


## Accelerating Fast Ewald Summation with Prolates for Molecular Dynamics Simulations

>Authors: Jiuyang Liang, Libin Lu, Alex Barnett, Leslie Greengard, Shidong Jiang

>2025-05-14

> http://arxiv.org/abs/2505.09727v1

Fast Ewald summation is the most widely used approach for computing
long-range Coulomb interactions in molecular dynamics (MD) simulations. While
the asymptotic scaling is nearly optimal, its performance on parallel
architectures is dominated by the global communication required for the
underlying fast Fourier transform (FFT). Here, we develop a novel method, ESP -
Ewald summation with prolate spheroidal wave functions (PSWFs) - that, for a
fixed precision, sharply reduces the size of this transform by performing the
Ewald split via a PSWF. In addition, PSWFs minimize the cost of spreading and
interpolation steps that move information between the particles and the
underlying uniform grid. We have integrated the ESP method into two widely-used
open-source MD packages: LAMMPS and GROMACS. Detailed benchmarks show that this
reduces the cost of computing far-field electrostatic interactions by an order
of magnitude, leading to better strong scaling with respect to number of cores.
The total execution time is reduced by a factor of 2 to 3 when using more than
one thousand cores, even after optimally tuning the existing internal
parameters in the native codes. We validate the accelerated codes in realistic
long-time biological simulations.


## Analog Foundation Models

>Authors: Julian BÃ¼chel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian

>2025-05-14

> http://arxiv.org/abs/2505.09663v1

Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output **quantization**. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and **quantization** constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be **quantize**d for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
**quantization**. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .


## MUST Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks

>Authors: Cunlai Pu, Fangrui Wu, Rajput Ramiz Sharafat, Guangzhao Dai, Xiangbo Shu

>2025-05-14

> http://arxiv.org/abs/2505.09331v1

Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and **sparse** nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of **sparsity**, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
**sparsity** by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and **sparse** UANETs.


## Optimal Transport-Based Domain Adaptation for Rotated Linear Regression

>Authors: Brian Britos, Mathias Bourel

>2025-05-14

> http://arxiv.org/abs/2505.09229v1

Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is **sparse**ly
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.


## Manifestly Covariant Canonical Formalism of Quadratic Gravity

>Authors: Ichiro Oda

>2025-05-14

> http://arxiv.org/abs/2505.09149v1

We present the manifestly covariant **quantization** of quadratic gravity or
higher-derivative gravity in the de Donder gauge condition (or harmonic gauge
condition) for general coordinate invariance on the basis of the BRST
transformation. We explicitly calculate various equal-time commutation
relations (ETCRs), in particlular, the ETCRs between the metric tensor and its
time derivatives in detail, and show that they are identically vanishing. We
also clarify global symmetries, the physical content of quadratic gravity, and
clearly show that this theory is not unitary and has a massive scalar, massive
ghost and massless graviton as physical modes. Finally, we comment on
confinement of the massive ghost, thereby recovering the unitarity of the
physical S-matrix in quadratic gravity.


## Dataflow & Tiling Strategies in Edge-AI FPGA Accelerators A Comprehensive Literature Review

>Authors: Zhaoqin Li

>2025-05-13

> http://arxiv.org/abs/2505.08992v1

Edge-AI applications demand high-throughput, low-latency inference on FPGAs
under tight resource and power constraints. This survey provides a
comprehensive review of two key architectural decisions for FPGA-based neural
network accelerators: (i) the dataflow (the order and manner in which data is
moved and reused on chip), and (ii) the tiling/blocking strategy (how large
tensors are partitioned to fit on-chip). We first present a broadened taxonomy
of canonical dataflow styles: Weight-Stationary, Output-Stationary,
Row-Stationary, and No-Local-Reuse, including formal definitions,
pseudocode/diagrams, and real FPGA examples. We then discuss analytical
frameworks (MAESTRO, Timeloop) and compare them with a concise feature table,
illustrating how they model reuse, performance, and hardware costs. Next, we
detail multi-level tiling and loop unrolling/pipelining strategies for FPGAs,
clarifying how each memory tier (registers, LUTRAM, BRAM, HBM) can be
exploited. Our four case studies - FINN, FINN-R, FlightLLM, and SSR - highlight
distinct dataflows (from binary streaming to hybrid **sparse** transformations) and
tiling patterns. We include a unified comparison matrix covering platform,
precision, throughput, resource utilization, and energy efficiency, plus small
block diagrams for each design. We conclude by examining design automation
trade-offs among HLS, DSL, and hand-coded RTL, offering a "lessons learned"
summary box, and charting future research directions in partial
reconfiguration, hybrid dataflows, and domain-specific compiler flows for
next-generation edge AI FPGA accelerators.


## A Formalism for the Transport and Matching of Coupled Beams in Accelerators

>Authors: Onur Gilanliogullari, Brahim Mustapha, Pavel Snopok

>2025-05-13

> http://arxiv.org/abs/2505.08987v1

Understanding transverse coupling dynamics is crucial for beam physics,
accelerator design, and operations. Currently, most accelerators are designed
for uncoupled beams, and coupling is treated as an error or perturbation. Many
transverse ($x$-$y$) coupling parametrizations exist: Edward-Teng, Mais-Ripken,
Levedev-Bogacz, and others. Here, we present an explicit and complete formalism
for transporting coupled beam optics functions based on Mais-Ripken and
Lebedev-Bogacz formalism. The formalism allows for matching generally coupled
beam optics functions but applies to uncoupled optics as well. A complete
transformation method for coupled optics provides easy matching routines that
can be added to known beam optics codes that lack this feature. For fully
coupled lattices, we present methods for extracting eigenmode emittances and
other beam parameters from observables that can be measured, which is essential
to diagnose and characterize the beam in a real machine.


## ITERA-LLM Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition

>Authors: Keran Zheng, Yinting Huang, Zhewen Yu, Christos-Savvas Bouganis

>2025-05-13

> http://arxiv.org/abs/2505.08981v1

Recent advancements in Large Language Models (LLMs) have demonstrated
impressive capabilities as their scale expands to billions of parameters.
Deploying these large-scale models on resource-constrained platforms presents
significant challenges, with post-training fixed-point **quantization** often used
as a model compression technique. However, **quantization**-only methods typically
lead to significant accuracy degradation in LLMs when precision falls below 8
bits. This paper addresses this challenge through a software-hardware co-design
framework, ITERA-LLM, which integrates sub-8-bit **quantization** with SVD-based
iterative low-rank tensor decomposition for error compensation, leading to
higher compression ratios and reduced computational complexity. The proposed
approach is complemented by a hardware-aware Design Space Exploration (DSE)
process that optimizes accuracy, latency, and resource utilization, tailoring
the configuration to the specific requirements of the targeted LLM. Our results
show that ITERA-LLM achieves linear layer latency reduction of up to 41.1%,
compared to **quantization**-only baseline approach while maintaining similar model
accuracy.


## ForeCite Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers

>Authors: Gavin Hull, Alex Bihlo

>2025-05-13

> http://arxiv.org/abs/2505.08941v1

Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the **acceleration** of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.


## Comparing Parallel Functional Array Languages Programming and Performance

>Authors: David van Balen, Tiziano De Matteis, Clemens Grelck, Troels Henriksen, Aaron W. Hsu, Gabriele K. Keller, Thomas Koopman, Trevor L. McDonell, Cosmin Oancea, Sven-Bodo Scholz, Artjoms Sinkarovs, Tom Smeding, Phil Trinder, Ivo Gabe de Wolff, Alexandros Nikolaos Ziogas

>2025-05-13

> http://arxiv.org/abs/2505.08906v1

Parallel functional array languages are an emerging class of programming
languages that promise to combine low-effort parallel programming with good
performance and performance portability. We systematically compare the designs
and implementations of five different functional array languages: Accelerate,
APL, DaCe, Futhark, and SaC. We demonstrate the expressiveness of functional
array programming by means of four challenging benchmarks, namely N-body
simulation, MultiGrid, Quickhull, and Flash Attention. These benchmarks
represent a range of application domains and parallel computational models. We
argue that the functional array code is much shorter and more comprehensible
than the hand-optimized baseline implementations because it omits
architecture-specific aspects. Instead, the language implementations generate
both multicore and GPU executables from a single source code base. Hence, we
further argue that functional array code could more easily be ported to, and
optimized for, new parallel architectures than conventional implementations of
numerical kernels. We demonstrate this potential by reporting the performance
of the five parallel functional array languages on a total of 39 instances of
the four benchmarks on both a 32-core AMD EPYC 7313 multicore system and on an
NVIDIA A30 GPU. We explore in-depth why each language performs well or not so
well on each benchmark and architecture. We argue that the results demonstrate
that mature functional array languages have the potential to deliver
performance competitive with the best available conventional techniques.


## SPAT Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models

>Authors: Suhan Guo, Jiahong Deng, Mengjun Yi, Furao Shen, Jian Zhao

>2025-05-13

> http://arxiv.org/abs/2505.08768v1

Attention-based architectures have achieved superior performance in
multivariate time series forecasting but are computationally expensive.
Techniques such as patching and adaptive masking have been developed to reduce
their sizes and latencies. In this work, we propose a structured **pruning**
method, SPAT ($\textbf{S}$ensitivity $\textbf{P}$runer for
$\textbf{At}$tention), which selectively removes redundant attention mechanisms
and yields highly effective models. Different from previous approaches, SPAT
aims to remove the entire attention module, which reduces the risk of
overfitting and enables speed-up without demanding specialized hardware. We
propose a dynamic sensitivity metric, $\textbf{S}$ensitivity
$\textbf{E}$nhanced $\textbf{N}$ormalized $\textbf{D}$ispersion (SEND) that
measures the importance of each attention module during the pre-training phase.
Experiments on multivariate datasets demonstrate that SPAT-pruned models
achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.
Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based
and LLM-based SOTA methods in both standard and zero-shot inference,
highlighting the importance of retaining only the most effective attention
mechanisms. We have made our code publicly available
https://anonymous.4open.science/r/SPAT-6042.


## PWC-MoE Privacy-Aware Wireless Collaborative Mixture of Experts

>Authors: Yang Su, Na Yan, Yansha Deng, Robert Schober

>2025-05-13

> http://arxiv.org/abs/2505.08719v1

Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a **sparse** privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.


## AC-PKAN Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks

>Authors: Hangwei Zhang, Zhimu Huang, Yan Wang

>2025-05-13

> http://arxiv.org/abs/2505.08687v1

Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving
partial differential equations (PDEs). Yet their original formulation is
computationally and memory intensive, motivating the introduction of Chebyshev
Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed
the vanilla KANs architecture, our rigorous theoretical analysis reveals that
they still suffer from rank collapse, ultimately limiting their expressive
capacity. To overcome these limitations, we enhance Chebyshev1KANs by
integrating wavelet-activated MLPs with learnable parameters and an internal
attention mechanism. We prove that this design preserves a full-rank Jacobian
and is capable of approximating solutions to PDEs of arbitrary order.
Furthermore, to alleviate the loss instability and imbalance introduced by the
Chebyshev polynomial basis, we externally incorporate a Residual Gradient
Attention (RGA) mechanism that dynamically re-weights individual loss terms
according to their gradient norms and residual magnitudes. By jointly
leveraging internal and external attention, we present AC-PKAN, a novel
architecture that constitutes an enhancement to weakly supervised
Physics-Informed Neural Networks (PINNs) and extends the expressive power of
KANs. Experimental results from nine benchmark tasks across three domains show
that AC-PKAN consistently outperforms or matches state-of-the-art models such
as PINNsFormer, establishing it as a highly effective tool for solving complex
real-world engineering problems in zero-data or data-**sparse** regimes. The code
will be made publicly available upon acceptance.


## TRAIL Trace Reasoning and Agentic Issue Localization

>Authors: Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian

>2025-05-13

> http://arxiv.org/abs/2505.08638v1

The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.


## Resource-Efficient Language Models Quantization for Fast and Accessible Inference

>Authors: Tollef Emil JÃ¸rgensen

>2025-05-13

> http://arxiv.org/abs/2505.08620v1

Large language models have significantly advanced natural language
processing, yet their heavy resource demands pose severe challenges regarding
hardware accessibility and energy consumption. This paper presents a focused
and high-level review of post-training **quantization** (PTQ) techniques designed
to optimize the inference efficiency of LLMs by the end-user, including details
on various **quantization** schemes, granularities, and trade-offs. The aim is to
provide a balanced overview between the theory and applications of
post-training **quantization**.


## Automatic Task Detection and Heterogeneous LLM Speculative Decoding

>Authors: Danying Ge, Jianhua Gao, Qizhi Jiang, Yifei Feng, Weixing Ji

>2025-05-13

> http://arxiv.org/abs/2505.08600v1

Speculative decoding, which combines a draft model with a target model, has
emerged as an effective approach to accelerate large language model (LLM)
inference. However, existing methods often face a trade-off between the
acceptance rate and decoding speed in downstream tasks due to the limited
capacity of the draft model, making it difficult to ensure efficiency across
diverse tasks. To address this problem, we propose a speculative decoding
algorithm tailored for downstream task optimization. It includes an automatic
task partitioning and assigning method, which automatically categorizes
downstream tasks into different sub-tasks and assigns them to a set of
heterogeneous draft models. Each draft model is aligned with the target model
using task-specific data, thereby enhancing the consistency of inference
results. In addition, our proposed method incorporates an online lightweight
prompt classifier to dynamically route prompts to the appropriate draft model.
Experimental results demonstrate that the proposed method improves draft
accuracy by 6% to 50% over vanilla speculative decoding, while achieving a
speedup of 1.10x to 2.64x in LLM inference.


## SPP-SBL Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery

>Authors: Yanhao Zhang, Zhihan Zhu, Yong Xia

>2025-05-13

> http://arxiv.org/abs/2505.08518v1

The recovery of block-**sparse** signals with unknown structural patterns remains
a fundamental challenge in structured **sparse** signal reconstruction. By
proposing a variance transformation framework, this paper unifies existing
pattern-based block **sparse** Bayesian learning methods, and introduces a novel
space power prior based on undirected graph models to adaptively capture the
unknown patterns of block-**sparse** signals. By combining the EM algorithm with
high-order equation root-solving, we develop a new structured **sparse** Bayesian
learning method, SPP-SBL, which effectively addresses the open problem of space
coupling parameter estimation in pattern-based methods. We further demonstrate
that learning the relative values of space coupling parameters is key to
capturing unknown block-**sparse** patterns and improving recovery accuracy.
Experiments validate that SPP-SBL successfully recovers various challenging
structured **sparse** signals (e.g., chain-structured signals and multi-pattern
**sparse** signals) and real-world multi-modal structured **sparse** signals (images,
audio), showing significant advantages in recovery accuracy across multiple
metrics.


## IterKey Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation

>Authors: Kazuki Hayashi, Hidetaka Kamigaito, Shinya Kouda, Taro Watanabe

>2025-05-13

> http://arxiv.org/abs/2505.08450v1

Retrieval-Augmented Generation (RAG) has emerged as a way to complement the
in-context knowledge of Large Language Models (LLMs) by integrating external
documents. However, real-world applications demand not only accuracy but also
interpretability. While dense retrieval methods provide high accuracy, they
lack interpretability; conversely, **sparse** retrieval methods offer transparency
but often fail to capture the full intent of queries due to their reliance on
keyword matching. To address these issues, we introduce IterKey, an LLM-driven
iterative keyword generation framework that enhances RAG via **sparse** retrieval.
IterKey consists of three LLM-driven stages: generating keywords for retrieval,
generating answers based on retrieved documents, and validating the answers. If
validation fails, the process iteratively repeats with refined keywords. Across
four QA tasks, experimental results show that IterKey achieves 5% to 20%
accuracy improvements over BM25-based RAG and simple baselines. Its performance
is comparable to dense retrieval-based RAG and prior iterative query refinement
methods using dense models. In summary, IterKey is a novel BM25-based approach
leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with
interpretability.


## Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments

>Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

>2025-05-13

> http://arxiv.org/abs/2505.08299v1

State-space models (SSMs), particularly the Mamba architecture, have emerged
as powerful alternatives to Transformers for sequence modeling, offering
linear-time complexity and competitive performance across diverse tasks.
However, their large parameter counts pose significant challenges for
deployment in resource-constrained environments. We propose a novel
unstructured **pruning** framework tailored for Mamba models that achieves up to
70\% parameter reduction while retaining over 95\% of the original performance.
Our approach integrates three key innovations: (1) a gradient-aware magnitude
**pruning** technique that combines weight magnitude and gradient information to
identify less critical parameters, (2) an iterative **pruning** schedule that
gradually increases **sparsity** to maintain model stability, and (3) a global
**pruning** strategy that optimizes parameter allocation across the entire model.
Through extensive experiments on WikiText-103, Long Range Arena, and ETT
time-series benchmarks, we demonstrate significant efficiency gains with
minimal performance degradation. Our analysis of **pruning** effects on Mamba's
components reveals critical insights into the architecture's redundancy and
robustness, enabling practical deployment in resource-constrained settings
while broadening Mamba's applicability.


## Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles

>Authors: Matteo Gallici, Ivan Masmitja, Mario MartÃ­n

>2025-05-13

> http://arxiv.org/abs/2505.08222v1

Autonomous vehicles (AV) offer a cost-effective solution for scientific
missions such as underwater tracking. Recently, reinforcement learning (RL) has
emerged as a powerful method for controlling AVs in complex marine
environments. However, scaling these techniques to a fleet--essential for
multi-target tracking or targets with rapid, unpredictable motion--presents
significant computational challenges. Multi-Agent Reinforcement Learning (MARL)
is notoriously sample-inefficient, and while high-fidelity simulators like
Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,
they offer no significant speedup for multi-vehicle scenarios, making MARL
training impractical. To address these limitations, we propose an iterative
distillation method that transfers high-fidelity simulations into a simplified,
GPU-accelerated environment while preserving high-level dynamics. This approach
achieves up to a 30,000x speedup over Gazebo through parallelization, enabling
efficient training via end-to-end GPU **acceleration**. Additionally, we introduce
a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent
policies invariant to the number of agents and targets, significantly improving
sample efficiency. Following large-scale curriculum learning conducted entirely
on GPU, we perform extensive evaluations in Gazebo, demonstrating that our
method maintains tracking errors below 5 meters over extended durations, even
in the presence of multiple fast-moving targets. This work bridges the gap
between large-scale MARL training and high-fidelity deployment, providing a
scalable framework for autonomous fleet control in real-world sea missions.


## Aitomia Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations

>Authors: Jinming Hu, Hassan Nawaz, Yuting Rui, Lijie Chi, Arif Ullah, Pavlo O. Dral

>2025-05-13

> http://arxiv.org/abs/2505.08195v1

We have developed Aitomia - a platform powered by AI to assist in performing
AI-driven atomistic and quantum chemical (QC) simulations. This intelligent
assistant platform is equipped with chatbots and AI agents to help experts and
guide non-experts in setting up and running the atomistic simulations,
monitoring their computation status, analyzing the simulation results, and
summarizing them for the user in text and graphical forms. We achieve these
goals by exploiting fine-tuned open-source large language models (LLMs),
rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia
leverages the versatility of our MLatom ecosystem for AI-enhanced computational
chemistry. This intelligent assistant is going to be integrated into the
Aitomistic Hub and XACS online computing services, with some functionality
already publicly available as described at http://mlatom.com/aitomia. Aitomia
is expected to lower the barrier to performing atomistic simulations,
accelerating research and development in the relevant fields.


## GDNTT an Area-Efficient Parallel NTT Accelerator Using Glitch-Driven Near-Memory Computing and Reconfigurable 10T SRAM

>Authors: Hengyu Ding, Houran Ji, Jia Li, Jinhang Chen, Chin-Wing Sham, Yao Wang

>2025-05-13

> http://arxiv.org/abs/2505.08162v1

With the rapid advancement of quantum computing technology, post-quantum
cryptography (PQC) has emerged as a pivotal direction for next-generation
encryption standards. Among these, lattice-based cryptographic schemes rely
heavily on the fast Number Theoretic Transform (NTT) over polynomial rings,
whose performance directly determines encryption/decryption throughput and
energy efficiency. However, existing software-based NTT implementations
struggle to meet the real-time performance and low-power requirements of IoT
and edge devices. To address this challenge, this paper proposes an
area-efficient highly parallel NTT accelerator with glitch-driven near-memory
computing (GDNTT). The design integrates a 10T SRAM for data storage, enabling
flexible row/column data access and streamlining circuit mapping strategies.
Furthermore, a glitch generator is incorporated into the near-memory computing
unit, significantly reducing the latency of butterfly operations. Evaluation
results show that the proposed NTT accelerator achieves a 1.5~28* improvement
in throughput-per-area compared to the state-of-the-art.


## Self Rewarding Self Improving

>Authors: Toby Simonds, Kevin Lopez, Akira Yoshiyama, Dominique Garmier

>2025-05-12

> http://arxiv.org/abs/2505.08827v1

We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.


## Fused3S Fast Sparse Attention on Tensor Cores

>Authors: Zitong Li, Aparna Chandramowlishwaran

>2025-05-12

> http://arxiv.org/abs/2505.08098v1

Sparse attention is a core building block in many leading neural network
models, from graph-structured learning to **sparse** sequence modeling. It can be
decomposed into a sequence of three **sparse** matrix operations (3S): sampled
dense-dense matrix multiplication (SDDMM), softmax normalization, and **sparse**
matrix multiplication (SpMM). Efficiently executing the 3S computational
pattern on modern GPUs remains challenging due to (a) the mismatch between
unstructured **sparsity** and tensor cores optimized for dense operations, and (b)
the high cost of data movement. Previous works have optimized these **sparse**
operations individually or addressed one of these challenges. This paper
introduces Fused3S, the first fused 3S algorithm that jointly maximizes tensor
core utilization and minimizes data movement. Across real-world graph datasets,
Fused3S achieves $1.6- 16.3\times$ and $1.5-14\times$ speedup over
state-of-the-art on H100 and A30 GPUs. Furthermore, integrating Fused3S into
Graph Transformer inference accelerates end-to-end performance by
$1.05-5.36\times$, consistently outperforming all 3S baselines across diverse
datasets (single and batched graphs) and GPU architectures.


## Beyond Input Activations Identifying Influential Latents by Gradient Sparse Autoencoders

>Authors: Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu

>2025-05-12

> http://arxiv.org/abs/2505.08080v1

Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.


## An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits

>Authors: Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock

>2025-05-12

> http://arxiv.org/abs/2505.08823v1

Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training **quantization**
reduces memory and computation but often degrades accuracy, while
**quantization**-aware training can recover performance at the cost of extra
training. Pushing **quantization** to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
**quantization** schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-**low-bit** inference practical.


## RDD Robust Feature Detector and Descriptor using Deformable Transformer

>Authors: Gonglin Chen, Tianwen Fu, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, Yajie Zhao

>2025-05-12

> http://arxiv.org/abs/2505.08013v1

As a core step in structure-from-motion and SLAM, robust feature detection
and description under challenging scenarios such as significant viewpoint
changes remain unresolved despite their ubiquity. While recent works have
identified the importance of local features in modeling geometric
transformations, these methods fail to learn the visual cues present in
long-range relationships. We present Robust Deformable Detector (RDD), a novel
and robust keypoint detector/descriptor leveraging the deformable transformer,
which captures global context and geometric invariance through deformable
self-attention mechanisms. Specifically, we observed that deformable attention
focuses on key locations, effectively reducing the search space complexity and
modeling the geometric invariance. Furthermore, we collected an Air-to-Ground
dataset for training in addition to the standard MegaDepth dataset. Our
proposed method outperforms all state-of-the-art keypoint detection/description
methods in **sparse** matching tasks and is also capable of semi-dense matching. To
ensure comprehensive evaluation, we introduce two challenging benchmarks: one
emphasizing large viewpoint and scale variations, and the other being an
Air-to-Ground benchmark -- an evaluation setting that has recently gaining
popularity for 3D reconstruction across different altitudes.


## SpecRouter Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models

>Authors: Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai

>2025-05-12

> http://arxiv.org/abs/2505.07680v1

Large Language Models (LLMs) present a critical trade-off between inference
quality and computational cost: larger models offer superior capabilities but
incur significant latency, while smaller models are faster but less powerful.
Existing serving strategies often employ fixed model scales or static two-stage
speculative decoding, failing to dynamically adapt to the varying complexities
of user requests or fluctuations in system performance. This paper introduces
\systemname{}, a novel framework that reimagines LLM inference as an adaptive
routing problem solved through multi-level speculative decoding. \systemname{}
dynamically constructs and optimizes inference "paths" (chains of models) based
on real-time feedback, addressing the limitations of static approaches. Our
contributions are threefold: (1) An \textbf{adaptive model chain scheduling}
mechanism that leverages performance profiling (execution times) and predictive
similarity metrics (derived from token distribution divergence) to continuously
select the optimal sequence of draft and verifier models, minimizing predicted
latency per generated token. (2) A \textbf{multi-level collaborative
verification} framework where intermediate models within the selected chain can
validate speculative tokens, reducing the verification burden on the final,
most powerful target model. (3) A \textbf{synchronized state management} system
providing efficient, consistent **KV** cache handling across heterogeneous models
in the chain, including precise, low-overhead rollbacks tailored for
asynchronous batch processing inherent in multi-level speculation. Preliminary
experiments demonstrate the validity of our method.


## OnPrem.LLM A Privacy-Conscious Document Intelligence Toolkit

>Authors: Arun S. Maiya

>2025-05-12

> http://arxiv.org/abs/2505.07672v2

We present OnPrem$.$LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,
vLLM, and Hugging Face Transformers -- with **quantize**d model support, GPU
**acceleration**, and seamless backend switching. Although designed for fully local
execution, OnPrem$.$LLM also supports integration with a wide range of cloud
LLM providers when permitted, enabling hybrid deployments that balance
performance with data control. A no-code web interface extends accessibility to
non-technical users.


## Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using Edge AI

>Authors: Cong Le

>2025-05-12

> http://arxiv.org/abs/2505.07583v1

This research addresses the growing need for privacy-preserving and
accessible language translation by developing a fully offline Neural Machine
Translation (NMT) system for Vietnamese-English translation on iOS devices.
Given increasing concerns about data privacy and unreliable network
connectivity, on-device translation offers critical advantages. This project
confronts challenges in deploying complex NMT models on resource-limited mobile
devices, prioritizing efficiency, accuracy, and a seamless user experience.
Leveraging advances such as MobileBERT and, specifically, the lightweight
\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} **quantize**d
Transformer-based model is implemented and optimized. The application is
realized as a real-time iOS prototype, tightly integrating modern iOS
frameworks and privacy-by-design principles. Comprehensive documentation covers
model selection, technical architecture, challenges, and final implementation,
including functional Swift code for deployment.


## GIFStream 4D Gaussian-based Immersive Video with Feature Stream

>Authors: Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao

>2025-05-12

> http://arxiv.org/abs/2505.07539v1

Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware **pruning**. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream


## QuantX A Framework for Hardware-Aware Quantization of Generative AI Workloads

>Authors: Khurram Mazher, Saad Bin Nasir

>2025-05-12

> http://arxiv.org/abs/2505.07531v1

We present QuantX: a tailored suite of recipes for LLM and VLM **quantization**.
It is capable of quantizing down to 3-bit resolutions with minimal loss in
performance. The **quantization** strategies in QuantX take into account
hardware-specific constraints to achieve efficient de**quantization** during
inference ensuring flexible trade-off between runtime speed, memory requirement
and model accuracy. Our results demonstrate that QuantX achieves performance
within 6% of the un**quantize**d model for LlaVa-v1.6 **quantize**d down to 3-bits for
multiple end user tasks and outperforms recently published state-of-the-art
**quantization** techniques. This manuscript provides insights into the LLM
**quantization** process that motivated the range of recipes and options that are
incorporated in QuantX.


## Integrating Machine Learning with Triboelectric Nanogenerators Optimizing Electrode Materials and Doping Strategies for Intelligent Energy Harves

>Authors: Guanping Xu, Zirui Zhao, Zhong Lin Wang, Hai-Feng Li

>2025-05-12

> http://arxiv.org/abs/2505.07414v1

The integration of machine learning techniques with triboelectric
nanogenerators (TENGs) offers a transformative pathway for optimizing energy
harvesting technologies. In this study, we propose a comprehensive framework
that utilizes graph neural networks to predict and enhance the performance of
TENG electrode materials and doping strategies. By leveraging an extensive
dataset of experimental and computational results, the model effectively
classifies electrode materials, predicts optimal doping ratios, and establishes
robust structure-property relationships. Key findings include a 65.7% increase
in energy density for aluminum-doped PTFE and an 85.7% improvement for
fluorine-doped PTFE, highlighting the critical influence of doping materials
and their concentrations. The model further identifies PTFE as a highly
effective negative electrode material, achieving a maximum energy density of
1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as the
positive electrode. This data-driven approach not only accelerates material
discovery but also significantly reduces experimental costs, providing novel
insights into the fundamental factors influencing TENG performance. The
proposed methodology establishes a robust platform for intelligent material
design, advancing the development of sustainable energy technologies and
self-powered systems.


## Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption

>Authors: Jordan Frery, Roman Bredehoft, Jakub Klemsa, Arthur Meyre, Andrei Stoian

>2025-05-12

> http://arxiv.org/abs/2505.07329v1

Preserving data confidentiality during the fine-tuning of open-source Large
Language Models (LLMs) is crucial for sensitive applications. This work
introduces an interactive protocol adapting the Low-Rank Adaptation (LoRA)
technique for private fine-tuning. Homomorphic Encryption (HE) protects the
confidentiality of training data and gradients handled by remote worker nodes
performing the bulk of computations involving the base model weights. The data
owner orchestrates training, requiring minimal local computing power and
memory, thus alleviating the need for expensive client-side GPUs. We
demonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting
convergence results using HE-compatible **quantization** and performance benchmarks
for HE computations on GPU hardware. This approach enables applications such as
confidential knowledge base question answering, private codebase fine-tuning
for AI code assistants, AI agents for drafting emails based on a company's
email archive, and adapting models to analyze sensitive legal or healthcare
documents.


## Autonomous Robotic Pruning in Orchards and Vineyards a Review

>Authors: Alessandro Navone, Mauro Martini, Marcello Chiaberge

>2025-05-12

> http://arxiv.org/abs/2505.07318v1

Manual **pruning** is labor intensive and represents up to 25% of annual labor
costs in fruit production, notably in apple orchards and vineyards where
operational challenges and cost constraints limit the adoption of large-scale
machinery. In response, a growing body of research is investigating compact,
flexible robotic platforms capable of precise **pruning** in varied terrains,
particularly where traditional mechanization falls short.
  This paper reviews recent advances in autonomous robotic **pruning** for orchards
and vineyards, addressing a critical need in precision agriculture. Our review
examines literature published between 2014 and 2024, focusing on innovative
contributions across key system components. Special attention is given to
recent developments in machine vision, perception, plant skeletonization, and
control strategies, areas that have experienced significant influence from
advancements in artificial intelligence and machine learning. The analysis
situates these technological trends within broader agricultural challenges,
including rising labor costs, a decline in the number of young farmers, and the
diverse **pruning** requirements of different fruit species such as apple,
grapevine, and cherry trees.
  By comparing various robotic architectures and methodologies, this survey not
only highlights the progress made toward autonomous **pruning** but also identifies
critical open challenges and future research directions. The findings
underscore the potential of robotic systems to bridge the gap between manual
and mechanized operations, paving the way for more efficient, sustainable, and
precise agricultural practices.


## Semantic Retention and Extreme Compression in LLMs Can We Have Both?

>Authors: Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost

>2025-05-12

> http://arxiv.org/abs/2505.07289v1

The exponential growth in Large Language Model (LLM) deployment has
intensified the need for efficient model compression techniques to reduce
computational and memory costs. While **pruning** and **quantization** have shown
promise, their combined potential remains largely unexplored. In this paper, we
examine joint compression and how strategically combining **pruning** and
**quantization** could yield superior performance-to-compression ratios compared to
single-method approaches. Recognizing the challenges in accurately assessing
LLM performance, we address key limitations of previous evaluation frameworks
and introduce the Semantic Retention Compression Rate (SrCr), a novel metric
that quantifies the trade-off between model compression and semantic
preservation, facilitating the optimization of **pruning**-**quantization**
configurations. Experiments demonstrate that our recommended combination
achieves, on average, a 20% performance increase compared to an equivalent
**quantization**-only model at the same theoretical compression rate.


## UMoE Unifying Attention and FFN with Shared Experts

>Authors: Yuanhang Yang, Chaozheng Wang, Jing Li

>2025-05-12

> http://arxiv.org/abs/2505.07260v1

Sparse Mixture of Experts (MoE) architectures have emerged as a promising
approach for scaling Transformer models. While initial works primarily
incorporated MoE into feed-forward network (FFN) layers, recent studies have
explored extending the MoE paradigm to attention layers to enhance model
performance. However, existing attention-based MoE layers require specialized
implementations and demonstrate suboptimal performance compared to their
FFN-based counterparts. In this paper, we aim to unify the MoE designs in
attention and FFN layers by introducing a novel reformulation of the attention
mechanism, revealing an underlying FFN-like structure within attention modules.
Our proposed architecture, UMoE, achieves superior performance through
attention-based MoE layers while enabling efficient parameter sharing between
FFN and attention components.


## Comet Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity

>Authors: Guang Yan, Yuhui Zhang, Zimu Guo, Lutan Zhao, Xiaojun Chen, Chen Wang, Wenhao Wang, Dan Meng, Rui Hou

>2025-05-12

> http://arxiv.org/abs/2505.07239v1

With the growing use of large language models (LLMs) hosted on cloud
platforms to offer inference services, privacy concerns about the potential
leakage of sensitive information are escalating. Secure multi-party computation
(MPC) is a promising solution to protect the privacy in LLM inference. However,
MPC requires frequent inter-server communication, causing high performance
overhead.
  Inspired by the prevalent activation **sparsity** of LLMs, where most neuron are
not activated after non-linear activation functions, we propose an efficient
private inference system, Comet. This system employs an accurate and fast
predictor to predict the **sparsity** distribution of activation function output.
Additionally, we introduce a new private inference protocol. It efficiently and
securely avoids computations involving zero values by exploiting the spatial
locality of the predicted **sparse** distribution. While this computation-avoidance
approach impacts the spatiotemporal continuity of **KV** cache entries, we address
this challenge with a low-communication overhead cache refilling strategy that
merges miss requests and incorporates a prefetching mechanism. Finally, we
evaluate Comet on four common LLMs and compare it with six state-of-the-art
private inference systems. Comet achieves a 1.87x-2.63x speedup and a
1.94x-2.64x communication reduction.


## Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding

>Authors: Dianwen Ng, Kun Zhou, Yi-Wen Chao, Zhiwei Xiong, Bin Ma, Eng Siong Chng

>2025-05-12

> http://arxiv.org/abs/2505.07235v1

Achieving high-fidelity audio compression while preserving perceptual quality
across diverse content remains a key challenge in Neural Audio Coding (NAC). We
introduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC)
framework that leverages psychoacoustically guided multi-band frequency
reconstruction. At its core is a Multi-Band Spectral Residual Vector
Quantization (MBS-RVQ) module that allocates bitrate across frequency bands
based on perceptual salience. This design enables efficient compression while
disentangling speaker identity from content using distinct codebooks. MUFFIN
incorporates a transformer-inspired convolutional backbone and a modified snake
activation to enhance resolution in fine-grained spectral regions. Experimental
results on multiple benchmarks demonstrate that MUFFIN consistently outperforms
existing approaches in reconstruction quality. A high-compression variant
achieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves
effective in downstream generative tasks, highlighting its promise as a token
representation for integration with language models. Audio samples and code are
available.


## PrefillOnly An Inference Engine for Prefill-only Workloads in Large Language Model Applications

>Authors: Kuntai Du, Bowen Wang, Chen Zhang, Yiming Cheng, Qing Lan, Hejian Sang, Yihua Cheng, Jiayi Yao, Xiaoxuan Liu, Yifan Qiao, Ion Stoica, Junchen Jiang

>2025-05-12

> http://arxiv.org/abs/2505.07203v1

Besides typical generative applications, like ChatGPT, GitHub Copilot, and
Cursor, we observe an emerging trend that LLMs are increasingly used in
traditional discriminative tasks, such as recommendation, credit verification,
and data labeling. The key characteristic of these emerging use cases is that
the LLM generates only a single output token, rather than an arbitrarily long
sequence of tokens. We call this prefill-only workload. However, since existing
LLM engines assume arbitrary output lengths, they fail to leverage the unique
properties of prefill-only workloads. In this paper, we present PrefillOnly,
the first LLM inference engine that improves the inference throughput and
latency by fully embracing the properties of prefill-only workloads. First,
since it generates only one token, PrefillOnly only needs to store the **KV** cache
of only the last computed layer, rather than of all layers. This drastically
reduces the GPU memory footprint of LLM inference and allows handling long
inputs without using solutions that reduces throughput, such as cross-GPU **KV**
cache parallelization. Second, because the output length is fixed, rather than
arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of
each prefill-only request before it starts. This enables efficient JCT-aware
scheduling policies such as shortest remaining job first. PrefillOnly can
process upto 4x larger queries per second without inflating average and P99
latency.


## High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution

>Authors: Wei Shang, Dongwei Ren, Wanying Zhang, Pengfei Zhu, Qinghua Hu, Wangmeng Zuo

>2025-05-11

> http://arxiv.org/abs/2505.06975v1

The primary challenge in accelerating image super-resolution lies in reducing
computation while maintaining performance and adaptability. Motivated by the
observation that high-frequency regions (e.g., edges and textures) are most
critical for reconstruction, we propose a training-free adaptive masking module
for **acceleration** that dynamically focuses computation on these challenging
areas. Specifically, our method first extracts high-frequency components via
Gaussian blur subtraction and adaptively generates binary masks using K-means
clustering to identify regions requiring intensive processing. Our method can
be easily integrated with both CNNs and Transformers. For CNN-based
architectures, we replace standard $3 \times 3$ convolutions with an unfold
operation followed by $1 \times 1$ convolutions, enabling pixel-wise **sparse**
computation guided by the mask. For Transformer-based models, we partition the
mask into non-overlapping windows and selectively process tokens based on their
average values. During inference, unnecessary pixels or windows are pruned,
significantly reducing computation. Moreover, our method supports
dilation-based mask adjustment to control the processing scope without
retraining, and is robust to unseen degradations (e.g., noise, compression).
Extensive experiments on benchmarks demonstrate that our method reduces FLOPs
by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving
comparable or better quantitative metrics. The source code is available at
https://github.com/shangwei5/AMSR


## Ecco Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware Cache Compression

>Authors: Feng Cheng, Cong Guo, Chiyue Wei, Junyao Zhang, Changchun Zhou, Edward Hanson, Jiaqi Zhang, Xiaoxiao Liu, Hai "Helen" Li, Yiran Chen

>2025-05-11

> http://arxiv.org/abs/2505.06901v1

Large language models (LLMs) have demonstrated transformative capabilities
across diverse artificial intelligence applications, yet their deployment is
hindered by substantial memory and computational demands, especially in
resource-constrained environments. Quantization techniques have emerged as a
critical solution, reducing data precision to enhance memory and computational
efficiency. However, existing methods often suffer from high runtime overheads
and potential accuracy degradation. To address these challenges, we propose
Ecco, an entropy-based cache compression technique tailored for LLMs. Ecco
combines group-wise and non-uniform **quantization** with pre-defined shared
k-means patterns and Huffman coding to exploit the inherent entropy
characteristics of LLM cache data. Recognizing the inefficiencies of
traditional Huffman coding in terms of parallelism and latency, we introduce a
novel parallel Huffman-based decoding process with a multi-stage pipeline
design, reducing latency by two orders of magnitude and achieving throughput
comparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco
achieves an up to 2.9$\times$ and 1.9$\times$ speedup over the state-of-the-art
AWQ and SmoothQuant framework, 2.4$\times$ over the Olive accelerator, all
while increasing memory capacity by nearly 4$\times$ and maintaining
state-of-the-art LLM accuracy. These results underscore the effectiveness of
our entropy-based cache compression in enhancing LLM performance and
efficiency, paving the way for more deployable large-scale AI models.


## Learning Soft Sparse Shapes for Efficient Time-Series Classification

>Authors: Zhen Liu, Yicheng Luo, Boyuan Li, Emadeldeen Eldele, Min Wu, Qianli Ma

>2025-05-11

> http://arxiv.org/abs/2505.06892v1

Shapelets are discriminative subsequences (or shapes) with high
interpretability in time series classification. Due to the time-intensive
nature of shapelet discovery, existing shapelet-based methods mainly focus on
selecting discriminative shapes while discarding others to achieve candidate
subsequence sparsification. However, this approach may exclude beneficial
shapes and overlook the varying contributions of shapelets to classification
performance. To this end, we propose a \textbf{Soft} **sparse** \textbf{Shape}s
(\textbf{SoftShape}) model for efficient time series classification. Our
approach mainly introduces soft shape sparsification and soft shape learning
blocks. The former transforms shapes into soft representations based on
classification contribution scores, merging lower-scored ones into a single
shape to retain and differentiate all subsequence information. The latter
facilitates intra- and inter-shape temporal pattern learning, improving model
efficiency by using sparsified soft shapes as inputs. Specifically, we employ a
learnable router to activate a subset of class-specific expert networks for
intra-shape pattern learning. Meanwhile, a shared expert network learns
inter-shape patterns by converting sparsified shapes into sequences. Extensive
experiments show that SoftShape outperforms state-of-the-art methods and
produces interpretable results.


## FreqMoE Dynamic Frequency Enhancement for Neural PDE Solvers

>Authors: Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Zhenzhe Zhang, Tianchen Zhu, Shanghang Zhang, Jianxin Li

>2025-05-11

> http://arxiv.org/abs/2505.06858v1

Fourier Neural Operators (FNO) have emerged as promising solutions for
efficiently solving partial differential equations (PDEs) by learning
infinite-dimensional function mappings through frequency domain
transformations. However, the **sparsity** of high-frequency signals limits
computational efficiency for high-dimensional inputs, and fixed-pattern
truncation often causes high-frequency signal loss, reducing performance in
scenarios such as high-resolution inputs or long-term predictions. To address
these challenges, we propose FreqMoE, an efficient and progressive training
framework that exploits the dependency of high-frequency signals on
low-frequency components. The model first learns low-frequency weights and then
applies a **sparse** upward-cycling strategy to construct a mixture of experts
(MoE) in the frequency domain, effectively extending the learned weights to
high-frequency regions. Experiments on both regular and irregular grid PDEs
demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using
merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,
the approach demonstrates remarkable stability in long-term predictions and
generalizes seamlessly to various FNO variants and grid structures,
establishing a new ``Low frequency Pretraining, High frequency Fine-tuning''
paradigm for solving PDEs.


## Image processing Application Development on Software Configurable Processor Array

>Authors: Ganesh Prabhu, Steevan Rodrigues, Niranjan Chiplunkar, Niranjan U. C

>2025-05-11

> http://arxiv.org/abs/2505.06847v1

The software configurable processor finds best use in the embedded systems.
These processors have onchip logic like FPGA (Field Programmable Gate Array)
and thus can be configured to implement custom hardware functionality. The
digital computing tasks that need to accelerate in hardware can be compiled
down to a configuration file or bit stream that contains the information on how
the logic components are configured and wired together. Video and image
processing applications perform repeated pixel transformation and thus consume
more power and the processing time. Software configurable processor best
accelerates such compute intensive applications. This paper mainly focuses on
the implementation of an image processing application called median filtering
on a single processor and colour conversion algorithm on array of SCPs(Software
Configurable Processors). Median filtering result on Digital Video Recorder
(DVR) is discussed as a realtime application of SCP.


## Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers

>Authors: Parth Padalkar, Gopal Gupta

>2025-05-10

> http://arxiv.org/abs/2505.06745v1

Recent neuro-symbolic approaches have successfully extracted symbolic
rule-sets from CNN-based models to enhance interpretability. However, applying
similar techniques to Vision Transformers (ViTs) remains challenging due to
their lack of modular concept detectors and reliance on global self-attention
mechanisms. We propose a framework for symbolic rule extraction from ViTs by
introducing a **sparse** concept layer inspired by Sparse Autoencoders (SAEs). This
linear layer operates on attention-weighted patch representations and learns a
disentangled, binarized representation in which individual neurons activate for
high-level visual concepts. To encourage interpretability, we apply a
combination of L1 **sparsity**, entropy minimization, and supervised contrastive
loss. These binarized concept activations are used as input to the FOLD-SE-M
algorithm, which generates a rule-set in the form of logic programs. Our method
achieves a 5.14% better classification accuracy than the standard ViT while
enabling symbolic reasoning. Crucially, the extracted rule-set is not merely
post-hoc but acts as a logic-based decision layer that operates directly on the
**sparse** concept representations. The resulting programs are concise and
semantically meaningful. This work is the first to extract executable logic
programs from ViTs using **sparse** symbolic representations. It bridges the gap
between transformer-based vision models and symbolic logic programming,
providing a step forward in interpretable and verifiable neuro-symbolic AI.


## Gated Attention for Large Language Models Non-linearity, Sparsity, and Attention-Sink-Free

>Authors: Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin

>2025-05-10

> http://arxiv.org/abs/2505.06708v1

Gating mechanisms have been widely utilized, from early models like LSTMs and
Highway Networks to recent state space models, linear attention, and also
softmax attention. Yet, existing literature rarely examines the specific
effects of gating. In this work, we conduct comprehensive experiments to
systematically investigate gating-augmented softmax attention variants.
Specifically, we perform a comprehensive comparison over 30 variants of 15B
Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion
token dataset. Our central finding is that a simple modification-applying a
head-specific sigmoid gate after the Scaled Dot-Product Attention
(SDPA)-consistently improves performance. This modification also enhances
training stability, tolerates larger learning rates, and improves scaling
properties. By comparing various gating positions and computational variants,
we attribute this effectiveness to two key factors: (1) introducing
non-linearity upon the low-rank mapping in the softmax attention, and (2)
applying query-dependent **sparse** gating scores to modulate the SDPA output.
Notably, we find this **sparse** gating mechanism mitigates 'attention sink' and
enhances long-context extrapolation performance, and we also release related
$\href{https://github.com/qiuzh20/gated_attention}{codes}$ and
$\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate
future research.


## Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4) Analysis and Variations

>Authors: Patrick Blumenberg, Thomas Graave, Tim Fingscheidt

>2025-05-10

> http://arxiv.org/abs/2505.06653v1

Large language models (LLMs) demand extensive memory capacity during both
fine-tuning and inference. To enable memory-efficient fine-tuning, existing
methods apply block-wise **quantization** techniques, such as NF4 and AF4, to the
network weights. We show that these **quantization** techniques incur suboptimal
**quantization** errors. Therefore, as a first novelty, we propose an optimization
approach for block-wise **quantization**. Using this method, we design a family of
**quantize**rs named 4-bit block-wise optimal float (BOF4), which consistently
reduces the **quantization** error compared to both baseline methods. We provide
both a theoretical and a data-driven solution for the optimization process and
prove their practical equivalence. Secondly, we propose a modification to the
employed normalization method based on the signed absolute block maximum
(BOF4-S), enabling further reduction of the **quantization** error and empirically
achieving less degradation in language modeling performance. Thirdly, we
explore additional variations of block-wise **quantization** methods applied to
LLMs through an experimental study on the importance of accurately representing
zero and large-amplitude weights on the one hand, and optimization towards
various error metrics on the other hand. Lastly, we introduce a mixed-precision
**quantization** strategy dubbed outlier-preserving **quantization** (OPQ) to address
the distributional mismatch induced by outlier weights in block-wise
**quantization**. By storing outlier weights in 16-bit precision (OPQ) while
applying BOF4-S, we achieve top performance among 4-bit block-wise **quantization**
techniques w.r.t. perplexity.


## Efficient Telecom Specific LLM TSLAM-Mini with QLoRA and Digital Twin Data

>Authors: Vignesh Ethiraj, Divya Vijay, Sidhanth Menon, Heblin Berscilla

>2025-05-10

> http://arxiv.org/abs/2505.07877v1

General-purpose large language models (LLMs), despite their broad
capabilities accrued from open-world data, frequently exhibit suboptimal
performance when confronted with the nuanced and specialized demands inherent
in real-time telecommunications applications. This investigation addresses this
critical limitation through the meticulous fine-tuning of TSLAM-Mini developed
by NetoAI, a compact (3.8-billion parameter) causal language model
architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen
leverages a bespoke dataset comprising 100,000 samples, strategically
engineered to address 20 pivotal telecommunications use-cases, encompassing
domains such as Network Fundamentals, IP Routing, MPLS, Network Security,
Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical
AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched
with granular insights from venerated network Subject Matter Experts (SMEs) and
authoritative RFC documents, thereby capturing high-fidelity representations of
real-world network dynamics through simulations inspired by digital twin
paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art
Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial
training efficiency and enabled prospective deployment on resource-constrained
hardware. A novel evaluation framework, predicated on a high-capacity LLM
(Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to
rigorously assess instruction-following fidelity and response quality across
the specified telecom use-cases. Empirical results unequivocally demonstrate
TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring
the profound efficacy of domain-specific datasets and PEFT methodologies for
advancing intelligent network management.


## Challenging GPU Dominance When CPUs Outperform for On-Device LLM Inference

>Authors: Haolin Zhang, Jeff Huang

>2025-05-09

> http://arxiv.org/abs/2505.06461v1

The common assumption in on-device AI is that GPUs, with their superior
parallel processing, always provide the best performance for large language
model (LLM) inference. In this work, we challenge this notion by empirically
demonstrating that, under certain conditions, CPUs can outperform GPUs for LLM
inference on mobile devices. Using a 1-billion-parameter LLM deployed via
llama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two
threads, F16 precision) achieves 17 tokens per second, surpassing the 12.8
tokens per second obtained with GPU **acceleration**. We analyze the architectural
factors driving this counterintuitive result, revealing that GPU memory
transfer overhead and CPU thread optimization play a critical role.
Furthermore, we explore the impact of thread oversubscription, **quantization**
strategies, and hardware constraints, providing new insights into efficient
on-device AI execution. Our findings challenge conventional GPU-first thinking,
highlighting the untapped potential of optimized CPU inference and paving the
way for smarter deployment strategies in mobile AI. However, fully explaining
the observed CPU advantage remains difficult due to limited access to low-level
profiling tools on iOS.


## Decoding Algorithms for Two-dimensional Constacyclic Codes over $\mathbb{F}_q$

>Authors: Vidya Sagar, Shikha Patel, Shayan Srinivasa Garani

>2025-05-09

> http://arxiv.org/abs/2505.06201v1

We derive the spectral domain properties of two-dimensional (2-D)
$(\lambda_1, \lambda_2)$-constacyclic codes over $\mathbb{F}_q$ using the 2-D
finite field Fourier transform (FFFT). Based on the spectral nulls of 2-D
$(\lambda_1, \lambda_2)$-constacyclic codes, we characterize the structure of
2-D constacyclic coded arrays. The proposed 2-D construction has flexible code
rates and works for any code areas, be it odd or even area. We present an
algorithm to detect the location of 2-D errors. Further, we also propose
decoding algorithms for extracting the error values using both time and
frequency domain properties by exploiting the **sparsity** that arises due to
duality in the time and frequency domains. Through several illustrative
examples, we demonstrate the working of the proposed decoding algorithms.


## Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients

>Authors: Jinsheng Yuan, Yuhang Hao, Weisi Guo, Yun Wu, Chongyan Gu

>2025-05-09

> http://arxiv.org/abs/2505.06335v1

Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform **sparse** gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with **sparse** updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.


## New Advances in Phonons From Band Topology to Quasiparticle Chirality

>Authors: Tiantian Zhang, Yizhou Liu, Hu Miao, Shuichi Murakami

>2025-05-09

> http://arxiv.org/abs/2505.06179v1

Phonons, the **quantize**d vibrational modes of a crystal lattice, are ubiquitous
quasiparticles in solid-state systems. They play a central role in a wide range
of physical phenomena, from thermal transport, as primary carriers of heat in
insulators, to their involvement in symmetry-breaking orders such as charge
density waves and superconductivity. Traditionally considered as spinless
bosons, phonons have recently emerged as a fertile ground for exploring
topological physics, spurred by the rapid development of topological band
theory initially formulated for fermionic systems. It is now understood that
the phonon eigenstates, characterized by their eigenvalues and eigenvectors,
can carry nontrivial topological invariants such as Berry curvature and Chern
numbers, despite the absence of spin or charge. This new understanding opens up
avenues to investigate the interplay between lattice dynamics, topology, and
chirality in bosonic systems. In this article, we provide a comprehensive
review of recent theoretical and experimental advances in the field of
topological and chiral phonons. We begin by introducing the foundational
concepts, including the classification of phononic band structures,
symmetry-protected topological phases, and the definition of topological
invariants in bosonic systems. Special attention is given to the concept of
phonon angular momentum and its fundametal connection to Weyl phonons in
inversion-symmetry-breaking systems. We then discuss key experimental
realizations of topological and chiral phonons across a variety of material
platforms. Finally, we outline outstanding challenges and promising directions
for future research, such as the role of topology in phonon-mediated
quasiparticle interactions and the manipulation of phonon angular momentum in
quantum technologies.


## Turbo-ICL In-Context Learning-Based Turbo Equalization

>Authors: Zihang Song, Matteo Zecchin, Bipin Rajendran, Osvaldo Simeone

>2025-05-09

> http://arxiv.org/abs/2505.06175v1

This paper introduces a novel in-context learning (ICL) framework, inspired
by large language models (LLMs), for soft-input soft-output channel
equalization in coded multiple-input multiple-output (MIMO) systems. The
proposed approach learns to infer posterior symbol distributions directly from
a prompt of pilot signals and decoder feedback. A key innovation is the use of
prompt augmentation to incorporate extrinsic information from the decoder
output as additional context, enabling the ICL model to refine its symbol
estimates iteratively across turbo decoding iterations. Two model variants,
based on Transformer and state-space architectures, are developed and
evaluated. Extensive simulations demonstrate that, when traditional linear
assumptions break down, e.g., in the presence of low-resolution **quantization**,
ICL equalizers consistently outperform conventional model-based baselines, even
when the latter are provided with perfect channel state information. Results
also highlight the advantage of Transformer-based models under limited training
diversity, as well as the efficiency of state-space models in
resource-constrained scenarios.


## Fast recovery of parametric eigenvalues depending on several parameters and location of high order exceptional points

>Authors: Benoit Nennig, Martin Ghienne, Emmanuel Perrey-Debain

>2025-05-09

> http://arxiv.org/abs/2505.06141v1

A numerical algorithm is proposed to deal with parametric eigenvalue problems
involving non-Hermitian matrices and is exploited to find location of defective
eigenvalues in the parameter space of non-hermitian parametric eigenvalue
problems. These non-Hermitian degeneracies also called exceptional points (EP)
have raised considerable attention in the scientific community as these can
have a great impact in a variety of physical problems. The method first
requires the computation of high order derivatives of a few selected
eigenvalues with respect to each parameter involved. The second step is to
recombine these quantities to form new coefficients associated with a partial
characteristic polynomial (PCP). By construction, these coefficients are
regular functions in a large domain of the parameter space which means that the
PCP allows one to recover the selected eigenvalues as well as the localization
of high order EPs by simply using standard root-finding algorithms. The
versatility of the proposed approach is tested on several applications, from
mass-spring systems to guided acoustic waves with absorbing walls and rooms
acoustics. The scalability of the method to large **sparse** matrices arising from
conventional discretization techniques such as the finite element method is
demonstrated. The proposed approach can be extended to a large number of
applications where EPs play an important role in quantum mechanics, optics and
photonics or in mechanical engineering.


## Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities

>Authors: Hiari Pizzini Cavagna, Daniele Cesarini, Andrea Bartolini

>2025-05-09

> http://arxiv.org/abs/2505.06085v2

The increasing demand for generative AI as Large Language Models (LLMs)
services has driven the need for specialized hardware architectures that
optimize computational efficiency and energy consumption. This paper evaluates
the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic
linear algebra kernels at reduced numerical precision, a fundamental operation
in LLM computations. We present a detailed characterization of Grayskull's
execution model, gridsize, matrix dimensions, data formats, and numerical
precision impact computational efficiency. Furthermore, we compare Grayskull's
performance against state-of-the-art architectures with tensor **acceleration**,
including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).
Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a
competitive trade-off between power consumption and computational throughput,
reaching a peak of 1.55 TFLOPs/Watt with BF16.


## Functoriality of Enriched Data Types

>Authors: Lukas Mulder, Paige Randall North, Maximilien PÃ©roux

>2025-05-09

> http://arxiv.org/abs/2505.06059v1

In previous work, categories of algebras of endofunctors were shown to be
enriched in categories of coalgebras of the same endofunctor, and the extra
structure of that enrichment was used to define a generalization of inductive
data types. These generalized inductive data types are parametrized by a
coalgebra $C$, so we call them $C$-inductive data types; we call the morphisms
induced by their universal property $C$-inductive functions. We extend that
work by incorporating natural transformations into the theory: given a suitable
natural transformation between endofunctors, we show that this induces enriched
functors between their categories of algebras which preserve $C$-inductive data
types and $C$-inductive functions. Such $C$-inductive data types are often
finite versions of the corresponding inductive data type, and we show how our
framework can extend classical initial algebra semantics to these types. For
instance, we show that our theory naturally produces partially inductive
functions on lists, changes in list element types, and tree **pruning** functions.


## Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition

>Authors: Zhiyuan Chen, Keyi Li, Yifan Jia, Le Ye, Yufei Ma

>2025-05-09

> http://arxiv.org/abs/2505.05829v1

Diffusion transformer (DiT) models have achieved remarkable success in image
generation, thanks for their exceptional generative capabilities and
scalability. Nonetheless, the iterative nature of diffusion models (DMs)
results in high computation complexity, posing challenges for deployment.
Although existing cache-based **acceleration** methods try to utilize the inherent
temporal similarity to skip redundant computations of DiT, the lack of
correction may induce potential quality degradation. In this paper, we propose
increment-calibrated caching, a training-free method for DiT **acceleration**,
where the calibration parameters are generated from the pre-trained model
itself with low-rank approximation. To deal with the possible correction
failure arising from outlier activations, we introduce channel-aware Singular
Value Decomposition (SVD), which further strengthens the calibration effect.
Experimental results show that our method always achieve better performance
than existing naive caching methods with a similar computation resource budget.
When compared with 35-step DDIM, our method eliminates more than 45%
computation and improves IS by 12 at the cost of less than 0.06 FID increase.
Code is available at https://github.com/ccccczzy/icc.


## Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM

>Authors: Zehao Fan, Garrett Gagnon, Zhenyu Liu, Liu Liu

>2025-05-09

> http://arxiv.org/abs/2505.05772v1

Transformer-based models are the foundation of modern machine learning, but
their execution, particularly during autoregressive decoding in large language
models (LLMs), places significant pressure on memory systems due to frequent
memory accesses and growing key-value (**KV**) caches. This creates a bottleneck in
memory bandwidth, especially as context lengths increase. Processing-in-memory
(PIM) architectures are a promising solution, offering high internal bandwidth
and compute parallelism near memory. However, current PIM designs are primarily
optimized for dense attention and struggle with the dynamic, irregular access
patterns introduced by modern **KV** cache **sparsity** techniques. Consequently, they
suffer from workload imbalance, reducing throughput and resource utilization.
In this work, we propose STARC, a novel **sparsity**-optimized data mapping scheme
tailored specifically for efficient LLM decoding on PIM architectures. STARC
clusters **KV** pairs by semantic similarity and maps them to contiguous memory
regions aligned with PIM bank structures. During decoding, queries retrieve
relevant tokens at cluster granularity by matching against precomputed
centroids, enabling selective attention and parallel processing without
frequent reclustering or data movement overhead. Experiments on the HBM-PIM
system show that, compared to common token-wise **sparsity** methods, STARC reduces
attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a
**KV** cache budget of 1024, it achieves up to 54%--74% latency reduction and
45%--67% energy reduction compared to full **KV** cache retrieval. Meanwhile, STARC
maintains model accuracy comparable to state-of-the-art **sparse** attention
methods, demonstrating its effectiveness in enabling efficient and
hardware-friendly long-context LLM inference on PIM architectures.


## QoSBERT An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction

>Authors: Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Meng Yan

>2025-05-09

> http://arxiv.org/abs/2505.07863v1

Accurate prediction of Quality of Service (QoS) metrics is fundamental for
selecting and managing cloud based services. Traditional QoS models rely on
manual feature engineering and yield only point estimates, offering no insight
into the confidence of their predictions. In this paper, we propose QoSBERT,
the first framework that reformulates QoS prediction as a semantic regression
task based on pre trained language models. Unlike previous approaches relying
on **sparse** numerical features, QoSBERT automatically encodes user service
metadata into natural language descriptions, enabling deep semantic
understanding. Furthermore, we integrate a Monte Carlo Dropout based
uncertainty estimation module, allowing for trustworthy and risk-aware service
quality prediction, which is crucial yet underexplored in existing QoS models.
QoSBERT applies attentive pooling over contextualized embeddings and a
lightweight multilayer perceptron regressor, fine tuned jointly to minimize
absolute error. We further exploit the resulting uncertainty estimates to
select high quality training samples, improving robustness in low resource
settings. On standard QoS benchmark datasets, QoSBERT achieves an average
reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and
6.9% in MAE for throughput prediction compared to the strongest baselines,
while providing well calibrated confidence intervals for robust and trustworthy
service quality estimation. Our approach not only advances the accuracy of
service quality prediction but also delivers reliable uncertainty
quantification, paving the way for more trustworthy, data driven service
selection and optimization.


## Dome-DETR DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection

>Authors: Zhangchi Hu, Peixi Wu, Jie Chen, Huyue Zhu, Yijun Wang, Yansong Peng, Hebei Li, Xiaoyan Sun

>2025-05-09

> http://arxiv.org/abs/2505.05741v1

Tiny object detection plays a vital role in drone surveillance, remote
sensing, and autonomous systems, enabling the identification of small targets
across vast landscapes. However, existing methods suffer from inefficient
feature leverage and high computational costs due to redundant feature
processing and rigid query allocation. To address these challenges, we propose
Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation
for Efficient Tiny Object Detection. To reduce feature redundancies, we
introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered
compact foreground masks. Leveraging these masks, we incorporate Masked Window
Attention Sparsification (MWAS) to focus computational resources on the most
informative regions via **sparse** attention. Besides, we propose Progressive
Adaptive Query Initialization (PAQI), which adaptively modulates query density
across spatial areas for better query allocation. Extensive experiments
demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on
AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational
complexity and a compact model size. Code will be released upon acceptance.


## All-to-All Communication with Mobile Edge Adversary Almost Linearly More Faults, For Free

>Authors: Orr Fischer, Merav Parter

>2025-05-09

> http://arxiv.org/abs/2505.05735v1

Resilient computation in all-to-all-communication models has attracted
tremendous attention over the years. Most of these works assume the classical
faulty model which restricts the total number of corrupted edges (or vertices)
by some integer fault parameter $f$. A recent work by [Bodwin, Haeupler and
Parter, SODA 2024] introduced a stronger notion of fault-tolerance, in the
context of graph sparsification, which restricts the degree of the failing edge
set $F$, rather than its cardinality. For a subset of faulty edges $F$, the
faulty-degree $\mathrm{deg}(F)$ is the largest number of faults in $F$ incident
to any given node.
  In this work, we study the communication aspects of this faulty model which
allows us to handle almost linearly more edge faults (possibly quadratic), with
no extra cost. Our end results are general compilers that take any Congested
Clique algorithm and simulate it, in a round by round manner, in the presence
of a $\alpha$-Byzantine mobile adversary that controls a $\alpha$-fraction of
the edges incident to each node in the fully connected network. For every round
$i$, the mobile adversary is allowed to select a distinct set of corrupted
edges $F_i$ under the restriction that $\mathrm{deg}(F_i)\leq \alpha n$. In the
non-adaptive setting, the $F_i$ sets are selected at the beginning of the
simulation, while in the adaptive setting, these edges can be chosen based on
the entire history of the protocol up to round $i$.
  We show general compilers for the non-adaptive, adaptive, and deterministic
settings. A key component of our algorithms is a new resilient routing scheme
which may be of independent interest. Our approach is based on a combination of
techniques, including error-correcting-code, locally decodable codes,
cover-free families, and **sparse** recovery sketches.


## A framework for learning symbolic turbulence models from indirect observation data via neural networks and feature importance analysis

>Authors: Chutian Wu, Xin-Lei Zhang, Duo Xu, Guowei He

>2025-05-09

> http://arxiv.org/abs/2505.05716v1

Learning symbolic turbulence models from indirect observation data is of
significant interest as it not only improves the accuracy of posterior
prediction but also provides explicit model formulations with good
interpretability. However, it typically resorts to gradient-free evolutionary
algorithms, which can be relatively inefficient compared to gradient-based
approaches, particularly when the Reynolds-averaged Navier-Stokes (RANS)
simulations are involved in the training process. In view of this difficulty,
we propose a framework that uses neural networks and the associated feature
importance analysis to improve the efficiency of symbolic turbulence modeling.
In doing so, the gradient-based method can be used to efficiently learn neural
network-based representations of Reynolds stress from indirect data, which is
further transformed into simplified mathematical expressions with symbolic
regression. Moreover, feature importance analysis is introduced to accelerate
the convergence of symbolic regression by excluding insignificant input
features. The proposed training strategy is tested in the flow in a square
duct, where it correctly learns underlying analytic models from indirect
velocity data. Further, the method is applied in the flow over the periodic
hills, demonstrating that the feature importance analysis can significantly
improve the training efficiency and learn symbolic turbulence models with
satisfactory generalizability.

