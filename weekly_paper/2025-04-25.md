# 2025-04-25

# Table of Contents
* [Generalized Neighborhood Attention Multi-dimensional Sparse Attention at the Speed of Light](#Generalized-Neighborhood-Attention-Multi-dimensional-Sparse-Attention-at-the-Speed-of-Light)
* [New Primal-Dual Algorithm for Convex Problems](#New-Primal-Dual-Algorithm-for-Convex-Problems)
* [HEMA  A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](#HEMA--A-Hippocampus-Inspired-Extended-Memory-Architecture-for-Long-Context-AI-Conversations)
* [IRIS Interactive Research Ideation System for Accelerating Scientific Discovery](#IRIS-Interactive-Research-Ideation-System-for-Accelerating-Scientific-Discovery)
* [Rethinking Vision Transformer for Large-Scale Fine-Grained Image Retrieval](#Rethinking-Vision-Transformer-for-Large-Scale-Fine-Grained-Image-Retrieval)
* [DTVM Revolutionizing Smart Contract Execution with Determinism and Compatibility](#DTVM-Revolutionizing-Smart-Contract-Execution-with-Determinism-and-Compatibility)
* [QuaDMix Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](#QuaDMix-Quality-Diversity-Balanced-Data-Selection-for-Efficient-LLM-Pretraining)
* [Cross Paradigm Representation and Alignment Transformer for Image Deraining](#Cross-Paradigm-Representation-and-Alignment-Transformer-for-Image-Deraining)
* [Qubit Geometry through Holomorphic Quantization](#Qubit-Geometry-through-Holomorphic-Quantization)
* [Hamiltonian quantization of complex Chern-Simons theory at level-$k$](#Hamiltonian-quantization-of-complex-Chern-Simons-theory-at-level-$k$)
* [Transitive Array An Efficient GEMM Accelerator with Result Reuse](#Transitive-Array-An-Efficient-GEMM-Accelerator-with-Result-Reuse)
* [Discrete Codebook Design for Self-interference Suppression in mmWave ISAC](#Discrete-Codebook-Design-for-Self-interference-Suppression-in-mmWave-ISAC)
* [Learning Explainable Dense Reward Shapes via Bayesian Optimization](#Learning-Explainable-Dense-Reward-Shapes-via-Bayesian-Optimization)
* [COBRA Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference](#COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference)
* [Boosting Classifier Performance with Opposition-Based Data Transformation](#Boosting-Classifier-Performance-with-Opposition-Based-Data-Transformation)
* [TeLLMe An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](#TeLLMe-An-Energy-Efficient-Ternary-LLM-Accelerator-for-Prefilling-and-Decoding-on-Edge-FPGAs)
* [MMInference Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](#MMInference-Accelerating-Pre-filling-for-Long-Context-VLMs-via-Modality-Aware-Permutation-Sparse-Attention)
* [Muon Optimizer Accelerates Grokking](#Muon-Optimizer-Accelerates-Grokking)
* [ViSMaP Unsupervised Hour-long Video Summarisation by Meta-Prompting](#ViSMaP-Unsupervised-Hour-long-Video-Summarisation-by-Meta-Prompting)
* [SARI Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](#SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning)
* [MS-Occ Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction](#MS-Occ-Multi-Stage-LiDAR-Camera-Fusion-for-3D-Semantic-Occupancy-Prediction)
* [Quantum Discrete Variable Representations](#Quantum-Discrete-Variable-Representations)
* [BBAL A Bidirectional Block Floating Point-Based Quantisation Accelerator for Large Language Models](#BBAL-A-Bidirectional-Block-Floating-Point-Based-Quantisation-Accelerator-for-Large-Language-Models)
* [Exploring the Role of Large Language Models in Cybersecurity A Systematic Survey](#Exploring-the-Role-of-Large-Language-Models-in-Cybersecurity-A-Systematic-Survey)
* [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](#A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings)
* [SparseJEPA Sparse Representation Learning of Joint Embedding Predictive Architectures](#SparseJEPA-Sparse-Representation-Learning-of-Joint-Embedding-Predictive-Architectures)
* [In-context Ranking Preference Optimization](#In-context-Ranking-Preference-Optimization)
* [Bigram Subnetworks Mapping to Next Tokens in Transformer Language Models](#Bigram-Subnetworks-Mapping-to-Next-Tokens-in-Transformer-Language-Models)
* [KeyDiff Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](#KeyDiff-Key-Similarity-Based-KV-Cache-Eviction-for-Long-Context-LLM-Inference-in-Resource-Constrained-Environments)
* [Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs](#Integrating-Symbolic-Execution-into-the-Fine-Tuning-of-Code-Generating-LLMs)
* [Compute-Optimal LLMs Provably Generalize Better With Scale](#Compute-Optimal-LLMs-Provably-Generalize-Better-With-Scale)
* [Efficient Pretraining Length Scaling](#Efficient-Pretraining-Length-Scaling)
* [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](#Fast-Adversarial-Training-with-Weak-to-Strong-Spatial-Temporal-Consistency-in-the-Frequency-Domain-on-Videos)
* [StableQuant Layer Adaptive Post-Training Quantization for Speech Foundation Models](#StableQuant-Layer-Adaptive-Post-Training-Quantization-for-Speech-Foundation-Models)
* [An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV A Value of Information Perspective](#An-Enhanced-Dual-Currency-VCG-Auction-Mechanism-for-Resource-Allocation-in-IoV-A-Value-of-Information-Perspective)
* [DONOD Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning](#DONOD-Robust-and-Generalizable-Instruction-Fine-Tuning-for-LLMs-via-Model-Intrinsic-Dataset-Pruning)
* [gLLM Global Balanced Pipeline Parallelism System for Distributed LLM Serving with Token Throttling](#gLLM-Global-Balanced-Pipeline-Parallelism-System-for-Distributed-LLM-Serving-with-Token-Throttling)
* [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](#IXGS-Intraoperative-3D-Reconstruction-from-Sparse,-Arbitrarily-Posed-Real-X-rays)
* [Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking](#Designing-AI-Systems-that-Augment-Human-Performed-vs.-Demonstrated-Critical-Thinking)
* [Efficient Federated Split Learning for Large Language Models over Communication Networks](#Efficient-Federated-Split-Learning-for-Large-Language-Models-over-Communication-Networks)
* [HLSTester Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis](#HLSTester-Efficient-Testing-of-Behavioral-Discrepancies-with-LLMs-for-High-Level-Synthesis)
* [NoWag A Unified Framework for Shape Preserving Compression of Large Language Models](#NoWag-A-Unified-Framework-for-Shape-Preserving-Compression-of-Large-Language-Models)
* [Optimizing SLO-oriented LLM Serving with PD-Multiplexing](#Optimizing-SLO-oriented-LLM-Serving-with-PD-Multiplexing)
* [Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation](#Vision-Centric-Representation-Efficient-Fine-Tuning-for-Robust-Universal-Foreground-Segmentation)
* [SG-Reg Generalizable and Efficient Scene Graph Registration](#SG-Reg-Generalizable-and-Efficient-Scene-Graph-Registration)
* [Accelerating LLM Inference with Flexible NM Sparsity via A Fully Digital Compute-in-Memory Accelerator](#Accelerating-LLM-Inference-with-Flexible-NM-Sparsity-via-A-Fully-Digital-Compute-in-Memory-Accelerator)
* [Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses](#Diffusion-based-Dynamic-Contract-for-Federated-AI-Agent-Construction-in-Mobile-Metaverses)
* [RAMCT Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking](#RAMCT-Novel-Region-adaptive-Multi-channel-Tracker-with-Iterative-Tikhonov-Regularization-for-Thermal-Infrared-Tracking)
* [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](#Understanding-the-Repeat-Curse-in-Large-Language-Models-from-a-Feature-Perspective)
* [Efficient state transition algorithm with guaranteed optimality](#Efficient-state-transition-algorithm-with-guaranteed-optimality)
* [DConAD A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](#DConAD-A-Differencing-based-Contrastive-Representation-Learning-Framework-for-Time-Series-Anomaly-Detection)
* [Room-temperature high-average-power strong-field terahertz source based on industrial high-repetition-rate femtosecond laser](#Room-temperature-high-average-power-strong-field-terahertz-source-based-on-industrial-high-repetition-rate-femtosecond-laser)
* [FGMP Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference](#FGMP-Fine-Grained-Mixed-Precision-Weight-and-Activation-Quantization-for-Hardware-Accelerated-LLM-Inference)
* [HF4Rec Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](#HF4Rec-Human-Like-Feedback-Driven-Optimization-Framework-for-Explainable-Recommendation)
* [Lightweight Road Environment Segmentation using Vector Quantization](#Lightweight-Road-Environment-Segmentation-using-Vector-Quantization)
* [6G WavesFM A Foundation Model for Sensing, Communication, and Localization](#6G-WavesFM-A-Foundation-Model-for-Sensing,-Communication,-and-Localization)
* [Transformation of audio embeddings into interpretable, concept-based representations](#Transformation-of-audio-embeddings-into-interpretable,-concept-based-representations)
* [A Baseline for Self-state Identification and Classification in Mental Health Data CLPsych 2025 Task](#A-Baseline-for-Self-state-Identification-and-Classification-in-Mental-Health-Data-CLPsych-2025-Task)
* [Scaling sparse feature circuit finding for in-context learning](#Scaling-sparse-feature-circuit-finding-for-in-context-learning)
* [Learning to Attribute with Attention](#Learning-to-Attribute-with-Attention)
* [Gradual Binary Search and Dimension Expansion  A general method for activation quantization in LLMs](#Gradual-Binary-Search-and-Dimension-Expansion--A-general-method-for-activation-quantization-in-LLMs)
* [ViG3D-UNet Volumetric Vascular Connectivity-Aware Segmentation via 3D Vision Graph Representation](#ViG3D-UNet-Volumetric-Vascular-Connectivity-Aware-Segmentation-via-3D-Vision-Graph-Representation)
* [MAAM A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework](#MAAM-A-Lightweight-Multi-Agent-Aggregation-Module-for-Efficient-Image-Classification-Based-on-the-MindSpore-Framework)
* [High-Throughput LLM inference on Heterogeneous Clusters](#High-Throughput-LLM-inference-on-Heterogeneous-Clusters)
* [From Large to Super-Tiny End-to-End Optimization for Cost-Efficient LLMs](#From-Large-to-Super-Tiny-End-to-End-Optimization-for-Cost-Efficient-LLMs)
* [HMPEHeatMap Embedding for Efficient Transformer-Based Small Object Detection](#HMPEHeatMap-Embedding-for-Efficient-Transformer-Based-Small-Object-Detection)
* [HPU High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](#HPU-High-Bandwidth-Processing-Unit-for-Scalable,-Cost-effective-LLM-Inference-via-GPU-Co-processing)
* [From Bayesian Asymptotics to General Large-Scale MIMO Capacity](#From-Bayesian-Asymptotics-to-General-Large-Scale-MIMO-Capacity)
* [Let Me Grok for You Accelerating Grokking via Embedding Transfer from a Weaker Model](#Let-Me-Grok-for-You-Accelerating-Grokking-via-Embedding-Transfer-from-a-Weaker-Model)
* [Hadamard product in deep learning Introduction, Advances and Challenges](#Hadamard-product-in-deep-learning-Introduction,-Advances-and-Challenges)
* [Deep literature reviews an application of fine-tuned language models to migration research](#Deep-literature-reviews-an-application-of-fine-tuned-language-models-to-migration-research)
* [Taccel Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation](#Taccel-Scaling-Up-Vision-based-Tactile-Robotics-via-High-performance-GPU-Simulation)
* [Mirror, Mirror of the Flow How Does Regularization Shape Implicit Bias?](#Mirror,-Mirror-of-the-Flow-How-Does-Regularization-Shape-Implicit-Bias?)
* [Tailoring Electromagnetic Fields in RF Cavities](#Tailoring-Electromagnetic-Fields-in-RF-Cavities)
* [Chinese-Vicuna A Chinese Instruction-following Llama-based Model](#Chinese-Vicuna-A-Chinese-Instruction-following-Llama-based-Model)
* [TUMLS Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology](#TUMLS-Trustful-Fully-Unsupervised-Multi-Level-Segmentation-for-Whole-Slide-Images-of-Histology)
* [Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection](#Hierarchical-Vector-Quantized-Graph-Autoencoder-with-Annealing-Based-Code-Selection)
* [D$^{2}$MoE Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving](#D$^{2}$MoE-Dual-Routing-and-Dynamic-Scheduling-for-Efficient-On-Device-MoE-based-LLM-Serving)
* [The Paradox of Professional Input How Expert Collaboration with AI Systems Shapes Their Future Value](#The-Paradox-of-Professional-Input-How-Expert-Collaboration-with-AI-Systems-Shapes-Their-Future-Value)
* [Fast Computation of the Discrete Fourier Transform Rectangular Index Coefficients](#Fast-Computation-of-the-Discrete-Fourier-Transform-Rectangular-Index-Coefficients)
* [Dense Backpropagation Improves Training for Sparse Mixture-of-Experts](#Dense-Backpropagation-Improves-Training-for-Sparse-Mixture-of-Experts)
* [Spontaneous symmetry breaking in the Heisenberg antiferromagnet on a triangular lattice](#Spontaneous-symmetry-breaking-in-the-Heisenberg-antiferromagnet-on-a-triangular-lattice)
* [Activated LoRA Fine-tuned LLMs for Intrinsics](#Activated-LoRA-Fine-tuned-LLMs-for-Intrinsics)
* [HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks](#HLS-Eval-A-Benchmark-and-Framework-for-Evaluating-LLMs-on-High-Level-Synthesis-Design-Tasks)
* [SCENT Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields](#SCENT-Robust-Spatiotemporal-Learning-for-Continuous-Scientific-Data-via-Scalable-Conditioned-Neural-Fields)
* [Cobra Efficient Line Art COlorization with BRoAder References](#Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References)
* [Cost-Efficient LLM Serving in the Cloud VM Selection with KV Cache Offloading](#Cost-Efficient-LLM-Serving-in-the-Cloud-VM-Selection-with-KV-Cache-Offloading)
* [Light WIMPs and MeV Gamma-ray Detection with COSI](#Light-WIMPs-and-MeV-Gamma-ray-Detection-with-COSI)
* [Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs](#Shared-Disk-KV-Cache-Management-for-Efficient-Multi-Instance-Inference-in-RAG-Powered-LLMs)
* [Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models](#Unveiling-Hidden-Collaboration-within-Mixture-of-Experts-in-Large-Language-Models)
* [EdgePrompt A Distributed Key-Value Inference Framework for LLMs in 6G Networks](#EdgePrompt-A-Distributed-Key-Value-Inference-Framework-for-LLMs-in-6G-Networks)
* [An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World](#An-Online-Adaptation-Method-for-Robust-Depth-Estimation-and-Visual-Odometry-in-the-Open-World)
* [In Pursuit of Total Reproducibility](#In-Pursuit-of-Total-Reproducibility)
* [Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning](#Efficient-Hybrid-Language-Model-Compression-through-Group-Aware-SSM-Pruning)
* [From Gaze to Insight Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation](#From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation)
* [Optimizing LLM Inference Fluid-Guided Online Scheduling with Memory Constraints](#Optimizing-LLM-Inference-Fluid-Guided-Online-Scheduling-with-Memory-Constraints)
* [ConvShareViT Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators](#ConvShareViT-Enhancing-Vision-Transformers-with-Convolutional-Attention-Mechanisms-for-Free-Space-Optical-Accelerators)
* [AutoRAN Automated and Zero-Touch Open RAN Systems](#AutoRAN-Automated-and-Zero-Touch-Open-RAN-Systems)
* [VEXP A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers](#VEXP-A-Low-Cost-RISC-V-ISA-Extension-for-Accelerated-Softmax-Computation-in-Transformers)
* [Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models](#Enhancing-Autonomous-Driving-Systems-with-On-Board-Deployed-Large-Language-Models)
* [Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning](#Scalable-Transceiver-Design-for-Multi-User-Communication-in-FDD-Massive-MIMO-Systems-via-Deep-Learning)
* [A Unified Hardware Accelerator for Fast Fourier Transform and Number Theoretic Transform](#A-Unified-Hardware-Accelerator-for-Fast-Fourier-Transform-and-Number-Theoretic-Transform)
* [Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay](#Revealing-Covert-Attention-by-Analyzing-Human-and-Reinforcement-Learning-Agent-Gameplay)
* [Morphing-based Compression for Data-centric ML Pipelines](#Morphing-based-Compression-for-Data-centric-ML-Pipelines)
* [Easy3D A Simple Yet Effective Method for 3D Interactive Segmentation](#Easy3D-A-Simple-Yet-Effective-Method-for-3D-Interactive-Segmentation)
* [eARCO Efficient Automated Root Cause Analysis with Prompt Optimization](#eARCO-Efficient-Automated-Root-Cause-Analysis-with-Prompt-Optimization)
* [CSPLADE Learned Sparse Retrieval with Causal Language Models](#CSPLADE-Learned-Sparse-Retrieval-with-Causal-Language-Models)
* [The Sword of Damocles in ViTs Computational Redundancy Amplifies Adversarial Transferability](#The-Sword-of-Damocles-in-ViTs-Computational-Redundancy-Amplifies-Adversarial-Transferability)
* [GOAT-TTS LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture](#GOAT-TTS-LLM-based-Text-To-Speech-Generation-Optimized-via-A-Dual-Branch-Architecture)
* [3D Wavelet Convolutions with Extended Receptive Fields for Hyperspectral Image Classification](#3D-Wavelet-Convolutions-with-Extended-Receptive-Fields-for-Hyperspectral-Image-Classification)
* [Matrix representation and GPU-optimized parallel B-spline computing](#Matrix-representation-and-GPU-optimized-parallel-B-spline-computing)
* [Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning](#Epistemic-Uncertainty-aware-Recommendation-Systems-via-Bayesian-Deep-Ensemble-Learning)
* [Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery](#Probabilistic-Task-Parameterization-of-Tool-Tissue-Interaction-via-Sparse-Landmarks-Tracking-in-Robotic-Surgery)
* [FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and Singular Value Decomposition in AI](#FPGA-Optimized-Hardware-Accelerator-for-Fast-Fourier-Transform-and-Singular-Value-Decomposition-in-AI)
* [AlayaDB The Data Foundation for Efficient and Effective Long-context LLM Inference](#AlayaDB-The-Data-Foundation-for-Efficient-and-Effective-Long-context-LLM-Inference)
* [Analysis of Attention in Video Diffusion Transformers](#Analysis-of-Attention-in-Video-Diffusion-Transformers)
* [DiffMOD Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing](#DiffMOD-Progressive-Diffusion-Point-Denoising-for-Moving-Object-Detection-in-Remote-Sensing)
* [WG-IDENT Weak Group Identification of PDEs with Varying Coefficients](#WG-IDENT-Weak-Group-Identification-of-PDEs-with-Varying-Coefficients)
* [Nanoplastic Analysis with Nanoelectromechanical System Fourier Transform Infrared Spectroscopy NEMS-FTIR](#Nanoplastic-Analysis-with-Nanoelectromechanical-System-Fourier-Transform-Infrared-Spectroscopy-NEMS-FTIR)
* [Efficient Generative Model Training via Embedded Representation Warmup](#Efficient-Generative-Model-Training-via-Embedded-Representation-Warmup)
* [Enhancing LLM-based Recommendation through Semantic-Aligned Collaborative Knowledge](#Enhancing-LLM-based-Recommendation-through-Semantic-Aligned-Collaborative-Knowledge)
* [Unleashing Expert Opinion from Social Media for Stock Prediction](#Unleashing-Expert-Opinion-from-Social-Media-for-Stock-Prediction)
* [Mavors Multi-granularity Video Representation for Multimodal Large Language Model](#Mavors-Multi-granularity-Video-Representation-for-Multimodal-Large-Language-Model)
* [Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics](#Masked-Autoencoder-Self-Pre-Training-for-Defect-Detection-in-Microelectronics)
* [OctGPT Octree-based Multiscale Autoregressive Models for 3D Shape Generation](#OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation)
* [KeepKV Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference](#KeepKV-Eliminating-Output-Perturbation-in-KV-Cache-Compression-for-Efficient-LLMs-Inference)
* [TAMP Token-Adaptive Layerwise Pruning in Multimodal Large Language Models](#TAMP-Token-Adaptive-Layerwise-Pruning-in-Multimodal-Large-Language-Models)
* [RadarLLM Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence](#RadarLLM-Empowering-Large-Language-Models-to-Understand-Human-Motion-from-Millimeter-wave-Point-Cloud-Sequence)
* [CUT Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices](#CUT-Pruning-Pre-Trained-Multi-Task-Models-into-Compact-Models-for-Edge-Devices)
* [Understanding and Optimizing Multi-Stage AI Inference Pipelines](#Understanding-and-Optimizing-Multi-Stage-AI-Inference-Pipelines)


## Generalized Neighborhood Attention Multi-dimensional Sparse Attention at the Speed of Light

>Authors: Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi

>2025-04-23

> http://arxiv.org/abs/2504.16922v1

Many **sparse** attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable **sparsity** to escape
the O(n^2) complexity. In this paper, we study a class of promising **sparse**
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-**sparse** cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.


## New Primal-Dual Algorithm for Convex Problems

>Authors: Shuning Liu, Zexian Liu

>2025-04-23

> http://arxiv.org/abs/2504.16876v1

Primal-dual algorithm (PDA) is a classic and popular scheme for
convex-concave saddle point problems. It is universally acknowledged that the
proximal terms in the subproblems about the primal and dual variables are
crucial to the convergence theory and numerical performance of primal-dual
algorithms. By taking advantage of the information from the current and
previous iterative points, we exploit two new proximal terms for the
subproblems about the primal and dual variables. Based on two new proximal
terms, we present a new primal-dual algorithm for convex-concave saddle point
problems with bilinear coupling terms and establish its global convergence and
O(1/N ) ergodic convergence rate. When either the primal function or the dual
function is strongly convex, we accelerate the above proposed algorithm and
show that the corresponding algorithm can achieve O(1/N^2) convergence rate.
Since the conditions for the stepsizes of the proposed algorithm are related
directly to the spectral norm of the linear transform, which is difficult to
obtain in some applications, we also introduce a linesearch strategy for the
above proposed primal-dual algorithm and establish its global convergence and
O(1/N ) ergodic convergence rate . Some numerical experiments are conducted on
matrix game and LASSO problems by comparing with other state-of-the-art
algorithms, which demonstrate the effectiveness of the proposed three
primal-dual algorithms.


## HEMA  A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations

>Authors: Kwangseob Ahn

>2025-04-23

> http://arxiv.org/abs/2504.16754v1

Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted **pruning** reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.


## IRIS Interactive Research Ideation System for Accelerating Scientific Discovery

>Authors: Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan

>2025-04-23

> http://arxiv.org/abs/2504.16728v1

The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System


## Rethinking Vision Transformer for Large-Scale Fine-Grained Image Retrieval

>Authors: Xin Jiang, Hao Tang, Yonghua Pan, Zechao Li

>2025-04-23

> http://arxiv.org/abs/2504.16691v1

Large-scale fine-grained image retrieval (FGIR) aims to retrieve images
belonging to the same subcategory as a given query by capturing subtle
differences in a large-scale setting. Recently, Vision Transformers (ViT) have
been employed in FGIR due to their powerful self-attention mechanism for
modeling long-range dependencies. However, most Transformer-based methods focus
primarily on leveraging self-attention to distinguish fine-grained details,
while overlooking the high computational complexity and redundant dependencies
inherent to these models, limiting their scalability and effectiveness in
large-scale FGIR. In this paper, we propose an Efficient and Effective
ViT-based framework, termed \textbf{EET}, which integrates token **pruning** module
with a discriminative transfer strategy to address these limitations.
Specifically, we introduce a content-based token **pruning** scheme to enhance the
efficiency of the vanilla ViT, progressively removing background or
low-discriminative tokens at different stages by exploiting feature responses
and self-attention mechanism. To ensure the resulting efficient ViT retains
strong discriminative power, we further present a discriminative transfer
strategy comprising both \textit{discriminative knowledge transfer} and
\textit{discriminative region guidance}. Using a distillation paradigm, these
components transfer knowledge from a larger ``teacher'' ViT to a more efficient
``student'' model, guiding the latter to focus on subtle yet crucial regions in
a cost-free manner. Extensive experiments on two widely-used fine-grained
datasets and four large-scale fine-grained datasets demonstrate the
effectiveness of our method. Specifically, EET reduces the inference latency of
ViT-Small by 42.7\% and boosts the retrieval performance of 16-bit hash codes
by 5.15\% on the challenging NABirds dataset.


## DTVM Revolutionizing Smart Contract Execution with Determinism and Compatibility

>Authors: Wei Zhou, Changzheng Wei, Ying Yan, Wei Tang, Zhihao Chen, Xiong Xu, Xuebing Huang, Wengang Chen, Jie Zhang, Yang Chen, Xiaofu Zheng, Hanghang Wu, Shenglong Chen, Ermei Wang, Xiangfei Chen, Yang Yu, Meng Wu, Tao Zhu, Liwei Yuan, Feng Yu, Alex Zhang, Wei Wang, Ji Luo, Zhengyu He, Wenbiao Zhao

>2025-04-23

> http://arxiv.org/abs/2504.16552v1

We introduce the DeTerministic Virtual Machine (DTVM) Stack, a
next-generation smart contract execution framework designed to address critical
performance, determinism, and ecosystem compatibility challenges in blockchain
networks. Building upon WebAssembly (Wasm) while maintaining full Ethereum
Virtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle
Intermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to
balance compilation speed and execution efficiency. DTVM further accommodates
diverse instruction set architectures (e.g., EVM, RISC-V) through modular
adaptation layers. This enables seamless integration with DTVM's hybrid
lazy-JIT compilation engine, which dynamically optimizes performance while
preserving deterministic execution guarantees across heterogeneous
environments. The key contributions including: 1). The framework achieves up to
2$\times$ **acceleration** over evmone in dominant Ethereum contract (e.g.
ERC20/721/1155) execution and reduces fibonacci computation latency by
11.8$\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch
mechanism enables sub-millisecond (0.95ms) post-deployment invocation times,
outperforming up to about 23$\times$ in compilation and invocation efficiency.
3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and
AssemblyScript) through unified bytecode conversion while maintaining EVM ABI
compatibility for seamless invocation. It reduces machine code object sizes by
30.0$\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers
SmartCogent, an AI-driven full-stack development experience, leveraging
fine-tuned LLMs and retrieval-augmented generation to automate tasks across the
smart contract lifecycle: development, debugging, security auditing, and
deployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).


## QuaDMix Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining

>Authors: Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao

>2025-04-23

> http://arxiv.org/abs/2504.16511v1

Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.


## Cross Paradigm Representation and Alignment Transformer for Image Deraining

>Authors: Shun Zou, Yi Zou, Juncheng Li, Guangwei Gao, Guojun Qi

>2025-04-23

> http://arxiv.org/abs/2504.16455v1

Transformer-based networks have achieved strong performance in low-level
vision tasks like image deraining by utilizing spatial or channel-wise
self-attention. However, irregular rain patterns and complex geometric overlaps
challenge single-paradigm architectures, necessitating a unified framework to
integrate complementary global-local and spatial-channel representations. To
address this, we propose a novel Cross Paradigm Representation and Alignment
Transformer (CPRAformer). Its core idea is the hierarchical representation and
alignment, leveraging the strengths of both paradigms (spatial-channel and
global-local) to aid image reconstruction. It bridges the gap within and
between paradigms, aligning and coordinating them to enable deep interaction
and fusion of features. Specifically, we use two types of self-attention in the
Transformer blocks: **sparse** prompt channel self-attention (SPC-SA) and spatial
pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel
dependencies through dynamic **sparsity**, while SPR-SA focuses on spatial rain
distribution and fine-grained texture recovery. To address the feature
misalignment and knowledge differences between them, we introduce the Adaptive
Alignment Frequency Module (AAFM), which aligns and interacts with features in
a two-stage progressive manner, enabling adaptive guidance and complementarity.
This reduces the information gap within and between paradigms. Through this
unified cross-paradigm dynamic interaction framework, we achieve the extraction
of the most valuable interactive fusion information from the two paradigms.
Extensive experiments demonstrate that our model achieves state-of-the-art
performance on eight benchmark datasets and further validates CPRAformer's
robustness in other image restoration tasks and downstream applications.


## Qubit Geometry through Holomorphic Quantization

>Authors: Ahmad Hazazi Ahmad Sumadi, Nurisya Mohd Shah, Umair Abdul Halim, Hishamuddin Zainuddin

>2025-04-23

> http://arxiv.org/abs/2504.16426v1

We develop a wave mechanics formalism for qubit geometry using holomorphic
functions and Mobius transformations, providing a geometric perspective on
quantum computation. This framework extends the standard Hilbert space
description, offering a natural interpretation of standard quantum gates on the
Riemann sphere that is examined through their Mobius action on holomorphic
wavefunction. These wavefunctions emerge via a **quantization** process, with the
Riemann sphere serving as the classical phase space of qubit geometry. We
**quantize** this space using canonical group **quantization** with holomorphic
polarization, yielding holomorphic wavefunctions and spin angular momentum
operators that recover the standard $SU(2)$ algebra with interesting geometric
properties. Such properties reveal how geometric transformations induce quantum
logic gates on the Riemann sphere, providing a novel perspective in quantum
information processing. This result provides a new direction for exploring
quantum computation through Isham's canonical group **quantization** and its
holomorphic polarization method.


## Hamiltonian quantization of complex Chern-Simons theory at level-$k$

>Authors: Muxin Han

>2025-04-23

> http://arxiv.org/abs/2504.16367v1

This paper develops a framework for the Hamiltonian **quantization** of complex
Chern-Simons theory with gauge group $\mathrm{SL}(2,\mathbb{C})$ at an even
level $k\in\mathbb{Z}_+$. Our approach follows the procedure of combinatorial
**quantization** to construct the operator algebras of quantum holonomies on
2-surfaces and develop the representation theory. The $*$-representation of the
operator algebra is carried by the infinite dimensional Hilbert space
$\mathcal{H}_{\vec{\lambda}}$ and closely connects to the infinite-dimensional
$*$-representation of the quantum deformed Lorentz group
$\mathscr{U}_{\mathbf{q}}(sl_2)\otimes
\mathscr{U}_{\widetilde{\mathbf{q}}}(sl_2)$, where $\mathbf{q}=\exp[\frac{2\pi
i}{k}(1+b^2)]$ and $\widetilde{\mathbf{q}}=\exp[\frac{2\pi i}{k}(1+b^{-2})]$
with $|b|=1$. The quantum group $\mathscr{U}_{\mathbf{q}}(sl_2)\otimes
\mathscr{U}_{\widetilde{\mathbf{q}}}(sl_2)$ also emerges from the quantum gauge
transformations of the complex Chern-Simons theory. Focusing on a $m$-holed
sphere $\Sigma_{0,m}$, the physical Hilbert space $\mathcal{H}_{phys}$ is
identified by imposing the gauge invariance and the flatness constraint. The
states in $\mathcal{H}_{phys}$ are the $\mathscr{U}_{\mathbf{q}}(sl_2)\otimes
\mathscr{U}_{\widetilde{\mathbf{q}}}(sl_2)$-invariant linear functionals on a
dense domain in $\mathcal{H}_{\vec{\lambda}}$. Finally, we demonstrate that the
physical Hilbert space carries a Fenchel-Nielsen representation, where a set of
Wilson loop operators associated with a pants decomposition of $\Sigma_{0,m}$
are diagonalized.


## Transitive Array An Efficient GEMM Accelerator with Result Reuse

>Authors: Cong Guo, Chiyue Wei, Jiaming Tang, Bowen Duan, Song Han, Hai Li, Yiran Chen

>2025-04-23

> http://arxiv.org/abs/2504.16339v1

Deep Neural Networks (DNNs) and Large Language Models (LLMs) have
revolutionized artificial intelligence, yet their deployment faces significant
memory and computational challenges, especially in resource-constrained
environments. Quantization techniques have mitigated some of these issues by
reducing data precision, primarily focusing on General Matrix Multiplication
(GEMM). This study introduces a novel **sparsity** paradigm, transitive **sparsity**,
which leverages the reuse of previously computed results to substantially
minimize computational overhead in GEMM operations. By representing transitive
relations using a directed acyclic graph, we develop an efficient strategy for
determining optimal execution orders, thereby overcoming inherent challenges
related to execution dependencies and parallelism. Building on this foundation,
we present the Transitive Array, a multiplication-free accelerator designed to
exploit transitive **sparsity** in GEMM. Our architecture effectively balances
computational workloads across multiple parallel lanes, ensuring high
efficiency and optimal resource utilization. Comprehensive evaluations
demonstrate that the Transitive Array achieves approximately 7.46$\times$ and
3.97$\times$ speedup and 2.31$\times$ and 1.65$\times$ energy reduction
compared to state-of-the-art accelerators such as Olive and BitVert while
maintaining comparable model accuracy on LLaMA models.


## Discrete Codebook Design for Self-interference Suppression in mmWave ISAC

>Authors: Guang Chai, Zhibin Yu, Xiaofeng Wu, Giuseppe Caire

>2025-04-22

> http://arxiv.org/abs/2504.16309v1

This paper presents discrete codebook synthesis methods for self-interference
(SI) suppression in a mmWave device, designed to support FD ISAC. We formulate
a SINR maximization problem that optimizes the RX and TX codewords, aimed at
suppressing the near-field SI signal while maintaining the beamforming gain in
the far-field sensing directions. The formulation considers the practical
constraints of discrete RX and TX codebooks with **quantize**d phase settings, as
well as a TX beamforming gain requirement in the specified communication
direction. Under an alternating optimization framework, the RX and TX codewords
are iteratively optimized, with one fixed while the other is optimized. When
the TX codeword is fixed, we show that the RX codeword optimization problem can
be formulated as an integer quadratic fractional programming (IQFP) problem.
Using Dinkelbach's algorithm, we transform the problem into a sequence of
subproblems in which the numerator and the denominator of the objective
function are decoupled. These subproblems, subject to discrete constraints, are
then efficiently solved by the spherical search (SS) method. This overall
approach is referred to as FP-SS. When the RX codeword is fixed, the TX
codeword optimization problem can similarly be formulated as an IQFP problem,
whereas an additional TX beamforming constraint for communication needs to be
considered. The problem is solved through Dinkelbach's transformation followed
by the constrained spherical search (CSS), and we refer to this approach as
FP-CSS. Finally, we integrate the FP-SS and FP-CSS methods into a joint RX-TX
codebook design approach. Simulations show that, the proposed FP-SS and FP-CSS
achieve the same SI suppression performance as the corresponding exhaustive
search method, but with much lower complexity. Furthermore, the alternating
optimization framework achieved even better SI suppression performance.


## Learning Explainable Dense Reward Shapes via Bayesian Optimization

>Authors: Ryan Koo, Ian Yang, Vipul Raheja, Mingyi Hong, Kwang-Sung Jun, Dongyeop Kang

>2025-04-22

> http://arxiv.org/abs/2504.16272v1

Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to **sparse** feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.


## COBRA Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference

>Authors: Ye Qiao, Zhiheng Cheng, Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang

>2025-04-22

> http://arxiv.org/abs/2504.16269v1

Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.


## Boosting Classifier Performance with Opposition-Based Data Transformation

>Authors: Abdesslem Layeb

>2025-04-22

> http://arxiv.org/abs/2504.16268v1

In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or **sparse** learning
environments.


## TeLLMe An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs

>Authors: Ye Qiao, Zhiheng Cheng, Yifan Zhang, Yian Wang, Sitao Huang

>2025-04-22

> http://arxiv.org/abs/2504.16266v1

Deploying large language models (LLMs) on edge platforms is challenged by
their high computational and memory demands. Although recent **low-bit**
**quantization** methods (e.g., BitNet, DeepSeek) compress weights to as little as
1.58 bits with minimal accuracy loss, edge deployment is still constrained by
limited on-chip resources, power budgets, and the often-neglected latency of
the prefill phase. We present TeLLMe, the first ternary LLM accelerator for
low-power FPGAs (e.g., AMD **KV**260) that fully supports both prefill and
autoregressive decoding using 1.58-bit weights and 8-bit activations. Our
contributions include: (1) a table-lookup matrix engine for ternary matmul that
merges grouped activations with online precomputation to minimize resource use;
(2) a fused, bandwidth-efficient attention module featuring a reversed
reordering scheme to accelerate prefill; and (3) a tightly integrated
normalization and **quantization**--de**quantization** unit optimized for ultra-**low-bit**
inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput
over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128
token prompts, marking a significant energy-efficiency advance and establishing
a new edge FPGA benchmark for generative AI.


## MMInference Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention

>Authors: Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu

>2025-04-22

> http://arxiv.org/abs/2504.16083v1

The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
**sparse** attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique **sparse** pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different **sparse** distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal **sparse** patterns for each head, MMInference constructs the **sparse**
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient **sparse** computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.


## Muon Optimizer Accelerates Grokking

>Authors: Amund Tveit, Bjørn Remseth, Arve Skogvold

>2025-04-22

> http://arxiv.org/abs/2504.16041v1

This paper investigates the impact of different optimizers on the grokking
phenomenon, where models exhibit delayed generalization. We conducted
experiments across seven numerical tasks (primarily modular arithmetic) using a
modern Transformer architecture. The experimental configuration systematically
varied the optimizer (Muon vs. AdamW) and the softmax activation function
(standard softmax, stablemax, and **sparse**max) to assess their combined effect on
learning dynamics. Our empirical evaluation reveals that the Muon optimizer,
characterized by its use of spectral norm constraints and second-order
information, significantly accelerates the onset of grokking compared to the
widely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch
from 153.09 to 102.89 across all configurations, a statistically significant
difference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice
plays a crucial role in facilitating the transition from memorization to
generalization.


## ViSMaP Unsupervised Hour-long Video Summarisation by Meta-Prompting

>Authors: Jian Hu, Dimitrios Korkinof, Shaogang Gong, Mariano Beguerisse-Diaz

>2025-04-22

> http://arxiv.org/abs/2504.15921v1

We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a
system to summarise hour long videos with no-supervision. Most existing video
understanding models work well on short videos of pre-segmented events, yet
they struggle to summarise longer videos where relevant events are **sparse**ly
distributed and not pre-segmented. Moreover, long-form video understanding
often relies on supervised hierarchical training that needs extensive
annotations which are costly, slow and prone to inconsistency. With ViSMaP we
bridge the gap between short videos (where annotated data is plentiful) and
long ones (where it's not). We rely on LLMs to create optimised
pseudo-summaries of long videos using segment descriptions from short ones.
These pseudo-summaries are used as training data for a model that generates
long-form video summaries, bypassing the need for expensive annotations of long
videos. Specifically, we adopt a meta-prompting strategy to iteratively
generate and refine creating pseudo-summaries of long videos. The strategy
leverages short clip descriptions obtained from a supervised short video model
to guide the summary. Each iteration uses three LLMs working in sequence: one
to generate the pseudo-summary from clip descriptions, another to evaluate it,
and a third to optimise the prompt of the generator. This iteration is
necessary because the quality of the pseudo-summaries is highly dependent on
the generator prompt, and varies widely among videos. We evaluate our summaries
extensively on multiple datasets; our results show that ViSMaP achieves
performance comparable to fully supervised state-of-the-art models while
generalising across domains without sacrificing performance. Code will be
released upon publication.


## SARI Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning

>Authors: Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li

>2025-04-22

> http://arxiv.org/abs/2504.15900v1

Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.


## MS-Occ Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction

>Authors: Zhiqiang Wei, Lianqing Zheng, Jianan Liu, Tao Huang, Qing-Long Han, Wenwen Zhang, Fengdeng Zhang

>2025-04-22

> http://arxiv.org/abs/2504.15888v1

Accurate 3D semantic occupancy perception is essential for autonomous driving
in complex environments with diverse and irregular objects. While
vision-centric methods suffer from geometric inaccuracies, LiDAR-based
approaches often lack rich semantic information. To address these limitations,
MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes
middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's
geometric fidelity with camera-based semantic richness via hierarchical
cross-modal fusion. The framework introduces innovations at two critical
stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module
leverages Gaussian kernel rendering on **sparse** LiDAR depth maps to enhance 2D
image features with dense geometric priors, and the Semantic-Aware module
enriches LiDAR voxels with semantic context via deformable cross-attention; (2)
In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically
balances voxel features across modalities, while the High Classification
Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using
self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy
benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%
and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU
and +2.4% mIoU. Ablation studies further validate the contribution of each
module, with substantial improvements in small-object perception, demonstrating
the practical value of MS-Occ for safety-critical autonomous driving scenarios.


## Quantum Discrete Variable Representations

>Authors: Szymon Pliś, Emil Zak

>2025-04-22

> http://arxiv.org/abs/2504.15841v1

We present a fault-tolerant quantum algorithm for implementing the Discrete
Variable Representation (DVR) transformation, a technique widely used in
simulations of quantum-mechanical Hamiltonians. DVR provides a diagonal
representation of local operators and enables **sparse** Hamiltonian structures,
making it a powerful alternative to the finite basis representation (FBR),
particularly in high-dimensional problems. While DVR has been extensively used
in classical simulations, its quantum implementation, particularly using
Gaussian quadrature grids, remains underexplored. We develop a quantum circuit
that efficiently transforms FBR into DVR by following a recursive construction
based on quantum arithmetic operations, and we compare this approach with
methods that directly load DVR matrix elements using quantum read-only memory
(QROM). We analyze the quantum resources, including T-gate and qubit counts,
required for implementing the DVR unitary and discuss preferable choices of
QROM-based and recursive-based methods for a given matrix size and precision.
This study lays the groundwork for utilizing DVR Hamiltonians in quantum
algorithms such as quantum phase estimation with block encoding.


## BBAL A Bidirectional Block Floating Point-Based Quantisation Accelerator for Large Language Models

>Authors: Xiaomeng Han, Yuan Cheng, Jing Wang, Junyang Lu, Hui Wang, X. x. Zhang, Ning Xu, Dawei Yang, Zhe Jiang

>2025-04-22

> http://arxiv.org/abs/2504.15721v1

Large language models (LLMs), with their billions of parameters, pose
substantial challenges for deployment on edge devices, straining both memory
capacity and computational resources. Block Floating Point (BFP) quantisation
reduces memory and computational overhead by converting high-overhead floating
point operations into **low-bit** fixed point operations. However, BFP requires
aligning all data to the maximum exponent, which causes loss of small and
moderate values, resulting in quantisation error and degradation in the
accuracy of LLMs. To address this issue, we propose a Bidirectional Block
Floating Point (BBFP) data format, which reduces the probability of selecting
the maximum as shared exponent, thereby reducing quantisation error. By
utilizing the features in BBFP, we present a full-stack Bidirectional Block
Floating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily
comprising a processing element array based on BBFP, paired with proposed
cost-effective nonlinear computation unit. Experimental results show BBAL
achieves a 22% improvement in accuracy compared to an outlier-aware accelerator
at similar efficiency, and a 40% efficiency improvement over a BFP-based
accelerator at similar accuracy.


## Exploring the Role of Large Language Models in Cybersecurity A Systematic Survey

>Authors: Shuang Tian, Tao Zhang, Jiqiang Liu, Jiacheng Wang, Xuangou Wu, Xiaoqiang Zhu, Ruichen Zhang, Weiting Zhang, Zhenhui Yuan, Shiwen Mao, Dong In Kim

>2025-04-22

> http://arxiv.org/abs/2504.15622v1

With the rapid development of technology and the **acceleration** of
digitalisation, the frequency and complexity of cyber security threats are
increasing. Traditional cybersecurity approaches, often based on static rules
and predefined scenarios, are struggling to adapt to the rapidly evolving
nature of modern cyberattacks. There is an urgent need for more adaptive and
intelligent defence strategies. The emergence of Large Language Model (LLM)
provides an innovative solution to cope with the increasingly severe cyber
threats, and its potential in analysing complex attack patterns, predicting
threats and assisting real-time response has attracted a lot of attention in
the field of cybersecurity, and exploring how to effectively use LLM to defend
against cyberattacks has become a hot topic in the current research field. This
survey examines the applications of LLM from the perspective of the cyber
attack lifecycle, focusing on the three phases of defense reconnaissance,
foothold establishment, and lateral movement, and it analyzes the potential of
LLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how
LLM-based security solutions are deployed and applied in different network
scenarios. It also summarizes the internal and external risk issues faced by
LLM during its application. Finally, this survey also points out the facing
risk issues and possible future research directions in this domain.


## A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings

>Authors: Md Millat Hosen

>2025-04-22

> http://arxiv.org/abs/2504.15610v2

The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit **quantization** method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient **quantization**, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic **quantization** routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.


## SparseJEPA Sparse Representation Learning of Joint Embedding Predictive Architectures

>Authors: Max Hartman, Lav Varshney

>2025-04-22

> http://arxiv.org/abs/2504.16140v1

Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
**sparse** representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating **sparsity** not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.


## In-context Ranking Preference Optimization

>Authors: Junda Wu, Rohan Surana, Zhouhang Xie, Yiran Shen, Yu Xia, Tong Yu, Ryan A. Rossi, Prithviraj Ammanabrolu, Julian McAuley

>2025-04-21

> http://arxiv.org/abs/2504.15477v1

Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
**sparse** pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.


## Bigram Subnetworks Mapping to Next Tokens in Transformer Language Models

>Authors: Tyler A. Chang, Benjamin K. Bergen

>2025-04-21

> http://arxiv.org/abs/2504.15471v1

In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.


## KeyDiff Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments

>Authors: Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott

>2025-04-21

> http://arxiv.org/abs/2504.15364v2

In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free **KV** cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other **KV** cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a **KV** cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% **KV** cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.


## Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs

>Authors: Marina Sakharova, Abhinav Anand, Mira Mezini

>2025-04-21

> http://arxiv.org/abs/2504.15210v1

Code-generating Large Language Models (LLMs) have become essential tools in
modern software development, enhancing productivity and accelerating
development. This paper aims to investigate the fine-tuning of code-generating
LLMs using Reinforcement Learning and Direct Preference Optimization, further
improving their performance. To achieve this, we enhance the training data for
the reward model with the help of symbolic execution techniques, ensuring more
comprehensive and objective data. With symbolic execution, we create a custom
dataset that better captures the nuances in code evaluation. Our reward models,
fine-tuned on this dataset, demonstrate significant improvements over the
baseline, CodeRL, in estimating the quality of generated code. Our
code-generating LLMs, trained with the help of reward model feedback, achieve
similar results compared to the CodeRL benchmark.


## Compute-Optimal LLMs Provably Generalize Better With Scale

>Authors: Marc Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Christopher De Sa, J. Zico Kolter, Andrew Gordon Wilson

>2025-04-21

> http://arxiv.org/abs/2504.15208v1

Why do larger language models generalize better? To investigate this
question, we develop generalization bounds on the pretraining objective of
large language models (LLMs) in the compute-optimal regime, as described by the
Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type
martingale concentration inequality that tightens existing bounds by accounting
for the variance of the loss function. This generalization bound can be
decomposed into three interpretable components: the number of parameters per
token, the loss variance, and the **quantization** error at a fixed bitrate. As
compute-optimal language models are scaled up, the number of parameters per
data point remains constant; however, both the loss variance and the
**quantization** error decrease, implying that larger models should have smaller
generalization gaps. We examine why larger models tend to be more quantizable
from an information theoretic perspective, showing that the rate at which they
can integrate new information grows more slowly than their capacity on the
compute-optimal frontier. From these findings we produce a scaling law for the
generalization gap, with bounds that become predictably stronger with scale.


## Efficient Pretraining Length Scaling

>Authors: Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou

>2025-04-21

> http://arxiv.org/abs/2504.14992v2

Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative **KV** cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the **KV** cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same **KV** cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.


## Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos

>Authors: Songping Wang, Hanqing Liu, Yueming Lyu, Xiantao Hu, Ziwen He, Wei Wang, Caifeng Shan, Liang Wang

>2025-04-21

> http://arxiv.org/abs/2504.14921v2

Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.


## StableQuant Layer Adaptive Post-Training Quantization for Speech Foundation Models

>Authors: Yeona Hong, Hyewon Han, Woo-jin Chung, Hong-Goo Kang

>2025-04-21

> http://arxiv.org/abs/2504.14915v1

In this paper, we propose StableQuant, a novel adaptive post-training
**quantization** (PTQ) algorithm for widely used speech foundation models (SFMs).
While PTQ has been successfully employed for compressing large language models
(LLMs) due to its ability to bypass additional fine-tuning, directly applying
these techniques to SFMs may not yield optimal results, as SFMs utilize
distinct network architecture for feature extraction. StableQuant demonstrates
optimal **quantization** performance regardless of the network architecture type,
as it adaptively determines the **quantization** range for each layer by analyzing
both the scale distributions and overall performance. We evaluate our algorithm
on two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)
task, and achieve superior performance compared to traditional PTQ methods.
StableQuant successfully reduces the sizes of SFM models to a quarter and
doubles the inference speed while limiting the word error rate (WER)
performance drop to less than 0.3% with 8-bit **quantization**.


## An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV A Value of Information Perspective

>Authors: Wei Wang, Nan Cheng, Conghao Zhou, Haixia Peng, Haibo Zhou, Zhou Su, Xuemin, Shen

>2025-04-21

> http://arxiv.org/abs/2504.14824v1

The Internet of Vehicles (IoV) is undergoing a transformative evolution,
enabled by advancements in future 6G network technologies, to support
intelligent, highly reliable, and low-latency vehicular services. However, the
enhanced capabilities of loV have heightened the demands for efficient network
resource allocation while simultaneously giving rise to diverse vehicular
service requirements. For network service providers (NSPs), meeting the
customized resource-slicing requirements of vehicle service providers (VSPs)
while maximizing social welfare has become a significant challenge. This paper
proposes an innovative solution by integrating a mean-field multi-agent
reinforcement learning (MFMARL) framework with an enhanced
Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social
welfare maximization under the condition of unknown VSP utility functions. The
core of this solution is introducing the ``value of information" as a novel
monetary metric to estimate the expected benefits of VSPs, thereby ensuring the
effective execution of the VCG auction mechanism. MFMARL is employed to
optimize resource allocation for social welfare maximization while adapting to
the intelligent and dynamic requirements of IoV. The proposed enhanced VCG
auction mechanism not only protects the privacy of VSPs but also reduces the
likelihood of collusion among VSPs, and it is theoretically proven to be
dominant-strategy incentive compatible (DSIC). The simulation results
demonstrate that, compared to the VCG mechanism implemented using **quantization**
methods, the proposed mechanism exhibits significant advantages in convergence
speed, social welfare maximization, and resistance to collusion, providing new
insights into resource allocation in intelligent 6G networks.


## DONOD Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning

>Authors: Jucheng Hu, Surong Yang, Dongzhan Zhou, Lijun Wu

>2025-04-21

> http://arxiv.org/abs/2504.14810v1

Ad-hoc instruction fine-tuning of large language models (LLMs) is widely
adopted for domain-specific adaptation. While domain-specific supervised
fine-tuning (SFT) is effective and efficient, it often weakens cross-domain
generalization and struggles with noisy training data. To address these
challenges, we propose DONOD, a lightweight model-intrinsic data **pruning**
method. Our approach evaluates data using two model-parameter-based metrics:
Delta of Norm (DON), which captures the cumulative influence on model weights,
and Norm of Delta (NOD), which quantifies weight instability. Moreover, by
employing the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and
generalization-harming samples without relying on auxiliary models during the
SFT process. Experiments on mathematical tasks demonstrate that data selected
by DONOD achieve superior fine-tuning efficiency and improved robustness
against noisy data. By filtering out 70% of the full dataset, we improve
target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,
our selected data present superior cross-architecture generalization. Data
pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger
models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD
demonstrates comparable or superior performance while remaining
dataset-agnostic, enabling broader applicability.


## gLLM Global Balanced Pipeline Parallelism System for Distributed LLM Serving with Token Throttling

>Authors: Tianyu Guo, Xianwei Zhang, Jiangsu Du, Zhiguang Chen, Nong Xiao, Yutong Lu

>2025-04-21

> http://arxiv.org/abs/2504.14775v1

Pipeline parallelism has emerged as a predominant approach for deploying
large language models (LLMs) across distributed nodes, owing to its lower
communication overhead compared to tensor parallelism. While demonstrating high
throughput in request serving, pipeline parallelism often suffers from
performance limitations caused by pipeline bubbles, which are primarily
resulted from imbalanced computation delays across batches. Existing methods
like Sarathi-Serve attempt to address this through hybrid scheduling of chunked
prefill and decode tokens using a fixed token budget. However, such methods may
experience significant fluctuations due to either insufficient prefill tokens
or uneven distribution of decode tokens, ultimately leading to computational
imbalance. To overcome these inefficiencies, we present gLLM, a globally
balanced pipeline parallelism system incorporating Token Throttling to
effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a
fine-grained scheduling policy that independently regulates the quantities of
prefill and decode tokens, thus enabling balanced computation by leveraging
global information from the inference system. Specifically, for decode tokens,
gLLM maintains near-consistent token count across processing batches. For
prefill tokens, it dynamically adjusts batch sizes based on both total pending
tokens and the memory utilization rates of key-value cache (**KV** cache).
Furthermore, gLLM runtime adopts an asynchronous execution and message passing
architecture specifically optimized for pipeline parallelism characteristics.
Experimental evaluations with representative LLMs show that gLLM achieves
significant performance improvements, delivering 11% to 398% higher maximum
throughput compared to state-of-the-art pipeline or tensor parallelism systems,
while simultaneously maintaining lower latency.


## IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays

>Authors: Sascha Jecklin, Aidana Massalimova, Ruyi Zha, Lilian Calvet, Christoph J. Laux, Mazda Farshad, Philipp Fürnstahl

>2025-04-20

> http://arxiv.org/abs/2504.14699v1

Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
**sparse** fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to **sparse**, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary **sparse**-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.


## Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking

>Authors: Katelyn Xiaoying Mei, Nic Weber

>2025-04-20

> http://arxiv.org/abs/2504.14689v1

The recent rapid advancement of LLM-based AI systems has accelerated our
search and production of information. While the advantages brought by these
systems seemingly improve the performance or efficiency of human activities,
they do not necessarily enhance human capabilities. Recent research has started
to examine the impact of generative AI on individuals' cognitive abilities,
especially critical thinking. Based on definitions of critical thinking across
psychology and education, this position paper proposes the distinction between
demonstrated and performed critical thinking in the era of generative AI and
discusses the implication of this distinction in research and development of AI
systems that aim to augment human critical thinking.


## Efficient Federated Split Learning for Large Language Models over Communication Networks

>Authors: Kai Zhao, Zhaohui Yang

>2025-04-20

> http://arxiv.org/abs/2504.14667v1

Fine-tuning pre-trained large language models (LLM) in a distributed manner
poses significant challenges on resource-constrained edge devices. To address
this challenge, we propose FedsLLM, a novel framework that integrates split
federated learning with parameter-efficient fine-tuning techniques. By
leveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the
computational burden on edge devices. Furthermore, the introduction of a
federated server facilitates parallel training and enhances privacy. To
accommodate heterogeneous communication conditions and diverse computational
capabilities of edge devices, as well as the impact of LoRA rank selection on
model convergence and training cost, we formulate a joint optimization problem.
The formulated problem jointly optimizes subchannel allocation, power control,
model splitting point selection, and LoRA rank configuration, all aimed at
minimizing total training delay. An alternating optimization algorithm is
developed to efficiently solve this problem and accelerate the training
process. Simulation results demonstrate that the proposed FedsLLM framework
achieves comparable model accuracy while significantly reducing client-side
computational requirements. Furthermore, the proposed resource allocation
scheme and adaptive LoRA rank selection strategy notably reduce the training
latency compared to conventional approaches.


## HLSTester Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis

>Authors: Kangwei Xu, Bing Li, Grace Li Zhang, Ulf Schlichtmann

>2025-04-20

> http://arxiv.org/abs/2504.14641v1

In high-level synthesis (HLS), C/C++ programs with synthesis directives are
used to generate circuits for FPGA implementations. However, hardware-specific
and platform-dependent characteristics in these implementations can introduce
behavioral discrepancies between the original C/C++ programs and the circuits
after high-level synthesis. Existing methods for testing behavioral
discrepancies in HLS are still immature, and the testing workflow requires
significant human efforts. To address this challenge, we propose HLSTester, a
large language model (LLM) aided testing framework that efficiently detects
behavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance
prompt quality, the testbenches for original C/C++ programs are leveraged to
guide LLMs in generating HLS-compatible testbenches, effectively eliminating
certain traditional C/C++ constructs that are incompatible with HLS tools. Key
variables are pinpointed through a backward slicing technique in both C/C++ and
HLS programs to monitor their runtime spectra, enabling an in-depth analysis of
the discrepancy symptoms. To reduce test time, a testing input generation
mechanism is introduced to integrate dynamic mutation with insights from an
LLM-based progressive reasoning chain. In addition, repetitive hardware testing
is skipped by a redundancy-aware filtering technique for the generated test
inputs. Experimental results demonstrate that the proposed LLM-aided testing
framework significantly accelerates the testing workflow while achieving higher
testbench simulation pass rates compared with the traditional method and the
direct use of LLMs on the same HLS programs.


## NoWag A Unified Framework for Shape Preserving Compression of Large Language Models

>Authors: Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, Lin F. Yang

>2025-04-20

> http://arxiv.org/abs/2504.14569v1

Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
**quantization** NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured **pruning** NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag


## Optimizing SLO-oriented LLM Serving with PD-Multiplexing

>Authors: Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo

>2025-04-20

> http://arxiv.org/abs/2504.14489v2

Modern LLM services demand high throughput and stringent SLO guarantees
across two distinct inference phases-prefill and decode-and complex multi-turn
workflows. However, current systems face a fundamental tradeoff: out-of-place
compute partition enables per-phase SLO attainment, while in-place memory
sharing maximizes throughput via **KV** cache reuse. Moreover, existing in-place
compute partition also encounters low utilization and high overhead due to
phase-coupling design. We present Drift, a new LLM serving framework that
resolves this tension via PD multiplexing, enabling in-place and
phase-decoupled compute partition. Drift leverages low-level GPU partitioning
techniques to multiplex prefill and decode phases spatially and adaptively on
shared GPUs, while preserving in-place memory sharing. To fully leverage the
multiplexing capability, Drift introduces an adaptive gang scheduling
mechanism, a contention-free modeling method, and a SLO-aware dispatching
policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput
improvement (up to $17.5\times$) over state-of-the-art baselines, while
consistently meeting SLO targets under complex LLM workloads.


## Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation

>Authors: Guoyi Zhang, Siyang Chen, Guangsheng Xu, Han Wang, Xiaohu Zhang

>2025-04-20

> http://arxiv.org/abs/2504.14481v1

Foreground segmentation is crucial for scene understanding, yet
parameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often
fails in complex scenarios, such as camouflage and infrared imagery. We
attribute this challenge to the inherent texture bias in VFMs, which is
exacerbated during fine-tuning and limits generalization in texture-**sparse**
environments. To address this, we propose Ladder Shape-bias Representation
Side-tuning (LSR-ST), a lightweight PEFT framework that enhances model
robustness by introducing shape-biased inductive priors. LSR-ST captures
shape-aware features using a simple HDConv Block, which integrates large-kernel
attention and residual learning. The method satisfies three key conditions for
inducing shape bias: large receptive fields, multi-order feature interactions,
and **sparse** connectivity. Our analysis reveals that these improvements stem from
representation efficiency-the ability to extract task-relevant, structurally
grounded features while minimizing redundancy. We formalize this concept via
Information Bottleneck theory and advocate for it as a key PEFT objective.
Unlike traditional NLP paradigms that focus on optimizing parameters and
memory, visual tasks require models that extract task-defined semantics, rather
than just relying on pre-encoded features. This shift enables our approach to
move beyond conventional trade-offs, offering more robust and generalizable
solutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves
consistent improvements across 17 datasets and 6 tasks using only 4.719M
trainable parameters. These results highlight the potential of representation
efficiency for robust and adaptable VFMs within complex visual environments.


## SG-Reg Generalizable and Efficient Scene Graph Registration

>Authors: Chuhao Liu, Zhijian Qiao, Jieqi Shi, Ke Wang, Peize Liu, Shaojie Shen

>2025-04-20

> http://arxiv.org/abs/2504.14440v1

This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a **sparse** and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.


## Accelerating LLM Inference with Flexible NM Sparsity via A Fully Digital Compute-in-Memory Accelerator

>Authors: Akshat Ramachandran, Souvik Kundu, Arnab Raha, Shamik Kundu, Deepak K. Mathaikutty, Tushar Krishna

>2025-04-19

> http://arxiv.org/abs/2504.14365v1

Large language model (LLM) **pruning** with fixed N:M structured **sparsity**
significantly limits the expressivity of the **sparse** model, yielding sub-optimal
performance. In contrast, supporting multiple N:M patterns to provide **sparse**
representational freedom introduces costly overhead in hardware. To address
these challenges for LLMs, we first present a flexible layer-wise
outlier-density-aware N:M **sparsity** (FLOW) selection method. FLOW enables the
identification of optimal layer-wise N and M values (from a given range) by
simultaneously accounting for the presence and distribution of outliers,
allowing a higher degree of representational freedom. To deploy **sparse** models
with such N:M flexibility, we then introduce a flexible, low-overhead digital
compute-in-memory architecture (FlexCiM). FlexCiM supports diverse **sparsity**
patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,
which are adaptively aggregated and disaggregated through distribution and
merging mechanisms for different N and M values. Extensive experiments on both
transformer-based and recurrence-based state space foundation models (SSMs)
demonstrate that FLOW outperforms existing alternatives with an accuracy
improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference
latency and 1.5x lower energy consumption compared to existing **sparse**
accelerators. Code is available at: https://github.com/FLOW-open-project/FLOW


## Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses

>Authors: Jinbo Wen, Jiawen Kang, Yang Zhang, Yue Zhong, Dusit Niyato, Jie Xu, Jianhang Tang, Chau Yuen

>2025-04-19

> http://arxiv.org/abs/2504.14326v1

Mobile metaverses have attracted significant attention from both academia and
industry, which are envisioned as the next-generation Internet, providing users
with immersive and ubiquitous metaverse services through mobile devices. Driven
by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial
Intelligence (AI) agents hold the potential to empower the creation,
maintenance, and evolution of mobile metaverses. Currently, AI agents are
primarily constructed using cloud-based LLMs and VLMs. However, several
challenges hinder their effective implementation, including high service
latency and potential sensitive data leakage during perception and processing.
In this paper, we develop an edge-cloud collaboration-based federated AI agent
construction framework in mobile metaverses. Specifically, Edge Servers (ESs),
acting as agent infrastructures, collaboratively create agent modules in a
distributed manner. The cloud server then integrates these modules into AI
agents and deploys them at the edge, thereby enabling low-latency AI agent
services for users. Considering that ESs may exhibit dynamic levels of
willingness to participate in federated AI agent construction, we design a
two-period dynamic contract model to continuously motivate ESs to participate
in agent module creation, effectively addressing the dynamic information
asymmetry between the cloud server and the ESs. Furthermore, we propose an
Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to
efficiently generate optimal dynamic contracts, in which dynamic structured
**pruning** is applied to DM-based actor networks to enhance denoising efficiency
and policy learning performance. Extensive simulations demonstrate the
effectiveness and superiority of the EDMSAC algorithm and the proposed contract
model.


## RAMCT Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking

>Authors: Shang Zhang, Yuke Hou, Guoqiang Gong, Ruoyan Xiong, Yue Zhang

>2025-04-19

> http://arxiv.org/abs/2504.14278v1

Correlation filter (CF)-based trackers have gained significant attention for
their computational efficiency in thermal infrared (TIR) target tracking.
However, ex-isting methods struggle with challenges such as low-resolution
imagery, occlu-sion, background clutter, and target deformation, which severely
impact tracking performance. To overcome these limitations, we propose RAMCT, a
region-adaptive **sparse** correlation filter tracker that integrates multi-channel
feature opti-mization with an adaptive regularization strategy. Firstly, we
refine the CF learn-ing process by introducing a spatially adaptive binary
mask, which enforces spar-sity in the target region while dynamically
suppressing background interference. Secondly, we introduce generalized
singular value decomposition (GSVD) and propose a novel GSVD-based
region-adaptive iterative Tikhonov regularization method. This enables flexible
and robust optimization across multiple feature channels, improving resilience
to occlusion and background variations. Thirdly, we propose an online
optimization strategy with dynamic discrepancy-based pa-rameter adjustment.
This mechanism facilitates real time adaptation to target and background
variations, thereby improving tracking accuracy and robustness. Ex-tensive
experiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks
demonstrate that RAMCT outperforms other state-of-the-art trackers in terms of
accuracy and robustness.


## Understanding the Repeat Curse in Large Language Models from a Feature Perspective

>Authors: Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang

>2025-04-19

> http://arxiv.org/abs/2504.14218v1

Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.


## Efficient state transition algorithm with guaranteed optimality

>Authors: Xiaojun Zhou, Chunhua Yang, Weihua Gui

>2025-04-19

> http://arxiv.org/abs/2504.14211v1

As a constructivism-based intelligent optimization method, state transition
algorithm (STA) has exhibited powerful search ability in optimization. However,
the standard STA still shows slow convergence at a later stage for flat
landscape and a user has to preset its maximum number of iterations (or
function evaluations) by experience. To resolve these two issues, efficient
state transition algorithm is proposed with guaranteed optimality. Firstly,
novel translation transformations based on predictive modeling are proposed to
generate more potential candidates by utilizing historical information.
Secondly, parameter control strategies are proposed to accelerate the
convergence. Thirdly, a specific termination condition is designed to guarantee
that the STA can stop automatically at an optimal point, which is equivalent to
the zero gradient in mathematical programming. Experimental results have
demonstrated the effectiveness and superiority of the proposed method.


## DConAD A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection

>Authors: Wenxin Zhang, Xiaojian Lin, Wenjun Yu, Guangzhen Yao, jingxiang Zhong, Yu Li, Renda Han, Songcheng Xu, Hao Shi, Cuicui Luo

>2025-04-19

> http://arxiv.org/abs/2504.14204v1

Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the **sparsity** of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.


## Room-temperature high-average-power strong-field terahertz source based on industrial high-repetition-rate femtosecond laser

>Authors: Deyin Kong, Yichen Su, Cheng Song, Xiaojun Wu

>2025-04-19

> http://arxiv.org/abs/2504.14196v1

Free-space strong-field terahertz (THz) pulses, generated via optical
rectification of femtosecond lasers in nonlinear crystals, are pivotal in
various applications. However, conventional Ti:sapphire lasers struggle to
produce high-average-power THz due to their limited output power. While
kilowatt ytterbium lasers are increasingly adopted, their application in THz
generation faces challenges: low optical-to-THz conversion efficiency
(attributed to long pulse durations and low energy) and crystal damage under
high pumping power. Here, we report a high-average-power strong-field THz
source using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,
50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By
systematically optimizing TPFP implementations and comparing grating- and
echelon-type configurations, we achieve a THz source with 64.5 mW average power
at 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at
0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in
cobalt-iron ferromagnetic nanofilms. This high-repetition-rate,
high-average-power THz system, combined with its potential capabilities in high
signal-to-noise spectroscopy and imaging, promises transformative impacts in
quantum matter manipulation, non-destructive testing, and biomedicine.


## FGMP Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference

>Authors: Coleman Hooper, Charbel Sakr, Ben Keller, Rangharajan Venkatesan, Kurt Keutzer, Sophia Shao, Brucek Khailany

>2025-04-19

> http://arxiv.org/abs/2504.14152v1

Quantization is a powerful tool to improve large language model (LLM)
inference efficiency by utilizing more energy-efficient low-precision datapaths
and reducing memory footprint. However, accurately quantizing LLM weights and
activations to low precision is challenging without degrading model accuracy.
We propose fine-grained mixed precision (FGMP) **quantization**, a post-training
mixed-precision **quantization** hardware-software co-design methodology that
maintains accuracy while quantizing the majority of weights and activations to
reduced precision. Our work makes the following contributions: 1) We develop a
policy that uses the perturbation in each value, weighted by the Fisher
information, to select which weight and activation blocks to keep in higher
precision. This approach preserves accuracy by identifying which weight and
activation blocks need to be retained in higher precision to minimize the
perturbation in the model loss. 2) We also propose a sensitivity-weighted
clipping approach for fine-grained **quantization** which helps retain accuracy for
blocks that are **quantize**d to low precision. 3) We then propose hardware
augmentations to leverage the efficiency benefits of FGMP **quantization**. Our
hardware implementation encompasses i) datapath support for FGMP at block
granularity, and ii) a mixed-precision activation **quantization** unit to assign
activation blocks to high or low precision on the fly with minimal runtime and
energy overhead. Our design, prototyped using NVFP4 (an FP4 format with
microscaling) as the low-precision datatype and FP8 as the high-precision
datatype, facilitates efficient FGMP **quantization**, attaining <1% perplexity
degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8
baseline design while consuming 14% less energy during inference and requiring
30% less weight memory.


## HF4Rec Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation

>Authors: Jiakai Tang, Jingsen Zhang, Zihang Tian, Xueyang Feng, Lei Wang, Xu Chen

>2025-04-19

> http://arxiv.org/abs/2504.14147v1

Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in **sparse** interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.


## Lightweight Road Environment Segmentation using Vector Quantization

>Authors: Jiyong Kwag, Alper Yilmaz, Charles Toth

>2025-04-19

> http://arxiv.org/abs/2504.14113v1

Road environment segmentation plays a significant role in autonomous driving.
Numerous works based on Fully Convolutional Networks (FCNs) and Transformer
architectures have been proposed to leverage local and global contextual
learning for efficient and accurate semantic segmentation. In both
architectures, the encoder often relies heavily on extracting continuous
representations from the image, which limits the ability to represent
meaningful discrete information. To address this limitation, we propose
segmentation of the autonomous driving environment using vector **quantization**.
Vector **quantization** offers three primary advantages for road environment
segmentation. (1) Each continuous feature from the encoder is mapped to a
discrete vector from the codebook, helping the model discover distinct features
more easily than with complex continuous features. (2) Since a discrete feature
acts as compressed versions of the encoder's continuous features, they also
compress noise or outliers, enhancing the image segmentation task. (3) Vector
**quantization** encourages the latent space to form coarse clusters of continuous
features, forcing the model to group similar features, making the learned
representations more structured for the decoding process. In this work, we
combined vector **quantization** with the lightweight image segmentation model
MobileUNETR and used it as a baseline model for comparison to demonstrate its
efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,
outperforming the baseline by 2.9 % without increasing the model's initial size
or complexity.


## 6G WavesFM A Foundation Model for Sensing, Communication, and Localization

>Authors: Ahmed Aboulfotouh, Elsayed Mohammed, Hatem Abou-Zeid

>2025-04-18

> http://arxiv.org/abs/2504.14100v1

This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.


## Transformation of audio embeddings into interpretable, concept-based representations

>Authors: Alice Zhang, Edison Thomaz, Lie Lu

>2025-04-18

> http://arxiv.org/abs/2504.14076v1

Advancements in audio neural networks have established state-of-the-art
results on downstream audio tasks. However, the black-box structure of these
models makes it difficult to interpret the information encoded in their
internal audio representations. In this work, we explore the semantic
interpretability of audio embeddings extracted from these neural networks by
leveraging CLAP, a contrastive learning model that brings audio and text into a
shared embedding space. We implement a post-hoc method to transform CLAP
embeddings into concept-based, **sparse** representations with semantic
interpretability. Qualitative and quantitative evaluations show that the
concept-based representations outperform or match the performance of original
audio embeddings on downstream tasks while providing interpretability.
Additionally, we demonstrate that fine-tuning the concept-based representations
can further improve their performance on downstream tasks. Lastly, we publish
three audio-specific vocabularies for concept-based interpretability of audio
embeddings.


## A Baseline for Self-state Identification and Classification in Mental Health Data CLPsych 2025 Task

>Authors: Laerdon Kim

>2025-04-18

> http://arxiv.org/abs/2504.14066v1

We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
**quantize**d Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.


## Scaling sparse feature circuit finding for in-context learning

>Authors: Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez, Arthur Conmy, Neel Nanda

>2025-04-18

> http://arxiv.org/abs/2504.13756v1

Sparse autoencoders (SAEs) are a popular tool for interpreting large language
model activations, but their utility in addressing open questions in
interpretability remains unclear. In this work, we demonstrate their
effectiveness by using SAEs to deepen our understanding of the mechanism behind
in-context learning (ICL). We identify abstract SAE features that (i) encode
the model's knowledge of which task to execute and (ii) whose latent vectors
causally induce the task zero-shot. This aligns with prior work showing that
ICL is mediated by task vectors. We further demonstrate that these task vectors
are well approximated by a **sparse** sum of SAE latents, including these
task-execution features. To explore the ICL mechanism, we adapt the **sparse**
feature circuits methodology of Marks et al. (2024) to work for the much larger
Gemma-1 2B model, with 30 times as many parameters, and to the more complex
task of ICL. Through circuit finding, we discover task-detecting features with
corresponding SAE latents that activate earlier in the prompt, that detect when
tasks have been performed. They are causally linked with task-execution
features through the attention and MLP sublayers.


## Learning to Attribute with Attention

>Authors: Benjamin Cohen-Wang, Yung-Sung Chuang, Aleksander Madry

>2025-04-18

> http://arxiv.org/abs/2504.13752v1

Given a sequence of tokens generated by a language model, we may want to
identify the preceding tokens that influence the model to generate this
sequence. Performing such token attribution is expensive; a common approach is
to ablate preceding tokens and directly measure their effects. To reduce the
cost of token attribution, we revisit attention weights as a heuristic for how
a language model uses previous tokens. Naive approaches to attribute model
behavior with attention (e.g., averaging attention weights across attention
heads to estimate a token's influence) have been found to be unreliable. To
attain faithful attributions, we propose treating the attention weights of
different attention heads as features. This way, we can learn how to
effectively leverage attention weights for attribution (using signal from
ablations). Our resulting method, Attribution with Attention (AT2), reliably
performs on par with approaches that involve many ablations, while being
significantly more efficient. To showcase the utility of AT2, we use it to
prune less important parts of a provided context in a question answering
setting, improving answer quality. We provide code for AT2 at
https://github.com/MadryLab/AT2 .


## Gradual Binary Search and Dimension Expansion  A general method for activation quantization in LLMs

>Authors: Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello

>2025-04-18

> http://arxiv.org/abs/2504.13989v1

Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of **quantization**
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving **low-bit** **quantization**.
Our method based on a gradual binary search enables 3-bit **quantization** for
weights, activations, and key-value (**KV**) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit **quantization** for weights, activations, and **KV** cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
**quantization**.


## ViG3D-UNet Volumetric Vascular Connectivity-Aware Segmentation via 3D Vision Graph Representation

>Authors: Bowen Liu, Chunlei Meng, Wei Lin, Hongda Zhang, Ziqing Zhou, Zhongxue Gan, Chun Ouyang

>2025-04-18

> http://arxiv.org/abs/2504.13599v1

Accurate vascular segmentation is essential for coronary visualization and
the diagnosis of coronary heart disease. This task involves the extraction of
**sparse** tree-like vascular branches from the volumetric space. However, existing
methods have faced significant challenges due to discontinuous vascular
segmentation and missing endpoints. To address this issue, a 3D vision graph
neural network framework, named ViG3D-UNet, was introduced. This method
integrates 3D graph representation and aggregation within a U-shaped
architecture to facilitate continuous vascular segmentation. The ViG3D module
captures volumetric vascular connectivity and topology, while the convolutional
module extracts fine vascular details. These two branches are combined through
channel attention to form the encoder feature. Subsequently, a paperclip-shaped
offset decoder minimizes redundant computations in the **sparse** feature space and
restores the feature map size to match the original input dimensions. To
evaluate the effectiveness of the proposed approach for continuous vascular
segmentation, evaluations were performed on two public datasets, ASOCA and
ImageCAS. The segmentation results show that the ViG3D-UNet surpassed competing
methods in maintaining vascular segmentation connectivity while achieving high
segmentation accuracy. Our code will be available soon.


## MAAM A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework

>Authors: Zhenkai Qin, Feng Zhu, Huan Zeng, Xunyi Nong

>2025-04-18

> http://arxiv.org/abs/2504.13574v1

The demand for lightweight models in image classification tasks under
resource-constrained environments necessitates a balance between computational
efficiency and robust feature representation. Traditional attention mechanisms,
despite their strong feature modeling capability, often struggle with high
computational complexity and structural rigidity, limiting their applicability
in scenarios with limited computational resources (e.g., edge devices or
real-time systems). To address this, we propose the Multi-Agent Aggregation
Module (MAAM), a lightweight attention architecture integrated with the
MindSpore framework. MAAM employs three parallel agent branches with
independently parameterized operations to extract heterogeneous features,
adaptively fused via learnable scalar weights, and refined through a
convolutional compression layer. Leveraging MindSpore's dynamic computational
graph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10
dataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)
models, while improving training efficiency by 30%. Ablation studies confirm
the critical role of agent attention (accuracy drops to 32.0% if removed) and
compression modules (25.5% if omitted), validating their necessity for
maintaining discriminative feature learning. The framework's hardware
**acceleration** capabilities and minimal memory footprint further demonstrate its
practicality, offering a deployable solution for image classification in
resource-constrained scenarios without compromising accuracy.


## High-Throughput LLM inference on Heterogeneous Clusters

>Authors: Yi Xiong, Jinqi Huang, Wenjie Huang, Xuebing Yu, Entong Li, Zhixiong Ning, Jinhua Zhou, Li Zeng, Xin Chen

>2025-04-18

> http://arxiv.org/abs/2504.15303v1

Nowadays, many companies possess various types of AI accelerators, forming
heterogeneous clusters. Efficiently leveraging these clusters for
high-throughput large language model (LLM) inference services can significantly
reduce costs and expedite task processing. However, LLM inference on
heterogeneous clusters presents two main challenges. Firstly, different
deployment configurations can result in vastly different performance. The
number of possible configurations is large, and evaluating the effectiveness of
a specific setup is complex. Thus, finding an optimal configuration is not an
easy task. Secondly, LLM inference instances within a heterogeneous cluster
possess varying processing capacities, leading to different processing speeds
for handling inference requests. Evaluating these capacities and designing a
request scheduling algorithm that fully maximizes the potential of each
instance is challenging. In this paper, we propose a high-throughput inference
service system on heterogeneous clusters. First, the deployment configuration
is optimized by modeling the resource amount and expected throughput and using
the exhaustive search method. Second, a novel mechanism is proposed to schedule
requests among instances, which fully considers the different processing
capabilities of various instances. Extensive experiments show that the proposed
scheduler improves throughput by 122.5% and 33.6% on two heterogeneous
clusters, respectively.


## From Large to Super-Tiny End-to-End Optimization for Cost-Efficient LLMs

>Authors: Jiliang Ni, Jiachen Pu, Zhongyi Yang, Kun Zhou, Hui Wang, Xiaoliang Xiao, Dakui Wang, Xin Li, Jingfeng Luo, Conggang Hu

>2025-04-18

> http://arxiv.org/abs/2504.13471v2

In recent years, Large Language Models (LLMs) have significantly advanced
artificial intelligence by optimizing traditional Natural Language Processing
(NLP) pipelines, improving performance and generalization. This has spurred
their integration into various systems. Many NLP systems, including ours,
employ a "one-stage" pipeline directly incorporating LLMs. While effective,
this approach incurs substantial costs and latency due to the need for large
model parameters to achieve satisfactory outcomes. This paper introduces a
three-stage cost-efficient end-to-end LLM deployment pipeline-including
prototyping, knowledge transfer, and model compression-to tackle the
cost-performance dilemma in LLM-based frameworks. Our approach yields a super
tiny model optimized for cost and performance in online systems, simplifying
the system architecture. Initially, by transforming complex tasks into a
function call-based LLM-driven pipeline, an optimal performance prototype
system is constructed to produce high-quality data as a teacher model. The
second stage combines techniques like rejection fine-tuning, reinforcement
learning, and knowledge distillation to transfer knowledge to a smaller 0.5B
student model, delivering effective performance at minimal cost. The final
stage applies **quantization** and **pruning** to extremely compress models to 0.4B,
achieving ultra-low latency and cost. The framework's modular design and
cross-domain capabilities suggest potential applicability in other NLP areas.


## HMPEHeatMap Embedding for Efficient Transformer-Based Small Object Detection

>Authors: YangChen Zeng

>2025-04-18

> http://arxiv.org/abs/2504.13469v1

Current Transformer-based methods for small object detection continue
emerging, yet they have still exhibited significant shortcomings. This paper
introduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization
technique that enhances object detection performance by dynamically integrating
positional encoding with semantic detection information through heatmap-guided
adaptive learning.We also innovatively visualize the HMPE method, offering
clear visualization of embedded information for parameter fine-tuning.We then
create Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced
High-Quality Queries for Decoder (HIDQ) modules. These are designed for the
encoder and decoder, respectively, to generate high-quality queries and reduce
background noise queries.Using both heatmap embedding and Linear-Snake
Conv(LSConv) feature engineering, we enhance the embedding of massively diverse
small object categories and reduced the decoder multihead layers, thereby
accelerating both inference and training.In the generalization experiments, our
approach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU
VHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing
HMPE-enhanced embedding, we are able to reduce the number of decoder layers
from eight to a minimum of three, significantly decreasing both inference and
training costs.


## HPU High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing

>Authors: Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hosik Kim

>2025-04-18

> http://arxiv.org/abs/2504.16112v1

The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of **KV** caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.


## From Bayesian Asymptotics to General Large-Scale MIMO Capacity

>Authors: Sheng Yang, Richard Combes

>2025-04-17

> http://arxiv.org/abs/2504.13325v1

We present a unifying framework that bridges Bayesian asymptotics and
information theory to analyze the asymptotic Shannon capacity of general
large-scale MIMO channels including ones with non-linearities or imperfect
hardware. We derive both an analytic capacity formula and an asymptotically
optimal input distribution in the large-antenna regime, each of which depends
solely on the single-output channel's Fisher information through a term we call
the (tilted) Jeffreys' factor. We demonstrate how our method applies broadly to
scenarios with clipping, coarse **quantization** (including 1-bit ADCs), phase
noise, fading with imperfect CSI, and even optical Poisson channels. Our
asymptotic analysis motivates a practical approach to constellation design via
a compander-like transformation. Furthermore, we introduce a low-complexity
receiver structure that approximates the log-likelihood by quantizing the
channel outputs into finitely many bins, enabling near-capacity performance
with computational complexity independent of the output dimension. Numerical
results confirm that the proposed method unifies and simplifies many previously
intractable MIMO capacity problems and reveals how the Fisher information alone
governs the channel's asymptotic behavior.


## Let Me Grok for You Accelerating Grokking via Embedding Transfer from a Weaker Model

>Authors: Zhiwei Xu, Zhiyu Ni, Yixin Wang, Wei Hu

>2025-04-17

> http://arxiv.org/abs/2504.13292v1

''Grokking'' is a phenomenon where a neural network first memorizes training
data and generalizes poorly, but then suddenly transitions to near-perfect
generalization after prolonged training. While intriguing, this delayed
generalization phenomenon compromises predictability and efficiency. Ideally,
models should generalize directly without delay. To this end, this paper
proposes GrokTransfer, a simple and principled method for accelerating grokking
in training neural networks, based on the key observation that data embedding
plays a crucial role in determining whether generalization is delayed.
GrokTransfer first trains a smaller, weaker model to reach a nontrivial (but
far from optimal) test performance. Then, the learned input embedding from this
weaker model is extracted and used to initialize the embedding in the target,
stronger model. We rigorously prove that, on a synthetic XOR task where delayed
generalization always occurs in normal training, GrokTransfer enables the
target model to generalize directly without delay. Moreover, we demonstrate
that, across empirical studies of different tasks, GrokTransfer effectively
reshapes the training dynamics and eliminates delayed generalization, for both
fully-connected neural networks and Transformers.


## Hadamard product in deep learning Introduction, Advances and Challenges

>Authors: Grigorios G Chrysos, Yongtao Wu, Razvan Pascanu, Philip Torr, Volkan Cevher

>2025-04-17

> http://arxiv.org/abs/2504.13112v1

While convolution and self-attention mechanisms have dominated architectural
design in deep learning, this survey examines a fundamental yet understudied
primitive: the Hadamard product. Despite its widespread implementation across
various applications, the Hadamard product has not been systematically analyzed
as a core architectural primitive. We present the first comprehensive taxonomy
of its applications in deep learning, identifying four principal domains:
higher-order correlation, multimodal data fusion, dynamic representation
modulation, and efficient pairwise operations. The Hadamard product's ability
to model nonlinear interactions with linear computational complexity makes it
particularly valuable for resource-constrained deployments and edge computing
scenarios. We demonstrate its natural applicability in multimodal fusion tasks,
such as visual question answering, and its effectiveness in representation
masking for applications including image inpainting and **pruning**. This
systematic review not only consolidates existing knowledge about the Hadamard
product's role in deep learning architectures but also establishes a foundation
for future architectural innovations. Our analysis reveals the Hadamard product
as a versatile primitive that offers compelling trade-offs between
computational efficiency and representational power, positioning it as a
crucial component in the deep learning toolkit.


## Deep literature reviews an application of fine-tuned language models to migration research

>Authors: Stefano M. Iacus, Haodong Qi, Jiyoung Han

>2025-04-17

> http://arxiv.org/abs/2504.13685v1

This paper presents a hybrid framework for literature reviews that augments
traditional bibliometric methods with large language models (LLMs). By
fine-tuning open-source LLMs, our approach enables scalable extraction of
qualitative insights from large volumes of research content, enhancing both the
breadth and depth of knowledge synthesis. To improve annotation efficiency and
consistency, we introduce an error-focused validation process in which LLMs
generate initial labels and human reviewers correct misclassifications.
Applying this framework to over 20000 scientific articles about human
migration, we demonstrate that a domain-adapted LLM can serve as a "specialist"
model - capable of accurately selecting relevant studies, detecting emerging
trends, and identifying critical research gaps. Notably, the LLM-assisted
review reveals a growing scholarly interest in climate-induced migration.
However, existing literature disproportionately centers on a narrow set of
environmental hazards (e.g., floods, droughts, sea-level rise, and land
degradation), while overlooking others that more directly affect human health
and well-being, such as air and water pollution or infectious diseases. This
imbalance highlights the need for more comprehensive research that goes beyond
physical environmental changes to examine their ecological and societal
consequences, particularly in shaping migration as an adaptive response.
Overall, our proposed framework demonstrates the potential of fine-tuned LLMs
to conduct more efficient, consistent, and insightful literature reviews across
disciplines, ultimately accelerating knowledge synthesis and scientific
discovery.


## Taccel Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation

>Authors: Yuyang Li, Wenxin Du, Chang Yu, Puhao Li, Zihang Zhao, Tengyu Liu, Chenfanfu Jiang, Yixin Zhu, Siyuan Huang

>2025-04-17

> http://arxiv.org/abs/2504.12908v1

Tactile sensing is crucial for achieving human-level robotic capabilities in
manipulation tasks. VBTSs have emerged as a promising solution, offering high
spatial resolution and cost-effectiveness by sensing contact through
camera-captured deformation patterns of elastic gel pads. However, these
sensors' complex physical characteristics and visual signal processing
requirements present unique challenges for robotic applications. The lack of
efficient and accurate simulation tools for VBTS has significantly limited the
scale and scope of tactile robotics research. Here we present Taccel, a
high-performance simulation platform that integrates IPC and ABD to model
robots, tactile sensors, and objects with both accuracy and unprecedented
speed, achieving an 18-fold **acceleration** over real-time across thousands of
parallel environments. Unlike previous simulators that operate at sub-real-time
speeds with limited parallelization, Taccel provides precise physics simulation
and realistic tactile signals while supporting flexible robot-sensor
configurations through user-friendly APIs. Through extensive validation in
object recognition, robotic grasping, and articulated object manipulation, we
demonstrate precise simulation and successful sim-to-real transfer. These
capabilities position Taccel as a powerful tool for scaling up tactile robotics
research and development. By enabling large-scale simulation and
experimentation with tactile sensing, Taccel accelerates the development of
more capable robotic systems, potentially transforming how robots interact with
and understand their physical environment.


## Mirror, Mirror of the Flow How Does Regularization Shape Implicit Bias?

>Authors: Tom Jacobs, Chao Zhou, Rebekka Burkholz

>2025-04-17

> http://arxiv.org/abs/2504.12883v1

Implicit bias plays an important role in explaining how overparameterized
models generalize well. Explicit regularization like weight decay is often
employed in addition to prevent overfitting. While both concepts have been
studied separately, in practice, they often act in tandem. Understanding their
interplay is key to controlling the shape and strength of implicit bias, as it
can be modified by explicit regularization. To this end, we incorporate
explicit regularization into the mirror flow framework and analyze its lasting
effects on the geometry of the training dynamics, covering three distinct
effects: positional bias, type of bias, and range shrinking. Our analytical
approach encompasses a broad class of problems, including **sparse** coding, matrix
sensing, single-layer attention, and LoRA, for which we demonstrate the utility
of our insights. To exploit the lasting effect of regularization and highlight
the potential benefit of dynamic weight decay schedules, we propose to switch
off weight decay during training, which can improve generalization, as we
demonstrate in experiments.


## Tailoring Electromagnetic Fields in RF Cavities

>Authors: Laurence Wroe, Walter Wuensch, Robert Apsimon

>2025-04-17

> http://arxiv.org/abs/2504.12780v1

Recent work introduced a systematic method for designing so-called
azimuthally modulated RF cavities that support transverse magnetic modes
composed of user-desired multipoles, enabling precision control of the
magnitude and orientation of multipolar components in RF cavity design. This
paper extends this method to practical implementation by deriving the
multipolar expansion of the longitudinal electric field in such RF cavities
with beam pipes, as well as the momentum change of ultra-relativistic particles
traversing these modes. The derived equations explicitly show the radial
variation of the change in longitudinal and transverse momentum follows a
polynomial rather than Bessel-function relationship. The expression for the
longitudinal electric field is then compared to a field map obtained from the
3D electromagnetic simulation of an azimuthally modulated cavity designed to
support a mode composed of monopole, dipole, and quadrupole components. Beam
dynamics studies are presented to assess the derived expressions for the change
in momentum, including the effects of relaxing the ultra-relativistic
assumption. Finally, two example applications are presented: the first
demonstrates the removal of unwanted transverse multipoles to create a
multipole-free accelerating structure with a single-port coupler, whereas the
second illustrates the synthesis of desired multipoles to create an RF cavity
that transforms the transverse distribution of a beam from Gaussian to uniform.


## Chinese-Vicuna A Chinese Instruction-following Llama-based Model

>Authors: Chenghao Fan, Zhenyi Lu, Jie Tian

>2025-04-17

> http://arxiv.org/abs/2504.12737v1

Chinese-Vicuna is an open-source, resource-efficient language model designed
to bridge the gap in Chinese instruction-following capabilities by fine-tuning
Meta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting
low-resource environments, it enables cost-effective deployment on consumer
GPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation
in fields like healthcare and law. By integrating hybrid datasets (BELLE and
Guanaco) and 4-bit **quantization** (QLoRA), the model achieves competitive
performance in tasks such as translation, code generation, and domain-specific
Q\&A. The project provides a comprehensive toolkit for model conversion, CPU
inference, and multi-turn dialogue interfaces, emphasizing accessibility for
researchers and developers. Evaluations indicate competitive performance across
medical tasks, multi-turn dialogue coherence, and real-time legal updates.
Chinese-Vicuna's modular design, open-source ecosystem, and community-driven
enhancements position it as a versatile foundation for Chinese LLM
applications.


## TUMLS Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology

>Authors: Walid Rehamnia, Alexandra Getmanskaya, Evgeniy Vasilyev, Vadim Turlapov

>2025-04-17

> http://arxiv.org/abs/2504.12718v1

Digital pathology, augmented by artificial intelligence (AI), holds
significant promise for improving the workflow of pathologists. However,
challenges such as the labor-intensive annotation of whole slide images (WSIs),
high computational demands, and trust concerns arising from the absence of
uncertainty estimation in predictions hinder the practical application of
current AI methodologies in histopathology. To address these issues, we present
a novel trustful fully unsupervised multi-level segmentation methodology
(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to
identify the different tissue types within low-resolution training data. It
selects representative patches from each identified group based on an
uncertainty measure and then does unsupervised nuclei segmentation in their
respective higher-resolution space without using any ML algorithms. Crucially,
this solution integrates seamlessly into clinicians workflows, transforming the
examination of a whole WSI into a review of concise, interpretable cross-level
insights. This integration significantly enhances and accelerates the workflow
while ensuring transparency. We evaluated our approach using the UPENN-GBM
dataset, where the AE achieved a mean squared error (MSE) of 0.0016.
Additionally, nucleus segmentation is assessed on the MoNuSeg dataset,
outperforming all unsupervised approaches with an F1 score of 77.46% and a
Jaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in
advancing the field of digital pathology.


## Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection

>Authors: Long Zeng, Jianxiang Yu, Jiapeng Zhu, Qingsong Zhong, Xiang Li

>2025-04-17

> http://arxiv.org/abs/2504.12715v1

Graph self-supervised learning has gained significant attention recently.
However, many existing approaches heavily depend on perturbations, and
inappropriate perturbations may corrupt the graph's inherent information. The
Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder
extensively used in fields such as computer vision; however, its application to
graph data remains underexplored. In this paper, we provide an empirical
analysis of vector **quantization** in the context of graph autoencoders,
demonstrating its significant enhancement of the model's capacity to capture
graph topology. Furthermore, we identify two key challenges associated with
vector **quantization** when applying in graph data: codebook underutilization and
codebook space **sparsity**. For the first challenge, we propose an annealing-based
encoding strategy that promotes broad code utilization in the early stages of
training, gradually shifting focus toward the most effective codes as training
progresses. For the second challenge, we introduce a hierarchical two-layer
codebook that captures relationships between embeddings through clustering. The
second layer codebook links similar codes, encouraging the model to learn
closer embeddings for nodes with similar features and structural topology in
the graph. Our proposed model outperforms 16 representative baseline methods in
self-supervised link prediction and node classification tasks across multiple
datasets.


## D$^{2}$MoE Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving

>Authors: Haodong Wang, Qihua Zhou, Zicong Hong, Song Guo

>2025-04-17

> http://arxiv.org/abs/2504.15299v1

The mixture of experts (MoE) model is a **sparse** variant of large language
models (LLMs), designed to hold a better balance between intelligent capability
and computational overhead. Despite its benefits, MoE is still too expensive to
deploy on resource-constrained edge devices, especially with the demands of
on-device inference services. Recent research efforts often apply model
compression techniques, such as **quantization**, **pruning** and merging, to restrict
MoE complexity. Unfortunately, due to their predefined static model
optimization strategies, they cannot always achieve the desired
quality-overhead trade-off when handling multiple requests, finally degrading
the on-device quality of service. These limitations motivate us to propose the
D$^2$MoE, an algorithm-system co-design framework that matches diverse task
requirements by dynamically allocating the most proper bit-width to each
expert. Specifically, inspired by the nested structure of matryoshka dolls, we
propose the matryoshka weight **quantization** (MWQ) to progressively compress
expert weights in a bit-nested manner and reduce the required runtime memory.
On top of it, we further optimize the I/O-computation pipeline and design a
heuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)
principle, which maximizes the expert parallelism between I/O and computation
queue under constrained memory budgets, thus significantly reducing the idle
temporal bubbles waiting for the experts to load. Evaluations on real edge
devices show that D$^2$MoE improves the overall inference throughput by up to
1.39$\times$ and reduces the peak memory footprint by up to 53% over the latest
on-device inference frameworks, while still preserving comparable serving
accuracy as its INT8 counterparts.


## The Paradox of Professional Input How Expert Collaboration with AI Systems Shapes Their Future Value

>Authors: Venkat Ram Reddy Ganuthula, Krishna Kumar Balaraman

>2025-04-17

> http://arxiv.org/abs/2504.12654v1

This perspective paper examines a fundamental paradox in the relationship
between professional expertise and artificial intelligence: as domain experts
increasingly collaborate with AI systems by externalizing their implicit
knowledge, they potentially accelerate the automation of their own expertise.
Through analysis of multiple professional contexts, we identify emerging
patterns in human-AI collaboration and propose frameworks for professionals to
navigate this evolving landscape. Drawing on research in knowledge management,
expertise studies, human-computer interaction, and labor economics, we develop
a nuanced understanding of how professional value may be preserved and
transformed in an era of increasingly capable AI systems. Our analysis suggests
that while the externalization of tacit knowledge presents certain risks to
traditional professional roles, it also creates opportunities for the evolution
of expertise and the emergence of new forms of professional value. We conclude
with implications for professional education, organizational design, and policy
development that can help ensure the codification of expert knowledge enhances
rather than diminishes the value of human expertise.


## Fast Computation of the Discrete Fourier Transform Rectangular Index Coefficients

>Authors: Saulo Queiroz, João P. Vilela, Benjamin Koon Kei Ng, Chan-Tong Lam, Edmundo Monteiro

>2025-04-17

> http://arxiv.org/abs/2504.12551v1

In~\cite{sic-magazine-2025}, the authors show that the square index
coefficients (SICs) of the \(N\)-point discrete Fourier transform (DFT) -- that
is, the coefficients \(X_{k\sqrt{N}}\) for \(k = 0, 1, \ldots, \sqrt{N} - 1\)
-- can be losslessly compressed from \(N\) to \(\sqrt{N}\) points, thereby
accelerating the computation of these specific DFT coefficients accordingly.
Following up on that, in this article we generalize SICs into what we refer to
as rectangular index coefficients (RICs) of the DFT, formalized as $X_{kL},
k=0,1,\cdots,C-1$, in which the integers $C$ and $L$ are generic roots of $N$
such that $N=LC$. We present an algorithm to compress the $N$-point input
signal $\mathbf{x}$ into a $C$-point signal $\mathbf{\hat{x}}$ at the expense
of $\mathcal{O}(N)$ complex sums and no complex multiplication. We show that a
DFT on $\mathbf{\hat{x}}$ is equivalent to a DFT on the RICs of $\mathbf{x}$.
In cases where specific frequencies of \(\mathbf{x}\) are of interest -- as in
harmonic analysis -- one can conveniently adjust the signal parameters (e.g.,
frequency resolution) to align the RICs with those frequencies, and use the
proposed algorithm to compute them significantly faster. If $N$ is a power of
two -- as required by the fast Fourier transform (FFT) algorithm -- then $C$
can be any power of two in the range $[2, N/2]$ and one can use our algorithm
along with FFT to compute all RICs in $\mathcal{O}(C\log C)$ time complexity.


## Dense Backpropagation Improves Training for Sparse Mixture-of-Experts

>Authors: Ashwinee Panda, Vatsal Baherwani, Zain Sarwar, Benjamin Therien, Supriyo Chakraborty, Tom Goldstein

>2025-04-16

> http://arxiv.org/abs/2504.12463v2

Mixture of Experts (MoE) pretraining is more scalable than dense Transformer
pretraining, because MoEs learn to route inputs to a **sparse** set of their
feedforward parameters. However, this means that MoEs only receive a **sparse**
backward update, leading to training instability and suboptimal performance. We
present a lightweight approximation method that gives the MoE router a dense
gradient update while continuing to **sparse**ly activate its parameters. Our
method, which we refer to as Default MoE, substitutes missing expert
activations with default outputs consisting of an exponential moving average of
expert outputs previously seen over the course of training. This allows the
router to receive signals from every expert for each token, leading to
significant improvements in training performance. Our Default MoE outperforms
standard TopK routing in a variety of settings without requiring significant
computational overhead. Code: https://github.com/vatsal0/default-moe.


## Spontaneous symmetry breaking in the Heisenberg antiferromagnet on a triangular lattice

>Authors: Bastián Pradenas, Grigor Adamyan, Oleg Tchernyshyov

>2025-04-16

> http://arxiv.org/abs/2504.12411v1

We present a detailed investigation of an overlooked symmetry structure in
non-collinear antiferromagnets that gives rise to an emergent quantum number
for magnons. Focusing on the triangular-lattice Heisenberg antiferromagnet, we
show that its spin order parameter transforms under an enlarged symmetry group,
$\mathrm{SO(3)_L \times SO(3)_R}$, rather than the conventional spin-rotation
group $\mathrm{SO(3)}$. Although this larger symmetry is spontaneously broken
by the ground state, a residual subgroup survives, leading to conserved Noether
charges that, upon **quantization**, endow magnons with an additional quantum
number -- \emph{isospin} -- beyond their energy and momentum. Our results
provide a comprehensive framework for understanding symmetry, degeneracy, and
quantum numbers in non-collinear magnetic systems, and bridge an unexpected
connection between the paradigms of symmetry breaking in non-collinear
antiferromagnets and chiral symmetry breaking in particle physics.


## Activated LoRA Fine-tuned LLMs for Intrinsics

>Authors: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

>2025-04-16

> http://arxiv.org/abs/2504.12397v1

Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is highly inefficient, as the key-value (**KV**) cache of the
entire turn history must be recomputed with the LoRA weights before generation
can begin. To address this problem, we propose Activated LoRA (aLoRA), which
modifies the LoRA framework to only adapt weights for the tokens in the
sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA
to accept the base model's **KV** cache of the input string, meaning that aLoRA can
be instantly activated whenever needed in a chain without recomputing the
cache. This enables building what we call \emph{intrinsics}, i.e. highly
specialized models invoked to perform well-defined operations on portions of an
input chain or conversation that otherwise uses the base model by default. We
use aLoRA to train a set of intrinsics models, demonstrating competitive
accuracy with standard LoRA while achieving significant inference benefits.


## HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks

>Authors: Stefan Abi-Karam, Cong Hao

>2025-04-16

> http://arxiv.org/abs/2504.12268v1

The rapid scaling of large language model (LLM) training and inference has
driven their adoption in semiconductor design across academia and industry.
While most prior work evaluates LLMs on hardware description language (HDL)
tasks, particularly Verilog, designers are increasingly using high-level
synthesis (HLS) to build domain-specific accelerators and complex hardware
systems. However, benchmarks and tooling to comprehensively evaluate LLMs for
HLS design tasks remain scarce.
  To address this, we introduce HLS-Eval, the first complete benchmark and
evaluation framework for LLM-driven HLS design. HLS-Eval targets two core
tasks: (1) generating HLS code from natural language descriptions, and (2)
performing HLS-specific code edits to optimize performance and hardware
efficiency. The benchmark includes 94 unique designs drawn from standard HLS
benchmarks and novel sources. Each case is prepared via a semi-automated flow
that produces a natural language description and a paired testbench for
C-simulation and synthesis validation, ensuring each task is "LLM-ready."
  Beyond the benchmark, HLS-Eval offers a modular Python framework for
automated, parallel evaluation of both local and hosted LLMs. It includes a
parallel evaluation engine, direct HLS tool integration, and abstractions for
to support different LLM interaction paradigms, enabling rapid prototyping of
new benchmarks, tasks, and LLM methods.
  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on
Vitis HLS, measuring outputs across four key metrics - parseability,
compilability, runnability, and synthesizability - reflecting the iterative HLS
design cycle. We also report pass@k metrics, establishing clear baselines and
reusable infrastructure for the broader LLM-for-hardware community.
  All benchmarks, framework code, and results are open-sourced at
https://github.com/stefanpie/hls-eval.


## SCENT Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields

>Authors: David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, Shinjae Yoo

>2025-04-16

> http://arxiv.org/abs/2504.12262v1

Spatiotemporal learning is challenging due to the intricate interplay between
spatial and temporal dependencies, the high dimensionality of the data, and
scalability constraints. These challenges are further amplified in scientific
domains, where data is often irregularly distributed (e.g., missing values from
sensor failures) and high-volume (e.g., high-fidelity simulations), posing
additional computational and modeling difficulties. In this paper, we present
SCENT, a novel framework for scalable and continuity-informed spatiotemporal
representation learning. SCENT unifies interpolation, reconstruction, and
forecasting within a single architecture. Built on a transformer-based
encoder-processor-decoder backbone, SCENT introduces learnable queries to
enhance generalization and a query-wise cross-attention mechanism to
effectively capture multi-scale dependencies. To ensure scalability in both
data size and model complexity, we incorporate a **sparse** attention mechanism,
enabling flexible output representations and efficient evaluation at arbitrary
resolutions. We validate SCENT through extensive simulations and real-world
experiments, demonstrating state-of-the-art performance across multiple
challenging tasks while achieving superior scalability.


## Cobra Efficient Line Art COlorization with BRoAder References

>Authors: Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan

>2025-04-16

> http://arxiv.org/abs/2504.12240v1

The comic production industry requires reference-based line art colorization
with high accuracy, efficiency, contextual consistency, and flexible control. A
comic page often involves diverse characters, objects, and backgrounds, which
complicates the coloring process. Despite advancements in diffusion models for
image generation, their application in line art colorization remains limited,
facing challenges related to handling extensive reference images,
time-consuming inference, and flexible control. We investigate the necessity of
extensive contextual image guidance on the quality of line art colorization. To
address these challenges, we introduce Cobra, an efficient and versatile method
that supports color hints and utilizes over 200 reference images while
maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,
which leverages specially designed positional encodings, causal **sparse**
attention, and Key-Value Cache to effectively manage long-context references
and ensure color identity consistency. Results demonstrate that Cobra achieves
accurate line art colorization through extensive contextual reference,
significantly enhancing inference speed and interactivity, thereby meeting
critical industrial demands. We release our codes and models on our project
page: https://zhuang2002.github.io/Cobra/.


## Cost-Efficient LLM Serving in the Cloud VM Selection with KV Cache Offloading

>Authors: Kihyun Kim, Jinwoo Kim, Hyunsun Chung, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim

>2025-04-16

> http://arxiv.org/abs/2504.11816v1

LLM inference is essential for applications like text summarization,
translation, and data analysis, but the high cost of GPU instances from Cloud
Service Providers (CSPs) like AWS is a major burden. This paper proposes
InferSave, a cost-efficient VM selection framework for cloud based LLM
inference. InferSave optimizes **KV** cache offloading based on Service Level
Objectives (SLOs) and workload charac teristics, estimating GPU memory needs,
and recommending cost-effective VM instances. Additionally, the Compute Time
Calibration Function (CTCF) improves instance selection accuracy by adjusting
for discrepancies between theoretical and actual GPU performance. Experiments
on AWS GPU instances show that selecting lower-cost instances without **KV** cache
offloading improves cost efficiency by up to 73.7% for online workloads, while
**KV** cache offloading saves up to 20.19% for offline workloads.


## Light WIMPs and MeV Gamma-ray Detection with COSI

>Authors: Yu Watanabe, Shigeki Matsumoto, Christopher M. Karwin, Tom Melia, Michela Negro, Thomas Siegert, Yuki Watanabe, Hiroki Yoneda, Tadayuki Takahashi

>2025-04-16

> http://arxiv.org/abs/2504.11810v1

Light weakly interacting massive particles (WIMPs), whose masses are in the
sub-GeV scale, have been attracting more attention due to the negative results
searching for traditional WIMPs. The light WIMPs are expected to produce gamma
rays from annihilation in the MeV energy region. Advancements in technology
have opened up possibilities to precisely detect MeV gamma rays, leading to the
upcoming space-based mission of the Compton Spectrometer and Imager (COSI). We
comprehensively and quantitatively study the phenomenology of light WIMPs to
determine if the COSI observations will probe their viable model parameter
regions. We first construct models to describe light WIMPs based on the
minimality and renormalizability of quantum field theory. Next, we impose
various constraints on the models obtained from cosmological observations (CMB,
BBN) and dark matter searches (accelerator, underground, astrophysical
experiments, etc.). Finally, we identify viable parameter regions in each model
and discuss whether or not COSI will be sensitive to the parameter regions. We
find that a velocity-dependent annihilation cross-section is predicted in some
regions, enabling COSI to detect the dark matter signal while avoiding severe
constraints from cosmological observations.


## Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs

>Authors: Hyungwoo Lee, Kihyun Kim, Jinwoo Kim, Jungmin So, Myung-Hoon Cha, Hong-Yeon Kim, James J. Kim, Youngjae Kim

>2025-04-16

> http://arxiv.org/abs/2504.11765v1

Recent large language models (LLMs) face increasing inference latency as
input context length and model size continue to grow. In particular, the
retrieval-augmented generation (RAG) technique, which enhances LLM responses by
incorporating external knowledge, exacerbates this issue by significantly
increasing the number of input tokens. This expansion in token length leads to
a substantial rise in computational overhead, particularly during the prefill
stage, resulting in prolonged time-to-first-token (TTFT). To address this
issue, this paper proposes a method to reduce TTFT by leveraging a disk-based
key-value (**KV**) cache to lessen the computational burden during the prefill
stage. We also introduce a disk-based shared **KV** cache management system, called
Shared RAG-DCache, for multi-instance LLM RAG service environments. This
system, together with an optimal system configuration, improves both throughput
and latency under given resource constraints. Shared RAG-DCache exploits the
locality of documents related to user queries in RAG, as well as the queueing
delay in LLM inference services. It proactively generates and stores disk **KV**
caches for query-related documents and shares them across multiple LLM
instances to enhance inference performance. In experiments on a single host
equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in
throughput and up to a 12~65% reduction in latency, depending on the resource
configuration.


## Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models

>Authors: Yuanbo Tang, Yan Tang, Naifan Zhang, Meixuan Chen, Yang Li

>2025-04-16

> http://arxiv.org/abs/2504.12359v1

Mixture-of-Experts based large language models (MoE LLMs) have shown
significant promise in multitask adaptability by dynamically routing inputs to
specialized experts. Despite their success, the collaborative mechanisms among
experts are still not well understood, limiting both the interpretability and
optimization of these models. In this paper, we focus on two critical issues:
(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs
through expert **pruning**. To address the first issue, we propose a hierarchical
**sparse** dictionary learning (HSDL) method that uncovers the collaboration
patterns among experts. For the second issue, we introduce the
Contribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes
low-contribution experts. Our extensive experiments demonstrate that expert
collaboration patterns are closely linked to specific input types and exhibit
semantic significance across various tasks. Moreover, **pruning** experiments show
that our approach improves overall performance by 2.5\% on average,
outperforming existing methods. These findings offer valuable insights into
enhancing the efficiency and interpretability of MoE LLMs, offering a clearer
understanding of expert interactions and improving model optimization.


## EdgePrompt A Distributed Key-Value Inference Framework for LLMs in 6G Networks

>Authors: Jiahong Ning, Pengyan Zhu, Ce Zheng, Gary Lee, Sumei Sun, Tingting Yang

>2025-04-16

> http://arxiv.org/abs/2504.11729v1

As sixth-generation (6G) networks advance, large language models (LLMs) are
increasingly integrated into 6G infrastructure to enhance network management
and intelligence. However, traditional LLMs architecture struggle to meet the
stringent latency and security requirements of 6G, especially as the increasing
in sequence length leads to greater task complexity. This paper proposes
Edge-Prompt, a cloud-edge collaborative framework based on a hierarchical
attention splicing mechanism. EdgePrompt employs distributed key-value (**KV**)
pair optimization techniques to accelerate inference and adapt to network
conditions. Additionally, to reduce the risk of data leakage, EdgePrompt
incorporates a privacy preserving strategy by isolating sensitive information
during processing. Experiments on public dataset show that EdgePrompt
effectively improves the inference throughput and reduces the latency, which
provides a reliable solution for LLMs deployment in 6G environments.


## An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World

>Authors: Xingwu Ji, Haochen Niu, Dexin Duan, Rendong Ying, Fei Wen, Peilin Liu

>2025-04-16

> http://arxiv.org/abs/2504.11698v1

Recently, learning-based robotic navigation systems have gained extensive
research attention and made significant progress. However, the diversity of
open-world scenarios poses a major challenge for the generalization of such
systems to practical scenarios. Specifically, learned systems for scene
measurement and state estimation tend to degrade when the application scenarios
deviate from the training data, resulting to unreliable depth and pose
estimation. Toward addressing this problem, this work aims to develop a visual
odometry system that can fast adapt to diverse novel environments in an online
manner. To this end, we construct a self-supervised online adaptation framework
for monocular visual odometry aided by an online-updated depth estimation
module. Firstly, we design a monocular depth estimation network with
lightweight refiner modules, which enables efficient online adaptation. Then,
we construct an objective for self-supervised learning of the depth estimation
module based on the output of the visual odometry system and the contextual
semantic information of the scene. Specifically, a **sparse** depth densification
module and a dynamic consistency enhancement module are proposed to leverage
camera poses and contextual semantics to generate pseudo-depths and valid masks
for the online adaptation. Finally, we demonstrate the robustness and
generalization capability of the proposed method in comparison with
state-of-the-art learning-based approaches on urban, in-house datasets and a
robot platform. Code is publicly available at:
https://github.com/jixingwu/SOL-SLAM.


## In Pursuit of Total Reproducibility

>Authors: Moritz E. Beber

>2025-04-15

> http://arxiv.org/abs/2504.11635v1

The vast majority of scientific contributions in the field of computational
systems biology are based on mathematical models. These models can be broadly
classified as either dynamic (kinetic) models or steady-state
(constraint-based) models. They are often described in specific markup
languages whose purpose is to aid in the distribution and standardization of
models. Despite numerous established standards in the field, reproducibility
remains problematic due to the substantial effort required for compliance,
diversity of implementations, and the lack of proportionate rewards for
researchers. This article explores the application of event sourcing - a
software engineering technique where system state is derived from sequential
recorded events - to address reproducibility challenges in computational
systems biology. Event sourcing, exemplified by systems like git, offers a
promising solution by maintaining complete, immutable records of all changes to
a model. Through examples including leader and follower applications, local and
remote computation, and contribution tracking, this work demonstrates how
event-sourced systems can automate standards compliance, provide comprehensive
audit trails, enable perfect replication of processes, facilitate
collaboration, and generate multiple specialized read models from a single
event log. An implementation of the outlined principles has the potential to
transform computational systems biology by providing unprecedented
transparency, reproducibility, and collaborative capabilities, ultimately
accelerating research through more effective model reuse and integration. An
event-sourced approach to modeling in computational systems biology may act as
an example to related disciplines and contribute to ending the reproducibility
crisis plaguing multiple major fields of science.


## Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning

>Authors: Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi Karnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov

>2025-04-15

> http://arxiv.org/abs/2504.11409v1

Hybrid LLM architectures that combine Attention and State Space Models (SSMs)
achieve state-of-the-art accuracy and runtime performance. Recent work has
demonstrated that applying compression and distillation to Attention-only
models yields smaller, more accurate models at a fraction of the training cost.
In this work, we explore the effectiveness of compressing Hybrid architectures.
We introduce a novel group-aware **pruning** strategy that preserves the structural
integrity of SSM blocks and their sequence modeling capabilities. Furthermore,
we demonstrate the necessity of such SSM **pruning** to achieve improved accuracy
and inference speed compared to traditional approaches. Our compression recipe
combines SSM, FFN, embedding dimension, and layer **pruning**, followed by
knowledge distillation-based retraining, similar to the MINITRON technique.
Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B
parameters with up to 40x fewer training tokens. The resulting model surpasses
the accuracy of similarly-sized models while achieving 2x faster inference,
significantly advancing the Pareto frontier.


## From Gaze to Insight Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation

>Authors: Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Tao Tan, Vicente Grau, Jungong Han

>2025-04-15

> http://arxiv.org/abs/2504.11368v1

Medical image segmentation remains challenging due to the high cost of
pixel-level annotations for training. In the context of weak supervision,
clinician gaze data captures regions of diagnostic interest; however, its
**sparsity** limits its use for segmentation. In contrast, vision-language models
(VLMs) provide semantic context through textual descriptions but lack the
explanation precision required. Recognizing that neither source alone suffices,
we propose a teacher-student framework that integrates both gaze and language
supervision, leveraging their complementary strengths. Our key insight is that
gaze data indicates where clinicians focus during diagnosis, while VLMs explain
why those regions are significant. To implement this, the teacher model first
learns from gaze points enhanced by VLM-generated descriptions of lesion
morphology, establishing a foundation for guiding the student model. The
teacher then directs the student through three strategies: (1) Multi-scale
feature alignment to fuse visual cues with textual semantics; (2)
Confidence-weighted consistency constraints to focus on reliable predictions;
(3) Adaptive masking to limit error propagation in uncertain areas. Experiments
on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves
Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over
gaze baselines without increasing the annotation burden. By preserving
correlations among predictions, gaze data, and lesion descriptions, our
framework also maintains clinical interpretability. This work illustrates how
integrating human visual attention with AI-generated semantic context can
effectively overcome the limitations of individual weak supervision signals,
thereby advancing the development of deployable, annotation-efficient medical
AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.


## Optimizing LLM Inference Fluid-Guided Online Scheduling with Memory Constraints

>Authors: Ruicheng Ao, Gan Luo, David Simchi-Levi, Xinshang Wang

>2025-04-15

> http://arxiv.org/abs/2504.11320v1

Large Language Models (LLMs) are indispensable in today's applications, but
their inference procedure -- generating responses by processing text in
segments and using a memory-heavy Key-Value (**KV**) cache -- demands significant
computational resources, particularly under memory constraints. This paper
formulates LLM inference optimization as a multi-stage online scheduling
problem where sequential prompt arrivals and **KV** cache growth render
conventional scheduling ineffective. We develop a fluid dynamics approximation
to provide a tractable benchmark that guides algorithm design. Building on
this, we propose the Waiting for Accumulated Inference Threshold (WAIT)
algorithm, which uses multiple thresholds to schedule incoming prompts
optimally when output lengths are known, and extend it to Nested WAIT for cases
with unknown output lengths. Theoretical analysis shows that both algorithms
achieve near-optimal performance against the fluid benchmark in heavy traffic
conditions, balancing throughput, latency, and Time to First Token (TTFT).
Experiments with the Llama-7B model on an A100 GPU using both synthetic and
real-world datasets demonstrate improved throughput and reduced latency
relative to established baselines like vLLM and Sarathi. This work bridges
operations research and machine learning, offering a rigorous framework for the
efficient deployment of LLMs under memory constraints.


## ConvShareViT Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators

>Authors: Riad Ibadulla, Thomas M. Chen, Constantino Carlos Reyes-Aldasoro

>2025-04-15

> http://arxiv.org/abs/2504.11517v1

This paper introduces ConvShareViT, a novel deep learning architecture that
adapts Vision Transformers (ViTs) to the 4f free-space optical system.
ConvShareViT replaces linear layers in multi-head self-attention (MHSA) and
Multilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared
weights across input channels. Through the development of ConvShareViT, the
behaviour of convolutions within MHSA and their effectiveness in learning the
attention mechanism were analysed systematically. Experimental results
demonstrate that certain configurations, particularly those using valid-padded
shared convolutions, can successfully learn attention, achieving comparable
attention scores to those obtained with standard ViTs. However, other
configurations, such as those using same-padded convolutions, show limitations
in attention learning and operate like regular CNNs rather than transformer
models. ConvShareViT architectures are specifically optimised for the 4f
optical system, which takes advantage of the parallelism and high-resolution
capabilities of optical systems. Results demonstrate that ConvShareViT can
theoretically achieve up to 3.04 times faster inference than GPU-based systems.
This potential **acceleration** makes ConvShareViT an attractive candidate for
future optical deep learning applications and proves that our ViT
(ConvShareViT) can be employed using only the convolution operation, via the
necessary optimisation of the ViT to balance performance and complexity.


## AutoRAN Automated and Zero-Touch Open RAN Systems

>Authors: Stefano Maxenti, Ravis Shirkhani, Maxime Elkael, Leonardo Bonati, Salvatore D'Oro, Tommaso Melodia, Michele Polese

>2025-04-15

> http://arxiv.org/abs/2504.11233v1

[...] This paper presents AutoRAN, an automated, intent-driven framework for
zero-touch provisioning of open, programmable cellular networks. Leveraging
cloud-native principles, AutoRAN employs virtualization, declarative
infrastructure-as-code templates, and disaggregated micro-services to abstract
physical resources and protocol stacks. Its orchestration engine integrates
Language Models (LLMs) to translate high-level intents into machine-readable
configurations, enabling closed-loop control via telemetry-driven
observability. Implemented on a multi-architecture OpenShift cluster with
heterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access
Network (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of
O-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS
core, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines.
Experimental results demonstrate that AutoRAN is capable of deploying an
end-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput,
validating its ability to streamline configuration, accelerate testing, and
reduce manual intervention with similar performance than non cloud-based
implementations. With its novel LLM-assisted intent translation mechanism, and
performance-optimized automation workflow for multi-vendor environments,
AutoRAN has the potential of advancing the robustness of next-generation
cellular supply chains through reproducible, intent-based provisioning across
public and private deployments.


## VEXP A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers

>Authors: Run Wang, Gamze Islamoglu, Andrea Belano, Viviane Potocnik, Francesco Conti, Angelo Garofalo, Luca Benini

>2025-04-15

> http://arxiv.org/abs/2504.11227v1

While Transformers are dominated by Floating-Point (FP)
Matrix-Multiplications, their aggressive **acceleration** through dedicated
hardware or many-core programmable systems has shifted the performance
bottleneck to non-linear functions like Softmax. Accelerating Softmax is
challenging due to its non-pointwise, non-linear nature, with exponentiation as
the most demanding step. To address this, we design a custom arithmetic block
for Bfloat16 exponentiation leveraging a novel approximation algorithm based on
Schraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of
the RISC-V cores of a compute cluster, through custom Instruction Set
Architecture (ISA) extensions, with a negligible area overhead of 1\%. By
optimizing the software kernels to leverage the extension, we execute Softmax
with 162.7$\times$ less latency and 74.3$\times$ less energy compared to the
baseline cluster, achieving an 8.2$\times$ performance improvement and
4.1$\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2
configuration. Moreover, the proposed approach enables a multi-cluster system
to efficiently execute end-to-end inference of pre-trained Transformer models,
such as GPT-2, GPT-3 and ViT, achieving up to 5.8$\times$ and 3.6$\times$
reduction in latency and energy consumption, respectively, without requiring
re-training and with negligible accuracy loss.


## Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models

>Authors: Nicolas Baumann, Cheng Hu, Paviththiren Sivasothilingam, Haotong Qin, Lei Xie, Michele Magno, Luca Benini

>2025-04-15

> http://arxiv.org/abs/2504.11514v1

Neural Networks (NNs) trained through supervised learning struggle with
managing edge-case scenarios common in real-world driving due to the
intractability of exhaustive datasets covering all edge-cases, making
knowledge-driven approaches, akin to how humans intuitively detect unexpected
driving behavior, a suitable complement to data-driven methods. This work
proposes a hybrid architecture combining low-level Model Predictive Controller
(MPC) with locally deployed Large Language Models (LLMs) to enhance
decision-making and Human Machine Interaction (HMI). The DecisionxLLM module
evaluates robotic state information against natural language instructions to
ensure adherence to desired driving behavior. The MPCxLLM module then adjusts
MPC parameters based on LLM-generated insights, achieving control adaptability
while preserving the safety and constraint guarantees of traditional MPC
systems. Further, to enable efficient on-board deployment and to eliminate
dependency on cloud connectivity, we shift processing to the on-board computing
platform: We propose an approach that exploits Retrieval Augmented Generation
(RAG), Low Rank Adaptation (LoRA) fine-tuning, and **quantization**. Experimental
results demonstrate that these enhancements yield significant improvements in
reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%,
and up to 10.5x increase in computational efficiency (tokens/s), validating the
proposed framework's practicality for real-time deployment even on down-scaled
robotic platforms. This work bridges high-level decision-making with low-level
control adaptability, offering a synergistic framework for knowledge-driven and
adaptive Autonomous Driving Systems (ADS).


## Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning

>Authors: Lin Zhu, Weifeng Zhu, Shuowen Zhang, Shuguang Cui, Liang Liu

>2025-04-15

> http://arxiv.org/abs/2504.11162v1

This paper addresses the joint transceiver design, including pilot
transmission, channel feature extraction and feedback, as well as precoding,
for low-overhead downlink massive multiple-input multiple-output (MIMO)
communication in frequency-division duplex (FDD) systems. Although deep
learning (DL) has shown great potential in tackling this problem, existing
methods often suffer from poor scalability in practical systems, as the
solution obtained in the training phase merely works for a fixed feedback
capacity and a fixed number of users in the deployment phase. To address this
limitation, we propose a novel DL-based framework comprised of choreographed
neural networks, which can utilize one training phase to generate all the
transceiver solutions used in the deployment phase with varying sizes of
feedback codebooks and numbers of users. The proposed framework includes a
residual vector-**quantize**d variational autoencoder (RVQ-VAE) for efficient
channel feedback and an edge graph attention network (EGAT) for robust
multiuser precoding. It can adapt to different feedback capacities by flexibly
adjusting the RVQ codebook sizes using the hierarchical codebook structure, and
scale with the number of users through a feedback module sharing scheme and the
inherent scalability of EGAT. Moreover, a progressive training strategy is
proposed to further enhance data transmission performance and generalization
capability. Numerical results on a real-world dataset demonstrate the superior
scalability and performance of our approach over existing methods.


## A Unified Hardware Accelerator for Fast Fourier Transform and Number Theoretic Transform

>Authors: Rishabh Shrivastava, Chaitanya Prasad Ratnala, Durga Manasa Puli, Utsav Banerjee

>2025-04-15

> http://arxiv.org/abs/2504.11124v1

The Number Theoretic Transform (NTT) is an indispensable tool for computing
efficient polynomial multiplications in post-quantum lattice-based
cryptography. It has strong resemblance with the Fast Fourier Transform (FFT),
which is the most widely used algorithm in digital signal processing. In this
work, we demonstrate a unified hardware accelerator supporting both 512-point
complex FFT as well as 256-point NTT for the recently standardized NIST
post-quantum key encapsulation and digital signature algorithms ML-KEM and
ML-DSA respectively. Our proposed architecture effectively utilizes the
arithmetic circuitry required for complex FFT, and the only additional circuits
required are for modular reduction along with modifications in the control
logic. Our implementation achieves performance comparable to state-of-the-art
ML-KEM / ML-DSA NTT accelerators on FPGA, thus demonstrating how an FFT
accelerator can be augmented to support NTT and the unified hardware can be
used for both digital signal processing and post-quantum lattice-based
cryptography applications.


## Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay

>Authors: Henrik Krauss, Takehisa Yairi

>2025-04-15

> http://arxiv.org/abs/2504.11118v1

This study introduces a novel method for revealing human covert attention
patterns using gameplay data alone, utilizing offline attention techniques from
reinforcement learning (RL). We propose the contextualized, task-relevant (CTR)
attention network, which generates attention maps from both human and RL agent
gameplay in Atari environments. These maps are **sparse** yet retain the necessary
information for the current player's decision making. We compare the
CTR-derived attention maps with a temporally integrated overt attention (TIOA)
model based on eye-tracking data, serving as a point of comparison and
discussion. Visual inspection reveals distinct attention patterns: human CTR
maps focus on the player and rather nearby opponents, occasionally shifting
between stronger focus and broader views - sometimes even attending to empty
space ahead. In contrast, agent maps maintain a consistent broad focus on most
objects, including distant ones and the player. Quantitative analysis further
demonstrates that human CTR maps align more closely with TIOA than agent maps
do. Our findings indicate that the CTR attention network can effectively reveal
human covert attention patterns from gameplay alone, without the need for
additional data like brain activity recordings. This work contributes to
understanding human-agent attention differences and enables the development of
RL agents augmented with human covert attention.


## Morphing-based Compression for Data-centric ML Pipelines

>Authors: Sebastian Baunsgaard, Matthias Boehm

>2025-04-15

> http://arxiv.org/abs/2504.11067v1

Data-centric ML pipelines extend traditional machine learning (ML) pipelines
-- of feature transformations and ML model training -- by outer loops for data
cleaning, augmentation, and feature engineering to create high-quality input
data. Existing lossless matrix compression applies lightweight compression
schemes to numeric matrices and performs linear algebra operations such as
matrix-vector multiplications directly on the compressed representation but
struggles to efficiently rediscover structural data redundancy. Compressed
operations are effective at fitting data in available memory, reducing I/O
across the storage-memory-cache hierarchy, and improving instruction
parallelism. The applied data cleaning, augmentation, and feature
transformations provide a rich source of information about data characteristics
such as distinct items, column **sparsity**, and column correlations. In this
paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless
matrix compression -- that pushes compression through feature transformations
and engineering to leverage information about structural transformations.
Besides compressed feature transformations, we introduce a novel technique for
lightweight morphing of a compressed representation into workload-optimized
compressed representations without decompression. BWARE shows substantial
end-to-end runtime improvements, reducing the execution time for training
data-centric ML pipelines from days to hours.


## Easy3D A Simple Yet Effective Method for 3D Interactive Segmentation

>Authors: Andrea Simonelli, Norman Müller, Peter Kontschieder

>2025-04-15

> http://arxiv.org/abs/2504.11024v1

The increasing availability of digital 3D environments, whether through
image-based 3D reconstruction, generation, or scans obtained by robots, is
driving innovation across various applications. These come with a significant
demand for 3D interaction, such as 3D Interactive Segmentation, which is useful
for tasks like object selection and manipulation. Additionally, there is a
persistent need for solutions that are efficient, precise, and performing well
across diverse settings, particularly in unseen environments and with
unfamiliar objects. In this work, we introduce a 3D interactive segmentation
method that consistently surpasses previous state-of-the-art techniques on both
in-domain and out-of-domain datasets. Our simple approach integrates a
voxel-based **sparse** encoder with a lightweight transformer-based decoder that
implements implicit click fusion, achieving superior performance and maximizing
efficiency. Our method demonstrates substantial improvements on benchmark
datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on
unseen geometric distributions such as the ones obtained by Gaussian Splatting.
The project web-page is available at https://simonelli-andrea.github.io/easy3d.


## eARCO Efficient Automated Root Cause Analysis with Prompt Optimization

>Authors: Drishti Goel, Raghav Magazine, Supriyo Ghosh, Akshay Nambi, Prathamesh Deshpande, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan

>2025-04-15

> http://arxiv.org/abs/2504.11505v1

Root cause analysis (RCA) for incidents in large-scale cloud systems is a
complex, knowledge-intensive task that often requires significant manual effort
from on-call engineers (OCEs). Improving RCA is vital for accelerating the
incident resolution process and reducing service downtime and manual efforts.
Recent advancements in Large-Language Models (LLMs) have proven to be effective
in solving different stages of the incident management lifecycle including RCA.
However, existing LLM-based RCA recommendations typically leverage default
finetuning or retrieval augmented generation (RAG) methods with static,
manually designed prompts, which lead to sub-optimal recommendations. In this
work, we leverage 'PromptWizard', a state-of-the-art prompt optimization
technique, to automatically identify the best optimized prompt instruction that
is combined with semantically similar historical examples for querying
underlying LLMs during inference. Moreover, by utilizing more than 180K
historical incident data from Microsoft, we developed cost-effective finetuned
small language models (SLMs) for RCA recommendation generation and demonstrate
the power of prompt optimization on such domain-adapted models. Our extensive
experimental results show that prompt optimization can improve the accuracy of
RCA recommendations by 21% and 13% on 3K test incidents over RAG-based LLMs and
finetuned SLMs, respectively. Lastly, our human evaluation with incident owners
have demonstrated the efficacy of prompt optimization on RCA recommendation
tasks. These findings underscore the advantages of incorporating prompt
optimization into AI for Operations (AIOps) systems, delivering substantial
gains without increasing computational overhead.


## CSPLADE Learned Sparse Retrieval with Causal Language Models

>Authors: Zhichao Xu, Aosong Feng, Yijun Tian, Haibo Ding, Lin Lee Cheong

>2025-04-15

> http://arxiv.org/abs/2504.10816v2

In recent years, dense retrieval has been the focus of information retrieval
(IR) research. While effective, dense retrieval produces uninterpretable dense
vectors, and suffers from the drawback of large index size. Learned **sparse**
retrieval (LSR) has emerged as promising alternative, achieving competitive
retrieval performance while also being able to leverage the classical inverted
index data structure for efficient retrieval. However, limited works have
explored scaling LSR beyond BERT scale. In this work, we identify two
challenges in training large language models (LLM) for LSR: (1) training
instability during the early stage of contrastive training; (2) suboptimal
performance due to pre-trained LLM's unidirectional attention. To address these
challenges, we propose two corresponding techniques: (1) a lightweight
adaptation training phase to eliminate training instability; (2) two model
variants to enable bidirectional information. With these techniques, we are
able to train LSR models with 8B scale LLM, and achieve competitive retrieval
performance with reduced index size. Furthermore, we are among the first to
analyze the performance-efficiency tradeoff of LLM-based LSR model through the
lens of model **quantization**. Our findings provide insights into adapting LLMs
for efficient retrieval modeling.


## The Sword of Damocles in ViTs Computational Redundancy Amplifies Adversarial Transferability

>Authors: Jiani Liu, Zhiyuan Wang, Zeliang Zhang, Chao Huang, Susan Liang, Yunlong Tang, Chenliang Xu

>2025-04-15

> http://arxiv.org/abs/2504.10804v1

Vision Transformers (ViTs) have demonstrated impressive performance across a
range of applications, including many safety-critical tasks. However, their
unique architectural properties raise new challenges and opportunities in
adversarial robustness. In particular, we observe that adversarial examples
crafted on ViTs exhibit higher transferability compared to those crafted on
CNNs, suggesting that ViTs contain structural characteristics favorable for
transferable attacks. In this work, we investigate the role of computational
redundancy in ViTs and its impact on adversarial transferability. Unlike prior
studies that aim to reduce computation for efficiency, we propose to exploit
this redundancy to improve the quality and transferability of adversarial
examples. Through a detailed analysis, we identify two forms of redundancy,
including the data-level and model-level, that can be harnessed to amplify
attack effectiveness. Building on this insight, we design a suite of
techniques, including attention **sparsity** manipulation, attention head
permutation, clean token regularization, ghost MoE diversification, and
test-time adversarial training. Extensive experiments on the ImageNet-1k
dataset validate the effectiveness of our approach, showing that our methods
significantly outperform existing baselines in both transferability and
generality across diverse model architectures.


## GOAT-TTS LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture

>Authors: Yaodong Song, Hongjie Chen, Jie Lian, Yuxin Zhang, Guangmin Xia, Zehan Li, Genliang Zhao, Jian Kang, Yongxiang Li, Jie Li

>2025-04-15

> http://arxiv.org/abs/2504.12339v1

While large language models (LLMs) have revolutionized text-to-speech (TTS)
synthesis through discrete tokenization paradigms, current architectures
exhibit fundamental tensions between three critical dimensions: 1) irreversible
loss of acoustic characteristics caused by **quantization** of speech prompts; 2)
stringent dependence on precisely aligned prompt speech-text pairs that limit
real-world deployment; and 3) catastrophic forgetting of the LLM's native text
comprehension during optimization for speech token generation. To address these
challenges, we propose an LLM-based text-to-speech Generation approach
Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework
introduces two key innovations: (1) The modality-alignment branch combines a
speech encoder and projector to capture continuous acoustic embeddings,
enabling bidirectional correlation between paralinguistic features (language,
timbre, emotion) and semantic text representations without transcript
dependency; (2) The speech-generation branch employs modular fine-tuning on
top-k layers of an LLM for speech token prediction while freezing the bottom-k
layers to preserve foundational linguistic knowledge. Moreover, multi-token
prediction is introduced to support real-time streaming TTS synthesis.
Experimental results demonstrate that our GOAT-TTS achieves performance
comparable to state-of-the-art TTS models while validating the efficacy of
synthesized dialect speech data.


## 3D Wavelet Convolutions with Extended Receptive Fields for Hyperspectral Image Classification

>Authors: Guandong Li, Mengxia Ye

>2025-04-15

> http://arxiv.org/abs/2504.10795v1

Deep neural networks face numerous challenges in hyperspectral image
classification, including high-dimensional data, **sparse** ground object
distributions, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To better adapt to ground
object distributions while expanding receptive fields without introducing
excessive parameters and skipping redundant information, this paper proposes
WCNet, an improved 3D-DenseNet model integrated with wavelet transforms. We
introduce wavelet transforms to effectively extend convolutional receptive
fields and guide CNNs to better respond to low frequencies through cascading,
termed wavelet convolution. Each convolution focuses on different frequency
bands of the input signal with gradually increasing effective ranges. This
process enables greater emphasis on low-frequency components while adding only
a small number of trainable parameters. This dynamic approach allows the model
to flexibly focus on critical spatial structures when processing different
regions, rather than relying on fixed receptive fields of single static
kernels. The Wavelet Conv module enhances model representation capability by
expanding receptive fields through 3D wavelet transforms without increasing
network depth or width. Experimental results demonstrate superior performance
on the IN, UP, and KSC datasets, outperforming mainstream hyperspectral image
classification methods.


## Matrix representation and GPU-optimized parallel B-spline computing

>Authors: Jiayu Wu, Qiang Zou

>2025-04-15

> http://arxiv.org/abs/2504.11498v1

B-spline modeling is fundamental to CAD systems, and its evaluation and
manipulation algorithms currently in use were developed decades ago,
specifically for CPU architectures. While remaining effective for many
applications, these algorithms become increasingly inadequate as CAD models
grow more complex, such as large-scale assemblies and microstructures. GPU
**acceleration** offers a promising solution, but most existing GPU B-spline
algorithms simply adapt CPU counterparts without accounting for the mismatch
between the unstructured, recursive nature of B-splines and the structured
nature of GPU kernels, ultimately failing to fully leverage GPU capabilities.
This paper presents a novel approach that transforms B-spline representations
into regular matrix structures, reducing all evaluation and manipulation
computations to matrix addition and multiplication, thus better aligning with
GPU architecture. By combining this matrix representation with GPU-optimized
task scheduling and memory access patterns, the paper demonstrates significant
performance improvements in the key B-spline operations of inversion and
projection. Experimental results show an improvement of about two orders of
magnitude in computational speed compared to existing methods.


## Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning

>Authors: Radin Cheraghi, Amir Mohammad Mahfoozi, Sepehr Zolfaghari, Mohammadshayan Shabani, Maryam Ramezani, Hamid R. Rabiee

>2025-04-14

> http://arxiv.org/abs/2504.10753v1

Recommending items to users has long been a fundamental task, and studies
have tried to improve it ever since. Most well-known models commonly employ
representation learning to map users and items into a unified embedding space
for matching assessment. These approaches have primary limitations, especially
when dealing with explicit feedback and **sparse** data contexts. Two primary
limitations are their proneness to overfitting and failure to incorporate
epistemic uncertainty in predictions. To address these problems, we propose a
novel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To
improve model generalization and quality, we utilize Bayesian Neural Networks,
which incorporate uncertainty within their weight parameters. In addition, we
introduce a new interpretable non-linear matching approach for the user and
item embeddings, leveraging the advantages of the attention mechanism.
Furthermore, we endorse the implementation of an ensemble-based supermodel to
generate more robust and reliable predictions, resulting in a more complete
model. Empirical evaluation through extensive experiments and ablation studies
across a range of publicly accessible real-world datasets with differing
**sparsity** characteristics confirms our proposed method's effectiveness and the
importance of its components.


## Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery

>Authors: Yiting Wang, Yunxin Fan, Fei Liu

>2025-04-14

> http://arxiv.org/abs/2504.11495v1

Accurate modeling of tool-tissue interactions in robotic surgery requires
precise tracking of deformable tissues and integration of surgical domain
knowledge. Traditional methods rely on labor-intensive annotations or rigid
assumptions, limiting flexibility. We propose a framework combining **sparse**
keypoint tracking and probabilistic modeling that propagates expert-annotated
landmarks across endoscopic frames, even with large tissue deformations.
Clustered tissue keypoints enable dynamic local transformation construction via
PCA, and tool poses, tracked similarly, are expressed relative to these frames.
Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)
integrates data-driven observations with labeled clinical expertise,
effectively predicting relative tool-tissue poses and enhancing visual
understanding of robotic surgical motions directly from video data.


## FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and Singular Value Decomposition in AI

>Authors: Hong Ding, Chia Chao Kang, SuYang Xi, Zehang Liu, Xuan Zhang, Yi Ding

>2025-04-14

> http://arxiv.org/abs/2504.10411v1

This research introduces an FPGA-based hardware accelerator to optimize the
Singular Value Decomposition (SVD) and Fast Fourier transform (FFT) operations
in AI models. The proposed design aims to improve processing speed and reduce
computational latency. Through experiments, we validate the performance
benefits of the hardware accelerator and show how well it handles FFT and SVD
operations. With its strong security and durability, the accelerator design
achieves significant speedups over software implementations, thanks to its
modules for data flow control, watermark embedding, FFT, and SVD.


## AlayaDB The Data Foundation for Efficient and Effective Long-context LLM Inference

>Authors: Yangshen Deng, Zhengxin You, Long Xiang, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang

>2025-04-14

> http://arxiv.org/abs/2504.10326v1

AlayaDB is a cutting-edge vector database system natively architected for
efficient and effective long-context inference for Large Language Models (LLMs)
at AlayaDB AI. Specifically, it decouples the **KV** cache and attention
computation from the LLM inference systems, and encapsulates them into a novel
vector database system. For the Model as a Service providers (MaaS), AlayaDB
consumes fewer hardware resources and offers higher generation quality for
various workloads with different kinds of Service Level Objectives (SLOs), when
comparing with the existing alternative solutions (e.g., **KV** cache
disaggregation, retrieval-based **sparse** attention). The crux of AlayaDB is that
it abstracts the attention computation and cache management for LLM inference
into a query processing procedure, and optimizes the performance via a native
query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via
(i) three use cases from our industry partners, and (ii) extensive experimental
results on LLM inference benchmarks.


## Analysis of Attention in Video Diffusion Transformers

>Authors: Yuxin Wen, Jim Wu, Ajay Jain, Tom Goldstein, Ashwinee Panda

>2025-04-14

> http://arxiv.org/abs/2504.10317v1

We conduct an in-depth analysis of attention in video diffusion transformers
(VDiTs) and report a number of novel findings. We identify three key properties
of attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe
that attention patterns across different VDiTs exhibit similar structure across
different prompts, and that we can make use of the similarity of attention
patterns to unlock video editing via self-attention map transfer. Sparse: We
study attention **sparsity** in VDiTs, finding that proposed **sparsity** methods do
not work for all VDiTs, because some layers that are seemingly **sparse** cannot be
sparsified. Sinks: We make the first study of attention sinks in VDiTs,
comparing and contrasting them to attention sinks in language models. We
propose a number of future directions that can make use of our insights to
improve the efficiency-quality Pareto frontier for VDiTs.


## DiffMOD Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing

>Authors: Jinyue Zhang, Xiangrong Zhang, Zhongjian Huang, Tianyang Zhang, Yifei Jiang, Licheng Jiao

>2025-04-14

> http://arxiv.org/abs/2504.10278v1

Moving object detection (MOD) in remote sensing is significantly challenged
by low resolution, extremely small object sizes, and complex noise
interference. Current deep learning-based MOD methods rely on probability
density estimation, which restricts flexible information interaction between
objects and across temporal frames. To flexibly capture high-order inter-object
and temporal relationships, we propose a point-based MOD in remote sensing.
Inspired by diffusion models, the network optimization is formulated as a
progressive denoising process that iteratively recovers moving object centers
from **sparse** noisy points. Specifically, we sample scattered features from the
backbone outputs as atomic units for subsequent processing, while global
feature embeddings are aggregated to compensate for the limited coverage of
**sparse** point features. By modeling spatial relative positions and semantic
affinities, Spatial Relation Aggregation Attention is designed to enable
high-order interactions among point-level features for enhanced object
representation. To enhance temporal consistency, the Temporal Propagation and
Global Fusion module is designed, which leverages an implicit memory reasoning
mechanism for robust cross-frame feature integration. To align with the
progressive denoising process, we propose a progressive MinK optimal transport
assignment strategy that establishes specialized learning objectives at each
denoising level. Additionally, we introduce a missing loss function to
counteract the clustering tendency of denoised points around salient objects.
Experiments on the RsData remote sensing MOD dataset show that our MOD method
based on scattered point denoising can more effectively explore potential
relationships between **sparse** moving objects and improve the detection
capability and temporal consistency.


## WG-IDENT Weak Group Identification of PDEs with Varying Coefficients

>Authors: Cheng Tang, Roy Y. He, Hao Liu

>2025-04-14

> http://arxiv.org/abs/2504.10212v1

Partial Differential Equations (PDEs) identification is a data-driven method
for mathematical modeling, and has received a lot of attentions recently. The
stability and precision in identifying PDE from heavily noisy spatiotemporal
data present significant difficulties. This problem becomes even more complex
when the coefficients of the PDEs are subject to spatial variation. In this
paper, we propose a Weak formulation of Group-**sparsity**-based framework for
IDENTifying PDEs with varying coefficients, called WG-IDENT, to tackle this
challenge. Our approach utilizes the weak formulation of PDEs to reduce the
impact of noise. We represent test functions and unknown PDE coefficients using
B-splines, where the knot vectors of test functions are optimally selected
based on spectral analysis of the noisy data. To facilitate feature selection,
we propose to integrate group **sparse** regression with a newly designed group
feature trimming technique, called GF-trim, to eliminate unimportant features.
Extensive and comparative ablation studies are conducted to validate our
proposed method. The proposed method not only demonstrates greater robustness
to high noise levels compared to state-of-the-art algorithms but also achieves
superior performance while exhibiting reduced sensitivity to hyperparameter
selection.


## Nanoplastic Analysis with Nanoelectromechanical System Fourier Transform Infrared Spectroscopy NEMS-FTIR

>Authors: Jelena Timarac-Popović, Johannes Hiesberger, Eldira Šesto, Niklas Luhmann, Ariane Giesriegl, Hajrudin Bešić, Josiane P. Lafleur, Silvan Schmid

>2025-04-14

> http://arxiv.org/abs/2504.10192v1

This paper presents a photothermal infrared (IR) spectroscopy technique based
on a nanoelectromechanical system, which is coupled to a commercial Fourier
transform infrared spectrometer (NEMS--FTIR) as a promising solution for the
chemical characterization and quantification of nanoplastics. Polystyrene (PS),
polypropylene (PP), and polyvinyl chloride (PVC) nanoparticles with nominal
diameters of 100, 54, and 262~nm, respectively, were analyzed by NEMS--FTIR
with limits of detection (LoD) of 353~pg for PS, 102~pg for PP, and 355~pg for
PVC. The PS mass deposited on the NEMS chips was estimated from the measured
absorptance values and the attenuation coefficient of PS. The wide spectral
range of the FTIR allowed the identification of individual polymer
nanoparticles from a mixture. The potential of NEMS--FTIR for the analysis of
real--world samples was evaluated by confirming the presence of polyamide (PA)
particles released from commercial tea bags during brewing. Accelerated aging
of the tea bags under elevated temperature and UV radiation showed continuous
release of PA particles over time.


## Efficient Generative Model Training via Embedded Representation Warmup

>Authors: Deyuan Liu, Peng Sun, Xufeng Li, Tao Lin

>2025-04-14

> http://arxiv.org/abs/2504.10188v1

Diffusion models excel at generating high-dimensional data but fall short in
training efficiency and representation quality compared to self-supervised
methods. We identify a key bottleneck: the underutilization of high-quality,
semantically rich representations during training notably slows down
convergence. Our systematic analysis reveals a critical representation
processing region -- primarily in the early layers -- where semantic and
structural pattern learning takes place before generation can occur. To address
this, we propose Embedded Representation Warmup (ERW), a plug-and-play
framework where in the first stage we get the ERW module serves as a warmup
that initializes the early layers of the diffusion model with high-quality,
pretrained representations. This warmup minimizes the burden of learning
representations from scratch, thereby accelerating convergence and boosting
performance. Our theoretical analysis demonstrates that ERW's efficacy depends
on its precise integration into specific neural network layers -- termed the
representation processing region -- where the model primarily processes and
transforms feature representations for later generation. We further establish
that ERW not only accelerates training convergence but also enhances
representation quality: empirically, our method achieves a 40$\times$
**acceleration** in training speed compared to REPA, the current state-of-the-art
methods. Code is available at https://github.com/LINs-lab/ERW.


## Enhancing LLM-based Recommendation through Semantic-Aligned Collaborative Knowledge

>Authors: Zihan Wang, Jinghao Lin, Xiaocui Yang, Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang

>2025-04-14

> http://arxiv.org/abs/2504.10107v1

Large Language Models (LLMs) demonstrate remarkable capabilities in
leveraging comprehensive world knowledge and sophisticated reasoning mechanisms
for recommendation tasks. However, a notable limitation lies in their inability
to effectively model **sparse** identifiers (e.g., user and item IDs), unlike
conventional collaborative filtering models (Collabs.), thus hindering LLM to
learn distinctive user-item representations and creating a performance
bottleneck. Prior studies indicate that integrating collaborative knowledge
from Collabs. into LLMs can mitigate the above limitations and enhance their
recommendation performance. Nevertheless, the significant discrepancy in
knowledge distribution and semantic space between LLMs and Collab. presents
substantial challenges for effective knowledge transfer. To tackle these
challenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving
alignment between the semantic spaces of Collabs. and LLMs. This alignment
fosters effective knowledge fusion, mitigating the influence of discriminative
noise and facilitating the deep integration of knowledge from diverse models.
Specifically, three special tokens with collaborative knowledge are embedded
into the LLM's semantic space through a hybrid projection layer and integrated
into task-specific prompts to guide the recommendation process. Experiments
conducted on two public benchmark datasets (MovieLens-1M and Amazon Book)
demonstrate that SeLLa-Rec achieves state-of-the-art performance.


## Unleashing Expert Opinion from Social Media for Stock Prediction

>Authors: Wanyun Zhou, Saizhuo Wang, Xiang Li, Yiyan Qi, Jian Guo, Xiaowen Chu

>2025-04-14

> http://arxiv.org/abs/2504.10078v1

While stock prediction task traditionally relies on volume-price and
fundamental data to predict the return ratio or price movement trend, sentiment
factors derived from social media platforms such as StockTwits offer a
complementary and useful source of real-time market information. However, we
find that most social media posts, along with the public sentiment they
reflect, provide limited value for trading predictions due to their noisy
nature. To tackle this, we propose a novel dynamic expert tracing algorithm
that filters out non-informative posts and identifies both true and inverse
experts whose consistent predictions can serve as valuable trading signals. Our
approach achieves significant improvements over existing expert identification
methods in stock trend prediction. However, when using binary expert
predictions to predict the return ratio, similar to all other expert
identification methods, our approach faces a common challenge of signal
**sparsity** with expert signals cover only about 4% of all stock-day combinations
in our dataset. To address this challenge, we propose a dual graph attention
neural network that effectively propagates expert signals across related
stocks, enabling accurate prediction of return ratios and significantly
increasing signal coverage. Empirical results show that our propagated
expert-based signals not only exhibit strong predictive power independently but
also work synergistically with traditional financial features. These combined
signals significantly outperform representative baseline models in all
quant-related metrics including predictive accuracy, return metrics, and
correlation metrics, resulting in more robust investment strategies. We hope
this work inspires further research into leveraging social media data for
enhancing quantitative investment strategies. The code can be seen in
https://github.com/wanyunzh/DualGAT.


## Mavors Multi-granularity Video Representation for Multimodal Large Language Model

>Authors: Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, Di Zhang

>2025-04-14

> http://arxiv.org/abs/2504.10068v1

Long-context video understanding in multimodal large language models (MLLMs)
faces a critical challenge: balancing computational efficiency with the
retention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,
**sparse** sampling, dense sampling with low resolution, and token compression)
suffer from significant information loss in temporal dynamics, spatial details,
or subtle interactions, particularly in videos with complex motion or varying
resolutions. To address this, we propose $\mathbf{Mavors}$, a novel framework
that introduces $\mathbf{M}$ulti-gr$\mathbf{a}$nularity
$\mathbf{v}$ide$\mathbf{o}$ $\mathbf{r}$epre$\mathbf{s}$entation for holistic
long-video modeling. Specifically, Mavors directly encodes raw video content
into latent representations through two core components: 1) an Intra-chunk
Vision Encoder (IVE) that preserves high-resolution spatial features via 3D
convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator
(IFA) that establishes temporal coherence across chunks using transformer-based
dependency modeling with chunk-level rotary position encodings. Moreover, the
framework unifies image and video understanding by treating images as
single-frame videos via sub-image decomposition. Experiments across diverse
benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity
and temporal continuity, significantly outperforming existing methods in tasks
requiring fine-grained spatio-temporal reasoning.


## Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics

>Authors: Nikolai Röhrich, Alwin Hoffmann, Richard Nordsieck, Emilio Zarbali, Alireza Javanmardi

>2025-04-14

> http://arxiv.org/abs/2504.10021v1

Whereas in general computer vision, transformer-based architectures have
quickly become the gold standard, microelectronics defect detection still
heavily relies on convolutional neural networks (CNNs). We hypothesize that
this is due to the fact that a) transformers have an increased need for data
and b) labelled image generation procedures for microelectronics are costly,
and labelled data is therefore **sparse**. Whereas in other domains, pre-training
on large natural image datasets can mitigate this problem, in microelectronics
transfer learning is hindered due to the dissimilarity of domain data and
natural images. Therefore, we evaluate self pre-training, where models are
pre-trained on the target dataset, rather than another dataset. We propose a
vision transformer (ViT) pre-training framework for defect detection in
microelectronics based on masked autoencoders (MAE). In MAE, a large share of
image patches is masked and reconstructed by the model during pre-training. We
perform pre-training and defect detection using a dataset of less than 10.000
scanning acoustic microscopy (SAM) images labelled using transient thermal
analysis (TTA). Our experimental results show that our approach leads to
substantial performance gains compared to a) supervised ViT, b) ViT pre-trained
on natural image datasets, and c) state-of-the-art CNN-based defect detection
models used in the literature. Additionally, interpretability analysis reveals
that our self pre-trained models, in comparison to ViT baselines, correctly
focus on defect-relevant features such as cracks in the solder material. This
demonstrates that our approach yields fault-specific feature representations,
making our self pre-trained models viable for real-world defect detection in
microelectronics.


## OctGPT Octree-based Multiscale Autoregressive Models for 3D Shape Generation

>Authors: Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang

>2025-04-14

> http://arxiv.org/abs/2504.09975v2

Autoregressive models have achieved remarkable success across various
domains, yet their performance in 3D shape generation lags significantly behind
that of diffusion models. In this paper, we introduce OctGPT, a novel
multiscale autoregressive model for 3D shape generation that dramatically
improves the efficiency and performance of prior 3D autoregressive approaches,
while rivaling or surpassing state-of-the-art diffusion models. Our method
employs a serialized octree representation to efficiently capture the
hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded
via octree structures, while fine-grained details are represented by binary
tokens generated using a vector **quantize**d variational autoencoder (VQVAE),
transforming 3D shapes into compact multiscale binary sequences suitable for
autoregressive prediction. To address the computational challenges of handling
long sequences, we incorporate octree-based transformers enhanced with 3D
rotary positional encodings, scale-specific embeddings, and token-parallel
generation schemes. These innovations reduce training time by 13 folds and
generation time by 69 folds, enabling the efficient training of high-resolution
3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.
OctGPT showcases exceptional versatility across various tasks, including text-,
sketch-, and image-conditioned generation, as well as scene-level synthesis
involving multiple objects. Extensive experiments demonstrate that OctGPT
accelerates convergence and improves generation quality over prior
autoregressive methods, offering a new paradigm for high-quality, scalable 3D
content creation. Our code and trained models are available at
https://github.com/octree-nn/octgpt.


## KeepKV Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference

>Authors: Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang

>2025-04-14

> http://arxiv.org/abs/2504.09936v1

Efficient inference of large language models (LLMs) is hindered by an
ever-growing key-value (**KV**) cache, making **KV** cache compression a critical
research direction. Traditional methods selectively evict less important **KV**
cache entries based on attention scores or position heuristics, which leads to
information loss and hallucinations. Recently, merging-based strategies have
been explored to retain more information by merging **KV** pairs that would be
discarded; however, these existing approaches inevitably introduce
inconsistencies in attention distributions before and after merging, causing
output perturbation and degraded generation quality. To overcome this
challenge, we propose Keep**KV**, a novel adaptive **KV** cache merging method designed
to eliminate output perturbation while preserving performance under strict
memory constraints. Keep**KV** introduces the Electoral Votes mechanism that
records merging history and adaptively adjusts attention scores. Moreover, it
further leverages a novel Zero Inference-Perturbation Merging methods, keeping
attention consistency and compensating for attention loss resulting from cache
merging. Keep**KV** successfully retains essential context information within a
significantly compressed cache. Extensive experiments on various benchmarks and
LLM architectures demonstrate that Keep**KV** substantially reduces memory usage,
enhances inference throughput by more than 2x and keeps superior generation
quality even with 10% **KV** cache budgets.


## TAMP Token-Adaptive Layerwise Pruning in Multimodal Large Language Models

>Authors: Jaewoo Lee, Keyang Xuan, Chanakya Ekbote, Sandeep Polisetty, Yi R. Fung, Paul Pu Liang

>2025-04-14

> http://arxiv.org/abs/2504.09897v2

Multimodal Large Language Models (MLLMs) have shown remarkable versatility in
understanding diverse multimodal data and tasks. However, these capabilities
come with an increased model scale. While post-training **pruning** reduces model
size in unimodal models, its application to MLLMs often yields limited success.
Our analysis discovers that conventional methods fail to account for the unique
token attributes across layers and modalities inherent to MLLMs. Inspired by
this observation, we propose TAMP, a simple yet effective **pruning** framework
tailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,
which adjusts **sparsity** ratio per layer based on diversities among multimodal
output tokens, preserving more parameters in high-diversity layers; and (2)
Adaptive Multimodal Input Activation, which identifies representative
multimodal input tokens using attention scores to guide unstructured weight
**pruning**. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,
designed for vision-language tasks, and VideoLLaMA2, capable of processing
audio, visual, and language modalities. Empirical experiments across various
multimodal evaluation benchmarks demonstrate that each component of our
approach substantially outperforms existing **pruning** techniques.


## RadarLLM Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence

>Authors: Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, Ling Pei

>2025-04-14

> http://arxiv.org/abs/2504.09862v1

Millimeter-wave radar provides a privacy-preserving solution for human motion
analysis, yet its **sparse** point clouds pose significant challenges for semantic
understanding. We present Radar-LLM, the first framework that leverages large
language models (LLMs) for human motion understanding using millimeter-wave
radar as the sensing modality. Our approach introduces two key innovations: (1)
a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that
incorporates deformable body templates and masked trajectory modeling to encode
spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware
language model that establishes cross-modal alignment between radar and text in
a shared embedding space. To address data scarcity, we introduce a
physics-aware synthesis pipeline that generates realistic radar-text pairs from
motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves
state-of-the-art performance across both synthetic and real-world benchmarks,
enabling accurate translation of millimeter-wave signals to natural language
descriptions. This breakthrough facilitates comprehensive motion understanding
in privacy-sensitive applications like healthcare and smart homes. We will
release the full implementation to support further research on
https://inowlzy.github.io/RadarLLM/.


## CUT Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices

>Authors: Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong

>2025-04-14

> http://arxiv.org/abs/2504.09803v1

Multi-task learning has garnered widespread attention in the industry due to
its efficient data utilization and strong generalization capabilities, making
it particularly suitable for providing high-quality intelligent services to
users. Edge devices, as the primary platforms directly serving users, play a
crucial role in delivering multi-task services. However, current multi-task
models are often large, and user task demands are increasingly diverse.
Deploying such models directly on edge devices not only increases the burden on
these devices but also leads to task redundancy. To address this issue, this
paper innovatively proposes a pre-trained multi-task model **pruning** method
specifically designed for edge computing. The goal is to utilize existing
pre-trained multi-task models to construct a compact multi-task model that
meets the needs of edge devices. The specific implementation steps are as
follows: First, decompose the tasks within the pre-trained multi-task model and
select tasks based on actual user needs. Next, while retaining the knowledge of
the original pre-trained model, evaluate parameter importance and use a
parameter fusion method to effectively integrate shared parameters among tasks.
Finally, obtain a compact multi-task model suitable for edge devices. To
validate the effectiveness of the proposed method, we conducted experiments on
three public image datasets. The experimental results fully demonstrate the
superiority and efficiency of this method, providing a new solution for
multi-task learning on edge devices.


## Understanding and Optimizing Multi-Stage AI Inference Pipelines

>Authors: Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna

>2025-04-14

> http://arxiv.org/abs/2504.09775v3

The rapid evolution of Large Language Models (LLMs) has driven the need for
increasingly sophisticated inference pipelines and hardware platforms. Modern
LLM serving extends beyond traditional prefill-decode workflows, incorporating
multi-stage processes such as Retrieval Augmented Generation (RAG), key-value
(**KV**) cache retrieval, dynamic model routing, and multi step reasoning. These
stages exhibit diverse computational demands, requiring distributed systems
that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,
existing simulators lack the fidelity to model these heterogeneous,
multi-engine workflows, limiting their ability to inform architectural
decisions.
  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM
inference Execution Simulator. HERMES models diverse request stages; including
RAG, **KV** retrieval, reasoning, prefill, and decode across complex hardware
hierarchies. HERMES supports heterogeneous clients executing multiple models
concurrently unlike prior frameworks while incorporating advanced batching
strategies and multi-level memory hierarchies. By integrating real hardware
traces with analytical modeling, HERMES captures critical trade-offs such as
memory bandwidth contention, inter-cluster communication latency, and batching
efficiency in hybrid CPU-accelerator deployments. Through case studies, we
explore the impact of reasoning stages on end-to-end latency, optimal batching
strategies for hybrid pipelines, and the architectural implications of remote
**KV** cache retrieval. HERMES empowers system designers to navigate the evolving
landscape of LLM inference, providing actionable insights into optimizing
hardware-software co-design for next-generation AI workloads.

