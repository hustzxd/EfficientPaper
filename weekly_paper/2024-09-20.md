# WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification

>Authors: Junzuo Zhou,Jiangyan Yi,Yong Ren,Jianhua Tao,Tao Wang,Chu Yuan Zhang

>2024-09-18

> http://arxiv.org/abs/2409.12121v1

Recent advances in speech spoofing necessitate stronger verification
mechanisms in neural speech codecs to ensure authenticity. Current methods
embed numerical watermarks before compression and extract them from
reconstructed speech for verification, but face limitations such as separate
training processes for the watermark and codec, and insufficient cross-modal
information integration, leading to reduced watermark imperceptibility,
extraction accuracy, and capacity. To address these issues, we propose WMCodec,
the first neural speech codec to jointly train compression-reconstruction and
watermark embedding-extraction in an end-to-end manner, optimizing both
imperceptibility and extractability of the watermark. Furthermore, We design an
iterative Attention Imprint Unit (AIU) for deeper feature integration of
watermark and speech, reducing the impact of quantization noise on the
watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec
in most quality metrics for watermark imperceptibility and consistently exceeds
both AudioSeal with Encodec and reinforced TraceableSpeech in extraction
accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16
bps, WMCodec maintains over 99% extraction accuracy under common attacks,
demonstrating strong robustness.


# Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference

>Authors: Edresson Casanova,Ryan Langman,Paarth Neekhara,Shehzeen Hussain,Jason Li,Subhankar Ghosh,Ante Jukić,Sang-gil Lee

>2024-09-18

> http://arxiv.org/abs/2409.12117v1

Large language models (LLMs) have significantly advanced audio processing
through audio codecs that convert audio into discrete tokens, enabling the
application of language modeling techniques to audio data. However, audio
codecs often operate at high frame rates, resulting in slow training and
inference, especially for autoregressive models. To address this challenge, we
present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that
leverages finite scalar quantization and adversarial training with large speech
language models to achieve high-quality audio compression with a 1.89 kbps
bitrate and 21.5 frames per second. We demonstrate that our novel codec can
make the inference of LLM-based text-to-speech models around three times faster
while improving intelligibility and producing quality comparable to previous
models.


# Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics

>Authors: Paul Garnier,Jonathan Viquerat,Elie Hachem

>2024-09-18

> http://arxiv.org/abs/2409.11899v1

Advancement in finite element methods have become essential in various
disciplines, and in particular for Computational Fluid Dynamics (CFD), driving
research efforts for improved precision and efficiency. While Convolutional
Neural Networks (CNNs) have found success in CFD by mapping meshes into images,
recent attention has turned to leveraging Graph Neural Networks (GNNs) for
direct mesh processing. This paper introduces a novel model merging
Self-Attention with Message Passing in GNNs, achieving a 15\% reduction in RMSE
on the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh
pruning technique based on Self-Attention is proposed, that leads to a robust
GNN-based multigrid approach, also reducing RMSE by 15\%. Additionally, a new
self-supervised training method based on BERT is presented, resulting in a 25\%
RMSE reduction. The paper includes an ablation study and outperforms
state-of-the-art models on several challenging datasets, promising advancements
similar to those recently achieved in natural language and image processing.
Finally, the paper introduces a dataset with meshes larger than existing ones
by at least an order of magnitude. Code and Datasets will be released at
https://github.com/DonsetPG/multigrid-gnn.


# Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview

>Authors: Yanshu Wang,Tong Yang,Xiyan Liang,Guoan Wang,Hanning Lu,Xu Zhe,Yaoming Li,Li Weitao

>2024-09-18

> http://arxiv.org/abs/2409.11650v1

This paper provides a comprehensive overview of the principles, challenges,
and methodologies associated with quantizing large-scale neural network models.
As neural networks have evolved towards larger and more complex architectures
to address increasingly sophisticated tasks, the computational and energy costs
have escalated significantly. We explore the necessity and impact of model size
growth, highlighting the performance benefits as well as the computational
challenges and environmental considerations. The core focus is on model
quantization as a fundamental approach to mitigate these challenges by reducing
model size and improving efficiency without substantially compromising
accuracy. We delve into various quantization techniques, including both
post-training quantization (PTQ) and quantization-aware training (QAT), and
analyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),
ZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine
how these methods address issues like outliers, importance weighting, and
activation quantization, ultimately contributing to more sustainable and
accessible deployment of large-scale models.


# Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models

>Authors: Bishwash Khanal,Jeffery M. Capone

>2024-09-17

> http://arxiv.org/abs/2409.11233v1

Large language models (LLMs) offer powerful capabilities but incur
substantial computational costs, driving the need for efficient compression
techniques. This study evaluates the impact of popular compression methods -
Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on
the trade-offs between model size reduction, downstream task performance, and
the role of calibration data. Our findings reveal that while SparseGPT and
Wanda preserve perplexity even at 50% sparsity, they suffer significant
degradation on downstream tasks, highlighting the inadequacy of perplexity as
the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)
Divergence as a more comprehensive metric that captures nuanced changes in
model behavior post-compression. We further demonstrate that task-specific
calibration data significantly enhances the downstream performance of
compressed models compared to general calibration data. This research
underscores the necessity for diverse evaluation metrics and careful
calibration data selection to fully understand the complexities of LLM
compression and its implications for practical applications.


# A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B

>Authors: Jemin Lee,Sihyeong Park,Jinse Kwon,Jihun Oh,Yongin Kwon

>2024-09-17

> http://arxiv.org/abs/2409.11055v1

Prior research works have evaluated quantized LLMs using limited metrics such
as perplexity or a few basic knowledge tasks and old datasets. Additionally,
recent large-scale models such as Llama 3.1 with up to 405B have not been
thoroughly examined. This paper evaluates the performance of instruction-tuned
LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on
models ranging from 7B to 405B. Using 13 benchmarks, we assess performance
across six task types: commonsense Q\&A, knowledge and language understanding,
instruction following, hallucination detection, mathematics, and dialogue. Our
key findings reveal that (1) quantizing a larger LLM to a similar size as a
smaller FP16 LLM generally performs better across most benchmarks, except for
hallucination detection and instruction following; (2) performance varies
significantly with different quantization methods, model size, and bit-width,
with weight-only methods often yielding better results in larger models; (3)
task difficulty does not significantly impact accuracy degradation due to
quantization; and (4) the MT-Bench evaluation method has limited discriminatory
power among recent high-performing LLMs.


# Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation

>Authors: Rui Yu,Runkai Zhao,Jiagen Li,Qingsong Zhao,Songhao Zhu,HuaiCheng Yan,Meng Wang

>2024-09-17

> http://arxiv.org/abs/2409.11018v1

The LiDAR-based 3D object detector that strikes a balance between accuracy
and speed is crucial for achieving real-time perception in autonomous driving
and robotic navigation systems. To enhance the accuracy of point cloud
detection, integrating global context for visual understanding improves the
point clouds ability to grasp overall spatial information. However, many
existing LiDAR detection models depend on intricate feature transformation and
extraction processes, leading to poor real-time performance and high resource
consumption, which limits their practical effectiveness. In this work, we
propose a Faster LiDAR 3D object detection framework, called FASD, which
implements heterogeneous model distillation by adaptively uniform cross-model
voxel features. We aim to distill the transformer's capacity for
high-performance sequence modeling into Mamba models with low FLOPs, achieving
a significant improvement in accuracy through knowledge transfer. Specifically,
Dynamic Voxel Group and Adaptive Attention strategies are integrated into the
sparse backbone, creating a robust teacher model with scale-adaptive attention
for effective global visual context modeling. Following feature alignment with
the Adapter, we transfer knowledge from the Transformer to the Mamba through
latent space feature supervision and span-head distillation, resulting in
improved performance and an efficient student model. We evaluated the framework
on the Waymo and nuScenes datasets, achieving a 4x reduction in resource
consumption and a 1-2\% performance improvement over the current SoTA methods.


# Ideal flat and resolved SU(3) Landau levels in three dimensions

>Authors: Mian Peng,Qiang Wei,Jiale Yuan,Da-Wei Wang,Mou Yan,Han Cai,Gang Chen

>2024-09-16

> http://arxiv.org/abs/2409.10785v1

Landau levels (LLs) are of great importance for understanding the quantum
Hall effect and associated many-body physics. Recently, their three-dimensional
(3D) counterparts, i.e., dispersionless 3D LLs with well-defined quantum
numbers, have attracted significant attention but have not yet been reported.
Here we theoretically propose and experimentally observe 3D LLs with a sharply
quantized spectrum in a diamond acoustic lattice, where the eigenstates are
characterized by SU(3) quantum numbers. The engineered inhomogeneous hopping
strengths not only introduce pseudomagnetic fields that quantize the nodal
lines into LLs but also provide three bosonic degrees of freedom, embedding a
generic SU(3) symmetry into the LLs. Using a phased array of acoustic sources,
we selectively excite distinct eigenstates within the degenerate LL multiplets
and visualize their 3D eigenmodes. Importantly, our approach enables the
precise reconstruction of SU(3) quantum numbers directly from eigenmode
correlations. Our results establish SU(3) LLs as a tractable model in
artificial platforms, and pave the way for synthesizing LLs with zero
dispersion and countable quantum numbers in arbitrary dimensions.


# RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval

>Authors: Di Liu,Meng Chen,Baotong Lu,Huiqiang Jiang,Zhenhua Han,Qianxi Zhang,Qi Chen,Chengruidong Zhang,Bailu Ding,Kai Zhang,Chen Chen,Fan Yang,Yuqing Yang,Lili Qiu

>2024-09-16

> http://arxiv.org/abs/2409.10516v2

Transformer-based Large Language Models (LLMs) have become increasingly
important. However, due to the quadratic time complexity of attention
computation, scaling LLMs to longer contexts incurs extremely slow inference
latency and high GPU memory consumption for caching key-value (KV) vectors.
This paper proposes RetrievalAttention, a training-free approach to both
accelerate attention computation and reduce GPU memory consumption. By
leveraging the dynamic sparsity of attention mechanism, RetrievalAttention
proposes to use approximate nearest neighbor search (ANNS) indexes for KV
vectors in CPU memory and retrieves the most relevant ones with vector search
during generation. Unfortunately, we observe that the off-the-shelf ANNS
indexes are often ineffective for such retrieval tasks due to the
out-of-distribution (OOD) between query vectors and key vectors in attention
mechanism. RetrievalAttention addresses the OOD challenge by designing an
attention-aware vector search algorithm that can adapt to the distribution of
query vectors. Our evaluation shows that RetrievalAttention only needs to
access 1--3% of data while maintaining high model accuracy. This leads to
significant reduction in the inference cost of long-context LLMs with much
lower GPU memory footprint. In particular, RetrievalAttention only needs a
single NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B
parameters, which is capable of generating one token in 0.188 seconds.


# DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction

>Authors: John Wu,David Wu,Jimeng Sun

>2024-09-16

> http://arxiv.org/abs/2409.10504v1

Predicting high-dimensional or extreme multilabels, such as in medical
coding, requires both accuracy and interpretability. Existing works often rely
on local interpretability methods, failing to provide comprehensive
explanations of the overall mechanism behind each label prediction within a
multilabel set. We propose a mechanistic interpretability module called
DIctionary Label Attention (\method) that disentangles uninterpretable dense
embeddings into a sparse embedding space, where each nonzero element (a
dictionary feature) represents a globally learned medical concept. Through
human evaluations, we show that our sparse embeddings are more human
understandable than its dense counterparts by at least 50 percent. Our
automated dictionary feature identification pipeline, leveraging large language
models (LLMs), uncovers thousands of learned medical concepts by examining and
summarizing the highest activating tokens for each dictionary feature. We
represent the relationships between dictionary features and medical codes
through a sparse interpretable matrix, enhancing the mechanistic and global
understanding of the model's predictions while maintaining competitive
performance and scalability without extensive human annotation.


# CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios

>Authors: Luning Wang,Shiyao Li,Xuefei Ning,Zhihang Yuan,Shengen Yan,Guohao Dai,Yu Wang

>2024-09-16

> http://arxiv.org/abs/2409.10593v1

Large Language Models (LLMs) have been widely adopted to process long-context
tasks. However, the large memory overhead of the key-value (KV) cache poses
significant challenges in long-context scenarios. Existing training-free KV
cache compression methods typically focus on quantization and token pruning,
which have compression limits, and excessive sparsity can lead to severe
performance degradation. Other methods design new architectures with less KV
overhead but require significant training overhead. To address the above two
drawbacks, we further explore the redundancy in the channel dimension and apply
an architecture-level design with minor training costs. Therefore, we introduce
CSKV, a training-efficient Channel Shrinking technique for KV cache
compression: (1) We first analyze the singular value distribution of the KV
cache, revealing significant redundancy and compression potential along the
channel dimension. Based on this observation, we propose using low-rank
decomposition for key and value layers and storing the low-dimension features.
(2) To preserve model performance, we introduce a bi-branch KV cache, including
a window-based full-precision KV cache and a low-precision compressed KV cache.
(3) To reduce the training costs, we minimize the layer-wise reconstruction
loss for the compressed KV cache instead of retraining the entire LLMs.
Extensive experiments show that CSKV can reduce the memory overhead of the KV
cache by 80% while maintaining the model's long-context capability. Moreover,
we show that our method can be seamlessly combined with quantization to further
reduce the memory overhead, achieving a compression ratio of up to 95%.


# Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation

>Authors: Tianrui Song,Wenshuo Chao,Hao Liu

>2024-09-16

> http://arxiv.org/abs/2409.10343v1

Implicit feedback, often used to build recommender systems, unavoidably
confronts noise due to factors such as misclicks and position bias. Previous
studies have attempted to alleviate this by identifying noisy samples based on
their diverged patterns, such as higher loss values, and mitigating the noise
through sample dropping or reweighting. Despite the progress, we observe
existing approaches struggle to distinguish hard samples and noise samples, as
they often exhibit similar patterns, thereby limiting their effectiveness in
denoising recommendations. To address this challenge, we propose a Large
Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,
we construct an LLM-based scorer to evaluate the semantic consistency of items
with the user preference, which is quantified based on summarized historical
user interactions. The resulting scores are used to assess the hardness of
samples for the pointwise or pairwise training objectives. To ensure
efficiency, we introduce a variance-based sample pruning strategy to filter
potential hard samples before scoring. Besides, we propose an iterative
preference update module designed to continuously refine summarized user
preference, which may be biased due to false-positive user-item interactions.
Extensive experiments on three real-world datasets and four backbone
recommenders demonstrate the effectiveness of our approach.


# Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation

>Authors: Minghan Chen,Guikun Chen,Wenguan Wang,Yi Yang

>2024-09-16

> http://arxiv.org/abs/2409.10262v1

DETR introduces a simplified one-stage framework for scene graph generation
(SGG). However, DETR-based SGG models face two challenges: i) Sparse
supervision, as each image typically contains fewer than 10 relation
annotations, while the models employ over 100 relation queries. This sparsity
arises because each ground truth relation is assigned to only one single query
during training. ii) False negative samples, since one ground truth relation
may have multiple queries with similar matching scores. These suboptimally
matched queries are simply treated as negative samples, causing the loss of
valuable supervisory signals. As a response, we devise Hydra-SGG, a one-stage
SGG method that adopts a new Hybrid Relation Assignment. This assignment
combines a One-to-One Relation Assignment with a newly introduced IoU-based
One-to-Many Relation Assignment. Specifically, each ground truth is assigned to
multiple relation queries with high IoU subject-object boxes. This Hybrid
Relation Assignment increases the number of positive training samples,
alleviating sparse supervision. Moreover, we, for the first time, empirically
show that self-attention over relation queries helps reduce duplicated relation
predictions. We, therefore, propose Hydra Branch, a parameter-sharing auxiliary
decoder without a self-attention layer. This design promotes One-to-Many
Relation Assignment by enabling different queries to predict the same relation.
Hydra-SGG achieves state-of-the-art performance with 10.6 mR@20 and 16.0 mR@50
on VG150, while only requiring 12 training epochs. It also sets a new
state-of-the-art on Open Images V6 and and GQA.


# From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs

>Authors: Navya Jain,Zekun Wu,Cristian Munoz,Airlie Hilliard,Adriano Koshiyama,Emre Kazim,Philip Treleaven

>2024-09-16

> http://arxiv.org/abs/2409.10245v1

As the demand for human-like interactions with LLMs continues to grow, so
does the interest in manipulating their personality traits, which has emerged
as a key area of research. Methods like prompt-based In-Context Knowledge
Editing (IKE) and gradient-based Model Editor Networks (MEND) have been
explored but show irregularity and variability. IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLORA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,
despite their absence in the PEFT data. For instance, Llama-2-7B-chat generated
emojis in 99.5% of extraversion-related test instances, while
Mistral-8B-Instruct did so in 92.5% of openness-related test instances.
Explainability analysis indicated that the LLMs used emojis intentionally to
express these traits. This paper provides a number of novel contributions.
First, introducing an Opinion QA dataset for PEFT-driven personality
manipulation; second, developing metric models to benchmark LLM personality
traits; third, demonstrating PEFT's superiority over IKE in personality
manipulation; and finally, analyzing and validating emoji usage through
explainability methods such as mechanistic interpretability and in-context
learning explainability methods.


# Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models

>Authors: Weihao Ye,Qiong Wu,Wenhao Lin,Yiyi Zhou

>2024-09-16

> http://arxiv.org/abs/2409.10197v1

Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.


# Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving

>Authors: Yunsheng Ma,Amr Abdelraouf,Rohit Gupta,Ziran Wang,Kyungtae Han

>2024-09-16

> http://arxiv.org/abs/2409.11182v1

Multimodal large language models (MLLMs) have demonstrated remarkable
potential for enhancing scene understanding in autonomous driving systems
through powerful logical reasoning capabilities. However, the deployment of
these models faces significant challenges due to their substantial parameter
sizes and computational demands, which often exceed the constraints of onboard
computation. One major limitation arises from the large number of visual tokens
required to capture fine-grained and long-context visual information, leading
to increased latency and memory consumption. To address this issue, we propose
Video Token Sparsification (VTS), a novel approach that leverages the inherent
redundancy in consecutive video frames to significantly reduce the total number
of visual tokens while preserving the most salient information. VTS employs a
lightweight CNN-based proposal model to adaptively identify key frames and
prune less informative tokens, effectively mitigating hallucinations and
increasing inference throughput without compromising performance. We conduct
comprehensive experiments on the DRAMA and LingoQA benchmarks, demonstrating
the effectiveness of VTS in achieving up to a 33\% improvement in inference
throughput and a 28\% reduction in memory usage compared to the baseline
without compromising performance.


# Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens

>Authors: Joseph Clinton,Robert Lieck

>2024-09-14

> http://arxiv.org/abs/2409.09513v1

Supervised learning approaches to offline reinforcement learning,
particularly those utilizing the Decision Transformer, have shown effectiveness
in continuous environments and for sparse rewards. However, they often struggle
with long-horizon tasks due to the high compounding error of auto-regressive
models. To overcome this limitation, we go beyond next-token prediction and
introduce Planning Tokens, which contain high-level, long time-scale
information about the agent's future. Predicting dual time-scale tokens at
regular intervals enables our model to use these long-horizon Planning Tokens
as a form of implicit planning to guide its low-level policy and reduce
compounding error. This architectural modification significantly enhances
performance on long-horizon tasks, establishing a new state-of-the-art in
complex D4RL environments. Additionally, we demonstrate that Planning Tokens
improve the interpretability of the model's policy through the interpretable
plan visualisations and attention map.


# Pathfinder for Low-altitude Aircraft with Binary Neural Network

>Authors: Kaijie Yin,Tian Gao,Hui Kong

>2024-09-13

> http://arxiv.org/abs/2409.08824v1

A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the
performance of autonomous mapping by a ground mobile robot. However, the prior
map is usually incomplete due to lacking labeling in partial paths. To solve
this problem, this paper proposes an OSM maker using airborne sensors carried
by low-altitude aircraft, where the core of the OSM maker is a novel efficient
pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream
road segmentation model. Specifically, a multi-scale feature extraction based
on the UNet architecture is implemented for images and point clouds. To reduce
the effect caused by the sparsity of point cloud, an attention-guided gated
block is designed to integrate image and point-cloud features. For enhancing
the efficiency of the model, we propose a binarization streamline to each model
component, including a variant of vision transformer (ViT) architecture as the
encoder of the image branch, and new focal and perception losses to optimize
the model training. The experimental results on two datasets demonstrate that
our pathfinder method achieves SOTA accuracy with high efficiency in finding
paths from the low-level airborne sensors, and we can create complete OSM prior
maps based on the segmented road skeletons. Code and data are available
at:https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder.


# Dynamic Link and Flow Prediction in Bank Transfer Networks

>Authors: Shu Takahashi,Kento Yamamoto,Shumpei Kobayashi,Ryoma Kondo,Ryohei Hisano

>2024-09-13

> http://arxiv.org/abs/2409.08718v1

The prediction of both the existence and weight of network links at future
time points is essential as complex networks evolve over time. Traditional
methods, such as vector autoregression and factor models, have been applied to
small, dense networks, but become computationally impractical for large-scale,
sparse, and complex networks. Some machine learning models address dynamic link
prediction, but few address the simultaneous prediction of both link presence
and weight. Therefore, we introduce a novel model that dynamically predicts
link presence and weight by dividing the task into two sub-tasks: predicting
remittance ratios and forecasting the total remittance volume. We use a
self-attention mechanism that combines temporal-topological neighborhood
features to predict remittance ratios and use a separate model to forecast the
total remittance volume. We achieve the final prediction by multiplying the
outputs of these models. We validated our approach using two real-world
datasets: a cryptocurrency network and bank transfer network.


# LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs

>Authors: Han Xu,Yutong Li,Shihao Ji

>2024-09-12

> http://arxiv.org/abs/2409.11424v1

Large language models (LLMs) have demonstrated remarkable abilities in
natural language processing. However, their deployment on resource-constrained
embedded devices remains difficult due to memory and computational demands. In
this paper, we present an FPGA-based accelerator designed to improve LLM
inference performance on embedded FPGAs. We employ post-training quantization
to reduce model size and optimize for off-chip memory bandwidth. Our design
features asynchronous computation and a fully pipelined accelerator for
matrix-vector multiplication. Experiments of the TinyLlama 1.1B model on a
Xilinx ZCU102 platform show a 14.3-15.8x speedup and a 6.1x power efficiency
improvement over running exclusively on ZCU102 processing system (PS).


# SDformer: Efficient End-to-End Transformer for Depth Completion

>Authors: Jian Qian,Miao Sun,Ashley Lee,Jie Li,Shenglong Zhuo,Patrick Yin Chiang

>2024-09-12

> http://arxiv.org/abs/2409.08159v1

Depth completion aims to predict dense depth maps with sparse depth
measurements from a depth sensor. Currently, Convolutional Neural Network (CNN)
based models are the most popular methods applied to depth completion tasks.
However, despite the excellent high-end performance, they suffer from a limited
representation area. To overcome the drawbacks of CNNs, a more effective and
powerful method has been presented: the Transformer, which is an adaptive
self-attention setting sequence-to-sequence model. While the standard
Transformer quadratically increases the computational cost from the key-query
dot-product of input resolution which improperly employs depth completion
tasks. In this work, we propose a different window-based Transformer
architecture for depth completion tasks named Sparse-to-Dense Transformer
(SDformer). The network consists of an input module for the depth map and RGB
image features extraction and concatenation, a U-shaped encoder-decoder
Transformer for extracting deep features, and a refinement module.
Specifically, we first concatenate the depth map features with the RGB image
features through the input model. Then, instead of calculating self-attention
with the whole feature maps, we apply different window sizes to extract the
long-range depth dependencies. Finally, we refine the predicted features from
the input module and the U-shaped encoder-decoder Transformer module to get the
enriching depth features and employ a convolution layer to obtain the dense
depth map. In practice, the SDformer obtains state-of-the-art results against
the CNN-based depth completion models with lower computing loads and parameters
on the NYU Depth V2 and KITTI DC datasets.


# Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG

>Authors: Gabriel de Souza P. Moreira,Ronay Ak,Benedikt Schifferer,Mengyao Xu,Radek Osmulski,Even Oldridge

>2024-09-12

> http://arxiv.org/abs/2409.07691v1

Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.


# STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM

>Authors: Qijiong Liu,Jieming Zhu,Lu Fan,Zhou Zhao,Xiao-Ming Wu

>2024-09-11

> http://arxiv.org/abs/2409.07276v2

Traditional recommendation models often rely on unique item identifiers (IDs)
to distinguish between items, which can hinder their ability to effectively
leverage item content information and generalize to long-tail or cold-start
items. Recently, semantic tokenization has been proposed as a promising
solution that aims to tokenize each item's semantic representation into a
sequence of discrete tokens. In this way, it preserves the item's semantics
within these tokens and ensures that semantically similar items are represented
by similar tokens. These semantic tokens have become fundamental in training
generative recommendation models. However, existing generative recommendation
methods typically involve multiple sub-models for embedding, quantization, and
recommendation, leading to an overly complex system. In this paper, we propose
to streamline the semantic tokenization and generative recommendation process
with a unified framework, dubbed STORE, which leverages a single large language
model (LLM) for both tasks. Specifically, we formulate semantic tokenization as
a text-to-token task and generative recommendation as a token-to-token task,
supplemented by a token-to-text reconstruction task and a text-to-token
auxiliary task. All these tasks are framed in a generative manner and trained
using a single LLM backbone. Extensive experiments have been conducted to
validate the effectiveness of our STORE framework across various recommendation
tasks and datasets. We will release the source code and configurations for
reproducible research.


# RICAU-Net: Residual-block Inspired Coordinate Attention U-Net for Segmentation of Small and Sparse Calcium Lesions in Cardiac CT

>Authors: Doyoung Park,Jinsoo Kim,Qi Chang,Shuang Leng,Liang Zhong,Lohendran Baskaran

>2024-09-11

> http://arxiv.org/abs/2409.06993v1

The Agatston score, which is the sum of the calcification in the four main
coronary arteries, has been widely used in the diagnosis of coronary artery
disease (CAD). However, many studies have emphasized the importance of the
vessel-specific Agatston score, as calcification in a specific vessel is
significantly correlated with the occurrence of coronary heart disease (CHD).
In this paper, we propose the Residual-block Inspired Coordinate Attention
U-Net (RICAU-Net), which incorporates coordinate attention in two distinct
manners and a customized combo loss function for lesion-specific coronary
artery calcium (CAC) segmentation. This approach aims to tackle the high
class-imbalance issue associated with small and sparse lesions, particularly
for CAC in the left main coronary artery (LM) which is generally small and the
scarcest in the dataset due to its anatomical structure. The proposed method
was compared with six different methods using Dice score, precision, and
recall. Our approach achieved the highest per-lesion Dice scores for all four
lesions, especially for CAC in LM compared to other methods. The ablation
studies demonstrated the significance of positional information from the
coordinate attention and the customized loss function in segmenting small and
sparse lesions with a high class-imbalance problem.


# LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for Heterogeneous Graphs

>Authors: Siqing Li,Jin-Duk Park,Wei Huang,Xin Cao,Won-Yong Shin,Zhiqiang Xu

>2024-09-10

> http://arxiv.org/abs/2409.06323v1

Heterogeneous graph neural networks (HGNNs) have significantly propelled the
information retrieval (IR) field. Still, the effectiveness of HGNNs heavily
relies on high-quality labels, which are often expensive to acquire. This
challenge has shifted attention towards Heterogeneous Graph Contrastive
Learning (HGCL), which usually requires pre-defined meta-paths. However, our
findings reveal that meta-path combinations significantly affect performance in
unsupervised settings, an aspect often overlooked in current literature.
Existing HGCL methods have considerable variability in outcomes across
different meta-path combinations, thereby challenging the optimization process
to achieve consistent and high performance. In response, we introduce
\textsf{LAMP} (\underline{\textbf{L}}earn\underline{\textbf{A}}ble
\underline{\textbf{M}}eta-\underline{\textbf{P}}ath), a novel adversarial
contrastive learning approach that integrates various meta-path sub-graphs into
a unified and stable structure, leveraging the overlap among these sub-graphs.
To address the denseness of this integrated sub-graph, we propose an
adversarial training strategy for edge pruning, maintaining sparsity to enhance
model performance and robustness. \textsf{LAMP} aims to maximize the difference
between meta-path and network schema views for guiding contrastive learning to
capture the most meaningful information. Our extensive experimental study
conducted on four diverse datasets from the Heterogeneous Graph Benchmark (HGB)
demonstrates that \textsf{LAMP} significantly outperforms existing
state-of-the-art unsupervised models in terms of accuracy and robustness.


# STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning

>Authors: Jaeseong Lee,seung-won hwang,Aurick Qiao,Daniel F Campos,Zhewei Yao,Yuxiong He

>2024-09-10

> http://arxiv.org/abs/2409.06211v1

Mixture-of-experts (MoEs) have been adopted for reducing inference costs by
sparsely activating experts in Large language models (LLMs). Despite this
reduction, the massive number of experts in MoEs still makes them expensive to
serve. In this paper, we study how to address this, by pruning MoEs. Among
pruning methodologies, unstructured pruning has been known to achieve the
highest performance for a given pruning ratio, compared to structured pruning,
since the latter imposes constraints on the sparsification structure. This is
intuitive, as the solution space of unstructured pruning subsumes that of
structured pruning. However, our counterintuitive finding reveals that expert
pruning, a form of structured pruning, can actually precede unstructured
pruning to outperform unstructured-only pruning. As existing expert pruning,
requiring $O(\frac{k^n}{\sqrt{n}})$ forward passes for $n$ experts, cannot
scale for recent MoEs, we propose a scalable alternative with $O(1)$
complexity, yet outperforming the more expensive methods. The key idea is
leveraging a latent structure between experts, based on behavior similarity,
such that the greedy decision of whether to prune closely captures the joint
pruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized
MoE with 128 experts, our method needs only one H100 and two hours to achieve
nearly no loss in performance with 40% sparsity, even in generative tasks such
as GSM8K, where state-of-the-art unstructured pruning fails to. The code will
be made publicly available.


# AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration

>Authors: Hongyi Cai,Mohammad Mahdinur Rahman,Mohammad Shahid Akhtar,Jie Li,Jingyu Wu,Zhili Fang

>2024-09-10

> http://arxiv.org/abs/2409.06206v1

Image Transformers show a magnificent success in Image Restoration tasks.
Nevertheless, most of transformer-based models are strictly bounded by
exorbitant memory occupancy. Our goal is to reduce the memory consumption of
Swin Transformer and at the same time speed up the model during training
process. Thus, we introduce AgileIR, group shifted attention mechanism along
with window attention, which sparsely simplifies the model in architecture. We
propose Group Shifted Window Attention (GSWA) to decompose Shift Window
Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA)
into groups across their attention heads, contributing to shrinking memory
usage in back propagation. In addition to that, we keep shifted window masking
and its shifted learnable biases during training, in order to induce the model
interacting across windows within the channel. We also re-allocate projection
parameters to accelerate attention matrix calculation, which we found a
negligible decrease in performance. As a result of experiment, compared with
our baseline SwinIR and other efficient quantization models, AgileIR keeps the
performance still at 32.20 dB on Set5 evaluation dataset, exceeding other
methods with tailor-made efficient methods and saves over 50% memory while a
large batch size is employed.


# MCDGLN: Masked Connection-based Dynamic Graph Learning Network for Autism Spectrum Disorder

>Authors: Peng Wang,Xin Wen,Ruochen Cao,Chengxin Gao,Yanrong Hao,Rui Cao

>2024-09-10

> http://arxiv.org/abs/2409.06163v1

Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized
by complex physiological processes. Previous research has predominantly focused
on static cerebral interactions, often neglecting the brain's dynamic nature
and the challenges posed by network noise. To address these gaps, we introduce
the Masked Connection-based Dynamic Graph Learning Network (MCDGLN). Our
approach first segments BOLD signals using sliding temporal windows to capture
dynamic brain characteristics. We then employ a specialized weighted edge
aggregation (WEA) module, which uses the cross convolution with channel-wise
element-wise convolutional kernel, to integrate dynamic functional connectivity
and to isolating task-relevant connections. This is followed by topological
feature extraction via a hierarchical graph convolutional network (HGCN), with
key attributes highlighted by a self-attention module. Crucially, we refine
static functional connections using a customized task-specific mask, reducing
noise and pruning irrelevant links. The attention-based connection encoder
(ACE) then enhances critical connections and compresses static features. The
combined features are subsequently used for classification. Applied to the
Autism Brain Imaging Data Exchange I (ABIDE I) dataset, our framework achieves
a 73.3\% classification accuracy between ASD and Typical Control (TC) groups
among 1,035 subjects. The pivotal roles of WEA and ACE in refining connectivity
and enhancing classification accuracy underscore their importance in capturing
ASD-specific features, offering new insights into the disorder.


# Optical Spiking Neurons Enable High-Speed and Energy-Efficient Optical Neural Networks

>Authors: Bo Xu,Zefeng Huang,Yuetong Fang,Xin Wang,Bojun Cheng,Shaoliang Yu,Zhongrui Wang,Renjing Xu

>2024-09-09

> http://arxiv.org/abs/2409.05726v1

Optical neural networks (ONNs) perform extensive computations using photons
instead of electrons, resulting in passively energy-efficient and low-latency
computing. Among various ONNs, the diffractive optical neural networks (DONNs)
particularly excel in energy efficiency, bandwidth, and parallelism, therefore
attract considerable attention. However, their performance is limited by the
inherent constraints of traditional frame-based sensors, which process and
produce dense and redundant information at low operating frequency. Inspired by
the spiking neurons in human neural system, which utilize a thresholding
mechanism to transmit information sparsely and efficiently, we propose
integrating a threshold-locking method into neuromorphic vision sensors to
generate sparse and binary information, achieving microsecond-level accurate
perception similar to human spiking neurons. By introducing novel Binary Dual
Adaptive Training (BAT) and Optically Parallel Mixture of Experts (OPMoE)
inference methods, the high-speed, spike-based diffractive optical neural
network (S2NN) demonstrates an ultra-fast operating speed of 3649 FPS, which is
30 fold faster than that of reported DONNs, delivering a remarkable
computational speed of 417.96 TOPS and a system energy efficiency of 12.6
TOPS/W. Our work demonstrates the potential of incorporating neuromorphic
architecture to facilitate optical neural network applications in real-world
scenarios for both low-level and high-level machine vision tasks.


# RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network

>Authors: Zhiwei Lin,Zhe Liu,Yongtao Wang,Le Zhang,Ce Zhu

>2024-09-08

> http://arxiv.org/abs/2409.04979v1

Perceiving the surrounding environment is a fundamental task in autonomous
driving. To obtain highly accurate perception results, modern autonomous
driving systems typically employ multi-modal sensors to collect comprehensive
environmental data. Among these, the radar-camera multi-modal perception system
is especially favored for its excellent sensing capabilities and
cost-effectiveness. However, the substantial modality differences between radar
and camera sensors pose challenges in fusing information. To address this
problem, this paper presents RCBEVDet, a radar-camera fusion 3D object
detection framework. Specifically, RCBEVDet is developed from an existing
camera-based 3D object detector, supplemented by a specially designed radar
feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF)
module. Firstly, RadarBEVNet encodes sparse radar points into a dense
bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar
Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a
deformable attention mechanism to align radar and camera BEV features and
adopts channel and spatial fusion layers to fuse them. To further enhance
RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF
through sparse fusion, supports query-based multi-view camera perception
models, and adapts to a broader range of perception tasks. Extensive
experiments on the nuScenes show that our method integrates seamlessly with
existing camera-based 3D perception models and improves their performance
across various perception tasks. Furthermore, our method achieves
state-of-the-art radar-camera fusion results in 3D object detection, BEV
semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L
as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object
detection without test-time augmentation or model ensembling.


# An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing

>Authors: Ashkan Moradifirouzabadi,Divya Sri Dodla,Mingu Kang

>2024-09-08

> http://arxiv.org/abs/2409.04940v1

The attention mechanism is a key computing kernel of Transformers,
calculating pairwise correlations across the entire input sequence. The
computing complexity and frequent memory access in computing self-attention put
a huge burden on the system especially when the sequence length increases. This
paper presents an analog and digital hybrid processor to accelerate the
attention mechanism for transformers in 65nm CMOS technology. We propose an
analog computing-in-memory (CIM) core, which prunes ~75% of low-score tokens on
average during runtime at ultra-low power and delay. Additionally, a digital
processor performs precise computations only for ~25% unpruned tokens selected
by the analog CIM core, preventing accuracy degradation. Measured results show
peak energy efficiency of 14.8 and 1.65 TOPS/W, and peak area efficiency of
976.6 and 79.4 GOPS/mm$^\mathrm{2}$ in the analog core and the system-on-chip
(SoC), respectively.


# Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in Large-Scale Environments

>Authors: Rodrigo Pérez-Dattari,Zhaoting Li,Robert Babuška,Jens Kober,Cosimo Della Santina

>2024-09-07

> http://arxiv.org/abs/2409.04775v2

Planning methods struggle with computational intractability in solving
task-level problems in large-scale environments. This work explores leveraging
the commonsense knowledge encoded in LLMs to empower planning techniques to
deal with these complex scenarios. We achieve this by efficiently using LLMs to
prune irrelevant components from the planning problem's state space,
substantially simplifying its complexity. We demonstrate the efficacy of this
system through extensive experiments within a household simulation environment,
alongside real-world validation using a 7-DoF manipulator (video
https://youtu.be/6ro2UOtOQS4).


# LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs

>Authors: Yongxin Deng,Xihe Qiu,Xiaoyu Tan,Wei Chu,Yinghui Xu

>2024-09-07

> http://arxiv.org/abs/2409.04744v1

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.


# IR2: Implicit Rendezvous for Robotic Exploration Teams under Sparse Intermittent Connectivity

>Authors: Derek Ming Siang Tan,Yixiao Ma,Jingsong Liang,Yi Cheng Chng,Yuhong Cao,Guillaume Sartoretti

>2024-09-07

> http://arxiv.org/abs/2409.04730v1

Information sharing is critical in time-sensitive and realistic multi-robot
exploration, especially for smaller robotic teams in large-scale environments
where connectivity may be sparse and intermittent. Existing methods often
overlook such communication constraints by assuming unrealistic global
connectivity. Other works account for communication constraints (by maintaining
close proximity or line of sight during information exchange), but are often
inefficient. For instance, preplanned rendezvous approaches typically involve
unnecessary detours resulting from poorly timed rendezvous, while pursuit-based
approaches often result in short-sighted decisions due to their greedy nature.
We present IR2, a deep reinforcement learning approach to information sharing
for multi-robot exploration. Leveraging attention-based neural networks trained
via reinforcement and curriculum learning, IR2 allows robots to effectively
reason about the longer-term trade-offs between disconnecting for solo
exploration and reconnecting for information sharing. In addition, we propose a
hierarchical graph formulation to maintain a sparse yet informative graph,
enabling our approach to scale to large-scale environments. We present
simulation results in three large-scale Gazebo environments, which show that
our approach yields 6.6-34.1% shorter exploration paths and significantly
improved mapped area consistency among robots when compared to state-of-the-art
baselines. Our simulation training and testing code is available at
https://github.com/marmotlab/IR2.


# Sparse Rewards Can Self-Train Dialogue Agents

>Authors: Barrett Martin Lattimer,Varun Gangal,Ryan McDonald,Yi Yang

>2024-09-06

> http://arxiv.org/abs/2409.04617v1

Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)
agents, especially in multi-turn dialogue tasks, have been primarily driven by
supervised fine-tuning and high-quality human feedback. However, as base LLM
models continue to improve, acquiring meaningful human feedback has become
increasingly challenging and costly. In certain domains, base LLM agents may
eventually exceed human capabilities, making traditional feedback-driven
methods impractical. In this paper, we introduce a novel self-improvement
paradigm that empowers LLM agents to autonomously enhance their performance
without external human feedback. Our method, Juxtaposed Outcomes for Simulation
Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward
simulation environment to extract ideal behaviors and further train the LLM on
its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation
environment derived from MultiWOZ. We demonstrate that models trained with
JOSH, both small and frontier, significantly improve tool-based interactions
while preserving general model capabilities across diverse benchmarks. Our code
and data are publicly available on GitHub.


# SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation

>Authors: Yi Tian,Juan Andrade-Cetto

>2024-09-06

> http://arxiv.org/abs/2409.04082v1

Event cameras generate asynchronous and sparse event streams capturing
changes in light intensity. They offer significant advantages over conventional
frame-based cameras, such as a higher dynamic range and an extremely faster
data rate, making them particularly useful in scenarios involving fast motion
or challenging lighting conditions. Spiking neural networks (SNNs) share
similar asynchronous and sparse characteristics and are well-suited for
processing data from event cameras. Inspired by the potential of transformers
and spike-driven transformers (spikeformers) in other computer vision tasks, we
propose two solutions for fast and robust optical flow estimation for event
cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial
neural network (ANN) architecture with spatiotemporal shifted window
self-attention (swin) transformer encoders, while SDformerFlow presents its
fully spiking counterpart, incorporating swin spikeformer encoders.
Furthermore, we present two variants of the spiking version with different
neuron models. Our work is the first to make use of spikeformers for dense
optical flow estimation. We conduct end-to-end training for all models using
supervised learning. Our results yield state-of-the-art performance among
SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and
show significant reduction in power consumption compared to the equivalent
ANNs.


# Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task

>Authors: Jing Wang,Ao Ma,Jiasong Feng,Dawei Leng,Yuhui Yin,Xiaodan Liang

>2024-09-06

> http://arxiv.org/abs/2409.04005v1

The global self-attention mechanism in diffusion transformers involves
redundant computation due to the sparse and redundant nature of visual
information, and the attention map of tokens within a spatial window shows
significant similarity. To address this redundancy, we propose the Proxy Token
Diffusion Transformer (PT-DiT), which employs sparse representative token
attention (where the number of representative tokens is much smaller than the
total number of tokens) to model global visual information efficiently.
Specifically, in each transformer block, we randomly sample one token from each
spatial-temporal window to serve as a proxy token for that region. The global
semantics are captured through the self-attention of these proxy tokens and
then injected into all latent tokens via cross-attention. Simultaneously, we
introduce window and shift window attention to address the limitations in
detail modeling caused by the sparse attention mechanism. Building on the
well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a
variety of models for T2I, T2V, and T2MV tasks. Experimental results show that
PT-DiT achieves competitive performance while reducing the computational
complexity in both image and video generation tasks (e.g., a 48% reduction
compared to DiT and a 35% reduction compared to Pixart-alpha). Our source code
is available at https://github.com/360CVGroup/Qihoo-T2X.


# OPAL: Outlier-Preserved Microscaling Quantization A ccelerator for Generative Large Language Models

>Authors: Jahyun Koo,Dahoon Park,Sangwoo Jung,Jaeha Kung

>2024-09-06

> http://arxiv.org/abs/2409.05902v1

To overcome the burden on the memory size and bandwidth due to
ever-increasing size of large language models (LLMs), aggressive weight
quantization has been recently studied, while lacking research on quantizing
activations. In this paper, we present a hardware-software co-design method
that results in an energy-efficient LLM accelerator, named OPAL, for generation
tasks. First of all, a novel activation quantization method that leverages the
microscaling data format while preserving several outliers per sub-tensor block
(e.g., four out of 128 elements) is proposed. Second, on top of preserving
outliers, mixed precision is utilized that sets 5-bit for inputs to sensitive
layers in the decoder block of an LLM, while keeping inputs to less sensitive
layers to 3-bit. Finally, we present the OPAL hardware architecture that
consists of FP units for handling outliers and vectorized INT multipliers for
dominant non-outlier related operations. In addition, OPAL uses log2-based
approximation on softmax operations that only requires shift and subtraction to
maximize power efficiency. As a result, we are able to improve the energy
efficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible
accuracy loss, i.e., <1 perplexity increase.


# Sirius: Contextual Sparsity with Correction for Efficient LLMs

>Authors: Yang Zhou,Zhuoming Chen,Zhaozhuo Xu,Victoria Lin,Beidi Chen

>2024-09-05

> http://arxiv.org/abs/2409.03856v1

With the blossom of large language models (LLMs), inference efficiency
becomes increasingly important. Various approximation methods are proposed to
reduce the cost at inference time. Contextual Sparsity (CS) is appealing for
its training-free nature and its ability to reach a higher compression ratio
seemingly without quality degradation. However, after a comprehensive
evaluation of contextual sparsity methods on various complex generation tasks,
we find that although CS succeeds in prompt-understanding tasks, CS
significantly degrades the model performance for reasoning, deduction, and
knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that
sparse models often share general problem-solving logic and require only a few
token corrections to recover the original model performance. This paper
introduces Sirius, an efficient correction mechanism, which significantly
recovers CS models quality on reasoning tasks while maintaining its efficiency
gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in
reasoning, math, and coding and shows consistent effectiveness and efficiency.
Also, we carefully develop a system implementation for Sirius and show that
Sirius achieves roughly 20% reduction in latency for 8B model on-chip and 35%
reduction for 70B model offloading. We open-source our implementation of Sirius
at https://github.com/Infini-AI-Lab/Sirius.git.


# LLM-CI: Assessing Contextual Integrity Norms in Language Models

>Authors: Yan Shvartzshnaider,Vasisht Duddu,John Lacalamita

>2024-09-05

> http://arxiv.org/abs/2409.03735v1

Large language models (LLMs), while memorizing parts of their training data
scraped from the Internet, may also inadvertently encode societal preferences
and norms. As these models are integrated into sociotechnical systems, it is
crucial that the norms they encode align with societal expectations. These
norms could vary across models, hyperparameters, optimization techniques, and
datasets. This is especially challenging due to prompt sensitivity$-$small
variations in prompts yield different responses, rendering existing assessment
methodologies unreliable. There is a need for a comprehensive framework
covering various models, optimization, and datasets, along with a reliable
methodology to assess encoded norms.
  We present LLM-CI, the first open-sourced framework to assess privacy norms
encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette
methodology to assess the encoded norms across different contexts and LLMs. We
propose the multi-prompt assessment methodology to address prompt sensitivity
by assessing the norms from only the prompts that yield consistent responses
across multiple variants. Using LLM-CI and our proposed methodology, we
comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior
work, examining the impact of model properties (e.g., hyperparameters,
capacity) and optimization strategies (e.g., alignment, quantization).


# How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data

>Authors: Yejie Wang,Keqing He,Dayuan Fu,Zhuoma Gongque,Heyang Xu,Yanxu Chen,Zhexu Wang,Yujia Fu,Guanting Dong,Muxi Diao,Jingang Wang,Mengdi Zhang,Xunliang Cai,Weiran Xu

>2024-09-05

> http://arxiv.org/abs/2409.03810v1

Recently, there has been a growing interest in studying how to construct
better code instruction tuning data. However, we observe Code models trained
with these datasets exhibit high performance on HumanEval but perform worse on
other benchmarks such as LiveCodeBench. Upon further investigation, we find
that many datasets suffer from severe data leakage. After cleaning up most of
the leaked data, some well-known high-quality datasets perform poorly. This
discovery reveals a new challenge: identifying which dataset genuinely qualify
as high-quality code instruction data. To address this, we propose an efficient
code data pruning strategy for selecting good samples. Our approach is based on
three dimensions: instruction complexity, response quality, and instruction
diversity. Based on our selected data, we present XCoder, a family of models
finetuned from LLaMA3. Our experiments show XCoder achieves new
state-of-the-art performance using fewer training data, which verify the
effectiveness of our data strategy. Moreover, we perform a comprehensive
analysis on the data composition and find existing code datasets have different
characteristics according to their construction methods, which provide new
insights for future code LLMs. Our models and dataset are released in
https://github.com/banksy23/XCoder


# Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction

>Authors: Rui Peng,Shihe Shen,Kaiqiang Xiong,Huachen Gao,Jianbo Jiao,Xiaodong Gu,Ronggang Wang

>2024-09-05

> http://arxiv.org/abs/2409.03634v1

Reconstructing the high-fidelity surface from multi-view images, especially
sparse images, is a critical and practical task that has attracted widespread
attention in recent years. However, existing methods are impeded by the memory
constraint or the requirement of ground-truth depths and cannot recover
satisfactory geometric details. To this end, we propose SuRF, a new
Surface-centric framework that incorporates a new Region sparsification based
on a matching Field, achieving good trade-offs between performance, efficiency
and scalability. To our knowledge, this is the first unsupervised method
achieving end-to-end sparsification powered by the introduced matching field,
which leverages the weight distribution to efficiently locate the boundary
regions containing surface. Instead of predicting an SDF value for each voxel,
we present a new region sparsification approach to sparse the volume by judging
whether the voxel is inside the surface region. In this way, our model can
exploit higher frequency features around the surface with less memory and
computational consumption. Extensive experiments on multiple benchmarks
containing complex large-scale scenes show that our reconstructions exhibit
high-quality details and achieve new state-of-the-art performance, i.e., 46%
improvements with 80% less memory consumption. Code is available at
https://github.com/prstrive/SuRF.


# HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual POI Retrieval at Baidu Maps

>Authors: Jizhou Huang,Haifeng Wang,Yibo Sun,Miao Fan,Zhengjie Huang,Chunyuan Yuan,Yawen Li

>2024-09-05

> http://arxiv.org/abs/2409.03504v1

The increasing interest in international travel has raised the demand of
retrieving point of interests in multiple languages. This is even superior to
find local venues such as restaurants and scenic spots in unfamiliar languages
when traveling abroad. Multilingual POI retrieval, enabling users to find
desired POIs in a demanded language using queries in numerous languages, has
become an indispensable feature of today's global map applications such as
Baidu Maps. This task is non-trivial because of two key challenges: (1)
visiting sparsity and (2) multilingual query-POI matching. To this end, we
propose a Heterogeneous Graph Attention Matching Network (HGAMN) to
concurrently address both challenges. Specifically, we construct a
heterogeneous graph that contains two types of nodes: POI node and query node
using the search logs of Baidu Maps. To alleviate challenge \#1, we construct
edges between different POI nodes to link the low-frequency POIs with the
high-frequency ones, which enables the transfer of knowledge from the latter to
the former. To mitigate challenge \#2, we construct edges between POI and query
nodes based on the co-occurrences between queries and POIs, where queries in
different languages and formulations can be aggregated for individual POIs.
Moreover, we develop an attention-based network to jointly learn node
representations of the heterogeneous graph and further design a cross-attention
module to fuse the representations of both types of nodes for query-POI
relevance scoring. Extensive experiments conducted on large-scale real-world
datasets from Baidu Maps demonstrate the superiority and effectiveness of
HGAMN. In addition, HGAMN has already been deployed in production at Baidu
Maps, and it successfully keeps serving hundreds of millions of requests every
day.


# OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving

>Authors: Julong Wei,Shanshuai Yuan,Pengfei Li,Qingda Hu,Zhongxue Gan,Wenchao Ding

>2024-09-05

> http://arxiv.org/abs/2409.03272v1

The rise of multi-modal large language models(MLLMs) has spurred their
applications in autonomous driving. Recent MLLM-based methods perform action by
learning a direct mapping from perception to action, neglecting the dynamics of
the world and the relations between action and world dynamics. In contrast,
human beings possess world model that enables them to simulate the future
states based on 3D internal visual representation and plan actions accordingly.
To this end, we propose OccLLaMA, an occupancy-language-action generative world
model, which uses semantic occupancy as a general visual representation and
unifies vision-language-action(VLA) modalities through an autoregressive model.
Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently
discretize and reconstruct semantic occupancy scenes, considering its sparsity
and classes imbalance. Then, we build a unified multi-modal vocabulary for
vision, language and action. Furthermore, we enhance LLM, specifically LLaMA,
to perform the next token/scene prediction on the unified vocabulary to
complete multiple tasks in autonomous driving. Extensive experiments
demonstrate that OccLLaMA achieves competitive performance across multiple
tasks, including 4D occupancy forecasting, motion planning, and visual question
answering, showcasing its potential as a foundation model in autonomous
driving.


# Regularized Multi-output Gaussian Convolution Process with Domain Adaptation

>Authors: Wang Xinming,Wang Chao,Song Xuan,Kirby Levi,Wu Jianguo

>2024-09-04

> http://arxiv.org/abs/2409.02778v1

Multi-output Gaussian process (MGP) has been attracting increasing attention
as a transfer learning method to model multiple outputs. Despite its high
flexibility and generality, MGP still faces two critical challenges when
applied to transfer learning. The first one is negative transfer, which occurs
when there exists no shared information among the outputs. The second challenge
is the input domain inconsistency, which is commonly studied in transfer
learning yet not explored in MGP. In this paper, we propose a regularized MGP
modeling framework with domain adaptation to overcome these challenges. More
specifically, a sparse covariance matrix of MGP is proposed by using
convolution process, where penalization terms are added to adaptively select
the most informative outputs for knowledge transfer. To deal with the domain
inconsistency, a domain adaptation method is proposed by marginalizing
inconsistent features and expanding missing features to align the input domains
among different outputs. Statistical properties of the proposed method are
provided to guarantee the performance practically and asymptotically. The
proposed framework outperforms state-of-the-art benchmarks in comprehensive
simulation studies and one real case study of a ceramic manufacturing process.
The results demonstrate the effectiveness of our method in dealing with both
the negative transfer and the domain inconsistency.


# Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations

>Authors: Chunyan An,Yunhan Li,Qiang Yang,Winston K. G. Seah,Zhixu Li,Conghao Yang

>2024-09-04

> http://arxiv.org/abs/2409.02702v2

Session-based Social Recommendation (SSR) leverages social relationships
within online networks to enhance the performance of Session-based
Recommendation (SR). However, existing SSR algorithms often encounter the
challenge of "friend data sparsity". Moreover, significant discrepancies can
exist between the purchase preferences of social network friends and those of
the target user, reducing the influence of friends relative to the target
user's own preferences. To address these challenges, this paper introduces the
concept of "Like-minded Peers" (LMP), representing users whose preferences
align with the target user's current session based on their historical
sessions. This is the first work, to our knowledge, that uses LMP to enhance
the modeling of social influence in SSR. This approach not only alleviates the
problem of friend data sparsity but also effectively incorporates users with
similar preferences to the target user. We propose a novel model named
Transformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec),
which includes the TEGAA module and the GAT-based social aggregation module.
The TEGAA module captures and merges both long-term and short-term interests
for target users and LMP users. Concurrently, the GAT-based social aggregation
module is designed to aggregate the target users' dynamic interests and social
influence in a weighted manner. Extensive experiments on four real-world
datasets demonstrate the efficacy and superiority of our proposed model and
ablation studies are done to illustrate the contributions of each component in
TEGAARec.


# Accelerating Large Language Model Training with Hybrid GPU-based Compression

>Authors: Lang Xu,Quentin Anthony,Qinghua Zhou,Nawras Alnaasan,Radha R. Gulhane,Aamir Shafi,Hari Subramoni,Dhabaleswar K. Panda

>2024-09-04

> http://arxiv.org/abs/2409.02423v1

Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP)
are the three strategies widely adopted to enable fast and efficient Large
Language Model (LLM) training. However, these approaches rely on data-intensive
communication routines to collect, aggregate, and re-distribute gradients,
activations, and other important model information, which pose significant
overhead. Co-designed with GPU-based compression libraries, MPI libraries have
been proven to reduce message size significantly, and leverage interconnect
bandwidth, thus increasing training efficiency while maintaining acceptable
accuracy.
  In this work, we investigate the efficacy of compression-assisted MPI
collectives under the context of distributed LLM training using 3D parallelism
and ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen
supercomputer. First, we enabled a na\"ive compression scheme across all
collectives and observed a 22.5\% increase in TFLOPS per GPU and a 23.6\%
increase in samples per second for GPT-NeoX-20B training. Nonetheless, such a
strategy ignores the sparsity discrepancy among messages communicated in each
parallelism degree, thus introducing more errors and causing degradation in
training loss. Therefore, we incorporated hybrid compression settings toward
each parallel dimension and adjusted the compression intensity accordingly.
Given their low-rank structure (arXiv:2301.02654), we apply aggressive
compression on gradients when performing DP All-reduce. We adopt milder
compression to preserve precision while communicating activations, optimizer
states, and model parameters in TP and PP. Using the adjusted hybrid
compression scheme, we demonstrate a 17.3\% increase in TFLOPS per GPU and a
12.7\% increase in samples per second while reaching baseline loss convergence.


# Anomalous Hall effects in magnetic weak topological insulator films

>Authors: Rui Chen,Xiao-Xia Yi,Bin Zhou,Dong-Hui Xu

>2024-09-04

> http://arxiv.org/abs/2409.02412v1

The interplay between magnetism and strong topological insulator gives rise
to distinct new topological phases and various intriguing phenomena, attracting
significant attention in recent years. However, magnetic effects in weak
topological insulators remain largely unexplored. In this work, we
systematically investigate the magnetic effect on thin films of weak
topological insulators. We focus on ferromagnetic and antiferromagnetic
effects, which have been extensively studied in strong topological insulators,
as well as the recently highlighted altermagnetic effect. We reveal that the
interplay between magnetism and weak topological insulators leads to a variety
of Hall effects in the absence of an external magnetic field, including the
metallic quantum anomalous Hall effect without chiral edge states, the quantum
anomalous Hall effect with a higher Hall conductance plateau, the quantized
layer Hall effect, the metallic half-quantized valley-like Hall effect, and a
quantized valley-like Hall effect. This work provides valuable insights for
exploring magnetic effect on weak topological insulators.


# Foundations of Large Language Model Compression -- Part 1: Weight Quantization

>Authors: Sean I. Young

>2024-09-03

> http://arxiv.org/abs/2409.02026v1

In recent years, compression of large language models (LLMs) has emerged as
an important problem to allow language model deployment on resource-constrained
devices, reduce computational costs, and mitigate the environmental footprint
of large-scale AI infrastructure. In this paper, we present the foundations of
LLM quantization from a convex optimization perspective and propose a
quantization method that builds on these foundations and outperforms previous
methods. Our quantization framework, CVXQ, scales to models containing hundreds
of billions of weight parameters and provides users with the flexibility to
compress models to any specified model size, post-training. A reference
implementation of CVXQ can be obtained from https://github.com/seannz/cvxq.


# Contemporary Model Compression on Large Language Models Inference

>Authors: Dong Liu

>2024-09-03

> http://arxiv.org/abs/2409.01990v1

Large Language Models (LLMs) have revolutionized natural language processing
by achieving state-of-the-art results across a variety of tasks. However, the
computational demands of LLM inference, including high memory consumption and
slow processing speeds, pose significant challenges for real-world
applications, particularly on resource-constrained devices. Efficient inference
is crucial for scaling the deployment of LLMs to a broader range of platforms,
including mobile and edge devices.
  This survey explores contemporary techniques in model compression that
address these challenges by reducing the size and computational requirements of
LLMs while maintaining their performance. We focus on model-level compression
methods, including quantization, knowledge distillation, and pruning, as well
as system-level optimizations like KV cache efficient design. Each of these
methodologies offers a unique approach to optimizing LLMs, from reducing
numerical precision to transferring knowledge between models and structurally
simplifying neural networks. Additionally, we discuss emerging trends in
system-level design that further enhance the efficiency of LLM inference. This
survey aims to provide a comprehensive overview of current advancements in
model compression and their potential to make LLMs more accessible and
practical for diverse applications.


# Dual Advancement of Representation Learning and Clustering for Sparse and Noisy Images

>Authors: Wenlin Li,Yucheng Xu,Xiaoqing Zheng,Suoya Han,Jun Wang,Xiaobo Sun

>2024-09-03

> http://arxiv.org/abs/2409.01781v2

Sparse and noisy images (SNIs), like those in spatial gene expression data,
pose significant challenges for effective representation learning and
clustering, which are essential for thorough data analysis and interpretation.
In response to these challenges, we propose Dual Advancement of Representation
Learning and Clustering (DARLC), an innovative framework that leverages
contrastive learning to enhance the representations derived from masked image
modeling. Simultaneously, DARLC integrates cluster assignments in a cohesive,
end-to-end approach. This integrated clustering strategy addresses the "class
collision problem" inherent in contrastive learning, thus improving the quality
of the resulting representations. To generate more plausible positive views for
contrastive learning, we employ a graph attention network-based technique that
produces denoised images as augmented data. As such, our framework offers a
comprehensive approach that improves the learning of representations by
enhancing their local perceptibility, distinctiveness, and the understanding of
relational semantics. Furthermore, we utilize a Student's t mixture model to
achieve more robust and adaptable clustering of SNIs. Extensive experiments,
conducted across 12 different types of datasets consisting of SNIs, demonstrate
that DARLC surpasses the state-of-the-art methods in both image clustering and
generating image representations that accurately capture gene interactions.
Code is available at https://github.com/zipging/DARLC.


# Leveraging Large Language Models for Solving Rare MIP Challenges

>Authors: Teng Wang,Wing-Yin Yu,Ruifeng She,Wenhan Yang,Taijie Chen,Jianping Zhang

>2024-09-03

> http://arxiv.org/abs/2409.04464v2

Mixed Integer Programming (MIP) has been extensively applied in areas
requiring mathematical solvers to address complex instances within tight time
constraints. However, as the problem scale increases, the complexity of model
formulation and finding feasible solutions escalates significantly. In
contrast, the model-building cost for end-to-end models, such as large language
models (LLMs), remains largely unaffected by problem scale due to their pattern
recognition capabilities. While LLMs, like GPT-4, without fine-tuning, can
handle some traditional medium-scale MIP problems, they struggle with uncommon
or highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible
solutions for medium-scale MIP instances, but these models typically fail to
explore diverse solutions when constrained by a low and constant temperature,
limiting their performance. In this paper, we propose and evaluate a
recursively dynamic temperature method integrated with a chain-of-thought
approach. Our findings show that starting with a high temperature and gradually
lowering it leads to better feasible solutions compared to other dynamic
temperature strategies. Additionally, by comparing results generated by the LLM
with those from Gurobi, we demonstrate that the LLM can produce solutions that
complement traditional solvers by accelerating the pruning process and
improving overall efficiency.


# Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers

>Authors: Sohan Anisetty,James Hays

>2024-09-03

> http://arxiv.org/abs/2409.01591v1

Our research presents a novel motion generation framework designed to produce
whole-body motion sequences conditioned on multiple modalities simultaneously,
specifically text and audio inputs. Leveraging Vector Quantized Variational
Autoencoders (VQVAEs) for motion discretization and a bidirectional Masked
Language Modeling (MLM) strategy for efficient token prediction, our approach
achieves improved processing efficiency and coherence in the generated motions.
By integrating spatial attention mechanisms and a token critic we ensure
consistency and naturalness in the generated motions. This framework expands
the possibilities of motion generation, addressing the limitations of existing
approaches and opening avenues for multimodal motion synthesis.


# CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification

>Authors: Junhui He,Shangyu Wu,Weidong Wen,Chun Jason Xue,Qingan Li

>2024-09-02

> http://arxiv.org/abs/2409.01366v1

Deploying large language models (LLMs) on edge devices presents significant
challenges due to the substantial computational overhead and memory
requirements. Activation sparsification can mitigate these challenges by
reducing the number of activated neurons during inference. Existing methods
typically employ thresholding-based sparsification based on the statistics of
activation tensors. However, these methods do not explicitly model the impact
of activation sparsification on performance, leading to suboptimal performance
degradation. To address this issue, this paper reformulates the activation
sparsification problem by introducing a new objective that optimizes the
sparsification decisions. Building on this reformulation, we propose CHESS, a
general activation sparsification approach via CHannel-wise thrEsholding and
Selective Sparsification. First, channel-wise thresholding assigns a unique
threshold to each activation channel in the feed-forward network (FFN) layers.
Then, selective sparsification involves applying thresholding-based activation
sparsification to specific layers within the attention modules. Finally, we
detail the implementation of sparse kernels to accelerate LLM inference.
Experimental results demonstrate that the proposed CHESS achieves lower
performance degradation over 8 downstream tasks while activating fewer
parameters compared to existing methods, thus speeding up the LLM inference by
up to 1.27x.


# GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling

>Authors: Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille

>2024-09-02

> http://arxiv.org/abs/2409.02720v2

Depth estimation plays a pivotal role in autonomous driving, facilitating a
comprehensive understanding of the vehicle's 3D surroundings. Radar, with its
robustness to adverse weather conditions and capability to measure distances,
has drawn significant interest for radar-camera depth estimation. However,
existing algorithms process the inherently noisy and sparse radar data by
projecting 3D points onto the image plane for pixel-level feature extraction,
overlooking the valuable geometric information contained within the radar point
cloud. To address this gap, we propose GET-UP, leveraging attention-enhanced
Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D
information from radar data. This approach effectively enriches the feature
representation by incorporating spatial relationships compared to traditional
methods that rely only on 2D feature extraction. Furthermore, we incorporate a
point cloud upsampling task to densify the radar point cloud, rectify point
positions, and derive additional 3D features under the guidance of lidar data.
Finally, we fuse radar and camera features during the decoding phase for depth
estimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving
state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE
over the previously best-performing model. Code:
https://github.com/harborsarah/GET-UP


# Balancing Performance and Efficiency: A Multimodal Large Language Model Pruning Method based Image Text Interaction

>Authors: Gaotong Yu,Yi Chen,Jian Xu

>2024-09-02

> http://arxiv.org/abs/2409.01162v1

Recently, multimodal large language models (MM-LLMs) have achieved great
success in many multimodal tasks, but their high computational costs limit
their further promotion and application. In the MM-LLMs framework, the main
computational consumption step is the processing of concatenated text and
visual tokens at the LLM layer. The length of the input token for LLM directly
affects the overall training and inference efficiency. In response to this
issue, we further studied the visual tokens of MM-LLMs. We found that the
similarity between visual and CLS tokens in the visual encoder follows a
long-tail distribution. In other words, only a few visual tokens are highly
similar to CLS tokens. Therefore, we designed a dynamic pruning algorithm to
address this issue. Firstly, for different input samples, we search for the
inflection point of their visual CLS token similarity curve and use it as the
corresponding segmentation point to trim the visual markers. This process
mainly reduces the output of the visual encoder to accelerate the model. Then,
in the LLM layer, the concatenated visual text tokens are pruned for the second
time. During this process, due to the interaction between visual and textual
features, visual and textual tokens with low text correlation are further
filtered, achieving a balance between efficiency and performance. The results
on multiple datasets show that our proposed method can achieve performance that
competes with the original performance when using an average of 22% of the
original token quantity. Our source code will be made publicly available
following acceptance.


# Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression

>Authors: Dingyuan Zhang,Dingkang Liang,Zichang Tan,Xiaoqing Ye,Cheng Zhang,Jingdong Wang,Xiang Bai

>2024-09-01

> http://arxiv.org/abs/2409.00633v1

Slow inference speed is one of the most crucial concerns for deploying
multi-view 3D detectors to tasks with high real-time requirements like
autonomous driving. Although many sparse query-based methods have already
attempted to improve the efficiency of 3D detectors, they neglect to consider
the backbone, especially when using Vision Transformers (ViT) for better
performance. To tackle this problem, we explore the efficient ViT backbones for
multi-view 3D detection via token compression and propose a simple yet
effective method called TokenCompression3D (ToC3D). By leveraging history
object queries as foreground priors of high quality, modeling 3D motion
information in them, and interacting them with image tokens through the
attention mechanism, ToC3D can effectively determine the magnitude of
information densities of image tokens and segment the salient foreground
tokens. With the introduced dynamic router design, ToC3D can weigh more
computing resources to important foreground tokens while compressing the
information loss, leading to a more efficient ViT-based multi-view 3D detector.
Extensive results on the large-scale nuScenes dataset show that our method can
nearly maintain the performance of recent SOTA with up to 30% inference
speedup, and the improvements are consistent after scaling up the ViT and input
resolution. The code will be made at https://github.com/DYZhang09/ToC3D.


# TinyAgent: Function Calling at the Edge

>Authors: Lutfi Eren Erdogan,Nicholas Lee,Siddharth Jha,Sehoon Kim,Ryan Tabrizi,Suhong Moon,Coleman Hooper,Gopala Anumanchipalli,Kurt Keutzer,Amir Gholami

>2024-09-01

> http://arxiv.org/abs/2409.00608v1

Recent large language models (LLMs) have enabled the development of advanced
agentic systems that can integrate various tools and APIs to fulfill user
queries through function calling. However, the deployment of these LLMs on the
edge has not been explored since they typically require cloud-based
infrastructure due to their substantial model size and computational demands.
To this end, we present TinyAgent, an end-to-end framework for training and
deploying task-specific small language model agents capable of function calling
for driving agentic systems at the edge. We first show how to enable accurate
function calling for open-source models via the LLMCompiler framework. We then
systematically curate a high-quality dataset for function calling, which we use
to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient
inference, we introduce a novel tool retrieval method to reduce the input
prompt length and utilize quantization to further accelerate the inference
speed. As a driving application, we demonstrate a local Siri-like system for
Apple's MacBook that can execute user commands through text or voice input. Our
results show that our models can achieve, and even surpass, the
function-calling capabilities of larger models like GPT-4-Turbo, while being
fully deployed at the edge. We open-source our dataset, models, and installable
package and provide a demo video for our MacBook assistant agent.


# Sparse Mamba: Reinforcing Controllability In Structural State Space Models

>Authors: Emadeldeen Hamdan,Hongyi Pan,Ahmet Enis Cetin

>2024-08-31

> http://arxiv.org/abs/2409.00563v1

In this article, we introduce the concept of controllability and
observability to the M amba architecture in our Sparse-Mamba (S-Mamba) for
natural language processing (NLP) applications. The structured state space
model (SSM) development in recent studies, such as Mamba and Mamba2,
outperformed and solved the computational inefficiency of transformers and
large language models (LLMs) on longer sequences in small to medium NLP tasks.
The Mamba SSMs architecture drops the need for attention layer or MLB blocks in
transformers. However, the current Mamba models do not reinforce the
controllability on state space equations in the calculation of A, B, C, and D
matrices at each time step, which increase the complexity and the computational
cost needed. In this article we show that the number of parameters can be
significantly decreased by reinforcing controllability in the state space
equations in the proposed Sparse-Mamba (S-Mamba), while maintaining the
performance. The controllable n x n state matrix A is sparse and it has only n
free parameters. Our novel approach will ensure a controllable system and could
be the gate key for Mamba 3.


# Generalized Orthogonal Chirp Division Multiplexing in Doubly Selective Channels

>Authors: Yun Liu,Hao Zhao,Huazhen Yao,Zeng Hu,Yinming Cui,Dehuan Wan

>2024-08-31

> http://arxiv.org/abs/2409.00402v1

In recent years, orthogonal chirp division modulation (OCDM) has gained
attention as a robust communication waveform due to its strong resistance to
both time-domain and frequency-domain interference. However, similar to
orthogonal frequency division multiplexing (OFDM), OCDM suffers from a high
peak-to-average power ratio (PAPR), resulting in increased hardware costs and
reduced energy efficiency of the transmitter's power amplifiers. In this work,
we introduce a novel unitary transform called the Generalized Discrete Fresnel
Transform (GDFnT) and propose a new waveform based on this transform, named
Generalized Orthogonal Chirp Division Modulation (GOCDM). In GOCDM, data
symbols from the constellation diagram are independently placed in the
Generalized Fresnel (GF) domain. We derive the GF-domain channel matrix for the
GOCDM system under time-frequency doubly selective channels and leverages the
sparsity of the GF-domain channel matrix to design an iterative receiver based
on the message-passing algorithm. Simulation results demonstrate that GOCDM
achieves better PAPR performance than OCDM without compromising bit error rate
(BER) performance.


# Benchmarking the Performance of Large Language Models on the Cerebras Wafer Scale Engine

>Authors: Zuoning Zhang,Dhruv Parikh,Youning Zhang,Viktor Prasanna

>2024-08-30

> http://arxiv.org/abs/2409.00287v1

Transformer based Large Language Models (LLMs) have recently reached state of
the art performance in Natural Language Processing (NLP) and Computer Vision
(CV) domains. LLMs use the Multi-Headed Self-Attention (MHSA) mechanism to
capture long-range global attention relationships among input words or image
patches, drastically improving its performance over prior deep learning
approaches. In this paper, we evaluate the performance of LLMs on the Cerebras
Wafer Scale Engine (WSE). Cerebras WSE is a high performance computing system
with 2.6 trillion transistors, 850,000 cores and 40 GB on-chip memory. Cerebras
WSE's Sparse Linear Algebra Compute (SLAC) cores eliminates multiply-by-zeros
operations and its 40 GB of on-chip memory is uniformly distributed among SLAC
cores, enabling fast local access to model parameters. Moreover, Cerebras
software configures routing between cores at runtime, optimizing communication
overhead among cores. As LLMs are becoming more commonly used, new hardware
architectures are needed to accelerate LLMs training and inference. We
benchmark the effectiveness of this hardware architecture at accelerating LLMs
training and inference. Additionally, we analyze if Cerebras WSE can scale the
memory-wall associated with traditionally memory-bound compute tasks using its
20 PB/s high bandwidth memory. Furthermore, we examine the performance
scalability of Cerebras WSE through a roofline model. By plotting performance
metrics against computational intensity, we aim to assess their effectiveness
at handling high compute-intensive LLMs training and inference tasks.


# Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations

>Authors: Yilin Zhuang,Sibo Cheng,Karthik Duraisamy

>2024-08-30

> http://arxiv.org/abs/2409.00230v1

Diffusion models have gained attention for their ability to represent complex
distributions and incorporate uncertainty, making them ideal for robust
predictions in the presence of noisy or incomplete data. In this study, we
develop and enhance score-based diffusion models in field reconstruction tasks,
where the goal is to estimate complete spatial fields from partial
observations. We introduce a condition encoding approach to construct a
tractable mapping mapping between observed and unobserved regions using a
learnable integration of sparse observations and interpolated fields as an
inductive bias. With refined sensing representations and an unraveled temporal
dimension, our method can handle arbitrary moving sensors and effectively
reconstruct fields. Furthermore, we conduct a comprehensive benchmark of our
approach against a deterministic interpolation-based method across various
static and time-dependent PDEs. Our study attempts to addresses the gap in
strong baselines for evaluating performance across varying sampling
hyperparameters, noise levels, and conditioning methods. Our results show that
diffusion models with cross-attention and the proposed conditional encoding
generally outperform other methods under noisy conditions, although the
deterministic method excels with noiseless data. Additionally, both the
diffusion models and the deterministic method surpass the numerical approach in
accuracy and computational cost for the steady problem. We also demonstrate the
ability of the model to capture possible reconstructions and improve the
accuracy of fused results in covariance-based correction tasks using ensemble
sampling.


# Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model

>Authors: Zhen Ye,Peiwen Sun,Jiahe Lei,Hongzhan Lin,Xu Tan,Zheqi Dai,Qiuqiang Kong,Jianyi Chen,Jiahao Pan,Qifeng Liu,Yike Guo,Wei Xue

>2024-08-30

> http://arxiv.org/abs/2408.17175v2

Recent advancements in audio generation have been significantly propelled by
the capabilities of Large Language Models (LLMs). The existing research on
audio LLM has primarily focused on enhancing the architecture and scale of
audio language models, as well as leveraging larger datasets, and generally,
acoustic codecs, such as EnCodec, are used for audio tokenization. However,
these codecs were originally designed for audio compression, which may lead to
suboptimal performance in the context of audio LLM. Our research aims to
address the shortcomings of current audio LLM codecs, particularly their
challenges in maintaining semantic integrity in generated audio. For instance,
existing methods like VALL-E, which condition acoustic token generation on text
transcriptions, often suffer from content inaccuracies and elevated word error
rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in
word skipping and errors. To overcome these issues, we propose a
straightforward yet effective approach called X-Codec. X-Codec incorporates
semantic features from a pre-trained semantic encoder before the Residual
Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss
after RVQ. By enhancing the semantic ability of the codec, X-Codec
significantly reduces WER in speech synthesis tasks and extends these benefits
to non-speech applications, including music and sound generation. Our
experiments in text-to-speech, music continuation, and text-to-sound tasks
demonstrate that integrating semantic information substantially improves the
overall performance of language models in audio generation. Our code and demo
are available (Demo: https://x-codec-audio.github.io Code:
https://github.com/zhenye234/xcodec)


# H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical Image Registration

>Authors: Yufeng Zhou,Wenming Cao

>2024-08-29

> http://arxiv.org/abs/2408.16719v1

The integration of Convolutional Neural Network (ConvNet) and Transformer has
emerged as a strong candidate for image registration, leveraging the strengths
of both models and a large parameter space. However, this hybrid model,
treating brain MRI volumes as grid or sequence structures, faces challenges in
accurately representing anatomical connectivity, diverse brain regions, and
vital connections contributing to the brain's internal architecture. Concerns
also arise regarding the computational expense and GPU memory usage associated
with this model. To tackle these issues, a lightweight hybrid sparse graph
attention network (H-SGANet) has been developed. This network incorporates a
central mechanism, Sparse Graph Attention (SGA), based on a Vision Graph Neural
Network (ViG) with predetermined anatomical connections. The SGA module expands
the model's receptive field and seamlessly integrates into the network. To
further amplify the advantages of the hybrid network, the Separable
Self-Attention (SSA) is employed as an enhanced token mixer, integrated with
depth-wise convolution to constitute SSAFormer. This strategic integration is
designed to more effectively extract long-range dependencies. As a hybrid
ConvNet-ViG-Transformer model, H-SGANet offers threefold benefits for
volumetric medical image registration. It optimizes fixed and moving images
concurrently through a hybrid feature fusion layer and an end-to-end learning
framework. Compared to VoxelMorph, a model with a similar parameter count,
H-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% in
Dice score on the OASIS dataset and LPBA40 dataset, respectively.


# A GREAT Architecture for Edge-Based Graph Problems Like TSP

>Authors: Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár

>2024-08-29

> http://arxiv.org/abs/2408.16717v1

In the last years, many neural network-based approaches have been proposed to
tackle combinatorial optimization problems such as routing problems. Many of
these approaches are based on graph neural networks (GNNs) or related
transformers, operating on the Euclidean coordinates representing the routing
problems. However, GNNs are inherently not well suited to operate on dense
graphs, such as in routing problems. Furthermore, models operating on Euclidean
coordinates cannot be applied to non-Euclidean versions of routing problems
that are often found in real-world settings. To overcome these limitations, we
propose a novel GNN-related edge-based neural model called Graph Edge Attention
Network (GREAT). We evaluate the performance of GREAT in the
edge-classification task to predict optimal edges in the Traveling Salesman
Problem (TSP). We can use such a trained GREAT model to produce sparse TSP
graph instances, keeping only the edges GREAT finds promising. Compared to
other, non-learning-based methods to sparsify TSP graphs, GREAT can produce
very sparse graphs while keeping most of the optimal edges. Furthermore, we
build a reinforcement learning-based GREAT framework which we apply to
Euclidean and non-Euclidean asymmetric TSP. This framework achieves
state-of-the-art results.


# HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications

>Authors: Rishi Kalra,Zekun Wu,Ayesha Gulley,Airlie Hilliard,Xin Guan,Adriano Koshiyama,Philip Treleaven

>2024-08-29

> http://arxiv.org/abs/2409.09046v1

While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.


# Lattice spectra of $DDK$ three-body system with Lorentz covariant kinematic

>Authors: Qi-Chao Xiao,Jin-Yi Pang,Jia-Jun Wu

>2024-08-29

> http://arxiv.org/abs/2408.16590v1

The $DDK$ system has gain increasing attention in recent research due to its
potential to contain a three-hadron bound state. This article utilizes an
extension of the Non-Relativistic Effective Field Theory (NREFT) and the finite
volume particle-dimer framework to derive Lorentz-invariant quantization
conditions for the $DDK$ three-body system. Using current model input
conditions, the finite volume energy spectrum of the $DDK$ three-body system
was calculated. This new calculation incorporates relativistic kinematics,
allowing it to be applicable across a broader energy range starting from the
threshold. In this work, we present a comprehensive \( O(p^{2}) \) calculation.
The spurious pole is effectively subtracted within the framework of
relativistic kinematics. The spectra in the moving frame are also obtained.
These analyses provide a broader testing ground for future lattice simulations.
They are expected to reveal more detailed properties of the $DDK$ system and
other three-hadron systems.


# WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling

>Authors: Shengpeng Ji,Ziyue Jiang,Xize Cheng,Yifu Chen,Minghui Fang,Jialong Zuo,Qian Yang,Ruiqi Li,Ziang Zhang,Xiaoda Yang,Rongjie Huang,Yidi Jiang,Qian Chen,Siqi Zheng,Wen Wang,Zhou Zhao

>2024-08-29

> http://arxiv.org/abs/2408.16532v1

Language models have been effectively applied to modeling natural signals,
such as images, video, speech, and audio. A crucial component of these models
is the codec tokenizer, which compresses high-dimensional natural signals into
lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,
which offers several advantages over previous SOTA acoustic codec models in the
audio domain: 1)extreme compression. By compressing the layers of quantizers
and the temporal dimension of the discrete codec, one-second audio of 24kHz
sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved
subjective quality. Despite the reduced number of tokens, WavTokenizer achieves
state-of-the-art reconstruction quality with outstanding UTMOS scores and
inherently contains richer semantic information. Specifically, we achieve these
results by designing a broader VQ space, extended contextual windows, and
improved attention networks, as well as introducing a powerful multi-scale
discriminator and an inverse Fourier transform structure. We conducted
extensive reconstruction experiments in the domains of speech, audio, and
music. WavTokenizer exhibited strong performance across various objective and
subjective metrics compared to state-of-the-art models. We also tested semantic
information, VQ utilization, and adaptability to generative models.
Comprehensive ablation studies confirm the necessity of each module in
WavTokenizer. The related code, demos, and pre-trained models are available at
https://github.com/jishengpeng/WavTokenizer.


# Locally Grouped and Scale-Guided Attention for Dense Pest Counting

>Authors: Chang-Hwan Son

>2024-08-29

> http://arxiv.org/abs/2408.16503v1

This study introduces a new dense pest counting problem to predict densely
distributed pests captured by digital traps. Unlike traditional detection-based
counting models for sparsely distributed objects, trap-based pest counting must
deal with dense pest distributions that pose challenges such as severe
occlusion, wide pose variation, and similar appearances in colors and textures.
To address these problems, it is essential to incorporate the local attention
mechanism, which identifies locally important and unimportant areas to learn
locally grouped features, thereby enhancing discriminative performance.
Accordingly, this study presents a novel design that integrates locally grouped
and scale-guided attention into a multiscale CenterNet framework. To group
local features with similar attributes, a straightforward method is introduced
using the heatmap predicted by the first hourglass containing pest centroid
information, which eliminates the need for complex clustering models. To
enhance attentiveness, the pixel attention module transforms the heatmap into a
learnable map. Subsequently, scale-guided attention is deployed to make the
object and background features more discriminative, achieving multiscale
feature fusion. Through experiments, the proposed model is verified to enhance
object features based on local grouping and discriminative feature attention
learning. Additionally, the proposed model is highly effective in overcoming
occlusion and pose variation problems, making it more suitable for dense pest
counting. In particular, the proposed model outperforms state-of-the-art models
by a large margin, with a remarkable contribution to dense pest counting.


# Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models

>Authors: Kengo Nakata,Daisuke Miyashita,Youyang Ng,Yasuto Hoshi,Jun Deguchi

>2024-08-29

> http://arxiv.org/abs/2408.16296v1

In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.


# FireFly-S: Exploiting Dual-Side Sparsity for Spiking Neural Networks Acceleration with Reconfigurable Spatial Architecture

>Authors: Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng

>2024-08-28

> http://arxiv.org/abs/2408.15578v1

Spiking Neural Networks (SNNs), with their brain-inspired structure using
discrete spikes instead of continuous activations, are gaining attention for
their potential of efficient processing on neuromorphic chips. While current
SNN hardware accelerators often prioritize temporal spike sparsity, exploiting
sparse synaptic weights offers significant untapped potential for even greater
efficiency. To address this, we propose FireFly-S, a Sparse extension of the
FireFly series. This co-optimized software-hardware design focusing on
leveraging dual-side sparsity for acceleration. On the software side, we
propose a novel algorithmic optimization framework that combines gradient
rewiring for pruning and modified Learned Step Size Quantization (LSQ) tailored
for SNNs, which achieves remarkable weight sparsity exceeding 85\% and enables
efficient 4-bit quantization with negligible accuracy loss. On the hardware
side, we present an efficient dual-side sparsity detector employing a
Bitmap-based sparse decoding logic to pinpoint the positions of non-zero
weights and input spikes. The logic allows for the direct bypassing of
redundant computations, thereby enhancing computational efficiency. Different
from the overlay architecture adopted by previous FireFly series, we adopt a
spatial architecture with inter-layer pipelining that can fully exploit the
nature of Field-Programmable Gate Arrays (FPGAs). A spatial-temporal dataflow
is also proposed to support such inter-layer pipelining and avoid long-term
temporal dependencies. In experiments conducted on the MNIST, DVS-Gesture and
CIFAR-10 datasets, the FireFly-S model achieves 85-95\% sparsity with 4-bit
quantization and the hardware accelerator effectively leverages the dual-side
sparsity, delivering outstanding performance metrics of 10,047 FPS/W on MNIST,
3,683 FPS/W on DVS-Gesture, and 2,327 FPS/W on CIFAR-10.


# The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical Study

>Authors: Minghai Qin

>2024-08-27

> http://arxiv.org/abs/2408.15301v1

We have observed a distinctive quantization-related behavior in the
LLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and
LLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying
large language models (LLMs) efficiently. Among various bit widths and
representations for weights and activations, the 8-bit integer weight and 8-bit
integer activation (W8A8) configuration is particularly popular due to its
widespread hardware support. However, the impact of W8A8 post-training
quantization on model accuracy remains contentious. While several studies have
suggested calibrating either weights or activations to mitigate accuracy
degradation, a comprehensive solution has yet to be identified. In this paper,
we empirically investigate multiple LLMs featured on an open LLM leaderboard,
discovering that the LLaMA3-70B model series have a unique accuracy degradation
behavior with W8A8 per-channel post-training quantization. In contrast, other
model series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and
Falcon demonstrate robust performance with W8A8, sometimes surpassing their
FP16 counterparts. Contrary to previous assertions attributing degradation to
the large dynamic range of activations, our findings indicate that the weight
distribution of the LLaMA3-70B is the primary factor behind the vulnerability.
By meticulously analyzing the distinct characteristics of weight distributions
across Transformer blocks, we propose a mixed strategy with less than 3% of the
layers enabling finer W8A8 quantization granularity, while the remaining 97% of
layers retain the per-channel configuration. As a result, the average accuracy
of LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of
LLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires
neither calibration nor fine-tuning.


# GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs

>Authors: Maxim Zhelnin,Viktor Moskvoretskii,Egor Shvetsov,Egor Venediktov,Mariya Krylova,Aleksandr Zuev,Evgeny Burnaev

>2024-08-27

> http://arxiv.org/abs/2408.15300v1

Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and
democratized the usage of Large Language Models (LLMs). Recent studies have
shown that a small subset of weights significantly impacts performance. Based
on this observation, we introduce a novel PEFT method, called Gaussian noise
Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only
salient columns, while injecting Gaussian noise into non-salient ones. To
identify these columns, we developeda generalized sensitivity metric that
extends and unifies metrics from previous studies. Experiments with LLaMA
models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT
methods under the same computational budget. Moreover, GIFT-SW offers practical
advantages to recover performance of models subjected to mixed-precision
quantization with keeping salient weights in full precision.


# NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals

>Authors: Wei-Bang Jiang,Yansen Wang,Bao-Liang Lu,Dongsheng Li

>2024-08-27

> http://arxiv.org/abs/2409.00101v1

Recent advancements for large-scale pre-training with neural signals such as
electroencephalogram (EEG) have shown promising results, significantly boosting
the development of brain-computer interfaces (BCIs) and healthcare. However,
these pre-trained models often require full fine-tuning on each downstream task
to achieve substantial improvements, limiting their versatility and usability,
and leading to considerable resource wastage. To tackle these challenges, we
propose NeuroLM, the first multi-task foundation model that leverages the
capabilities of Large Language Models (LLMs) by regarding EEG signals as a
foreign language, endowing the model with multi-task learning and inference
capabilities. Our approach begins with learning a text-aligned neural tokenizer
through vector-quantized temporal-frequency prediction, which encodes EEG
signals into discrete neural tokens. These EEG tokens, generated by the frozen
vector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG
information via multi-channel autoregression. Consequently, NeuroLM can
understand both EEG and language modalities. Finally, multi-task instruction
tuning adapts NeuroLM to various downstream tasks. We are the first to
demonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse
EEG tasks within a single model through instruction tuning. The largest variant
NeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and
is pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG
data. When evaluated on six diverse downstream datasets, NeuroLM showcases the
huge potential of this multi-task learning paradigm.


# SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models

>Authors: Shuaijie Shen,Chao Wang,Renzhuo Huang,Yan Zhong,Qinghai Guo,Zhichao Lu,Jianguo Zhang,Luziwei Leng

>2024-08-27

> http://arxiv.org/abs/2408.14909v1

Known as low energy consumption networks, spiking neural networks (SNNs) have
gained a lot of attention within the past decades. While SNNs are increasing
competitive with artificial neural networks (ANNs) for vision tasks, they are
rarely used for long sequence tasks, despite their intrinsic temporal dynamics.
In this work, we develop spiking state space models (SpikingSSMs) for long
sequence learning by leveraging on the sequence learning abilities of state
space models (SSMs). Inspired by dendritic neuron structure, we hierarchically
integrate neuronal dynamics with the original SSM block, meanwhile realizing
sparse synaptic computation. Furthermore, to solve the conflict of event-driven
neuronal dynamics with parallel computing, we propose a light-weight surrogate
dynamic network which accurately predicts the after-reset membrane potential
and compatible to learnable thresholds, enabling orders of acceleration in
training speed compared with conventional iterative methods. On the long range
arena benchmark task, SpikingSSM achieves competitive performance to
state-of-the-art SSMs meanwhile realizing on average 90\% of network sparsity.
On language modeling, our network significantly surpasses existing spiking
large language models (spikingLLMs) on the WikiText-103 dataset with only a
third of the model size, demonstrating its potential as backbone architecture
for low computation cost LLMs.


# Channel-wise Influence: Estimating Data Influence for Multivariate Time Series

>Authors: Muyao Wang,Zeke Xie,Bo Chen

>2024-08-27

> http://arxiv.org/abs/2408.14763v1

The influence function, a technique from robust statistics, measures the
impact on model parameters or related functions when training data is removed
or modified. This effective and valuable post-hoc method allows for studying
the interpretability of machine learning models without requiring costly model
retraining. It would provide extensions like increasing model performance,
improving model generalization, and offering interpretability. Recently,
Multivariate Time Series (MTS) analysis has become an important yet challenging
task, attracting significant attention. However, there is no preceding research
on the influence functions of MTS to shed light on the effects of modifying the
channel of training MTS. Given that each channel in an MTS plays a crucial role
in its analysis, it is essential to characterize the influence of different
channels. To fill this gap, we propose a channel-wise influence function, which
is the first method that can estimate the influence of different channels in
MTS, utilizing a first-order gradient approximation that leverages the more
informative average gradient of the data set. Additionally, we demonstrate how
this influence function can be used to estimate the impact of a channel in MTS.
Finally, we validated the accuracy and effectiveness of our influence
estimation function in critical MTS analysis tasks, such as MTS anomaly
detection and MTS forecasting. According to abundant experiments on real-world
dataset, the original influence function performs worse than our method and
even fail for the channel pruning problem, which demonstrate the superiority
and necessity of channel-wise influence function in MTS analysis tasks.


# PAT: Pruning-Aware Tuning for Large Language Models

>Authors: Yijiang Liu,Huanrui Yang,Youxin Chen,Rongyu Zhang,Miao Wang,Yuan Du,Li Du

>2024-08-27

> http://arxiv.org/abs/2408.14721v1

Large language models (LLMs) excel in language tasks, especially with
supervised fine-tuning after pre-training. However, their substantial memory
and computational requirements hinder practical applications. Structural
pruning, which reduces less significant weight dimensions, is one solution.
Yet, traditional post-hoc pruning often leads to significant performance loss,
with limited recovery from further fine-tuning due to reduced capacity. Since
the model fine-tuning refines the general and chaotic knowledge in pre-trained
models, we aim to incorporate structural pruning with the fine-tuning, and
propose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy
while preserving the model performance to the maximum extend. Specifically, we
insert the innovative Hybrid Sparsification Modules (HSMs) between the
Attention and FFN components to accordingly sparsify the upstream and
downstream linear modules. The HSM comprises a lightweight operator and a
globally shared trainable mask. The lightweight operator maintains a training
overhead comparable to that of LoRA, while the trainable mask unifies the
channels to be sparsified, ensuring structural pruning. Additionally, we
propose the Identity Loss which decouples the transformation and scaling
properties of the HSMs to enhance training robustness. Extensive experiments
demonstrate that PAT excels in both performance and efficiency. For example,
our Llama2-7b model with a 25\% pruning ratio achieves 1.33$\times$ speedup
while outperforming the LoRA-finetuned model by up to 1.26\% in accuracy with a
similar training cost. Code:
https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning


# Training-Free Activation Sparsity in Large Language Models

>Authors: James Liu,Pragaash Ponnusamy,Tianle Cai,Han Guo,Yoon Kim,Ben Athiwaratkun

>2024-08-26

> http://arxiv.org/abs/2408.14690v1

Activation sparsity can enable practical inference speedups in large language
models (LLMs) by reducing the compute and memory-movement required for matrix
multiplications during the forward pass. However, existing methods face
limitations that inhibit widespread adoption. Some approaches are tailored
towards older models with ReLU-based sparsity, while others require extensive
continued pre-training on up to hundreds of billions of tokens. This paper
describes TEAL, a simple training-free method that applies magnitude-based
activation sparsity to hidden states throughout the entire model. TEAL achieves
40-50% model-wide sparsity with minimal performance degradation across Llama-2,
Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve
existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to
1.53$\times$ and 1.8$\times$ at 40% and 50% model-wide sparsity. TEAL is
compatible with weight quantization, enabling further efficiency gains.


# Language-specific Calibration for Pruning Multilingual Language Models

>Authors: Simon Kurz,Jian-Jia Chen,Lucie Flek,Zhixue Zhao

>2024-08-26

> http://arxiv.org/abs/2408.14398v2

Recent advances in large language model (LLM) pruning have shown
state-of-the-art compression results in post-training and retraining-free
settings while maintaining high predictive performance. However, such research
mainly considers calibrating pruning using English text, despite the
multilingual nature of modern LLMs and their frequent uses in non-English
languages. In this paper, we set out to explore effective strategies for
calibrating the pruning of multilingual language models. We present the first
comprehensive empirical study, comparing different calibration languages for
pruning multilingual models across diverse tasks, models, and state-of-the-art
pruning techniques. Our results present practical suggestions, for example,
calibrating in the target language can efficiently yield lower perplexity, but
does not necessarily benefit downstream tasks. Our further analysis experiments
unveil that calibration in the target language mainly contributes to preserving
language-specific features related to fluency and coherence, but might not
contribute to capturing language-agnostic features such as language
understanding and reasoning. Last, we provide practical recommendations for
future practitioners.


# Streamline tractography of the fetal brain in utero with machine learning

>Authors: Weide Liu,Camilo Calixto,Simon K. Warfield,Davood Karimi

>2024-08-26

> http://arxiv.org/abs/2408.14326v1

Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive
tool for studying white matter tracts and structural connectivity of the brain.
These assessments rely heavily on tractography techniques, which reconstruct
virtual streamlines representing white matter fibers. Much effort has been
devoted to improving tractography methodology for adult brains, while
tractography of the fetal brain has been largely neglected. Fetal tractography
faces unique difficulties due to low dMRI signal quality, immature and rapidly
developing brain structures, and paucity of reference data. This work presents
the first machine learning model for fetal tractography. The model input
consists of five sources of information: (1) Fiber orientation, inferred from a
diffusion tensor fit to the dMRI signal; (2) Directions of recent propagation
steps; (3) Global spatial information, encoded as distances to keypoints in the
brain cortex; (4) Tissue segmentation information; and (5) Prior information
about the expected local fiber orientations supplied with an atlas. In order to
mitigate the local tensor estimation error, a large spatial context around the
current point in the diffusion tensor image is encoded using convolutional and
attention neural network modules. Moreover, the diffusion tensor information at
a hypothetical next point is included in the model input. Filtering rules based
on anatomically constrained tractography are applied to prune implausible
streamlines. We trained the model on manually-refined whole-brain fetal
tractograms and validated the trained model on an independent set of 11 test
scans with gestational ages between 23 and 36 weeks. Results show that our
proposed method achieves superior performance across all evaluated tracts. The
new method can significantly advance the capabilities of dMRI for studying
normal and abnormal brain development in utero.


# On-Device Language Models: A Comprehensive Review

>Authors: Jiajun Xu,Zhiyuan Li,Wei Chen,Qun Wang,Xin Gao,Qi Cai,Ziyuan Ling

>2024-08-26

> http://arxiv.org/abs/2409.00088v2

The advent of large language models (LLMs) revolutionized natural language
processing applications, and running LLMs on edge devices has become
increasingly attractive for reasons including reduced latency, data
localization, and personalized user experiences. This comprehensive review
examines the challenges of deploying computationally expensive LLMs on
resource-constrained devices and explores innovative solutions across multiple
domains. The paper investigates the development of on-device language models,
their efficient architectures, including parameter sharing and modular designs,
as well as state-of-the-art compression techniques like quantization, pruning,
and knowledge distillation. Hardware acceleration strategies and collaborative
edge-cloud deployment approaches are analyzed, highlighting the intricate
balance between performance and resource utilization. Case studies of on-device
language models from major mobile manufacturers demonstrate real-world
applications and potential benefits. The review also addresses critical aspects
such as adaptive learning, multi-modal capabilities, and personalization. By
identifying key research directions and open challenges, this paper provides a
roadmap for future advancements in on-device language models, emphasizing the
need for interdisciplinary efforts to realize the full potential of ubiquitous,
intelligent computing while ensuring responsible and ethical deployment. For a
comprehensive review of research work and educational resources on on-device
large language models (LLMs), please visit
https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device
LLMs, visit https://www.nexaai.com/models.


# FusionSAM: Latent Space driven Segment Anything Model for Multimodal Fusion and Segmentation

>Authors: Daixun Li,Weiying Xie,Mingxiang Cao,Yunke Wang,Jiaqing Zhang,Yunsong Li,Leyuan Fang,Chang Xu

>2024-08-26

> http://arxiv.org/abs/2408.13980v1

Multimodal image fusion and segmentation enhance scene understanding in
autonomous driving by integrating data from various sensors. However, current
models struggle to efficiently segment densely packed elements in such scenes,
due to the absence of comprehensive fusion features that can guide mid-process
fine-tuning and focus attention on relevant areas. The Segment Anything Model
(SAM) has emerged as a transformative segmentation method. It provides more
effective prompts through its flexible prompt encoder, compared to transformers
lacking fine-tuned control. Nevertheless, SAM has not been extensively studied
in the domain of multimodal fusion for natural images. In this paper, we
introduce SAM into multimodal image segmentation for the first time, proposing
a novel framework that combines Latent Space Token Generation (LSTG) and Fusion
Mask Prompting (FMP) modules to enhance SAM's multimodal fusion and
segmentation capabilities. Specifically, we first obtain latent space features
of the two modalities through vector quantization and embed them into a
cross-attention-based inter-domain fusion module to establish long-range
dependencies between modalities. Then, we use these comprehensive fusion
features as prompts to guide precise pixel-level segmentation. Extensive
experiments on several public datasets demonstrate that the proposed method
significantly outperforms SAM and SAM2 in multimodal autonomous driving
scenarios, achieving at least 3.9$\%$ higher segmentation mIoU than the
state-of-the-art approaches.


# MobileQuant: Mobile-friendly Quantization for On-device Language Models

>Authors: Fuwen Tan,Royson Lee,Łukasz Dudziak,Shell Xu Hu,Sourav Bhattacharya,Timothy Hospedales,Georgios Tzimiropoulos,Brais Martinez

>2024-08-25

> http://arxiv.org/abs/2408.13933v1

Large language models (LLMs) have revolutionized language processing,
delivering outstanding results across multiple applications. However, deploying
LLMs on edge devices poses several challenges with respect to memory, energy,
and compute costs, limiting their widespread use in devices such as mobile
phones. A promising solution is to reduce the number of bits used to represent
weights and activations. While existing works have found partial success at
quantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations
beyond 16 bits often leads to large computational overheads due to poor
on-device quantization support, or a considerable accuracy drop. Yet, 8-bit
activations are very attractive for on-device deployment as they would enable
LLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units
(NPUs). In this work, we make a first attempt to facilitate the on-device
deployment of LLMs using integer-only quantization. We first investigate the
limitations of existing quantization methods for on-device deployment, with a
special focus on activation quantization. We then address these limitations by
introducing a simple post-training quantization method, named MobileQuant, that
extends previous weight equivalent transformation works by jointly optimizing
the weight transformation and activation range parameters in an end-to-end
manner. MobileQuant demonstrates superior capabilities over existing methods by
1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)
reducing latency and energy consumption by 20\%-50\% compared to current
on-device quantization strategies, 3) requiring limited compute budget, 4)
being compatible with mobile-friendly compute units, e.g. NPU.


# Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models

>Authors: Seyed Amir Ahmad Safavi-Naini,Shuhaib Ali,Omer Shahab,Zahra Shahhoseini,Thomas Savage,Sara Rafiee,Jamil S Samaan,Reem Al Shabeeb,Farah Ladak,Jamie O Yang,Juan Echavarria,Sumbal Babar,Aasma Shaukat,Samuel Margolis,Nicholas P Tatonetti,Girish Nadkarni,Bara El Kurdi,Ali Soroush

>2024-08-25

> http://arxiv.org/abs/2409.00084v2

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.


# Multi-SIGATnet: A multimodal schizophrenia MRI classification algorithm using sparse interaction mechanisms and graph attention networks

>Authors: Yuhong Jiao,Jiaqing Miao,Jinnan Gong,Hui He,Ping Liang,Cheng Luo,Ying Tan

>2024-08-25

> http://arxiv.org/abs/2408.13830v1

Schizophrenia is a serious psychiatric disorder. Its pathogenesis is not
completely clear, making it difficult to treat patients precisely. Because of
the complicated non-Euclidean network structure of the human brain, learning
critical information from brain networks remains difficult. To effectively
capture the topological information of brain neural networks, a novel
multimodal graph attention network based on sparse interaction mechanism
(Multi-SIGATnet) was proposed for SZ classification was proposed for SZ
classification. Firstly, structural and functional information were fused into
multimodal data to obtain more comprehensive and abundant features for patients
with SZ. Subsequently, a sparse interaction mechanism was proposed to
effectively extract salient features and enhance the feature representation
capability. By enhancing the strong connections and weakening the weak
connections between feature information based on an asymmetric convolutional
network, high-order interactive features were captured. Moreover, sparse
learning strategies were designed to filter out redundant connections to
improve model performance. Finally, local and global features were updated in
accordance with the topological features and connection weight constraints of
the higher-order brain network, the features being projected to the
classification target space for disorder classification. The effectiveness of
the model is verified on the Center for Biomedical Research Excellence (COBRE)
and University of California Los Angeles (UCLA) datasets, achieving 81.9\% and
75.8\% average accuracy, respectively, 4.6\% and 5.5\% higher than the graph
attention network (GAT) method. Experiments showed that the Multi-SIGATnet
method exhibited good performance in identifying SZ.


# Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors

>Authors: Xueying Ding,Rui Xi,Leman Akoglu

>2024-08-24

> http://arxiv.org/abs/2408.13667v1

The astonishing successes of ML have raised growing concern for the fairness
of modern methods when deployed in real world settings. However, studies on
fairness have mostly focused on supervised ML, while unsupervised outlier
detection (OD), with numerous applications in finance, security, etc., have
attracted little attention. While a few studies proposed fairness-enhanced OD
algorithms, they remain agnostic to the underlying driving mechanisms or
sources of unfairness. Even within the supervised ML literature, there exists
debate on whether unfairness stems solely from algorithmic biases (i.e. design
choices) or from the biases encoded in the data on which they are trained. To
close this gap, this work aims to shed light on the possible sources of
unfairness in OD by auditing detection models under different data-centric
factors. By injecting various known biases into the input data -- as pertain to
sample size disparity, under-representation, feature measurement noise, and
group membership obfuscation -- we find that the OD algorithms under the study
all exhibit fairness pitfalls, although differing in which types of data bias
they are more susceptible to. Most notable of our study is to demonstrate that
OD algorithm bias is not merely a data bias problem. A key realization is that
the data properties that emerge from bias injection could as well be organic --
as pertain to natural group differences w.r.t. sparsity, base rate, variance,
and multi-modality. Either natural or biased, such data properties can give
rise to unfairness as they interact with certain algorithmic design choices.


# Focus on Neighbors and Know the Whole: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation

>Authors: Bonan Li,Zicheng Zhang,Xingyi Yang,Xinchao Wang

>2024-08-23

> http://arxiv.org/abs/2408.13149v2

Generating dense multiview images from text prompts is crucial for creating
high-fidelity 3D assets. Nevertheless, existing methods struggle with
space-view correspondences, resulting in sparse and low-quality outputs. In
this paper, we introduce CoSER, a novel consistent dense Multiview
Text-to-Image Generator for Text-to-3D, achieving both efficiency and quality
by meticulously learning neighbor-view coherence and further alleviating
ambiguity through the swift traversal of all views. For achieving neighbor-view
consistency, each viewpoint densely interacts with adjacent viewpoints to
perceive the global spatial structure, and aggregates information along motion
paths explicitly defined by physical principles to refine details. To further
enhance cross-view consistency and alleviate content drift, CoSER rapidly scan
all views in spiral bidirectional manner to aware holistic information and then
scores each point based on semantic material. Subsequently, we conduct weighted
down-sampling along the spatial dimension based on scores, thereby facilitating
prominent information fusion across all views with lightweight computation.
Technically, the core module is built by integrating the attention mechanism
with a selective state space model, exploiting the robust learning capabilities
of the former and the low overhead of the latter. Extensive evaluation shows
that CoSER is capable of producing dense, high-fidelity, content-consistent
multiview images that can be flexibly integrated into various 3D generation
models.


# The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities

>Authors: Venkatesh Balavadhani Parthasarathy,Ahtsham Zafar,Aafaq Khan,Arsalan Shahid

>2024-08-23

> http://arxiv.org/abs/2408.13296v1

This report examines the fine-tuning of Large Language Models (LLMs),
integrating theoretical insights with practical applications. It outlines the
historical evolution of LLMs from traditional Natural Language Processing (NLP)
models to their pivotal role in AI. A comparison of fine-tuning methodologies,
including supervised, unsupervised, and instruction-based approaches,
highlights their applicability to different tasks. The report introduces a
structured seven-stage pipeline for fine-tuning LLMs, spanning data
preparation, model initialization, hyperparameter tuning, and model deployment.
Emphasis is placed on managing imbalanced datasets and optimization techniques.
Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half
Fine-Tuning are explored for balancing computational efficiency with
performance. Advanced techniques such as memory fine-tuning, Mixture of Experts
(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized
networks and multi-agent collaboration. The report also examines novel
approaches like Proximal Policy Optimization (PPO) and Direct Preference
Optimization (DPO), which align LLMs with human preferences, alongside pruning
and routing optimizations to improve efficiency. Further sections cover
validation frameworks, post-deployment monitoring, and inference optimization,
with attention to deploying LLMs on distributed and cloud-based platforms.
Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and
challenges related to scalability, privacy, and accountability are also
addressed. This report offers actionable insights for researchers and
practitioners navigating LLM fine-tuning in an evolving landscape.


# An alternative formulation of attention pooling function in translation

>Authors: Eddie Conti

>2024-08-23

> http://arxiv.org/abs/2409.00068v1

The aim of this paper is to present an alternative formulation of the
attention scoring function in translation tasks. Generally speaking, language
is deeply structured, and this is reflected in the attention scoring matrix. We
exploit this property to define the attention pooling function, taking this
aspect into account. In the first chapters, we introduce the attention
mechanism in mathematical terms and explain its limitations and alternative
formulations. Next, we focus on the experimental session that led to the
alternative formulation. Essentially, we guide queries and keys to interact in
a specific manner, encoding the distinct roles of attention heads and directing
values on where to seek context. In mathematical terms, we can think of this
formula as projecting the attention scores matrix, say $H$, onto the space of
band matrices with fixed bandwidth. This convex subspace is clearly
finite-dimensional and therefore closed. As a consequence, the projection on
this space is well-posed and unique. However, at the price of losing the
uniqueness of the projection (i.e., the best approximation for $H$), we defined
a new space consisting of band matrices plus error sparse matrices. We prove
that this is a compact subspace which guarantees the existence of a matrix that
best approximates $H$. We conclude the thesis by validating the new formula,
namely calculating how well the new formula for attention scores approximates
the original one. Additionally, we explore the impact of different parameters
such as w (context windows) and num-pos (number of relevant words in a
sentence). These analyses provide deeper insights into how languages are
processed and translated, revealing nuances in the roles of context and word
relevance.


# In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting

>Authors: Haowei Du,Dongyan Zhao

>2024-08-23

> http://arxiv.org/abs/2408.13028v1

In-context learning (ICL) of large language models (LLMs) has attracted
increasing attention in the community where LLMs make predictions only based on
instructions augmented with a few examples. Existing example selection methods
for ICL utilize sparse or dense retrievers and derive effective performance.
However, these methods do not utilize direct feedback of LLM to train the
retriever and the examples selected can not necessarily improve the analogy
ability of LLM. To tackle this, we propose our policy-based reinforcement
learning framework for example selection (RLS), which consists of a language
model (LM) selector and an LLM generator. The LM selector encodes the candidate
examples into dense representations and selects the top-k examples into the
demonstration for LLM. The outputs of LLM are adopted to compute the reward and
policy gradient to optimize the LM selector. We conduct experiments on
different datasets and significantly outperform existing example selection
methods. Moreover, our approach shows advantages over supervised finetuning
(SFT) models in few shot setting. Further experiments show the balance of
abundance and the similarity with the test case of examples is important for
ICL performance of LLM.


# Robust Iterative Value Conversion: Deep Reinforcement Learning for Neurochip-driven Edge Robots

>Authors: Yuki Kadokawa,Tomohito Kodera,Yoshihisa Tsurumine,Shinya Nishimura,Takamitsu Matsubara

>2024-08-23

> http://arxiv.org/abs/2408.13018v1

A neurochip is a device that reproduces the signal processing mechanisms of
brain neurons and calculates Spiking Neural Networks (SNNs) with low power
consumption and at high speed. Thus, neurochips are attracting attention from
edge robot applications, which suffer from limited battery capacity. This paper
aims to achieve deep reinforcement learning (DRL) that acquires SNN policies
suitable for neurochip implementation. Since DRL requires a complex function
approximation, we focus on conversion techniques from Floating Point NN (FPNN)
because it is one of the most feasible SNN techniques. However, DRL requires
conversions to SNNs for every policy update to collect the learning samples for
a DRL-learning cycle, which updates the FPNN policy and collects the SNN policy
samples. Accumulative conversion errors can significantly degrade the
performance of the SNN policies. We propose Robust Iterative Value Conversion
(RIVC) as a DRL that incorporates conversion error reduction and robustness to
conversion errors. To reduce them, FPNN is optimized with the same number of
quantization bits as an SNN. The FPNN output is not significantly changed by
quantization. To robustify the conversion error, an FPNN policy that is applied
with quantization is updated to increase the gap between the probability of
selecting the optimal action and other actions. This step prevents unexpected
replacements of the policy's optimal actions. We verified RIVC's effectiveness
on a neurochip-driven robot. The results showed that RIVC consumed 1/15 times
less power and increased the calculation speed by five times more than an edge
CPU (quad-core ARM Cortex-A72). The previous framework with no countermeasures
against conversion errors failed to train the policies. Videos from our
experiments are available: https://youtu.be/Q5Z0-BvK1Tc.


# Growing Deep Neural Network Considering with Similarity between Neurons

>Authors: Taigo Sakai,Kazuhiro Hotta

>2024-08-23

> http://arxiv.org/abs/2408.13291v1

Deep learning has excelled in image recognition tasks through neural networks
inspired by the human brain. However, the necessity for large models to improve
prediction accuracy introduces significant computational demands and extended
training times.Conventional methods such as fine-tuning, knowledge
distillation, and pruning have the limitations like potential accuracy drops.
Drawing inspiration from human neurogenesis, where neuron formation continues
into adulthood, we explore a novel approach of progressively increasing neuron
numbers in compact models during training phases, thereby managing
computational costs effectively. We propose a method that reduces feature
extraction biases and neuronal redundancy by introducing constraints based on
neuron similarity distributions. This approach not only fosters efficient
learning in new neurons but also enhances feature extraction relevancy for
given tasks. Results on CIFAR-10 and CIFAR-100 datasets demonstrated accuracy
improvement, and our method pays more attention to whole object to be
classified in comparison with conventional method through Grad-CAM
visualizations. These results suggest that our method's potential to
decision-making processes.


# From Few to More: Scribble-based Medical Image Segmentation via Masked Context Modeling and Continuous Pseudo Labels

>Authors: Zhisong Wang,Yiwen Ye,Ziyang Chen,Minglei Shu,Yong Xia

>2024-08-23

> http://arxiv.org/abs/2408.12814v1

Scribble-based weakly supervised segmentation techniques offer comparable
performance to fully supervised methods while significantly reducing annotation
costs, making them an appealing alternative. Existing methods often rely on
auxiliary tasks to enforce semantic consistency and use hard pseudo labels for
supervision. However, these methods often overlook the unique requirements of
models trained with sparse annotations. Since the model must predict pixel-wise
segmentation maps with limited annotations, the ability to handle varying
levels of annotation richness is critical. In this paper, we adopt the
principle of `from few to more' and propose MaCo, a weakly supervised framework
designed for medical image segmentation. MaCo employs masked context modeling
(MCM) and continuous pseudo labels (CPL). MCM uses an attention-based masking
strategy to disrupt the input image, compelling the model's predictions to
remain consistent with those of the original image. CPL converts scribble
annotations into continuous pixel-wise labels by applying an exponential decay
function to distance maps, resulting in continuous maps that represent the
confidence of each pixel belonging to a specific category, rather than using
hard pseudo labels. We evaluate MaCo against other weakly supervised methods
using three public datasets. The results indicate that MaCo outperforms
competing methods across all datasets, setting a new record in weakly
supervised medical image segmentation.


# A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model

>Authors: Shuo Yang,Shizhen Li,Yanjun Huang,Hong Chen

>2024-08-23

> http://arxiv.org/abs/2408.12805v1

Autonomous driving systems with self-evolution capabilities have the
potential to independently evolve in complex and open environments, allowing to
handle more unknown scenarios. However, as a result of the safety-performance
trade-off mechanism of evolutionary algorithms, it is difficult to ensure safe
exploration without sacrificing the improvement ability. This problem is
especially prominent in dynamic traffic scenarios. Therefore, this paper
proposes a safe self-evolution algorithm for autonomous driving based on
data-driven risk quantification model. Specifically, a risk quantification
model based on the attention mechanism is proposed by modeling the way humans
perceive risks during driving, with the idea of achieving safety situation
estimation of the surrounding environment through a data-driven approach. To
prevent the impact of over-conservative safety guarding policies on the
self-evolution capability of the algorithm, a safety-evolutionary
decision-control integration algorithm with adjustable safety limits is
proposed, and the proposed risk quantization model is integrated into it.
Simulation and real-vehicle experiments results illustrate the effectiveness of
the proposed method. The results show that the proposed algorithm can generate
safe and reasonable actions in a variety of complex scenarios and guarantee
safety without losing the evolutionary potential of learning-based autonomous
driving systems.


# TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing

>Authors: Abhishek Moitra,Abhiroop Bhattacharjee,Youngeun Kim,Priyadarshini Panda

>2024-08-22

> http://arxiv.org/abs/2408.12742v1

Due to the high computation overhead of Vision Transformers (ViTs), In-memory
Computing architectures are being researched towards energy-efficient
deployment in edge-computing scenarios. Prior works have proposed efficient
algorithm-hardware co-design and IMC-architectural improvements to improve the
energy-efficiency of IMC-implemented ViTs. However, all prior works have
neglected the overhead and co-depencence of attention blocks on the
accuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose
TReX- an attention-reuse-driven ViT optimization framework that effectively
performs attention reuse in ViT models to achieve optimal
accuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer
encoders for attention reuse to achieve near iso-accuracy performance while
meeting the user-specified delay requirement. Based on our analysis on the
Imagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and
1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S
(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP
reduction compared to state-of-the-art token pruning and weight sharing
approaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal
accuracy compared to baseline at 1.6x lower EDAP.


# Tunneling photo-thermoelectric effect in monolayer graphene/bilayer hexagonal boron nitride/bilayer graphene asymmetric van der Waals tunnel junctions

>Authors: Sabin Park,Rai Moriya,Yijin Zhang,Kenji Watanabe,Takashi Taniguchi,Tomoki Machida

>2024-08-22

> http://arxiv.org/abs/2408.12501v1

Graphene is known to exhibit a pronounced photo-thermoelectric effect (PTE)
in its in-plane carrier transport and attracting attention toward various
optoelectronic applications. In this study, we demonstrate an out-of-plane PTE
by utilizing electron tunneling across a barrier, namely, the tunneling
photo-thermoelectric effect (TPTE). This was achieved in a monolayer graphene
(MLG)/bilayer hexagonal boron nitride (h-BN)/bilayer graphene (BLG) asymmetric
tunnel junction. MLG and BLG exhibit different cyclotron resonance (CR) optical
absorption energies when their energies are Landau quantized under an
out-of-plane magnetic field. We tuned the magnetic field under mid-infrared
(MIR) irradiation to bring MLG into CR conditions, whereas BLG was not in CR.
The CR absorption in the MLG generates an electron temperature difference
between the MLG and BLG, and induces an out-of-plane TPTE voltage across the
h-BN tunnel barrier. The TPTE exhibited a unique dependence on the Fermi energy
of the MLG, which differed from that of the in-plane PTE of the MLG. The TPTE
signal was large when the Fermi energy of the MLG was tuned near the phase
transition between the quantum Hall state (QHS) and non-QHS, that is, the
transition between carrier localization and delocalization. The TPTE provides
another degree of freedom for probing the electronic and optoelectronic
properties of two-dimensional material heterostructures.


# Self-supervised Learning for Geospatial AI: A Survey

>Authors: Yile Chen,Weiming Huang,Kaiqi Zhao,Yue Jiang,Gao Cong

>2024-08-22

> http://arxiv.org/abs/2408.12133v1

The proliferation of geospatial data in urban and territorial environments
has significantly facilitated the development of geospatial artificial
intelligence (GeoAI) across various urban applications. Given the vast yet
inherently sparse labeled nature of geospatial data, there is a critical need
for techniques that can effectively leverage such data without heavy reliance
on labeled datasets. This requirement aligns with the principles of
self-supervised learning (SSL), which has attracted increasing attention for
its adoption in geospatial data. This paper conducts a comprehensive and
up-to-date survey of SSL techniques applied to or developed for three primary
data (geometric) types prevalent in geospatial vector data: points, polylines,
and polygons. We systematically categorize various SSL techniques into
predictive and contrastive methods, discussing their application with respect
to each data type in enhancing generalization across various downstream tasks.
Furthermore, we review the emerging trends of SSL for GeoAI, and several
task-specific SSL techniques. Finally, we discuss several key challenges in the
current research and outline promising directions for future investigation. By
presenting a structured analysis of relevant studies, this paper aims to
inspire continued advancements in the integration of SSL with GeoAI,
encouraging innovative methods to harnessing the power of geospatial data.


# Matmul or No Matmul in the Era of 1-bit LLMs

>Authors: Jinendra Malekar,Mohammed E. Elbtity,Ramtin Zand

>2024-08-21

> http://arxiv.org/abs/2408.11939v2

The advent of 1-bit large language models (LLMs) has attracted considerable
attention and opened up new research opportunities. However, 1-bit LLMs only
improve a fraction of models by applying extreme quantization to the projection
layers while leaving attention heads unchanged. Therefore, to avoid
fundamentally wrong choices of goals in future research, it is crucial to
understand the actual improvements in computation and memory usage that 1-bit
LLMs can deliver. In this work, we present an adaptation of Amdahl's Law
tailored for the 1-bit LLM context, which illustrates how partial improvements
in 1-bit LLMs impact overall model performance. Through extensive experiments,
we uncover key nuances across different model architectures and hardware
configurations, offering a roadmap for future research in the era of 1-bit
LLMs.


# Practical token pruning for foundation models in few-shot conversational virtual assistant systems

>Authors: Haode Qi,Cheng Qian,Jian Ni,Pratyush Singh,Reza Fazeli,Gengyu Wang,Zhongzheng Shu,Eric Wayne,Juergen Bross

>2024-08-21

> http://arxiv.org/abs/2408.11799v1

In an enterprise Virtual Assistant (VA) system, intent classification is the
crucial component that determines how a user input is handled based on what the
user wants. The VA system is expected to be a cost-efficient SaaS service with
low training and inference time while achieving high accuracy even with a small
number of training samples. We pretrain a transformer-based sentence embedding
model with a contrastive learning objective and leverage the embedding of the
model as features when training intent classification models. Our approach
achieves the state-of-the-art results for few-shot scenarios and performs
better than other commercial solutions on popular intent classification
benchmarks. However, generating features via a transformer-based model
increases the inference time, especially for longer user inputs, due to the
quadratic runtime of the transformer's attention mechanism. On top of model
distillation, we introduce a practical multi-task adaptation approach that
configures dynamic token pruning without the need for task-specific training
for intent classification. We demonstrate that this approach improves the
inference speed of popular sentence transformer models without affecting model
performance.


# LLM Pruning and Distillation in Practice: The Minitron Approach

>Authors: Sharath Turuvekere Sreenivas,Saurav Muralidharan,Raviraj Joshi,Marcin Chochowski,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jan Kautz,Pavlo Molchanov

>2024-08-21

> http://arxiv.org/abs/2408.11796v2

We present a comprehensive report on compressing the Llama 3.1 8B and Mistral
NeMo 12B models to 4B and 8B parameters, respectively, using pruning and
distillation. We explore two distinct pruning strategies: (1) depth pruning and
(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on
common benchmarks from the LM Evaluation Harness. The models are then aligned
with NeMo Aligner and tested in instruct-tuned versions. This approach produces
a compelling 4B model from Llama 3.1 8B and a state-of-the-art
Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo
12B. We found that with no access to the original data, it is beneficial to
slightly fine-tune teacher models on the distillation dataset. We open-source
our base model weights on Hugging Face with a permissive license.


