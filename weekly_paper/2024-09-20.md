# LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling

>Authors: Xin Li,Anand Sarwate

>2024-09-16

> http://arxiv.org/abs/2409.11184v1

Learning compact and meaningful latent space representations has been shown
to be very useful in generative modeling tasks for visual data. One particular
example is applying Vector Quantization (VQ) in variational autoencoders
(VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance
in many modern generative modeling applications. Quantizing the latent space
has been justified by the assumption that the data themselves are inherently
discrete in the latent space (like pixel values). In this paper, we propose an
alternative representation of the latent space by relaxing the structural
assumption than the VQ formulation. Specifically, we assume that the latent
space can be approximated by a union of subspaces model corresponding to a
dictionary-based representation under a sparsity constraint. The dictionary is
learned/updated during the training process. We apply this approach to look at
two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs
with Generative Adversarial Networks (DL-GANs). We show empirically that our
more latent space is more expressive and has leads to better representations
than the VQ approach in terms of reconstruction quality at the expense of a
small computational overhead for the latent space computation. Our results thus
suggest that the true benefit of the VQ approach might not be from
discretization of the latent space, but rather the lossy compression of the
latent space. We confirm this hypothesis by showing that our sparse
representations also address the codebook collapse issue as found common in
VQ-family models.


# Robust Training of Neural Networks at Arbitrary Precision and Sparsity

>Authors: Chengxi Ye,Grace Chu,Yanfeng Liu,Yichi Zhang,Lukasz Lew,Andrew Howard

>2024-09-14

> http://arxiv.org/abs/2409.09245v1

The discontinuous operations inherent in quantization and sparsification
introduce obstacles to backpropagation. This is particularly challenging when
training deep neural networks in ultra-low precision and sparse regimes. We
propose a novel, robust, and universal solution: a denoising affine transform
that stabilizes training under these challenging conditions. By formulating
quantization and sparsification as perturbations during training, we derive a
perturbation-resilient approach based on ridge regression. Our solution employs
a piecewise constant backbone model to ensure a performance lower bound and
features an inherent noise reduction mechanism to mitigate perturbation-induced
corruption. This formulation allows existing models to be trained at
arbitrarily low precision and sparsity levels with off-the-shelf recipes.
Furthermore, our method provides a novel perspective on training temporal
binary neural networks, contributing to ongoing efforts to narrow the gap
between artificial and biological neural networks.


# S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training

>Authors: Yuezhou Hu,Jun Zhu,Jianfei Chen

>2024-09-13

> http://arxiv.org/abs/2409.09099v1

Training deep neural networks (DNNs) is costly. Fortunately, Nvidia Ampere
and Hopper GPUs can accelerate matrix multiplications twice as fast as a dense
equivalent by implementing 2:4 sparsity. However, previous STE-based 2:4
pre-training methods (e.g. STE with hard-thresholding, SR-STE) suffer from
optimization difficulties because of discontinuous pruning function. In this
study, we comprehensively analyse the bottleneck of traditional N:M sparse
training and recognize three drawbacks with discontinuity: incorrect descending
direction, inability to predict the amount of descent and sparse mask
oscillation. In the light of this statement, we propose S-STE, a simple yet
powerful 2:4 training method that contains two parts: to continuously project
weights to be 2:4 sparse, and to rescale sparse weights with a per-tensor fixed
scaling factor. Besides, we adopt minimum-variance unbiased estimation for
activation gradient and FP8 quantization for whole process. Results show that
our method surpass previous 2:4 pre-training recipes and is comparable even
with full parameter models.


# BBS: Bi-directional Bit-level Sparsity for Deep Learning Acceleration

>Authors: Yuzong Chen,Jian Meng,Jae-sun Seo,Mohamed S. Abdelfattah

>2024-09-08

> http://arxiv.org/abs/2409.05227v1

Bit-level sparsity methods skip ineffectual zero-bit operations and are
typically applicable within bit-serial deep learning accelerators. This type of
sparsity at the bit-level is especially interesting because it is both
orthogonal and compatible with other deep neural network (DNN) efficiency
methods such as quantization and pruning. In this work, we improve the
practicality and efficiency of bitlevel sparsity through a novel algorithmic
bit-pruning, averaging, and compression method, and a co-designed efficient
bit-serial hardware accelerator. On the algorithmic side, we introduce
bidirectional bit sparsity (BBS). The key insight of BBS is that we can
leverage bit sparsity in a symmetrical way to prune either zero-bits or
one-bits. This significantly improves the load balance of bit-serial computing
and guarantees the level of sparsity to be more than 50%. On top of BBS, we
further propose two bit-level binary pruning methods that require no
retraining, and can be seamlessly applied to quantized DNNs. Combining binary
pruning with a new tensor encoding scheme, BBS can both skip computation and
reduce the memory footprint associated with bi-directional sparse bit columns.
On the hardware side, we demonstrate the potential of BBS through BitVert, a
bitserial architecture with an efficient PE design to accelerate DNNs with low
overhead, exploiting our proposed binary pruning. Evaluation on seven
representative DNN models shows that our approach achieves: (1) on average
1.66$\times$ reduction in model sizewith negligible accuracy loss of < 0.5%;
(2) up to 3.03$\times$ speedupand 2.44$\times$ energy saving compared to prior
DNN accelerators.


# FireFly-S: Exploiting Dual-Side Sparsity for Spiking Neural Networks Acceleration with Reconfigurable Spatial Architecture

>Authors: Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng

>2024-08-28

> http://arxiv.org/abs/2408.15578v1

Spiking Neural Networks (SNNs), with their brain-inspired structure using
discrete spikes instead of continuous activations, are gaining attention for
their potential of efficient processing on neuromorphic chips. While current
SNN hardware accelerators often prioritize temporal spike sparsity, exploiting
sparse synaptic weights offers significant untapped potential for even greater
efficiency. To address this, we propose FireFly-S, a Sparse extension of the
FireFly series. This co-optimized software-hardware design focusing on
leveraging dual-side sparsity for acceleration. On the software side, we
propose a novel algorithmic optimization framework that combines gradient
rewiring for pruning and modified Learned Step Size Quantization (LSQ) tailored
for SNNs, which achieves remarkable weight sparsity exceeding 85\% and enables
efficient 4-bit quantization with negligible accuracy loss. On the hardware
side, we present an efficient dual-side sparsity detector employing a
Bitmap-based sparse decoding logic to pinpoint the positions of non-zero
weights and input spikes. The logic allows for the direct bypassing of
redundant computations, thereby enhancing computational efficiency. Different
from the overlay architecture adopted by previous FireFly series, we adopt a
spatial architecture with inter-layer pipelining that can fully exploit the
nature of Field-Programmable Gate Arrays (FPGAs). A spatial-temporal dataflow
is also proposed to support such inter-layer pipelining and avoid long-term
temporal dependencies. In experiments conducted on the MNIST, DVS-Gesture and
CIFAR-10 datasets, the FireFly-S model achieves 85-95\% sparsity with 4-bit
quantization and the hardware accelerator effectively leverages the dual-side
sparsity, delivering outstanding performance metrics of 10,047 FPS/W on MNIST,
3,683 FPS/W on DVS-Gesture, and 2,327 FPS/W on CIFAR-10.


# Training-Free Activation Sparsity in Large Language Models

>Authors: James Liu,Pragaash Ponnusamy,Tianle Cai,Han Guo,Yoon Kim,Ben Athiwaratkun

>2024-08-26

> http://arxiv.org/abs/2408.14690v1

Activation sparsity can enable practical inference speedups in large language
models (LLMs) by reducing the compute and memory-movement required for matrix
multiplications during the forward pass. However, existing methods face
limitations that inhibit widespread adoption. Some approaches are tailored
towards older models with ReLU-based sparsity, while others require extensive
continued pre-training on up to hundreds of billions of tokens. This paper
describes TEAL, a simple training-free method that applies magnitude-based
activation sparsity to hidden states throughout the entire model. TEAL achieves
40-50% model-wide sparsity with minimal performance degradation across Llama-2,
Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve
existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to
1.53$\times$ and 1.8$\times$ at 40% and 50% model-wide sparsity. TEAL is
compatible with weight quantization, enabling further efficiency gains.


# STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs

>Authors: Peijie Dong,Lujun Li,Dayou Du,Yuhan Chen,Zhenheng Tang,Qiang Wang,Wei Xue,Wenhan Luo,Qifeng Liu,Yike Guo,Xiaowen Chu

>2024-08-03

> http://arxiv.org/abs/2408.01803v1

In this paper, we present STBLLM, the first structural binarization framework
for compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs
have achieved remarkable performance, but their heavy memory requirements have
hindered widespread adoption, particularly on resource-constrained devices.
Binarization, which quantifies weights to a mere 1-bit, achieves a milestone in
increasing computational efficiency. However, we observe that some weights in
binarized LLMs can be randomly flipped without significant performance
degradation, indicating the potential for further compression. To exploit this,
our STBLLM employs an N:M sparsity to perform structural binarization of the
weights. First, we introduce a new Standardized Importance (SI) metric that
considers weight magnitude and input feature norm to better evaluate weight
significance. Then, we propose a layer-wise approach where different layers of
the LLM can be sparsified with varying N:M ratios, balancing compression and
accuracy. Finally, we use residual approximation with double binarization to
preserve information for salient weights. In addition, we utilize a
fine-grained grouping strategy for less important weights that applies
different quantization schemes to sparse, intermediate, and dense regions. We
conduct extensive experiments on various language models, including the
LLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.
The results demonstrate that our approach performs better than other compressed
binarization LLM methods while significantly reducing memory requirements.


