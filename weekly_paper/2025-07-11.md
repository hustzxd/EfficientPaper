# 2025-07-11

# Table of Contents
* [Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs](#Skip-a-Layer-or-Loop-it?-Test-Time-Depth-Adaptation-of-Pretrained-LLMs)
* [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](#Multi-Granular-Spatio-Temporal-Token-Merging-for-Training-Free-Acceleration-of-Video-LLMs)
* [MoSE Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](#MoSE-Skill-by-Skill-Mixture-of-Expert-Learning-for-Autonomous-Driving)
* [When Large Language Models Meet Law Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](#When-Large-Language-Models-Meet-Law-Dual-Lens-Taxonomy,-Technical-Advances,-and-Ethical-Governance)
* [On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions](#On-the-capabilities-of-LLMs-for-classifying-and-segmenting-time-series-of-fruit-picking-motions-into-primitive-actions)
* [KeyKnowledgeRAG (K^2RAG) An Enhanced RAG method for improved LLM question-answering capabilities](#KeyKnowledgeRAG-(K^2RAG)-An-Enhanced-RAG-method-for-improved-LLM-question-answering-capabilities)
* [Position We Need An Algorithmic Understanding of Generative AI](#Position-We-Need-An-Algorithmic-Understanding-of-Generative-AI)
* [MedReadCtrl Personalizing medical text generation with readability-controlled instruction learning](#MedReadCtrl-Personalizing-medical-text-generation-with-readability-controlled-instruction-learning)
* [KVFlow Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](#KVFlow-Efficient-Prefix-Caching-for-Accelerating-LLM-Based-Multi-Agent-Workflows)
* [Compute Can't Handle the Truth Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](#Compute-Can't-Handle-the-Truth-Why-Communication-Tax-Prioritizes-Memory-and-Interconnects-in-Modern-AI-Infrastructure)
* [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](#Neurosymbolic-Feature-Extraction-for-Identifying-Forced-Labor-in-Supply-Chains)
* [Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning](#Boosting-Parameter-Efficiency-in-LLM-Based-Recommendation-through-Sophisticated-Pruning)
* [QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks](#QoE-Optimization-for-Semantic-Self-Correcting-Video-Transmission-in-Multi-UAV-Networks)
* [CCQ Convolutional Code for Extreme Low-bit Quantization in LLMs](#CCQ-Convolutional-Code-for-Extreme-Low-bit-Quantization-in-LLMs)
* [Direct Regret Optimization in Bayesian Optimization](#Direct-Regret-Optimization-in-Bayesian-Optimization)
* [SpindleKV A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](#SpindleKV-A-Novel-KV-Cache-Reduction-Method-Balancing-Both-Shallow-and-Deep-Layers)
* [MoFE-Time Mixture of Frequency Domain Experts for Time-Series Forecasting Models](#MoFE-Time-Mixture-of-Frequency-Domain-Experts-for-Time-Series-Forecasting-Models)
* [Video-RTS Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](#Video-RTS-Rethinking-Reinforcement-Learning-and-Test-Time-Scaling-for-Efficient-and-Enhanced-Video-Reasoning)
* [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](#Exploring-Task-Performance-with-Interpretable-Models-via-Sparse-Auto-Encoders)
* [Ampere Communication-Efficient and High-Accuracy Split Federated Learning](#Ampere-Communication-Efficient-and-High-Accuracy-Split-Federated-Learning)
* [SLDB An End-To-End Heterogeneous System-on-Chip Benchmark Suite for LLM-Aided Design](#SLDB-An-End-To-End-Heterogeneous-System-on-Chip-Benchmark-Suite-for-LLM-Aided-Design)
* [ETT Expanding the Long Context Understanding Capability of LLMs at Test-Time](#ETT-Expanding-the-Long-Context-Understanding-Capability-of-LLMs-at-Test-Time)
* [Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations](#Hierarchical-Interaction-Summarization-and-Contrastive-Prompting-for-Explainable-Recommendations)
* [Large Language Models for Agent-Based Modelling Current and possible uses across the modelling cycle](#Large-Language-Models-for-Agent-Based-Modelling-Current-and-possible-uses-across-the-modelling-cycle)
* [SenseCF LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](#SenseCF-LLM-Prompted-Counterfactuals-for-Intervention-and-Sensor-Data-Augmentation)
* [Helix Parallelism Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](#Helix-Parallelism-Rethinking-Sharding-Strategies-for-Interactive-Multi-Million-Token-LLM-Decoding)
* [StreamVLN Streaming Vision-and-Language Navigation via SlowFast Context Modeling](#StreamVLN-Streaming-Vision-and-Language-Navigation-via-SlowFast-Context-Modeling)
* [Cascade Token-Sharded Private LLM Inference](#Cascade-Token-Sharded-Private-LLM-Inference)
* [All in One Visual-Description-Guided Unified Point Cloud Segmentation](#All-in-One-Visual-Description-Guided-Unified-Point-Cloud-Segmentation)
* [CREW-WILDFIRE Benchmarking Agentic Multi-Agent Collaborations at Scale](#CREW-WILDFIRE-Benchmarking-Agentic-Multi-Agent-Collaborations-at-Scale)
* [The Case for Instance-Optimized LLMs in OLAP Databases](#The-Case-for-Instance-Optimized-LLMs-in-OLAP-Databases)
* [LIFT Automating Symbolic Execution Optimization with Large Language Models for AI Networks](#LIFT-Automating-Symbolic-Execution-Optimization-with-Large-Language-Models-for-AI-Networks)
* [DoPI Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine](#DoPI-Doctor-like-Proactive-Interrogation-LLM-for-Traditional-Chinese-Medicine)
* [FurniMAS Language-Guided Furniture Decoration using Multi-Agent System](#FurniMAS-Language-Guided-Furniture-Decoration-using-Multi-Agent-System)
* [Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems](#Who's-the-Mole?-Modeling-and-Detecting-Intention-Hiding-Malicious-Agents-in-LLM-Based-Multi-Agent-Systems)
* [LOOM-Scope a comprehensive and efficient LOng-cOntext Model evaluation framework](#LOOM-Scope-a-comprehensive-and-efficient-LOng-cOntext-Model-evaluation-framework)


## Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs

>Authors: Ziyue Li, Yang Li, Tianyi Zhou

>2025-07-10

> http://arxiv.org/abs/2507.07996v1

Can a pretrained neural network adapt its architecture to different inputs
without any finetuning? Do we need all layers for simple tasks, and are they
adequate for challenging tasks? We found that the layers of a pretrained large
language model (![key](https://img.shields.io/badge/LLM-FF8C00)) can be manipulated as separate modules to build a better
and even shallower model customized for each test sample. In particular, each
layer from the pretrained model can be skipped/pruned or repeated multiple
times as recurrent neural networks (RNN), and stacked with others in arbitrary
orders, yielding a chain-of-layers (CoLa) per sample. This compositional space
greatly expands the scope of existing works on looped/recurrent pretrained
modules, layer ![key](https://img.shields.io/badge/pruning-F08080), or early-exit networks. We develop a Monte Carlo Tree
Search (MCTS) protocol to explore and identify the optimal CoLa for each sample
from math and commonsense reasoning benchmarks. Compared to a static model of a
fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same
layer(s) (slow thinking), and combining both, offering more flexible, dynamic
architectures for different inputs. We conduct an extensive analysis of the
MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples
with correct predictions by the original ![key](https://img.shields.io/badge/LLM-FF8C00), we can find shorter CoLa,
suggesting a large space for improving inference efficiency; (2) For >60% of
samples with originally incorrect predictions, we can identify CoLa achieving
correct predictions, suggesting a large space of performance enhancement. Our
results highlight the shortcomings of using a fixed architecture of pre-trained
![key](https://img.shields.io/badge/LLM-FF8C00)s for inference on different samples and pave the way to unlock the
generalization power of test-time depth adaptation.


## Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs

>Authors: Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim

>2025-07-10

> http://arxiv.org/abs/2507.07990v1

Video large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) achieve strong video understanding by
leveraging a large number of spatio-temporal tokens, but suffer from quadratic
computational scaling with token count. To address this, we propose a
training-free spatio-temporal token merging method, named STTM. Our key insight
is to exploit local spatial and temporal redundancy in video data which has
been overlooked in prior work. STTM first transforms each frame into
multi-granular spatial tokens using a coarse-to-fine search over a quadtree
structure, then performs directed pairwise merging across the temporal
dimension. This decomposed merging approach outperforms existing token
reduction methods across six video QA benchmarks. Notably, STTM achieves a
2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and
a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is
query-agnostic, allowing ![key](https://img.shields.io/badge/KV-F08080) cache reuse across different questions for the same
video. The project page is available at https://www.jshyun.me/projects/sttm.


## MoSE Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving

>Authors: Lu Xu, Jiaqian Yu, Xiongfeng Peng, Yiwei Chen, Weiming Li, Jaewook Yoo, Sunghyun Chunag, Dongwook Lee, Daehyun Ji, Chao Zhang

>2025-07-10

> http://arxiv.org/abs/2507.07818v1

Recent studies show large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general ![key](https://img.shields.io/badge/LLM-FF8C00)s or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B ![key](https://img.shields.io/badge/sparse-F08080)ly activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.


## When Large Language Models Meet Law Dual-Lens Taxonomy, Technical Advances, and Ethical Governance

>Authors: Peizhang Shao, Linrui Xu, Jinxi Wang, Wei Zhou, Xingyu Wu

>2025-07-10

> http://arxiv.org/abs/2507.07748v1

This paper establishes the first comprehensive review of Large Language
Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based ![key](https://img.shields.io/badge/LLM-FF8C00)s, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like ![key](https://img.shields.io/badge/sparse-F08080) attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of ![key](https://img.shields.io/badge/LLM-FF8C00) introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/![key](https://img.shields.io/badge/LLM-FF8C00)s_Meet_Law.


## On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions

>Authors: Eleni Konstantinidou, Nikolaos Kounalakis, Nikolaos Efstathopoulos, Dimitrios Papageorgiou

>2025-07-10

> http://arxiv.org/abs/2507.07745v1

Despite their recent introduction to human society, Large Language Models
(![key](https://img.shields.io/badge/LLM-FF8C00)s) have significantly affected the way we tackle mental challenges in our
everyday lives. From optimizing our linguistic ![key](https://img.shields.io/badge/communication-F08080) to assisting us in
making important decisions, ![key](https://img.shields.io/badge/LLM-FF8C00)s, such as ChatGPT, are notably reducing our
cognitive load by gradually taking on an increasing share of our mental
activities. In the context of Learning by Demonstration (LbD), classifying and
segmenting complex motions into primitive actions, such as pushing, pulling,
twisting etc, is considered to be a key-step towards encoding a task. In this
work, we investigate the capabilities of ![key](https://img.shields.io/badge/LLM-FF8C00)s to undertake this task,
considering a finite set of predefined primitive actions found in fruit picking
operations. By utilizing ![key](https://img.shields.io/badge/LLM-FF8C00)s instead of simple supervised learning or analytic
methods, we aim at making the method easily applicable and deployable in a
real-life scenario. Three different fine-tuning approaches are investigated,
compared on datasets captured kinesthetically, using a UR10e robot, during a
fruit-picking scenario.


## KeyKnowledgeRAG (K^2RAG) An Enhanced RAG method for improved LLM question-answering capabilities

>Authors: Hruday Markondapatnaikuni, Basem Suleiman, Abdelkarim Erradi, Shijing Chen

>2025-07-10

> http://arxiv.org/abs/2507.07695v1

Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as ![key](https://img.shields.io/badge/LLM-FF8C00)s continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in ![key](https://img.shields.io/badge/LLM-FF8C00)s is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and ![key](https://img.shields.io/badge/sparse-F08080) vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.


## Position We Need An Algorithmic Understanding of Generative AI

>Authors: Oliver Eberle, Thomas McGee, Hamza Giaffar, Taylor Webb, Ida Momennejad

>2025-07-10

> http://arxiv.org/abs/2507.07544v1

What algorithms do ![key](https://img.shields.io/badge/LLM-FF8C00)s actually learn and use to solve problems? Studies
addressing this question are ![key](https://img.shields.io/badge/sparse-F08080), as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that ![key](https://img.shields.io/badge/LLM-FF8C00)s learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how ![key](https://img.shields.io/badge/LLM-FF8C00)s actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.


## MedReadCtrl Personalizing medical text generation with readability-controlled instruction learning

>Authors: Hieu Tran, Zonghai Yao, Won Seok Jang, Sharmin Sultana, Allen Chang, Yuan Zhang, Hong Yu

>2025-07-10

> http://arxiv.org/abs/2507.07419v1

Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI ![key](https://img.shields.io/badge/communication-F08080), where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables ![key](https://img.shields.io/badge/LLM-FF8C00)s to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.


## KVFlow Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows

>Authors: Zaifeng Pan, Ajjkumar Patel, Zhengding Hu, Yipeng Shen, Yue Guan, Wan-Lu Li, Lianhui Qin, Yida Wang, Yufei Ding

>2025-07-10

> http://arxiv.org/abs/2507.07400v1

Large language model (![key](https://img.shields.io/badge/LLM-FF8C00)) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing ![key](https://img.shields.io/badge/LLM-FF8C00) systems employ prefix caching to
reuse key-value (![key](https://img.shields.io/badge/KV-F08080)) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict ![key](https://img.shields.io/badge/KV-F08080) caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards ![key](https://img.shields.io/badge/KV-F08080) caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present ![key](https://img.shields.io/badge/KV-F08080)Flow, a workflow-aware ![key](https://img.shields.io/badge/KV-F08080)
cache management framework tailored for agentic workloads. ![key](https://img.shields.io/badge/KV-F08080)Flow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the ![key](https://img.shields.io/badge/KV-F08080) node
level, allowing ![key](https://img.shields.io/badge/KV-F08080)Flow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, ![key](https://img.shields.io/badge/KV-F08080)Flow introduces a
fully ![key](https://img.shields.io/badge/overlap-F08080)ped ![key](https://img.shields.io/badge/KV-F08080) prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, ![key](https://img.shields.io/badge/KV-F08080)Flow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.


## Compute Can't Handle the Truth Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure

>Authors: Myoungsoo Jung

>2025-07-09

> http://arxiv.org/abs/2507.07223v1

Modern AI workloads such as large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) and
retrieval-augmented generation (RAG) impose severe demands on memory,
![key](https://img.shields.io/badge/communication-F08080) bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU ![key](https://img.shields.io/badge/communication-F08080)
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in ![key](https://img.shields.io/badge/LLM-FF8C00)s. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.


## Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains

>Authors: Zili Wang, Frank Montabon, Kristin Yvonne Rozier

>2025-07-09

> http://arxiv.org/abs/2507.07217v1

Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very ![key](https://img.shields.io/badge/sparse-F08080) data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (![key](https://img.shields.io/badge/LLM-FF8C00)) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.


## Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning

>Authors: Shanle Zheng, Keqin Bao, Jizhi Zhang, Yang Zhang, Fuli Feng, Xiangnan He

>2025-07-09

> http://arxiv.org/abs/2507.07064v1

![key](https://img.shields.io/badge/LLM-FF8C00)-based recommender systems have made significant progress; however, the
deployment cost associated with the large parameter volume of ![key](https://img.shields.io/badge/LLM-FF8C00)s still
hinders their real-world applications. This work explores parameter ![key](https://img.shields.io/badge/pruning-F08080) to
improve parameter efficiency while maintaining recommendation quality, thereby
enabling easier deployment. Unlike existing approaches that focus primarily on
inter-layer redundancy, we uncover intra-layer redundancy within components
such as self-attention and MLP modules. Building on this analysis, we propose a
more fine-grained ![key](https://img.shields.io/badge/pruning-F08080) approach that integrates both intra-layer and
layer-wise ![key](https://img.shields.io/badge/pruning-F08080). Specifically, we introduce a three-stage ![key](https://img.shields.io/badge/pruning-F08080) strategy
that progressively prunes parameters at different levels and parts of the
model, moving from intra-layer to layer-wise ![key](https://img.shields.io/badge/pruning-F08080), or from width to depth.
Each stage also includes a performance restoration step using distillation
techniques, helping to strike a balance between performance and parameter
efficiency. Empirical results demonstrate the effectiveness of our approach:
across three datasets, our models achieve an average of 88% of the original
model's performance while ![key](https://img.shields.io/badge/pruning-F08080) more than 95% of the non-embedding
parameters. This underscores the potential of our method to significantly
reduce resource requirements without greatly compromising recommendation
quality. Our code will be available at: https://github.com/zheng-sl/PruneRec


## QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks

>Authors: Xuyang Chen, Chong Huang, Daquan Feng, Lei Luo, Yao Sun, Xiang-Gen Xia

>2025-07-09

> http://arxiv.org/abs/2507.06717v1

Real-time unmanned aerial vehicle (UAV) video streaming is essential for
time-sensitive applications, including remote surveillance, emergency response,
and environmental monitoring. However, it faces challenges such as limited
bandwidth, latency fluctuations, and high packet loss. To address these issues,
we propose a novel semantic self-correcting video transmission framework with
ultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded
into a compact semantic codebook space, and the transmitter adaptively sends a
subset of semantic indices based on bandwidth availability, enabling
fine-grained bitrate control for improved bandwidth efficiency. At the
receiver, a spatio-temporal vision ![key](https://img.shields.io/badge/transformer-FF8C00) (ST-ViT) performs multi-frame
joint decoding to reconstruct dropped semantic indices by modeling intra- and
inter-frame dependencies. To further improve performance under dynamic network
conditions, we integrate a multi-user proximal policy optimization (MUPPO)
reinforcement learning scheme that jointly optimizes ![key](https://img.shields.io/badge/communication-F08080) resource
allocation and semantic bitrate selection to maximize user Quality of
Experience (QoE). Extensive experiments demonstrate that the proposed SSCV-G
significantly outperforms state-of-the-art video codecs in coding efficiency,
bandwidth adaptability, and packet loss robustness. Moreover, the proposed
MUPPO-based QoE optimization consistently surpasses existing benchmarks.


## CCQ Convolutional Code for Extreme Low-bit Quantization in LLMs

>Authors: Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang

>2025-07-09

> http://arxiv.org/abs/2507.07145v1

The rapid scaling of Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) elevates inference costs
and compounds substantial deployment barriers. While quantization to 8 or 4
bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and
efficiency degradation. We propose Convolutional Code Quantization (CCQ), an
inference-optimized quantization approach compressing ![key](https://img.shields.io/badge/LLM-FF8C00)s to 2.0-2.75 bits
with minimal accuracy loss. Departing from error-prone scalar quantization or
slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding
and decoding solution with Convolutional Code, Hybrid Encoding, and Code
Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a
lookup-free encoding space, enabling a linear mapping between the codebook and
weight vectors, thereby optimizing inference performance. Meanwhile, by drawing
on the concept of data mapping from vector quantization, we minimize the
performance degradation of the model under extremely low-bit conditions.
Experiments demonstrate that CCQ achieves outstanding performance on ![key](https://img.shields.io/badge/LLM-FF8C00)s
across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to
184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE
4.5 and eliminating inter-card ![key](https://img.shields.io/badge/communication-F08080). The 2-bit ERNIE-4.5-300B-A47B
model and inference engine have been open-sourced.


## Direct Regret Optimization in Bayesian Optimization

>Authors: Fengxue Zhang, Yuxin Chen

>2025-07-09

> http://arxiv.org/abs/2507.06529v1

Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
![key](https://img.shields.io/badge/transformer-FF8C00) that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--![key](https://img.shields.io/badge/sparse-F08080) learning
paradigm: The decision ![key](https://img.shields.io/badge/transformer-FF8C00) is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.


## SpindleKV A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers

>Authors: Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang

>2025-07-09

> http://arxiv.org/abs/2507.06517v1

Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of ![key](https://img.shields.io/badge/KV-F08080) cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the ![key](https://img.shields.io/badge/KV-F08080) cache, demonstrating its
potential for reduction, particularly in deeper layers. However, ![key](https://img.shields.io/badge/KV-F08080) cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the ![key](https://img.shields.io/badge/KV-F08080) cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel ![key](https://img.shields.io/badge/KV-F08080) cache reduction method, Spindle![key](https://img.shields.io/badge/KV-F08080),
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, Spindle![key](https://img.shields.io/badge/KV-F08080) addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different ![key](https://img.shields.io/badge/LLM-FF8C00)s shown that Spindle![key](https://img.shields.io/badge/KV-F08080) obtained better ![key](https://img.shields.io/badge/KV-F08080)
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.


## MoFE-Time Mixture of Frequency Domain Experts for Time-Series Forecasting Models

>Authors: Yiwen Liu, Chenyu Zhang, Junjie Song, Siqi Chen, Sun Yin, Zihan Wang, Lingming Zeng, Yuji Cao, Junming Jiao

>2025-07-09

> http://arxiv.org/abs/2507.06502v1

As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s), the adoption of ![key](https://img.shields.io/badge/LLM-FF8C00)s as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional ![key](https://img.shields.io/badge/sparse-F08080) representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.


## Video-RTS Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning

>Authors: Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul Islam, Gedas Bertasius, Mohit Bansal

>2025-07-09

> http://arxiv.org/abs/2507.06485v1

Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a ![key](https://img.shields.io/badge/sparse-F08080)-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.


## Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders

>Authors: Shun Wang, Tyler Loakman, Youbo Lei, Yi Liu, Bohao Yang, Yuting Zhao, Dong Yang, Chenghua Lin

>2025-07-08

> http://arxiv.org/abs/2507.06427v1

Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective ![key](https://img.shields.io/badge/LLM-FF8C00) decomposition method using a dictionary-learning
approach with ![key](https://img.shields.io/badge/sparse-F08080) autoencoders. This helps extract monosemantic features
from polysemantic ![key](https://img.shields.io/badge/LLM-FF8C00) neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by ![key](https://img.shields.io/badge/LLM-FF8C00)s. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.


## Ampere Communication-Efficient and High-Accuracy Split Federated Learning

>Authors: Zihan Zhang, Leon Wong, Blesson Varghese

>2025-07-08

> http://arxiv.org/abs/2507.07130v1

A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large ![key](https://img.shields.io/badge/communication-F08080) overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server ![key](https://img.shields.io/badge/communication-F08080) while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the ![key](https://img.shields.io/badge/communication-F08080) overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and ![key](https://img.shields.io/badge/transformer-FF8C00)s show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server ![key](https://img.shields.io/badge/communication-F08080) overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.


## SLDB An End-To-End Heterogeneous System-on-Chip Benchmark Suite for LLM-Aided Design

>Authors: Elisavet Lydia Alvanaki, Kevin Lee, Luca P. Carloni

>2025-07-08

> http://arxiv.org/abs/2507.06376v1

Over the last few years, Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) have emerged as a
valuable tool for Electronic Design Automation (EDA). State-of-the-art research
in ![key](https://img.shields.io/badge/LLM-FF8C00)-aided design has demonstrated the ability of ![key](https://img.shields.io/badge/LLM-FF8C00)s to generate
syntactically correct RTL code, showcasing encouraging prospects for
integrating AI into the hardware design process. A key enabler of these
advancements is the availability of high-quality benchmarks to evaluate new
approaches. However, existing datasets and benchmarks fall short of
system-level design, as they focus primarily on component-level information and
low-complexity designs. To address this gap, we introduce the System-Level
Design Benchmark (SLDB), a dataset tailored for evaluating ![key](https://img.shields.io/badge/LLM-FF8C00)s in system-level
integration and configuration tasks. SLDB includes a curated benchmark suite of
10 baseline SoC designs, whose components can be combined into an exponential
number of distinct tile-based SoCs through a synthetic library. The dataset
provides full SoC configurations, accelerator integration code, ![key](https://img.shields.io/badge/communication-F08080)
parameters, and accelerator-aware system configurations, along with
testing-application code, compatible with the ESP platform[1].


## ETT Expanding the Long Context Understanding Capability of LLMs at Test-Time

>Authors: Kiarash Zahirnia, Zahra Golpayegani, Walid Ahmad, Yang Liu

>2025-07-08

> http://arxiv.org/abs/2507.06313v1

Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing ![key](https://img.shields.io/badge/LLM-FF8C00)s for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based ![key](https://img.shields.io/badge/LLM-FF8C00)s, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into ![key](https://img.shields.io/badge/overlap-F08080)ping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
![key](https://img.shields.io/badge/LLM-FF8C00)'s weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.


## Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations

>Authors: Yibin Liu, Ang Li, Shijian Li

>2025-07-08

> http://arxiv.org/abs/2507.06044v1

Explainable recommendations, which use the information of user and item with
interaction to generate a explanation for why the user would interact with the
item, are crucial for improving user trust and decision transparency to the
recommender system. Existing methods primarily rely on encoding features of
users and items to embeddings, which often leads to information loss due to
dimensionality reduction, ![key](https://img.shields.io/badge/sparse-F08080) interactions, and so on. With the advancements
of large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) in language comprehension, some methods use
embeddings as ![key](https://img.shields.io/badge/LLM-FF8C00) inputs for explanation generation. However, since embeddings
lack inherent semantics, ![key](https://img.shields.io/badge/LLM-FF8C00)s must adjust or extend their parameters to
interpret them, a process that inevitably incurs information loss. To address
this issue, we propose a novel approach combining profile generation via
hierarchical interaction summarization (PGHIS), which leverages a pretrained
![key](https://img.shields.io/badge/LLM-FF8C00) to hierarchically summarize user-item interactions, generating structured
textual profiles as explicit representations of user and item characteristics.
Additionally, we propose contrastive prompting for explanation generation
(CPEG) which employs contrastive learning to guide another reasoning language
models in producing high-quality ground truth recommendation explanations.
Finally, we use the textual profiles of user and item as input and high-quality
explanation as output to fine-tune a ![key](https://img.shields.io/badge/LLM-FF8C00) for generating explanations.
Experimental results on multiple datasets demonstrate that our approach
outperforms existing state-of-the-art methods, achieving a great improvement on
metrics about explainability (e.g., 5% on GPTScore) and text quality.
Furthermore, our generated ground truth explanations achieve a significantly
higher win rate compared to user-written reviews and those produced by other
methods, demonstrating the effectiveness of CPEG in generating high-quality
ground truths.


## Large Language Models for Agent-Based Modelling Current and possible uses across the modelling cycle

>Authors: Loïs Vanhée, Melania Borit, Peer-Olaf Siebers, Roger Cremades, Christopher Frantz, Önder Gürcan, František Kalvas, Denisa Reshef Kera, Vivek Nallur, Kavin Narasimhan, Martin Neumann

>2025-07-08

> http://arxiv.org/abs/2507.05723v1

The emergence of Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) with increasingly sophisticated
natural language understanding and generative capabilities has sparked interest
in the Agent-based Modelling (ABM) community. With their ability to summarize,
generate, analyze, categorize, transcribe and translate text, answer questions,
propose explanations, sustain dialogue, extract information from unstructured
text, and perform logical reasoning and problem-solving tasks, ![key](https://img.shields.io/badge/LLM-FF8C00)s have a good
potential to contribute to the modelling process. After reviewing the current
use of ![key](https://img.shields.io/badge/LLM-FF8C00)s in ABM, this study reflects on the opportunities and challenges of
the potential use of ![key](https://img.shields.io/badge/LLM-FF8C00)s in ABM. It does so by following the modelling cycle,
from problem formulation to documentation and ![key](https://img.shields.io/badge/communication-F08080) of model results,
and holding a critical stance.


## SenseCF LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation

>Authors: Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh

>2025-07-07

> http://arxiv.org/abs/2507.05541v1

Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot ![key](https://img.shields.io/badge/LLM-FF8C00)-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
![key](https://img.shields.io/badge/sparsity-F08080). Moreover, using ![key](https://img.shields.io/badge/LLM-FF8C00)-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.


## Helix Parallelism Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding

>Authors: Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani

>2025-07-07

> http://arxiv.org/abs/2507.07120v1

As ![key](https://img.shields.io/badge/LLM-FF8C00)s scale to multi-million-token ![key](https://img.shields.io/badge/KV-F08080) histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long ![key](https://img.shields.io/badge/KV-F08080) caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of ![key](https://img.shields.io/badge/KV-F08080) heads, it leads to inefficient ![key](https://img.shields.io/badge/KV-F08080)
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long ![key](https://img.shields.io/badge/KV-F08080) histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies ![key](https://img.shields.io/badge/KV-F08080)
parallelism during attention to shard ![key](https://img.shields.io/badge/KV-F08080) caches across GPUs, then reuses the
same GPUs for TP in dense ![key](https://img.shields.io/badge/LLM-FF8C00)s or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
![key](https://img.shields.io/badge/communication-F08080) step. To minimize the exposed ![key](https://img.shields.io/badge/communication-F08080) cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes ![key](https://img.shields.io/badge/communication-F08080) overhead through
batchwise ![key](https://img.shields.io/badge/overlap-F08080), preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.


## StreamVLN Streaming Vision-and-Language Navigation via SlowFast Context Modeling

>Authors: Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang

>2025-07-07

> http://arxiv.org/abs/2507.05240v1

Vision-and-Language Navigation (VLN) in real-world settings requires agents
to process continuous visual streams and generate actions with low latency
grounded in language instructions. While Video-based Large Language Models
(Video-![key](https://img.shields.io/badge/LLM-FF8C00)s) have driven recent progress, current VLN methods based on
Video-![key](https://img.shields.io/badge/LLM-FF8C00) often face trade-offs among fine-grained visual understanding,
long-term context modeling and computational efficiency. We introduce
StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context
modeling strategy to support multi-modal reasoning over interleaved vision,
language and action inputs. The fast-streaming dialogue context facilitates
responsive action generation through a sliding-window of active dialogues,
while the slow-updating memory context compresses historical visual states
using a 3D-aware token ![key](https://img.shields.io/badge/pruning-F08080) strategy. With this slow-fast design, StreamVLN
achieves coherent multi-turn dialogue through efficient ![key](https://img.shields.io/badge/KV-F08080) cache reuse,
supporting long video streams with bounded context size and inference cost.
Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with
stable low latency, ensuring robustness and efficiency in real-world
deployment. The project page is:
\href{https://streamvln.github.io/}{https://streamvln.github.io/}.


## Cascade Token-Sharded Private LLM Inference

>Authors: Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal

>2025-07-07

> http://arxiv.org/abs/2507.05228v1

As ![key](https://img.shields.io/badge/LLM-FF8C00)s continue to increase in parameter size, the computational resources
required to run them are available to fewer parties. Therefore, third-party
inference services -- where ![key](https://img.shields.io/badge/LLM-FF8C00)s are hosted by third parties with significant
computational resources -- are becoming increasingly popular. However, third
party inference raises critical concerns about user data privacy. To mitigate
these risks, privacy researchers have developed provably secure schemes for
third-party inference, such as Secure Multi-Party Computation (SMPC). However,
SMPC protocols have significant computational and ![key](https://img.shields.io/badge/communication-F08080) overhead, and
do not scale to large models. In this work, we propose a new multi-party
inference protocol, Cascade, that avoids these punitive costs by leveraging
sharding in the sequence dimension to maintain privacy, trading off
cryptographic privacy guarantees for increased performance and scalability. We
demonstrate that Cascade is resistant to a generalization of a recent attack
that is highly effective against other statistical privacy schemes, and that it
is further resistant to learning-based attacks. As Cascade is orders of
magnitude faster than existing schemes, our findings offer practical solutions
for secure deployment of modern state-of-the-art ![key](https://img.shields.io/badge/LLM-FF8C00)s.


## All in One Visual-Description-Guided Unified Point Cloud Segmentation

>Authors: Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer

>2025-07-07

> http://arxiv.org/abs/2507.05211v1

Unified segmentation of 3D point clouds is crucial for scene understanding,
but is hindered by its ![key](https://img.shields.io/badge/sparse-F08080) structure, limited annotations, and the challenge
of distinguishing fine-grained object classes in complex environments. Existing
methods often struggle to capture rich semantic and contextual information due
to limited supervision and a lack of diverse multimodal cues, leading to
suboptimal differentiation of classes and instances. To address these
challenges, we propose VDG-Uni3DSeg, a novel framework that integrates
pre-trained vision-language models (e.g., CLIP) and large language models
(![key](https://img.shields.io/badge/LLM-FF8C00)s) to enhance 3D segmentation. By leveraging ![key](https://img.shields.io/badge/LLM-FF8C00)-generated textual
descriptions and reference images from the internet, our method incorporates
rich multimodal cues, facilitating fine-grained class and instance separation.
We further design a Semantic-Visual Contrastive Loss to align point features
with multimodal queries and a Spatial Enhanced Module to model scene-wide
relationships efficiently. Operating within a closed-set paradigm that utilizes
multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art
results in semantic, instance, and panoptic segmentation, offering a scalable
and practical solution for 3D understanding. Our code is available at
https://github.com/Hanzy1996/VDG-Uni3DSeg.


## CREW-WILDFIRE Benchmarking Agentic Multi-Agent Collaborations at Scale

>Authors: Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen

>2025-07-07

> http://arxiv.org/abs/2507.05178v1

Despite rapid progress in large language model (![key](https://img.shields.io/badge/LLM-FF8C00))-based multi-agent
systems, current benchmarks fall short in evaluating their scalability,
robustness, and coordination capabilities in complex, dynamic, real-world
tasks. Existing environments typically focus on small-scale, fully observable,
or low-complexity domains, limiting their utility for developing and assessing
next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,
an open-source benchmark designed to close this gap. Built atop the human-AI
teaming CREW simulation platform, CREW-Wildfire offers procedurally generated
wildfire response scenarios featuring large maps, heterogeneous agents, partial
observability, stochastic dynamics, and long-horizon planning objectives. The
environment supports both low-level control and high-level natural language
interactions through modular Perception and Execution modules. We implement and
evaluate several state-of-the-art ![key](https://img.shields.io/badge/LLM-FF8C00)-based multi-agent Agentic AI frameworks,
uncovering significant performance gaps that highlight the unsolved challenges
in large-scale coordination, ![key](https://img.shields.io/badge/communication-F08080), spatial reasoning, and long-horizon
planning under uncertainty. By providing more realistic complexity, scalable
architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a
critical foundation for advancing research in scalable multi-agent Agentic
intelligence. All code, environments, data, and baselines will be released to
support future research in this emerging domain.


## The Case for Instance-Optimized LLMs in OLAP Databases

>Authors: Bardia Mohammadi, Laurent Bindschaedler

>2025-07-07

> http://arxiv.org/abs/2507.04967v1

Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) can enhance analytics systems with powerful data
summarization, cleaning, and semantic transformation capabilities. However,
deploying ![key](https://img.shields.io/badge/LLM-FF8C00)s at scale -- processing millions to billions of rows -- remains
prohibitively expensive in computation and memory. We present IOLM-DB, a novel
system that makes ![key](https://img.shields.io/badge/LLM-FF8C00)-enhanced database queries practical through
query-specific model optimization. Instead of using general-purpose ![key](https://img.shields.io/badge/LLM-FF8C00)s,
IOLM-DB generates lightweight, specialized models tailored to each query's
specific needs using representative data samples. IOLM-DB reduces model
footprints by up to 76% and increases throughput by up to 3.31$\times$ while
maintaining accuracy through aggressive compression techniques, including
quantization, sparsification, and structural ![key](https://img.shields.io/badge/pruning-F08080). We further show how our
approach enables higher parallelism on existing hardware and seamlessly
supports caching and batching strategies to reduce overheads. Our prototype
demonstrates that leveraging ![key](https://img.shields.io/badge/LLM-FF8C00) queries inside analytics systems is feasible
at scale, opening new possibilities for future OLAP applications.


## LIFT Automating Symbolic Execution Optimization with Large Language Models for AI Networks

>Authors: Ruoxi Wang, Kun Li, Minghui Xu, Yue Zhang, Kaidi Xu, Chunchi Liu, Yinhao Xiao, Xiuzhen Cheng

>2025-07-07

> http://arxiv.org/abs/2507.04931v1

Dynamic Symbolic Execution (DSE) is a key technique in program analysis,
widely used in software testing, vulnerability discovery, and formal
verification. In distributed AI systems, DSE plays a crucial role in
identifying hard-to-detect bugs, especially those arising from complex network
![key](https://img.shields.io/badge/communication-F08080) patterns. However, traditional approaches to symbolic execution
are often hindered by scalability issues and inefficiencies, particularly in
large-scale systems. This paper introduces LIFT (Large-language-model
Integrated Functional-equivalent-IR Transformation), a novel framework that
leverages Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)s) to automate the optimization of
Intermediate Representations (IRs) in symbolic execution. LIFT addresses the
challenges of symbolic execution by providing a scalable, context-sensitive
solution for IR transformation. The framework consists of two phases: IR
Analysis and Optimization, where ![key](https://img.shields.io/badge/LLM-FF8C00)s optimize time-intensive IR blocks, and
Symbolic Execution and Validation, which includes benchmarking and semantic
verification to ensure correctness and generalizability. Experiments on
real-world binaries demonstrated significant performance improvements,
including a 53.5\% reduction in execution time for bigtest and a 10.24\%
reduction for random, along with reductions in IR statements, PUT instructions,
and temporary variables. These results demonstrate that ![key](https://img.shields.io/badge/LLM-FF8C00)s simplify IRs while
maintaining functional correctness, enhancing symbolic execution in distributed
AI systems.


## DoPI Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine

>Authors: Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang

>2025-07-07

> http://arxiv.org/abs/2507.04877v1

Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)
diagnosis through multi-turn dialogues and knowledge graphs presents a
significant challenge for modern AI systems. Current large language models
(![key](https://img.shields.io/badge/LLM-FF8C00)s), despite their advancements, exhibit notable limitations in medical
applications, particularly in conducting effective multi-turn dialogues and
proactive questioning. These shortcomings hinder their practical application
and effectiveness in simulating real-world diagnostic scenarios. To address
these limitations, we propose DoPI, a novel ![key](https://img.shields.io/badge/LLM-FF8C00) system specifically designed
for the TCM domain. The DoPI system introduces a collaborative architecture
comprising a guidance model and an expert model. The guidance model conducts
multi-turn dialogues with patients and dynamically generates questions based on
a knowledge graph to efficiently extract critical symptom information.
Simultaneously, the expert model leverages deep TCM expertise to provide final
diagnoses and treatment plans. Furthermore, this study constructs a multi-turn
doctor-patient dialogue dataset to simulate realistic consultation scenarios
and proposes a novel evaluation methodology that does not rely on manually
collected real-world consultation data. Experimental results show that the DoPI
system achieves an accuracy rate of 84.68 percent in interrogation outcomes,
significantly enhancing the model's ![key](https://img.shields.io/badge/communication-F08080) ability during diagnosis
while maintaining professional expertise.


## FurniMAS Language-Guided Furniture Decoration using Multi-Agent System

>Authors: Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen

>2025-07-07

> http://arxiv.org/abs/2507.04770v1

Furniture decoration is an important task in various industrial applications.
However, achieving a high-quality decorative result is often time-consuming and
requires specialized artistic expertise. To tackle these challenges, we explore
how multi-agent systems can assist in automating the decoration process. We
propose FurniMAS, a multi-agent system for automatic furniture decoration.
Specifically, given a human prompt and a household furniture item such as a
working desk or a TV stand, our system suggests relevant assets with
appropriate styles and materials, and arranges them on the item, ensuring the
decorative result meets functionality, aesthetic, and ambiance preferences.
FurniMAS assembles a hybrid team of ![key](https://img.shields.io/badge/LLM-FF8C00)-based and non-![key](https://img.shields.io/badge/LLM-FF8C00) agents, each
fulfilling distinct roles in a typical decoration project. These agents
collaborate through ![key](https://img.shields.io/badge/communication-F08080), logical reasoning, and validation to
transform the requirements into the final outcome. Extensive experiments
demonstrate that our FurniMAS significantly outperforms other baselines in
generating high-quality 3D decor.


## Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems

>Authors: Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu

>2025-07-07

> http://arxiv.org/abs/2507.04724v1

Multi-agent systems powered by Large Language Models (![key](https://img.shields.io/badge/LLM-FF8C00)-MAS) demonstrate
remarkable capabilities in collaborative problem-solving. While ![key](https://img.shields.io/badge/LLM-FF8C00)-MAS exhibit
strong collaborative abilities, the security risks in their ![key](https://img.shields.io/badge/communication-F08080) and
coordination remain underexplored. We bridge this gap by systematically
investigating intention-hiding threats in ![key](https://img.shields.io/badge/LLM-FF8C00)-MAS, and design four
representative attack paradigms that subtly disrupt task completion while
maintaining high concealment. These attacks are evaluated in centralized,
decentralized, and layered ![key](https://img.shields.io/badge/communication-F08080) structures. Experiments conducted on
six benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,
and biographies, demonstrate that they exhibit strong disruptive capabilities.
To identify these threats, we propose a psychology-based detection framework
AgentXposed, which combines the HEXACO personality model with the Reid
Technique, using progressive questionnaire inquiries and behavior-based
monitoring. Experiments conducted on six types of attacks show that our
detection framework effectively identifies all types of malicious behaviors.
The detection rate for our intention-hiding attacks is slightly lower than that
of the two baselines, Incorrect Fact Injection and Dark Traits Injection,
demonstrating the effectiveness of intention concealment. Our findings reveal
the structural and behavioral risks posed by intention-hiding attacks and offer
valuable insights into securing ![key](https://img.shields.io/badge/LLM-FF8C00)-based multi-agent systems through
psychological perspectives, which contributes to a deeper understanding of
multi-agent safety. The code and data are available at
https://anonymous.4open.science/r/AgentXposed-F814.


## LOOM-Scope a comprehensive and efficient LOng-cOntext Model evaluation framework

>Authors: Zecheng Tang, Haitian Wang, Quantong Qiu, Baibei Ji, Ruoxi Sun, Keyan Zhou, Juntao Li, Min Zhang

>2025-07-07

> http://arxiv.org/abs/2507.04723v1

Long-context processing has become a fundamental capability for large
language models~(![key](https://img.shields.io/badge/LLM-FF8C00)s). To assess model's long-context performance, numerous
long-context evaluation benchmarks have been proposed. However, variations in
evaluation settings across these benchmarks lead to inconsistent results,
making it difficult to draw reliable comparisons. Besides, the high
computational cost of long-context evaluation poses a significant barrier for
the community to conduct comprehensive assessments of long-context models. In
this paper, we propose LOOM-Scope, a comprehensive and efficient framework for
long-context evaluation. LOOM-Scope standardizes evaluation settings across
diverse benchmarks, supports deployment of efficient long-context inference
![key](https://img.shields.io/badge/acceleration-F08080) methods, and introduces a holistic yet lightweight benchmark suite
to evaluate models comprehensively. Homepage: https://loomscope.github.io

