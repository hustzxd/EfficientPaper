# 2025-02-21

# Table of Contents
* [SPEX Scaling Feature Interaction Explanations for LLMs](#SPEX-Scaling-Feature-Interaction-Explanations-for-LLMs)
* [Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning](#Proving-Olympiad-Inequalities-by-Synergizing-LLMs-and-Symbolic-Reasoning)
* [Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method](#Adapting-Large-Language-Models-for-Time-Series-Modeling-via-a-Novel-Parameter-efficient-Adaptation-Method)
* [C2T A Classifier-Based Tree Construction Method in Speculative Decoding](#C2T-A-Classifier-Based-Tree-Construction-Method-in-Speculative-Decoding)
* [AI-Empowered Catalyst Discovery A Survey from Classical Machine Learning Approaches to Large Language Models](#AI-Empowered-Catalyst-Discovery-A-Survey-from-Classical-Machine-Learning-Approaches-to-Large-Language-Models)
* [Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton](#Fundamental-Bias-in-Inverting-Random-Sampling-Matrices-with-Application-to-Sub-sampled-Newton)
* [Unraveling the Localized Latents Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts](#Unraveling-the-Localized-Latents-Learning-Stratified-Manifold-Structures-in-LLM-Embedding-Space-with-Sparse-Mixture-of-Experts)
* [Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs](#Democratizing-Large-Language-Model-Based-Graph-Data-Augmentation-via-Latent-Knowledge-Graphs)
* [Activation-aware Probe-Query Effective Key-Value Retrieval for Long-Context LLMs Inference](#Activation-aware-Probe-Query-Effective-Key-Value-Retrieval-for-Long-Context-LLMs-Inference)
* [Train Small, Infer Large Memory-Efficient LoRA Training for Large Language Models](#Train-Small,-Infer-Large-Memory-Efficient-LoRA-Training-for-Large-Language-Models)
* [PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference](#PLDR-LLMs-Learn-A-Generalizable-Tensor-Operator-That-Can-Replace-Its-Own-Deep-Neural-Net-At-Inference)
* [K-Paths Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](#K-Paths-Reasoning-over-Graph-Paths-for-Drug-Repurposing-and-Drug-Interaction-Prediction)
* [Neural Attention Search](#Neural-Attention-Search)
* [MatterChat A Multi-Modal LLM for Material Science](#MatterChat-A-Multi-Modal-LLM-for-Material-Science)
* [The Role of GitHub Copilot on Software Development A Perspec-tive on Productivity, Security, Best Practices and Future Directions](#The-Role-of-GitHub-Copilot-on-Software-Development-A-Perspec-tive-on-Productivity,-Security,-Best-Practices-and-Future-Directions)
* [On-Device LLMs for Home Assistant Dual Role in Intent Detection and Response Generation](#On-Device-LLMs-for-Home-Assistant-Dual-Role-in-Intent-Detection-and-Response-Generation)
* [MoBA Mixture of Block Attention for Long-Context LLMs](#MoBA-Mixture-of-Block-Attention-for-Long-Context-LLMs)
* [NTP-INT Network Traffic Prediction-Driven In-band Network Telemetry for High-load Switches](#NTP-INT-Network-Traffic-Prediction-Driven-In-band-Network-Telemetry-for-High-load-Switches)
* [Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations](#Learning-Wall-Segmentation-in-3D-Vessel-Trees-using-Sparse-Annotations)
* [SparkAttention High-Performance Multi-Head Attention for Large Models on Volta GPU Architecture](#SparkAttention-High-Performance-Multi-Head-Attention-for-Large-Models-on-Volta-GPU-Architecture)
* [Geometric Flavours of Quantum Field Theory on a Cauchy Hypersurface Gaussian Analysis for the Hamiltonian Formalism and Applications to Cosmology](#Geometric-Flavours-of-Quantum-Field-Theory-on-a-Cauchy-Hypersurface-Gaussian-Analysis-for-the-Hamiltonian-Formalism-and-Applications-to-Cosmology)
* [A$^2$ATS Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](#A$^2$ATS-Retrieval-Based-KV-Cache-Reduction-via-Windowed-Rotary-Position-Embedding-and-Query-Aware-Vector-Quantization)
* [LiMo-Calib On-Site Fast LiDAR-Motor Calibration for Quadruped Robot-Based Panoramic 3D Sensing System](#LiMo-Calib-On-Site-Fast-LiDAR-Motor-Calibration-for-Quadruped-Robot-Based-Panoramic-3D-Sensing-System)
* [Generative AI Enabled Robust Data Augmentation for Wireless Sensing in ISAC Networks](#Generative-AI-Enabled-Robust-Data-Augmentation-for-Wireless-Sensing-in-ISAC-Networks)
* [A Primal Dual Active Set with Continuation Algorithm for $\ell_0$-Penalized High-dimensional Accelerated Failure Time Model](#A-Primal-Dual-Active-Set-with-Continuation-Algorithm-for-$\ell_0$-Penalized-High-dimensional-Accelerated-Failure-Time-Model)
* [PTQ1.61 Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models](#PTQ1.61-Push-the-Real-Limit-of-Extremely-Low-Bit-Post-Training-Quantization-Methods-for-Large-Language-Models)
* [Benchmarking Post-Training Quantization in LLMs Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis](#Benchmarking-Post-Training-Quantization-in-LLMs-Comprehensive-Taxonomy,-Unified-Evaluation,-and-Comparative-Analysis)
* [PASER Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](#PASER-Post-Training-Data-Selection-for-Efficient-Pruned-Large-Language-Model-Recovery)
* [G-Refer Graph Retrieval-Augmented Large Language Model for Explainable Recommendation](#G-Refer-Graph-Retrieval-Augmented-Large-Language-Model-for-Explainable-Recommendation)
* [HeadInfer Memory-Efficient LLM Inference by Head-wise Offloading](#HeadInfer-Memory-Efficient-LLM-Inference-by-Head-wise-Offloading)
* [NoKSR Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization](#NoKSR-Kernel-Free-Neural-Surface-Reconstruction-via-Point-Cloud-Serialization)
* [BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference](#BaKlaVa----Budgeted-Allocation-of-KV-cache-for-Long-context-Inference)
* [Boost, Disentangle, and Customize A Robust System2-to-System1 Pipeline for Code Generation](#Boost,-Disentangle,-and-Customize-A-Robust-System2-to-System1-Pipeline-for-Code-Generation)
* [Multi-Attribute Steering of Language Models via Targeted Intervention](#Multi-Attribute-Steering-of-Language-Models-via-Targeted-Intervention)
* [SparAMX Accelerating Compressed LLMs Token Generation on AMX-powered CPUs](#SparAMX-Accelerating-Compressed-LLMs-Token-Generation-on-AMX-powered-CPUs)
* [HopRAG Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation](#HopRAG-Multi-Hop-Reasoning-for-Logic-Aware-Retrieval-Augmented-Generation)
* [OCT Data is All You Need How Vision Transformers with and without Pre-training Benefit Imaging](#OCT-Data-is-All-You-Need-How-Vision-Transformers-with-and-without-Pre-training-Benefit-Imaging)
* [QuZO Quantized Zeroth-Order Fine-Tuning for Large Language Models](#QuZO-Quantized-Zeroth-Order-Fine-Tuning-for-Large-Language-Models)
* [Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory](#Hardware-Software-Co-Design-for-Accelerating-Transformer-Inference-Leveraging-Compute-in-Memory)
* [Independence Tests for Language Models](#Independence-Tests-for-Language-Models)
* [Gem5-AcceSys Enabling System-Level Exploration of Standard Interconnects for Novel Accelerators](#Gem5-AcceSys-Enabling-System-Level-Exploration-of-Standard-Interconnects-for-Novel-Accelerators)
* [Transformer Dynamics A neuroscientific approach to interpretability of large language models](#Transformer-Dynamics-A-neuroscientific-approach-to-interpretability-of-large-language-models)
* [AdaSplash Adaptive Sparse Flash Attention](#AdaSplash-Adaptive-Sparse-Flash-Attention)
* [Low-Rank Thinning](#Low-Rank-Thinning)
* [DLFR-VAE Dynamic Latent Frame Rate VAE for Video Generation](#DLFR-VAE-Dynamic-Latent-Frame-Rate-VAE-for-Video-Generation)
* [Accurate Expert Predictions in MoE Inference via Cross-Layer Gate](#Accurate-Expert-Predictions-in-MoE-Inference-via-Cross-Layer-Gate)
* [HAAN A Holistic Approach for Accelerating Normalization Operations in Large Language Models](#HAAN-A-Holistic-Approach-for-Accelerating-Normalization-Operations-in-Large-Language-Models)
* [SFTs a scalable data-analysis framework for long-duration gravitational-wave signals](#SFTs-a-scalable-data-analysis-framework-for-long-duration-gravitational-wave-signals)
* [Exploring Translation Mechanism of Large Language Models](#Exploring-Translation-Mechanism-of-Large-Language-Models)
* [JotlasNet Joint Tensor Low-Rank and Attention-based Sparse Unrolling Network for Accelerating Dynamic MRI](#JotlasNet-Joint-Tensor-Low-Rank-and-Attention-based-Sparse-Unrolling-Network-for-Accelerating-Dynamic-MRI)
* [Towards Reasoning Ability of Small Language Models](#Towards-Reasoning-Ability-of-Small-Language-Models)
* [Tactic Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs](#Tactic-Adaptive-Sparse-Attention-with-Clustering-and-Distribution-Fitting-for-Long-Context-LLMs)
* [Token Pruning in Multimodal Large Language Models Are We Solving the Right Problem?](#Token-Pruning-in-Multimodal-Large-Language-Models-Are-We-Solving-the-Right-Problem?)
* [Stop Looking for Important Tokens in Multimodal Language Models Duplication Matters More](#Stop-Looking-for-Important-Tokens-in-Multimodal-Language-Models-Duplication-Matters-More)
* [Dictionary-Learning-Based Data Pruning for System Identification](#Dictionary-Learning-Based-Data-Pruning-for-System-Identification)
* [Towards Efficient Pre-training Exploring FP4 Precision in Large Language Models](#Towards-Efficient-Pre-training-Exploring-FP4-Precision-in-Large-Language-Models)
* [Does RAG Really Perform Bad For Long-Context Processing?](#Does-RAG-Really-Perform-Bad-For-Long-Context-Processing?)
* [Sparse Autoencoder Features for Classifications and Transferability](#Sparse-Autoencoder-Features-for-Classifications-and-Transferability)
* [Teleportation With Null Space Gradient Projection for Optimization Acceleration](#Teleportation-With-Null-Space-Gradient-Projection-for-Optimization-Acceleration)
* [SAIF A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models](#SAIF-A-Sparse-Autoencoder-Framework-for-Interpreting-and-Steering-Instruction-Following-of-Language-Models)
* [Unveiling Environmental Impacts of Large Language Model Serving A Functional Unit View](#Unveiling-Environmental-Impacts-of-Large-Language-Model-Serving-A-Functional-Unit-View)
* [RT-DEMT A hybrid real-time acupoint detection model combining mamba and transformer](#RT-DEMT-A-hybrid-real-time-acupoint-detection-model-combining-mamba-and-transformer)
* [Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity](#Efficient-Long-Decoding-Inference-with-Reasoning-Aware-Attention-Sparsity)
* [Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation](#Ramp-Up-NTT-in-Record-Time-using-GPU-Accelerated-Algorithms-and-LLM-based-Code-Generation)
* [CacheFocus Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation](#CacheFocus-Dynamic-Cache-Re-Positioning-for-Efficient-Retrieval-Augmented-Generation)
* [Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time](#Mixture-of-Tunable-Experts---Behavior-Modification-of-DeepSeek-R1-at-Inference-Time)
* [SyncSpeech Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer](#SyncSpeech-Low-Latency-and-Efficient-Dual-Stream-Text-to-Speech-based-on-Temporal-Masked-Transformer)
* [Native Sparse Attention Hardware-Aligned and Natively Trainable Sparse Attention](#Native-Sparse-Attention-Hardware-Aligned-and-Natively-Trainable-Sparse-Attention)
* [Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks](#Streamlining-the-Collaborative-Chain-of-Models-into-A-Single-Forward-Pass-in-Generation-Based-Tasks)
* [Accelerating Anchors via Specialization and Feature Transformation](#Accelerating-Anchors-via-Specialization-and-Feature-Transformation)
* [DreamDDP Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization](#DreamDDP-Accelerating-Data-Parallel-Distributed-LLM-Training-with-Layer-wise-Scheduled-Partial-Synchronization)
* [GRIFFIN Effective Token Alignment for Faster Speculative Decoding](#GRIFFIN-Effective-Token-Alignment-for-Faster-Speculative-Decoding)
* [GS-GVINS A Tightly-integrated GNSS-Visual-Inertial Navigation System Augmented by 3D Gaussian Splatting](#GS-GVINS-A-Tightly-integrated-GNSS-Visual-Inertial-Navigation-System-Augmented-by-3D-Gaussian-Splatting)
* [A recurrent vision transformer shows signatures of primate visual attention](#A-recurrent-vision-transformer-shows-signatures-of-primate-visual-attention)
* [Order-agnostic Identifier for Large Language Model-based Generative Recommendation](#Order-agnostic-Identifier-for-Large-Language-Model-based-Generative-Recommendation)
* [1bit-Merging Dynamic Quantized Merging for Large Language Models](#1bit-Merging-Dynamic-Quantized-Merging-for-Large-Language-Models)
* [OPTISHEAR Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization](#OPTISHEAR-Towards-Efficient-and-Adaptive-Pruning-of-Large-Language-Models-via-Evolutionary-Optimization)
* [Lagrangian formalism in the theory of relativistic vector fields](#Lagrangian-formalism-in-the-theory-of-relativistic-vector-fields)
* [Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA](#Pushing-up-to-the-Limit-of-Memory-Bandwidth-and-Capacity-Utilization-for-Efficient-LLM-Decoding-on-Embedded-FPGA)
* [Time Parameterized Optimal Transport](#Time-Parameterized-Optimal-Transport)
* [Weighted quantization using MMD From mean field to mean shift via gradient flows](#Weighted-quantization-using-MMD-From-mean-field-to-mean-shift-via-gradient-flows)
* [Towards Watermarking of Open-Source LLMs](#Towards-Watermarking-of-Open-Source-LLMs)
* [Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding](#Text-guided-Sparse-Voxel-Pruning-for-Efficient-3D-Visual-Grounding)
* [Region-Adaptive Sampling for Diffusion Transformers](#Region-Adaptive-Sampling-for-Diffusion-Transformers)
* [Agentic Verification for Ambiguous Query Disambiguation](#Agentic-Verification-for-Ambiguous-Query-Disambiguation)
* [Step-Video-T2V Technical Report The Practice, Challenges, and Future of Video Foundation Model](#Step-Video-T2V-Technical-Report-The-Practice,-Challenges,-and-Future-of-Video-Foundation-Model)
* [Variational optical phase learning on a continuous-variable quantum compiler](#Variational-optical-phase-learning-on-a-continuous-variable-quantum-compiler)
* [Can Post-Training Quantization Benefit from an Additional QLoRA Integration?](#Can-Post-Training-Quantization-Benefit-from-an-Additional-QLoRA-Integration?)
* [Janus Collaborative Vision Transformer Under Dynamic Network Environment](#Janus-Collaborative-Vision-Transformer-Under-Dynamic-Network-Environment)
* [Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts](#Identifiable-Steering-via-Sparse-Autoencoding-of-Multi-Concept-Shifts)
* [EmbBERT-Q Breaking Memory Barriers in Embedded NLP](#EmbBERT-Q-Breaking-Memory-Barriers-in-Embedded-NLP)
* [X-Boundary Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability](#X-Boundary-Establishing-Exact-Safety-Boundary-to-Shield-LLMs-from-Multi-Turn-Jailbreaks-without-Compromising-Usability)
* [Granite Vision a lightweight, open-source multimodal model for enterprise Intelligence](#Granite-Vision-a-lightweight,-open-source-multimodal-model-for-enterprise-Intelligence)
* [INF^2 High-Throughput Generative Inference of Large Language Models using Near-Storage Processing](#INF^2-High-Throughput-Generative-Inference-of-Large-Language-Models-using-Near-Storage-Processing)
* [An Efficient Large Recommendation Model Towards a Resource-Optimal Scaling Law](#An-Efficient-Large-Recommendation-Model-Towards-a-Resource-Optimal-Scaling-Law)


## SPEX Scaling Feature Interaction Explanations for LLMs

>Authors: Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu

>2025-02-19

> http://arxiv.org/abs/2502.13870v1

Large language models (LLMs) have revolutionized machine learning due to
their ability to capture complex interactions between input features. Popular
post-hoc explanation methods like SHAP provide marginal feature attributions,
while their extensions to interaction importances only scale to small input
lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic
interaction attribution algorithm that efficiently scales to large input
lengths ($\approx 1000)$. SPEX exploits underlying natural **sparsity** among
interactions -- common in real-world data -- and applies a **sparse** Fourier
transform using a channel decoding algorithm to efficiently identify important
interactions. We perform experiments across three difficult long-context
datasets that require LLMs to utilize interactions between inputs to complete
the task. For large inputs, SPEX outperforms marginal attribution methods by up
to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX
successfully identifies key features and interactions that strongly influence
model output. For one of our datasets, HotpotQA, SPEX provides interactions
that align with human annotations. Finally, we use our model-agnostic approach
to generate explanations to demonstrate abstract reasoning in closed-source
LLMs (GPT-4o mini) and compositional reasoning in vision-language models.


## Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning

>Authors: Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma

>2025-02-19

> http://arxiv.org/abs/2502.13834v1

Large language models (LLMs) can prove mathematical theorems formally by
generating proof steps (\textit{a.k.a.} tactics) within a proof system.
However, the space of possible tactics is vast and complex, while the available
training data for formal proofs is limited, posing a significant challenge to
LLM-based tactic generation. To address this, we introduce a neuro-symbolic
tactic generator that synergizes the mathematical intuition learned by LLMs
with domain-specific insights encoded by symbolic methods. The key aspect of
this integration is identifying which parts of mathematical reasoning are best
suited to LLMs and which to symbolic methods. While the high-level idea of
neuro-symbolic integration is broadly applicable to various mathematical
problems, in this paper, we focus specifically on Olympiad inequalities
(Figure~1). We analyze how humans solve these problems and distill the
techniques into two types of tactics: (1) scaling, handled by symbolic methods,
and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with
LLMs to prune and rank the proof goals for efficient proof search. We evaluate
our framework on 161 challenging inequalities from multiple mathematics
competitions, achieving state-of-the-art performance and significantly
outperforming existing LLM and symbolic approaches without requiring additional
training data.


## Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method

>Authors: Juyuan Zhang, Wei Zhu, Jiechao Gao

>2025-02-19

> http://arxiv.org/abs/2502.13725v1

Time series modeling holds significant importance in many real-world
applications and has been extensively studied. While pre-trained foundation
models have made impressive strides in the fields of natural language
processing (NLP) and computer vision (CV), their development in time series
domains has been constrained by data **sparsity**. A series of recent studies have
demonstrated that large language models (LLMs) possess robust pattern
recognition and reasoning abilities over complex sequences of tokens. However,
the current literature have yet striked a high-quality balance between (a)
effectively aligning the time series and natural language modalities, and (b)
keeping the inference efficiency. To address the above issues, we now propose
the Time-LlaMA framework. Time-LlaMA first converts the time series input into
token embeddings through a linear tokenization mechanism. Second, the time
series token embeddings are aligned with the text prompts. Third, to further
adapt the LLM backbone for time series modeling, we have developed a dynamic
low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most
suitable LoRA modules at each layer of the Transformer backbone for each time
series input, enhancing the model's predictive capabilities. Our experimental
results on an extensive collection of challenging real-world time series tasks
confirm that our proposed method achieves the state-of-the-art (SOTA)
performance.


## C2T A Classifier-Based Tree Construction Method in Speculative Decoding

>Authors: Feiye Huo, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Shengli Sun

>2025-02-19

> http://arxiv.org/abs/2502.13652v1

The growing scale of Large Language Models (LLMs) has exacerbated inference
latency and computational costs. Speculative decoding methods, which aim to
mitigate these issues, often face inefficiencies in the construction of token
trees and the verification of candidate tokens. Existing strategies, including
chain mode, static tree, and dynamic tree approaches, have limitations in
accurately preparing candidate token trees for verification. We propose a novel
method named C2T that adopts a lightweight classifier to generate and prune
token trees dynamically. Our classifier considers additional feature variables
beyond the commonly used joint probability to predict the confidence score for
each draft token to determine whether it is the candidate token for
verification. This method outperforms state-of-the-art (SOTA) methods such as
EAGLE-2 on multiple benchmarks, by reducing the total number of candidate
tokens by 25% while maintaining or even improving the acceptance length.


## AI-Empowered Catalyst Discovery A Survey from Classical Machine Learning Approaches to Large Language Models

>Authors: Yuanyuan Xu, Hanchen Wang, Wenjie Zhang, Lexing Xie, Yin Chen, Flora Salim, Ying Zhang, Justin Gooding, Toby Walsh

>2025-02-19

> http://arxiv.org/abs/2502.13626v1

Catalysts are essential for accelerating chemical reactions and enhancing
selectivity, which is crucial for the sustainable production of energy,
materials, and bioactive compounds. Catalyst discovery is fundamental yet
challenging in computational chemistry and has garnered significant attention
due to the promising performance of advanced Artificial Intelligence (AI)
techniques. The development of Large Language Models (LLMs) notably accelerates
progress in the discovery of both homogeneous and heterogeneous catalysts,
where their chemical reactions differ significantly in material phases,
temperature, dynamics, etc. However, there is currently no comprehensive survey
that discusses the progress and latest developments in both areas, particularly
with the application of LLM techniques. To address this gap, this paper
presents a thorough and systematic survey of AI-empowered catalyst discovery,
employing a unified and general categorization for homogeneous and
heterogeneous catalysts. We examine the progress of AI-empowered catalyst
discovery, highlighting their individual advantages and disadvantages, and
discuss the challenges faced in this field. Furthermore, we suggest potential
directions for future research from the perspective of computer science. Our
goal is to assist researchers in computational chemistry, computer science, and
related fields in easily tracking the latest advancements, providing a clear
overview and roadmap of this area. We also organize and make accessible
relevant resources, including article lists and datasets, in an open repository
at
https://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.


## Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton

>Authors: Chengmei Niu, Zhenyu Liao, Zenan Ling, Michael W. Mahoney

>2025-02-19

> http://arxiv.org/abs/2502.13583v1

A substantial body of work in machine learning (ML) and randomized numerical
linear algebra (RandNLA) has exploited various sorts of random sketching
methodologies, including random sampling and random projection, with much of
the analysis using Johnson--Lindenstrauss and subspace embedding techniques.
Recent studies have identified the issue of inversion bias -- the phenomenon
that inverses of random sketches are not unbiased, despite the unbiasedness of
the sketches themselves. This bias presents challenges for the use of random
sketches in various ML pipelines, such as fast stochastic optimization,
scalable statistical estimators, and distributed optimization.
  In the context of random projection, the inversion bias can be easily
corrected for dense Gaussian projections (which are, however, too expensive for
many applications). Recent work has shown how the inversion bias can be
corrected for **sparse** sub-gaussian projections. In this paper, we show how the
inversion bias can be corrected for random sampling methods, both uniform and
non-uniform leverage-based, as well as for structured random projections,
including those based on the Hadamard transform. Using these results, we
establish problem-independent local convergence rates for sub-sampled Newton
methods.


## Unraveling the Localized Latents Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts

>Authors: Xin Li, Anand Sarwate

>2025-02-19

> http://arxiv.org/abs/2502.13577v1

However, real-world data often exhibit complex local structures that can be
challenging for single-model approaches with a smooth global manifold in the
embedding space to unravel. In this work, we conjecture that in the latent
space of these large language models, the embeddings live in a local manifold
structure with different dimensions depending on the perplexities and domains
of the input data, commonly referred to as a Stratified Manifold structure,
which in combination form a structured space known as a Stratified Space. To
investigate the validity of this structural claim, we propose an analysis
framework based on a Mixture-of-Experts (MoE) model where each expert is
implemented with a simple dictionary learning algorithm at varying **sparsity**
levels. By incorporating an attention-based soft-gating network, we verify that
our model learns specialized sub-manifolds for an ensemble of input data
sources, reflecting the semantic stratification in LLM embedding space. We
further analyze the intrinsic dimensions of these stratified sub-manifolds and
present extensive statistics on expert assignments, gating entropy, and
inter-expert distances. Our experimental results demonstrate that our method
not only validates the claim of a stratified manifold structure in the LLM
embedding space, but also provides interpretable clusters that align with the
intrinsic semantic variations of the input data.


## Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs

>Authors: Yushi Feng, Tsai Hor Chan, Guosheng Yin, Lequan Yu

>2025-02-19

> http://arxiv.org/abs/2502.13555v1

Data augmentation is necessary for graph representation learning due to the
scarcity and noise present in graph data. Most of the existing augmentation
methods overlook the context information inherited from the dataset as they
rely solely on the graph structure for augmentation. Despite the success of
some large language model-based (LLM) graph learning methods, they are mostly
white-box which require access to the weights or latent features from the
open-access LLMs, making them difficult to be democratized for everyone as
existing LLMs are mostly closed-source for commercial considerations. To
overcome these limitations, we propose a black-box context-driven graph data
augmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the
text prompt as context-related information, we task the LLM with generating
knowledge graphs (KGs), which allow us to capture the structural interactions
from the text outputs. We then design a dynamic merging schema to
stochastically integrate the LLM-generated KGs into the original graph during
training. To control the **sparsity** of the augmented graph, we further devise a
granularity-aware prompting strategy and an instruction fine-tuning module,
which seamlessly generates text prompts according to different granularity
levels of the dataset. Extensive experiments on various graph learning tasks
validate the effectiveness of our method over existing graph data augmentation
methods. Notably, our approach excels in scenarios involving electronic health
records (EHRs), which validates its maximal utilization of contextual
knowledge, leading to enhanced predictive performance and interpretability.


## Activation-aware Probe-Query Effective Key-Value Retrieval for Long-Context LLMs Inference

>Authors: Qingfa Xiao, Jiachuan Wang, Haoyang Li, Cheng Deng, Jiaqi Tang, Shuangyin Li, Yongqi Zhang, Jun Wang, Lei Chen

>2025-02-19

> http://arxiv.org/abs/2502.13542v1

Recent advances in large language models (LLMs) have showcased exceptional
performance in long-context tasks, while facing significant inference
efficiency challenges with limited GPU memory. Existing solutions first
proposed the sliding-window approach to accumulate a set of historical
\textbf{key-value} (**KV**) pairs for reuse, then further improvements selectively
retain its subsets at each step. However, due to the **sparse** attention
distribution across a long context, it is hard to identify and recall relevant
**KV** pairs, as the attention is distracted by massive candidate pairs.
Additionally, we found it promising to select representative tokens as
probe-Query in each sliding window to effectively represent the entire context,
which is an approach overlooked by existing methods. Thus, we propose
\textbf{ActQ**KV**}, a training-free, \textbf{Act}ivation-aware approach that
dynamically determines probe-\textbf{Q}uery and leverages it to retrieve the
relevant \textbf{**KV**} pairs for inference. Specifically, ActQ**KV** monitors a
token-level indicator, Activation Bias, within each context window, enabling
the proper construction of probe-Query for retrieval at pre-filling stage. To
accurately recall the relevant **KV** pairs and minimize the irrelevant ones, we
design a dynamic **KV** cut-off mechanism guided by information density across
layers at the decoding stage. Experiments on the Long-Bench and $\infty$
Benchmarks demonstrate its state-of-the-art performance with competitive
inference quality and resource efficiency.


## Train Small, Infer Large Memory-Efficient LoRA Training for Large Language Models

>Authors: Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou

>2025-02-19

> http://arxiv.org/abs/2502.13533v1

Large Language Models (LLMs) have significantly advanced natural language
processing with exceptional task generalization capabilities. Low-Rank Adaption
(LoRA) offers a cost-effective fine-tuning solution, freezing the original
model parameters and training only lightweight, low-rank adapter matrices.
However, the memory footprint of LoRA is largely dominated by the original
model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA
training scheme founded on the intuition that many neurons in
over-parameterized LLMs have low training utility but are essential for
inference. LoRAM presents a unique twist: it trains on a pruned (small) model
to obtain pruned low-rank matrices, which are then recovered and utilized with
the original (large) model for inference. Additionally, minimal-cost continual
pre-training, performed by the model publishers in advance, aligns the
knowledge discrepancy between pruned and original models. Our extensive
experiments demonstrate the efficacy of LoRAM across various **pruning** strategies
and downstream tasks. For a model with 70 billion parameters, LoRAM enables
training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA
training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by
structured **pruning** combined with 4-bit **quantization**, for LLaMA-3.1-70B
(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory
usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while
achieving dominant performance gains over both the original LLaMA-3.1-70B
(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).


## PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference

>Authors: Burc Gokden

>2025-02-19

> http://arxiv.org/abs/2502.13502v1

We show that Large Language Model from Power Law Decoder Representations
(PLDR-LLM) is a foundational model whose deductive outputs are invariant
tensors up to a small perturbation. PLDR-LLM learns a singularity condition for
the deductive outputs that enable the once-inferred energy-curvature tensor
$\mathbf{G}_{LM}$ to replace the deep neural network of power law graph
attention (PLGA) generating the deductive outputs at inference. We demonstrate
that a cache for $\mathbf{G}_{LM}$ (G-cache) and **KV**-cache can be implemented in
a straightforward manner to improve the inference time. The invariance and
generalizable nature of deductive outputs is at a very high fidelity where
deductive outputs have same RMSE and determinant values up to 15 decimal places
after caching, and zero-shot benchmark scores remain unchanged. Ablation
studies show that learned deductive outputs have distinct loss and accuracy
characteristics from models pretrained with transferred, randomly initialized
or identity tensors as a constant tensor operator and an LLM with scaled-dot
product attention (SDPA) is a special case of PLDR-LLM where $\mathbf{G}_{LM}$
is predefined as identity. The observed invariance characteristic introduces a
novel asymmetry between training and inference phases with caching. We outline
observed common characteristics of the deductive outputs for the learned
singularity condition. We provide an implementation of a training and inference
framework for PLDR-LLM with **KV**-cache and G-cache.


## K-Paths Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction

>Authors: Tassallah Abdullahi, Ioanna Gemou, Nihal V. Nayak, Ghulam Murtaza, Stephen H. Bach, Carsten Eickhoff, Ritambhara Singh

>2025-02-18

> http://arxiv.org/abs/2502.13344v1

Drug discovery is a complex and time-intensive process that requires
identifying and validating new therapeutic candidates. Computational approaches
using large-scale biomedical knowledge graphs (KGs) offer a promising solution
to accelerate this process. However, extracting meaningful insights from
large-scale KGs remains challenging due to the complexity of graph traversal.
Existing subgraph-based methods are tailored to graph neural networks (GNNs),
making them incompatible with other models, such as large language models
(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,
diverse, and biologically meaningful paths from KGs. Integrating these paths
enables LLMs and GNNs to effectively predict unobserved drug-drug and
drug-disease interactions. Unlike traditional path-ranking approaches, K-Paths
retrieves and transforms paths into a structured format that LLMs can directly
process, facilitating explainable reasoning. K-Paths employs a diversity-aware
adaptation of Yen's algorithm to retrieve the K shortest loopless paths between
entities in an interaction query, prioritizing biologically relevant and
diverse relationships. Our experiments on benchmark datasets show that K-Paths
improves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on
drug repurposing and 13.42 points on interaction severity prediction. We also
show that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,
respectively. K-Paths also improves the supervised training efficiency of
EmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining
strong predictive performance. Beyond its scalability and efficiency, K-Paths
uniquely bridges the gap between KGs and LLMs, providing explainable rationales
for predicted interactions. These capabilities show that K-Paths is a valuable
tool for efficient data-driven drug discovery.


## Neural Attention Search

>Authors: Difan Deng, Marius Lindauer

>2025-02-18

> http://arxiv.org/abs/2502.13251v2

We present Neural Attention Search (NAtS), a framework that automatically
evaluates the importance of each token within a sequence and determines if the
corresponding token can be dropped after several steps. This approach can
efficiently reduce the **KV** cache sizes required by transformer-based models
during inference and thus reduce inference costs. In this paper, we design a
search space that contains three token types: (i) Global Tokens will be
preserved and queried by all the following tokens. (ii) Local Tokens survive
until the next global token appears. (iii) Sliding Window Tokens have an impact
on the inference of a fixed size of the next following tokens. Similar to the
One-Shot Neural Architecture Search approach, this token-type information can
be learned jointly with the architecture weights via a learnable attention
mask. Experiments on both training a new transformer from scratch and
fine-tuning existing large language models show that NAtS can efficiently
reduce the **KV** cache size required for the models while maintaining the models'
performance.


## MatterChat A Multi-Modal LLM for Material Science

>Authors: Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao

>2025-02-18

> http://arxiv.org/abs/2502.13107v1

Understanding and predicting the properties of inorganic materials is crucial
for accelerating advancements in materials science and driving applications in
energy, electronics, and beyond. Integrating material structure data with
language-based information through multi-modal large language models (LLMs)
offers great potential to support these efforts by enhancing human-AI
interaction. However, a key challenge lies in integrating atomic structures at
full resolution into LLMs. In this work, we introduce MatterChat, a versatile
structure-aware multi-modal LLM that unifies material structural data and
textual inputs into a single cohesive model. MatterChat employs a bridging
module to effectively align a pretrained machine learning interatomic potential
with a pretrained LLM, reducing training costs and enhancing flexibility. Our
results demonstrate that MatterChat significantly improves performance in
material property prediction and human-AI interaction, surpassing
general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in
applications such as more advanced scientific reasoning and step-by-step
material synthesis.


## The Role of GitHub Copilot on Software Development A Perspec-tive on Productivity, Security, Best Practices and Future Directions

>Authors: Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi

>2025-02-18

> http://arxiv.org/abs/2502.13199v1

GitHub Copilot is transforming software development by automating tasks and
boosting productivity through AI-driven code generation. In this paper, we
con-duct a literature survey to synthesize insights on Copilot's impact on
productivity and security. We review academic journal databases, industry
reports, and official docu-mentation to highlight key findings and challenges.
While Copilot accelerates coding and prototyping, concerns over security
vulnerabilities and intellectual property risks persist. Drawing from the
literature, we provide a perspective on best practices and future directions
for responsible AI adoption in software engineering, offering action-able
insights for developers and organizations to integrate Copilot effectively
while maintaining high standards of quality and security.


## On-Device LLMs for Home Assistant Dual Role in Intent Detection and Response Generation

>Authors: Rune Birkmose, Nathan Mørkeberg Reece, Esben Hofstedt Norvin, Johannes Bjerva, Mike Zhang

>2025-02-18

> http://arxiv.org/abs/2502.12923v1

This paper investigates whether Large Language Models (LLMs), fine-tuned on
synthetic but domain-representative data, can perform the twofold task of (i)
slot and intent detection and (ii) natural language response generation for a
smart home assistant, while running solely on resource-limited, CPU-only edge
hardware. We fine-tune LLMs to produce both JSON action calls and text
responses. Our experiments show that 16-bit and 8-bit **quantize**d variants
preserve high accuracy on slot and intent detection and maintain strong
semantic coherence in generated text, while the 4-bit model, while retaining
generative fluency, suffers a noticeable drop in device-service classification
accuracy. Further evaluations on noisy human (non-synthetic) prompts and
out-of-domain intents confirm the models' generalization ability, obtaining
around 80--86\% accuracy. While the average inference time is 5--6 seconds per
query -- acceptable for one-shot commands but suboptimal for multi-turn
dialogue -- our results affirm that an on-device LLM can effectively unify
command interpretation and flexible response generation for home automation
without relying on specialized hardware.


## MoBA Mixture of Block Attention for Long-Context LLMs

>Authors: Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu

>2025-02-18

> http://arxiv.org/abs/2502.13189v1

Scaling the effective context length is essential for advancing large
language models (LLMs) toward artificial general intelligence (AGI). However,
the quadratic increase in computational complexity inherent in traditional
attention mechanisms presents a prohibitive overhead. Existing approaches
either impose strongly biased structures, such as sink or window attention
which are task-specific, or radically modify the attention mechanism into
linear approximations, whose performance in complex reasoning tasks remains
inadequately explored.
  In this work, we propose a solution that adheres to the ``less structure''
principle, allowing the model to determine where to attend autonomously, rather
than introducing predefined biases. We introduce Mixture of Block Attention
(MoBA), an innovative approach that applies the principles of Mixture of
Experts (MoE) to the attention mechanism. This novel architecture demonstrates
superior performance on long-context tasks while offering a key advantage: the
ability to seamlessly transition between full and **sparse** attention, enhancing
efficiency without the risk of compromising performance. MoBA has already been
deployed to support Kimi's long-context requests and demonstrates significant
advancements in efficient attention computation for LLMs. Our code is available
at https://github.com/MoonshotAI/MoBA.


## NTP-INT Network Traffic Prediction-Driven In-band Network Telemetry for High-load Switches

>Authors: Penghui Zhang, Hua Zhang, Yuqi Dai, Cheng Zeng, Jingyu Wang, Jianxin Liao

>2025-02-18

> http://arxiv.org/abs/2502.12834v1

In-band network telemetry (INT) is essential to network management due to its
real-time visibility. However, because of the rapid increase in network devices
and services, it has become crucial to have targeted access to detailed network
information in a dynamic network environment. This paper proposes an
intelligent network telemetry system called NTP-INT to obtain more fine-grained
network information on high-load switches. Specifically, NTP-INT consists of
three modules: network traffic prediction module, network **pruning** module, and
probe path planning module. Firstly, the network traffic prediction module
adopts a Multi-Temporal Graph Neural Network (MTGNN) to predict future network
traffic and identify high-load switches. Then, we design the network **pruning**
algorithm to generate a subnetwork covering all high-load switches to reduce
the complexity of probe path planning. Finally, the probe path planning module
uses an attention-mechanism-based deep reinforcement learning (DEL) model to
plan efficient probe paths in the network slice. The experimental results
demonstrate that NTP-INT can acquire more precise network information on
high-load switches while decreasing the control overhead by 50\%.


## Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations

>Authors: Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth

>2025-02-18

> http://arxiv.org/abs/2502.12801v1

We propose a novel approach that uses **sparse** annotations from clinical
studies to train a 3D segmentation of the carotid artery wall. We use a
centerline annotation to sample perpendicular cross-sections of the carotid
artery and use an adversarial 2D network to segment them. These annotations are
then transformed into 3D pseudo-labels for training of a 3D convolutional
neural network, circumventing the creation of manual 3D masks. For pseudo-label
creation in the bifurcation area we propose the use of cross-sections
perpendicular to the bifurcation axis and show that this enhances segmentation
performance. Different sampling distances had a lesser impact. The proposed
method allows for efficient training of 3D segmentation, offering potential
improvements in the assessment of carotid artery stenosis and allowing the
extraction of 3D biomarkers such as plaque volume.


## SparkAttention High-Performance Multi-Head Attention for Large Models on Volta GPU Architecture

>Authors: Youxuan Xu, Tong Wu, Shigang Li, Xueying Wang, Jingjing Wang

>2025-02-18

> http://arxiv.org/abs/2502.12784v2

Transformer are widely used in various fields such as natural language
processing and computer vision. However, the training time for large
Transformer models can be challenging due to the Multi-Head Attention (MHA)
mechanism. Especially as models become larger, training becomes more costly. So
it is crucial to utilize various resources for efficient model training.
Currently, NVIDIA Volta GPU is still widely used. However, because the
computational shapes supported by Tensor Core Units (TCU) of Volta GPU differ
from other GPU architectures, most efforts have not focused on using them to
accelerate Transformer training. To address this issue, we propose
SparkAttention, an **acceleration** library designed to speed up MHA training on
the Volta GPU. SparkAttention leverages TCU and kernel fusion to reduce the
number of high bandwidth memory (HBM) accesses and overhead. Our End-to-End
experimental results on an NVIDIA V100 GPU show that SparkAttention achieves on
average 1.80$\times$ (up to 2.46$\times$) speedup compared to using PyTorch.


## Geometric Flavours of Quantum Field Theory on a Cauchy Hypersurface Gaussian Analysis for the Hamiltonian Formalism and Applications to Cosmology

>Authors: David Martínez-Crespo

>2025-02-18

> http://arxiv.org/abs/2502.12760v1

This thesis explores Quantum Field Theory (QFT) on curved spacetimes using a
geometric Hamiltonian approach to the Schr\"odinger-like representation. In
particular it studies the theory of the scalar field described through its
configurations over a Cauchy hypersurface. It is focused on mathematical
consistency based on analytic and geometric tools.
  The mathematical aspects of Gaussian integration theory in
infinite-dimensional Topological Vector Spaces (TVS) are thoroughly reviewed.
It also reviews the complex and holomorphic versions of important results and
concepts of Gaussian integration. For example, the Wiener-It\^o decomposition
theorem or the definition of Hida test functions.
  The physical framework builds upon three interconnected levels: classical
General Relativity (GR), Classical Statistical Field Theory (CSFT), and QFT.
The work begins by extending the Koopman-van Hove (KvH) formalism of classical
statistical mechanics to CSFT. This description is based upon pre**quantization**
theory. It reveals features inherent to both CSFT and QFT, that help delineate
the genuine quantum features of a theory.
  Upon the prequantum program, the QFT of the scalar field is built mixing
Geometric Quantization with the choice of Wick and Weyl orderings. Various
quantum representations are introduced: the holomorphic, Schr\"odinger,
field-momentum, and antiholomorphic. The relation among them is studied using
integral transforms, including novel infinite-dimensional Fourier transforms.
From a geometrical analysis, it is argued that a covariant time derivative that
modifies the evolution equations should be added to the Schr\"odinger equation.
This connection is unique and required by the geometrodynamical description
ofthe coupling of QFT and GR.
  Finally, studying the free model on cosmological spacetimes it obtains
particle creation effects on a dynamical equation.


## A$^2$ATS Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization

>Authors: Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li

>2025-02-18

> http://arxiv.org/abs/2502.12665v1

Long context large language models (LLMs) pose significant challenges for
efficient serving due to the large memory footprint and high access overhead of
**KV** cache. Retrieval-based **KV** cache reduction methods can mitigate these
challenges, typically by offloading the complete **KV** cache to CPU and retrieving
necessary tokens on demand during inference. However, these methods still
suffer from unsatisfactory accuracy degradation and extra retrieval overhead.
To address these limitations, this paper proposes A$^2$ATS, a novel
retrieval-based **KV** cache reduction method. A$^2$ATS aims to obtain an accurate
approximation of attention scores by applying the vector **quantization** technique
to key states, thereby enabling efficient and precise retrieval of the top-K
tokens. First, we propose Windowed Rotary Position Embedding, which decouples
the positional dependency from query and key states after position embedding.
Then, we propose query-aware vector **quantization** that optimizes the objective
of attention score approximation directly. Finally, we design the heterogeneous
inference architecture for **KV** cache offloading, enabling long context serving
with larger batch sizes. Experimental results demonstrate that A$^2$ATS can
achieve a lower performance degradation with similar or lower overhead compared
to existing methods, thereby increasing long context serving throughput by up
to $2.7 \times$.


## LiMo-Calib On-Site Fast LiDAR-Motor Calibration for Quadruped Robot-Based Panoramic 3D Sensing System

>Authors: Jianping Li, Zhongyuan Liu, Xinhang Xu, Jinxin Liu, Shenghai Yuan, Lihua Xie

>2025-02-18

> http://arxiv.org/abs/2502.12655v1

Conventional single LiDAR systems are inherently constrained by their limited
field of view (FoV), leading to blind spots and incomplete environmental
awareness, particularly on robotic platforms with strict payload limitations.
Integrating a motorized LiDAR offers a practical solution by significantly
expanding the sensor's FoV and enabling adaptive panoramic 3D sensing. However,
the high-frequency vibrations of the quadruped robot introduce calibration
challenges, causing variations in the LiDAR-motor transformation that degrade
sensing accuracy. Existing calibration methods that use artificial targets or
dense feature extraction lack feasibility for on-site applications and
real-time implementation. To overcome these limitations, we propose LiMo-Calib,
an efficient on-site calibration method that eliminates the need for external
targets by leveraging geometric features directly from raw LiDAR scans.
LiMo-Calib optimizes feature selection based on normal distribution to
accelerate convergence while maintaining accuracy and incorporates a
reweighting mechanism that evaluates local plane fitting quality to enhance
robustness. We integrate and validate the proposed method on a motorized LiDAR
system mounted on a quadruped robot, demonstrating significant improvements in
calibration efficiency and 3D sensing accuracy, making LiMo-Calib well-suited
for real-world robotic applications. The demo video is available at:
https://youtu.be/FMINa-sap7g


## Generative AI Enabled Robust Data Augmentation for Wireless Sensing in ISAC Networks

>Authors: Jiacheng Wang, Changyuan Zhao, Hongyang Du, Geng Sun, Jiawen Kang, Shiwen Mao, Dusit Niyato, Dong In Kim

>2025-02-18

> http://arxiv.org/abs/2502.12622v1

Integrated sensing and communication (ISAC) uses the same software and
hardware resources to achieve both communication and sensing functionalities.
Thus, it stands as one of the core technologies of 6G and has garnered
significant attention in recent years. In ISAC systems, a variety of machine
learning models are trained to analyze and identify signal patterns, thereby
ensuring reliable sensing and communications. However, considering factors such
as communication rates, costs, and privacy, collecting sufficient training data
from various ISAC scenarios for these models is impractical. Hence, this paper
introduces a generative AI (GenAI) enabled robust data augmentation scheme. The
scheme first employs a conditioned diffusion model trained on a limited amount
of collected CSI data to generate new samples, thereby expanding the sample
quantity. Building on this, the scheme further utilizes another diffusion model
to enhance the sample quality, thereby facilitating the data augmentation in
scenarios where the original sensing data is insufficient and unevenly
distributed. Moreover, we propose a novel algorithm to estimate the
**acceleration** and jerk of signal propagation path length changes from CSI. We
then use the proposed scheme to enhance the estimated parameters and detect the
number of targets based on the enhanced data. The evaluation reveals that our
scheme improves the detection performance by up to 70%, demonstrating
reliability and robustness, which supports the deployment and practical use of
the ISAC network.


## A Primal Dual Active Set with Continuation Algorithm for $\ell_0$-Penalized High-dimensional Accelerated Failure Time Model

>Authors: Peili Li, Ruoying Hu, Yanyun Ding, Yunhai Xiao

>2025-02-18

> http://arxiv.org/abs/2502.12621v1

The accelerated failure time model has garnered attention due to its
intuitive linear regression interpretation and has been successfully applied in
fields such as biostatistics, clinical medicine, economics, and social
sciences. This paper considers a weighted least squares estimation method with
an $\ell_0$-penalty based on right-censored data in a high-dimensional setting.
For practical implementation, we adopt an efficient primal dual active set
algorithm and utilize a continuous strategy to select the appropriate
regularization parameter. By employing the mutual incoherence property and
restricted isometry property of the covariate matrix, we perform an error
analysis for the estimated variables in the active set during the iteration
process. Furthermore, we identify a distinctive monotonicity in the active set
and show that the algorithm terminates at the oracle solution in a finite
number of steps. Finally, we perform extensive numerical experiments using both
simulated data and real breast cancer datasets to assess the performance
benefits of our method in comparison to other existing approaches.


## PTQ1.61 Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models

>Authors: Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Min Zhang

>2025-02-18

> http://arxiv.org/abs/2502.13179v1

Large Language Models (LLMs) suffer severe performance degradation when
facing extremely **low-bit** (sub 2-bit) **quantization**. Several existing sub 2-bit
post-training **quantization** (PTQ) methods utilize a mix-precision scheme by
leveraging an unstructured fine-grained mask to explicitly distinguish salient
weights, while which introduces an extra 1-bit or more per weight. To explore
the real limit of PTQ, we propose an extremely **low-bit** PTQ method called
PTQ1.61, which enables weight **quantization** to 1.61-bit for the first time.
Specifically, we first introduce a one-dimensional structured mask with
negligibly additional 0.0002-bit per weight based on input activations from the
perspective of reducing the upper bound of **quantization** error to allocate
corresponding salient weight channels to 4-bit. For non-salient channels
binarization, an efficient block-wise scaling factors optimization framework is
then presented to take implicit row-wise correlations and angular biases into
account. Different from prior works that concentrate on adjusting **quantization**
methodologies, we further propose a novel paradigm called **quantization**
preprocessing, where we argue that transforming the weight distribution of the
pretrained model before **quantization** can alleviate the difficulty in
per-channel extremely **low-bit** PTQ. Extensive experiments indicate our PTQ1.61
achieves state-of-the-art performance in extremely **low-bit** **quantization**. Codes
are available at https://github.com/zjq0455/PTQ1.61.


## Benchmarking Post-Training Quantization in LLMs Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis

>Authors: Jiaqi Zhao, Ming Wang, Miao Zhang, Yuzhang Shang, Xuebo Liu, Yaowei Wang, Min Zhang, Liqiang Nie

>2025-02-18

> http://arxiv.org/abs/2502.13178v1

Post-training Quantization (PTQ) technique has been extensively adopted for
large language models (LLMs) compression owing to its efficiency and low
resource requirement. However, current research lacks a in-depth analysis of
the superior and applicable scenarios of each PTQ strategy. In addition,
existing algorithms focus primarily on performance, overlooking the trade-off
among model size, performance, and **quantization** bitwidth. To mitigate these
confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,
in order to support our benchmark, we propose a comprehensive taxonomy for
existing mainstream methods by scrutinizing their computational strategies
(e.g., optimization-based, compensation-based, etc.). Then, we conduct
extensive experiments with the baseline within each class, covering models with
various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),
architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and
VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on
the results, we summarize the superior of each PTQ strategy and
modelsize-bitwidth trade-off considering the performance. For example, our
benchmark reveals that compensation-based technique demonstrates outstanding
cross-architecture robustness and extremely **low-bit** PTQ for ultra large models
should be reexamined. Finally, we further accordingly claim that a practical
combination of compensation and other PTQ strategy can achieve SOTA various
robustness. We believe that our benchmark will provide valuable recommendations
for the deployment of LLMs and future research on PTQ approaches.


## PASER Post-Training Data Selection for Efficient Pruned Large Language Model Recovery

>Authors: Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma

>2025-02-18

> http://arxiv.org/abs/2502.12594v1

Model **pruning** is an effective approach for compressing large language models.
However, this process often leads to significant degradation of model
capabilities. While post-training techniques such as instruction tuning are
commonly employed to recover model performance, existing methods often overlook
the uneven deterioration of model capabilities and incur high computational
costs. Moreover, some instruction data irrelevant to model capability recovery
may introduce negative effects. To address these challenges, we propose the
\textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for
\textbf{E}fficient pruned large language model \textbf{R}ecovery
(\textbf{PASER}). PASER aims to identify instructions where model capabilities
are most severely compromised within a certain recovery data budget. Our
approach first applies manifold learning and spectral clustering to group
recovery data in the semantic space, revealing capability-specific instruction
sets. We then adaptively allocate the data budget to different clusters based
on the degrees of model capability degradation. In each cluster, we prioritize
data samples where model performance has declined dramatically. To mitigate
potential negative transfer, we also detect and filter out conflicting or
irrelevant recovery data. Extensive experiments demonstrate that PASER
significantly outperforms conventional baselines, effectively recovering the
general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the
original post-training data.


## G-Refer Graph Retrieval-Augmented Large Language Model for Explainable Recommendation

>Authors: Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li

>2025-02-18

> http://arxiv.org/abs/2502.12586v1

Explainable recommendation has demonstrated significant advantages in
informing users about the logic behind recommendations, thereby increasing
system transparency, effectiveness, and trustworthiness. To provide
personalized and interpretable explanations, existing works often combine the
generation capabilities of large language models (LLMs) with collaborative
filtering (CF) information. CF information extracted from the user-item
interaction graph captures the user behaviors and preferences, which is crucial
for providing informative explanations. However, due to the complexity of graph
structure, effectively extracting the CF information from graphs still remains
a challenge. Moreover, existing methods often struggle with the integration of
extracted CF information with LLMs due to its implicit representation and the
modality gap between graph structures and natural language explanations. To
address these challenges, we propose G-Refer, a framework using graph
retrieval-augmented large language models (LLMs) for explainable
recommendation. Specifically, we first employ a hybrid graph retrieval
mechanism to retrieve explicit CF signals from both structural and semantic
perspectives. The retrieved CF information is explicitly formulated as
human-understandable text by the proposed graph translation and accounts for
the explanations generated by LLMs. To bridge the modality gap, we introduce
knowledge **pruning** and retrieval-augmented fine-tuning to enhance the ability of
LLMs to process and utilize the retrieved CF information to generate
explanations. Extensive experiments show that G-Refer achieves superior
performance compared with existing methods in both explainability and
stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.


## HeadInfer Memory-Efficient LLM Inference by Head-wise Offloading

>Authors: Cheng Luo, Zefan Cai, Hanshi Sun, Jinqi Xiao, Bo Yuan, Wen Xiao, Junjie Hu, Jiawei Zhao, Beidi Chen, Anima Anandkumar

>2025-02-18

> http://arxiv.org/abs/2502.12574v1

Transformer-based large language models (LLMs) demonstrate impressive
performance in long context generation. Extending the context length has
disproportionately shifted the memory footprint of LLMs during inference to the
key-value cache (**KV** cache). In this paper, we propose HEADINFER, which offloads
the **KV** cache to CPU RAM while avoiding the need to fully store the **KV** cache for
any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise
offloading strategy, maintaining only selective attention heads **KV** cache on the
GPU while computing attention output dynamically. Through roofline analysis, we
demonstrate that HEADINFER maintains computational efficiency while
significantly reducing memory footprint. We evaluate HEADINFER on the
Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory
footprint of the **KV** cache from 128 GB to 1 GB and the total GPU memory usage
from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline
inference. Notably, HEADINFER enables 4-million-token inference with an 8B
model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without
approximation methods.


## NoKSR Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization

>Authors: Zhen Li, Weiwei Sun, Shrisudhan Govindarajan, Shaobo Xia, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi

>2025-02-18

> http://arxiv.org/abs/2502.12534v2

We present a novel approach to large-scale point cloud surface reconstruction
by developing an efficient framework that converts an irregular point cloud
into a signed distance field (SDF). Our backbone builds upon recent
transformer-based architectures (i.e., PointTransformerV3), that serializes the
point cloud into a locality-preserving sequence of tokens. We efficiently
predict the SDF value at a point by aggregating nearby tokens, where fast
approximate neighbors can be retrieved thanks to the serialization. We
serialize the point cloud at different levels/scales, and non-linearly
aggregate a feature to predict the SDF value. We show that aggregating across
multiple scales is critical to overcome the approximations introduced by the
serialization (i.e. false negatives in the neighborhood). Our frameworks sets
the new state-of-the-art in terms of accuracy and efficiency (better or similar
performance with half the latency of the best prior method, coupled with a
simpler implementation), particularly on outdoor datasets where **sparse**-grid
methods have shown limited performance.


## BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference

>Authors: Ahmed Burak Gulhan, Krishna Teja Chitty-Venkata, Murali Emani, Mahmut Kandemir, Venkatram Vishwanath

>2025-02-18

> http://arxiv.org/abs/2502.13176v1

In Large Language Model (LLM) inference, Key-Value (**KV**) caches (**KV**-caches)
are essential for reducing time complexity. However, they result in a linear
increase in GPU memory as the context length grows. While recent work explores
**KV**-cache eviction and compression policies to reduce memory usage, they often
consider uniform **KV**-caches across all attention heads, leading to suboptimal
performance. We introduce BaKlaVa, a method to allocate optimal memory for
individual **KV**-caches across the model by estimating the importance of each
**KV**-cache. Our empirical analysis demonstrates that not all **KV**-caches are
equally critical for LLM performance. Using a one-time profiling approach,
BaKlaVa assigns optimal memory budgets to each **KV**-cache. We evaluated our
method on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\% compression
ratio while keeping baseline performance and delivering up to an
order-of-magnitude accuracy improvement at higher compression levels.


## Boost, Disentangle, and Customize A Robust System2-to-System1 Pipeline for Code Generation

>Authors: Kounianhua Du, Hanjing Wang, Jianxing Liu, Jizheng Chen, Xinyi Dai, Yasheng Wang, Ruiming Tang, Yong Yu, Jun Wang, Weinan Zhang

>2025-02-18

> http://arxiv.org/abs/2502.12492v1

Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, particularly in system 1 tasks, yet the intricacies of their
problem-solving mechanisms in system 2 tasks are not sufficiently explored.
Recent research on System2-to-System1 methods surge, exploring the System 2
reasoning knowledge via inference-time computation and compressing the explored
knowledge into System 1 process. In this paper, we focus on code generation,
which is a representative System 2 task, and identify two primary challenges:
(1) the complex hidden reasoning processes and (2) the heterogeneous data
distributions that complicate the exploration and training of robust LLM
solvers. To tackle these issues, we propose a novel BDC framework that explores
insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with
mutual \textbf{B}oosting, \textbf{D}isentangles the heterogeneous training data
for composable LoRA-experts, and obtain \textbf{C}ustomized problem solver for
each data instance with an input-aware hypernetwork to weight over the
LoRA-experts, offering effectiveness, flexibility, and robustness. This
framework leverages multiple LLMs through mutual verification and boosting,
integrated into a Monte-Carlo Tree Search process enhanced by reflection-based
**pruning** and refinement. Additionally, we introduce the DisenLora algorithm,
which clusters heterogeneous data to fine-tune LLMs into composable Lora
experts, enabling the adaptive generation of customized problem solvers through
an input-aware hypernetwork. This work lays the groundwork for advancing LLM
capabilities in complex reasoning tasks, offering a novel System2-to-System1
solution.


## Multi-Attribute Steering of Language Models via Targeted Intervention

>Authors: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

>2025-02-18

> http://arxiv.org/abs/2502.12446v1

Inference-time intervention (ITI) has emerged as a promising method for
steering large language model (LLM) behavior in a particular direction (e.g.,
improving helpfulness) by intervening on token representations without costly
updates to the LLM's parameters. However, existing ITI approaches fail to scale
to multi-attribute settings with conflicts, such as enhancing helpfulness while
also reducing toxicity. To address this, we introduce Multi-Attribute Targeted
Steering (MAT-Steer), a novel steering framework designed for selective
token-level intervention across multiple attributes. MAT-Steer learns steering
vectors using an alignment objective that shifts the model's internal
representations of undesirable outputs closer to those of desirable ones while
enforcing **sparsity** and orthogonality among vectors for different attributes,
thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two
distinct settings: (i) on question answering (QA) tasks where we balance
attributes like truthfulness, bias, and toxicity; (ii) on generative tasks
where we simultaneously improve attributes like helpfulness, correctness, and
coherence. MAT-Steer outperforms existing ITI and parameter-efficient
finetuning approaches across both task types (e.g., 3% average accuracy gain
across QA tasks and 55.82% win rate against the best ITI baseline).


## SparAMX Accelerating Compressed LLMs Token Generation on AMX-powered CPUs

>Authors: Ahmed F. AbouElhamayed, Jordan Dotzel, Yash Akhauri, Chi-Chih Chang, Sameh Gobriel, J. Pablo Muñoz, Vui Seng Chua, Nilesh Jain, Mohamed S. Abdelfattah

>2025-02-18

> http://arxiv.org/abs/2502.12444v1

Large language models have high compute, latency, and memory requirements.
While specialized accelerators such as GPUs and TPUs typically run these
workloads, CPUs are more widely available and consume less energy. Accelerating
LLMs with CPUs enables broader AI access at a lower cost and power consumption.
This **acceleration** potential for CPUs is especially relevant during the
memory-bound decoding stage of LLM inference, which processes one token at a
time and is becoming increasingly utilized with reasoning models. We utilize
Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with
unstructured **sparsity** to achieve a $1.42 \times$ reduction in end-to-end
latency compared to the current PyTorch implementation by applying our
technique in linear layers. We provide a set of open-source customized **sparse**
kernels that can speed up any PyTorch model by automatically replacing all
linear layers with our custom **sparse** implementation. Furthermore, we
demonstrate for the first time the use of unstructured **sparsity** in the
attention computation achieving a $1.14 \times$ speedup over the current
systems without compromising accuracy. Code:
https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX


## HopRAG Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation

>Authors: Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang

>2025-02-18

> http://arxiv.org/abs/2502.12442v1

Retrieval-Augmented Generation (RAG) systems often struggle with imperfect
retrieval, as traditional retrievers focus on lexical or semantic similarity
rather than logical relevance. To address this, we propose HopRAG, a novel RAG
framework that augments retrieval with logical reasoning through
graph-structured knowledge exploration. During indexing, HopRAG constructs a
passage graph, with text chunks as vertices and logical connections established
via LLM-generated pseudo-queries as edges. During retrieval, it employs a
retrieve-reason-prune mechanism: starting with lexically or semantically
similar passages, the system explores multi-hop neighbors guided by
pseudo-queries and LLM reasoning to identify truly relevant ones. Extensive
experiments demonstrate HopRAG's superiority, achieving 76.78\% higher answer
accuracy and 65.07\% improved retrieval F1 score compared to conventional
methods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.


## OCT Data is All You Need How Vision Transformers with and without Pre-training Benefit Imaging

>Authors: Zihao Han, Philippe De Wilde

>2025-02-17

> http://arxiv.org/abs/2502.12379v1

Optical Coherence Tomography (OCT) provides high-resolution cross-sectional
images useful for diagnosing various diseases, but their distinct
characteristics from natural images raise questions about whether large-scale
pre-training on datasets like ImageNet is always beneficial. In this paper, we
investigate the impact of ImageNet-based pre-training on Vision Transformer
(ViT) performance for OCT image classification across different dataset sizes.
Our experiments cover four-category retinal pathologies (CNV, DME, Drusen,
Normal). Results suggest that while pre-training can accelerate convergence and
potentially offer better performance in smaller datasets, training from scratch
may achieve comparable or even superior accuracy when sufficient OCT data is
available. Our findings highlight the importance of matching domain
characteristics in pre-training and call for further study on large-scale
OCT-specific pre-training.


## QuZO Quantized Zeroth-Order Fine-Tuning for Large Language Models

>Authors: Jiajun Zhou, Yifan Yang, Kai Zhen, Ziyue Liu, Yequan Zhao, Ershad Banijamali, Athanasios Mouchtaris, Ngai Wong, Zheng Zhang

>2025-02-17

> http://arxiv.org/abs/2502.12346v1

Language Models (LLMs) are often **quantize**d to lower precision to reduce the
memory cost and latency in inference. However, **quantization** often degrades
model performance, thus fine-tuning is required for various down-stream tasks.
Traditional fine-tuning methods such as stochastic gradient descent and Adam
optimization require backpropagation, which are error-prone in the
low-precision settings. To overcome these limitations, we propose the Quantized
Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs
through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid
the error-prone low-precision straight-through estimator, and utilizes
optimized stochastic rounding to mitigate the increased bias. QuZO simplifies
the training process, while achieving results comparable to first-order methods
in ${\rm FP}8$ and superior accuracy in ${\rm INT}8$ and ${\rm INT}4$ training.
Experiments demonstrate that **low-bit** training QuZO achieves performance
comparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks,
while reducing memory cost by $2.94 \times$ in LLaMA2-7B fine-tuning compared
to **quantize**d first-order methods.


## Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory

>Authors: Dong Eun Kim, Tanvi Sharma, Kaushik Roy

>2025-02-17

> http://arxiv.org/abs/2502.12344v1

Transformers have become the backbone of neural network architecture for most
machine learning applications. Their widespread use has resulted in multiple
efforts on accelerating attention, the basic building block of transformers.
This paper tackles the challenges associated with accelerating attention
through a hardware-software co-design approach while leveraging
compute-in-memory(CIM) architecture. In particular, our energy- and
area-efficient CIM based accelerator, named HASTILY, aims to accelerate softmax
computation, an integral operation in attention, and minimize their high
on-chip memory requirements that grows quadratically with input sequence
length. Our architecture consists of novel CIM units called unified compute and
lookup modules(UCLMs) that integrate both lookup and multiply-accumulate
functionality within the same SRAM array, incurring minimal area overhead over
standard CIM arrays. Designed in TSMC 65nm, UCLMs can be used to concurrently
perform exponential and matrix-vector multiplication operations. Complementing
the proposed architecture, HASTILY features a fine-grained pipelining strategy
for scheduling both attention and feed-forward layers, to reduce the quadratic
dependence on sequence length to linear dependence. Further, for fast softmax
computation which involves computing the maxima and sum of exponential values,
such operations are parallelized across multiple cores using reduce and gather
strategy. We evaluate our proposed architecture using a compiler tailored
towards attention computation and a standard cycle-level CIM simulator. Our
evaluation shows end-to-end throughput(TOPS) improvement of 4.4x-9.8x and
1.7x-5.9x over Nvidia A40 GPU and baseline CIM hardware, respectively, for BERT
models with INT-8 precision. Additionally, it shows gains of 16x-36x in
energy-efficiency(TOPS/W) over A40 GPU and similar energy-efficiency as
baseline CIM hardware.


## Independence Tests for Language Models

>Authors: Sally Zhu, Ahmed Ahmed, Rohith Kuditipudi, Percy Liang

>2025-02-17

> http://arxiv.org/abs/2502.12292v1

We consider the following problem: given the weights of two models, can we
test whether they were trained independently -- i.e., from independent random
initializations? We consider two settings: constrained and unconstrained. In
the constrained setting, we make assumptions about model architecture and
training and propose a family of statistical tests that yield exact p-values
with respect to the null hypothesis that the models are trained from
independent random initializations. These p-values are valid regardless of the
composition of either model's training data; we compute them by simulating
exchangeable copies of each model under our assumptions and comparing various
similarity measures of weights and activations between the original two models
versus these copies. We report the p-values from these tests on pairs of 21
open-weight models (210 total pairs) and correctly identify all pairs of
non-independent models. Our tests remain effective even if one model was
fine-tuned for many tokens. In the unconstrained setting, where we make no
assumptions about training procedures, can change model architecture, and allow
for adversarial evasion attacks, the previous tests no longer work. Instead, we
propose a new test which matches hidden activations between two models, and
which is robust to adversarial transformations and to changes in model
architecture. The test can also do localized testing: identifying specific
non-independent components of models. Though we no longer obtain exact p-values
from this, empirically we find it behaves as one and reliably identifies
non-independent models. Notably, we can use the test to identify specific parts
of one model that are derived from another (e.g., how Llama 3.1-8B was pruned
to initialize Llama 3.2-3B, or shared layers between Mistral-7B and
StripedHyena-7B), and it is even robust to retraining individual layers of
either model from scratch.


## Gem5-AcceSys Enabling System-Level Exploration of Standard Interconnects for Novel Accelerators

>Authors: Qunyou Liu, Marina Zapater, David Atienza

>2025-02-17

> http://arxiv.org/abs/2502.12273v1

The growing demand for efficient, high-performance processing in machine
learning (ML) and image processing has made hardware accelerators, such as GPUs
and Data Streaming Accelerators (DSAs), increasingly essential. These
accelerators enhance ML and image processing tasks by offloading computation
from the CPU to dedicated hardware. These accelerators rely on interconnects
for efficient data transfer, making interconnect design crucial for
system-level performance. This paper introduces Gem5-AcceSys, an innovative
framework for system-level exploration of standard interconnects and
configurable memory hierarchies. Using a matrix multiplication accelerator
tailored for transformer workloads as a case study, we evaluate PCIe
performance across diverse memory types (DDR4, DDR5, GDDR6, HBM2) and
configurations, including host-side and device-side memory. Our findings
demonstrate that optimized interconnects can achieve up to 80% of device-side
memory performance and, in some scenarios, even surpass it. These results offer
actionable insights for system architects, enabling a balanced approach to
performance and cost in next-generation accelerator design.


## Transformer Dynamics A neuroscientific approach to interpretability of large language models

>Authors: Jesseba Fernando, Grigori Guitchounts

>2025-02-17

> http://arxiv.org/abs/2502.12131v1

As artificial intelligence models have exploded in scale and capability,
understanding of their internal mechanisms remains a critical challenge.
Inspired by the success of dynamical systems approaches in neuroscience, here
we propose a novel framework for studying computations in deep learning
systems. We focus on the residual stream (RS) in transformer models,
conceptualizing it as a dynamical system evolving across layers. We find that
activations of individual RS units exhibit strong continuity across layers,
despite the RS being a non-privileged basis. Activations in the RS accelerate
and grow denser over layers, while individual units trace unstable periodic
orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with
attractor-like dynamics in the lower layers. These insights bridge dynamical
systems theory and mechanistic interpretability, establishing a foundation for
a "neuroscience of AI" that combines theoretical rigor with large-scale data
analysis to advance our understanding of modern neural networks.


## AdaSplash Adaptive Sparse Flash Attention

>Authors: Nuno Gonçalves, Marcos Treviso, André F. T. Martins

>2025-02-17

> http://arxiv.org/abs/2502.12082v1

The computational cost of softmax-based attention in transformers limits
their applicability to long-context tasks. Adaptive **sparsity**, of which
$\alpha$-entmax attention is an example, offers a flexible data-dependent
alternative, but existing implementations are inefficient and do not leverage
the **sparsity** to obtain runtime and memory gains. In this work, we propose
AdaSplash, which combines the efficiency of GPU-optimized algorithms with the
**sparsity** benefits of $\alpha$-entmax. We first introduce a hybrid
Halley-bisection algorithm, resulting in a 7-fold reduction in the number of
iterations needed to compute the $\alpha$-entmax transformation. Then, we
implement custom Triton kernels to efficiently handle adaptive **sparsity**.
Experiments with RoBERTa and ModernBERT for text classification and
single-vector retrieval, along with GPT-2 for language modeling, show that our
method achieves substantial improvements in runtime and memory efficiency
compared to existing $\alpha$-entmax implementations. It approaches -- and in
some cases surpasses -- the efficiency of highly optimized softmax
implementations like FlashAttention-2, enabling long-context training while
maintaining strong task performance.


## Low-Rank Thinning

>Authors: Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey

>2025-02-17

> http://arxiv.org/abs/2502.12063v1

The goal in thinning is to summarize a dataset using a small set of
representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel
Halving and Compress can match the quality of uniform subsampling while
substantially reducing the number of summary points. However, existing
guarantees cover only a restricted range of distributions and kernel-based
quality measures and suffer from pessimistic dimension dependence. To address
these deficiencies, we introduce a new low-rank analysis of sub-Gaussian
thinning that applies to any distribution and any kernel, guaranteeing
high-quality compression whenever the kernel or data matrix is approximately
low-rank. To demonstrate the broad applicability of the techniques, we design
practical sub-Gaussian thinning approaches that improve upon the best known
guarantees for approximating attention in transformers, accelerating stochastic
gradient training through reordering, and distinguishing distributions in
near-linear time.


## DLFR-VAE Dynamic Latent Frame Rate VAE for Video Generation

>Authors: Zhihang Yuan, Siyuan Wang, Rui Xie, Hanling Zhang, Tongcheng Fang, Yuzhang Shang, Shengen Yan, Guohao Dai, Yu Wang

>2025-02-17

> http://arxiv.org/abs/2502.11897v1

In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a
training-free paradigm that can make use of adaptive temporal compression in
latent space. While existing video generative models apply fixed compression
rates via pretrained VAE, we observe that real-world video content exhibits
substantial temporal non-uniformity, with high-motion segments containing more
information than static scenes. Based on this insight, DLFR-VAE dynamically
adjusts the latent frame rate according to the content complexity.
Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent
Frame Rate Scheduler that partitions videos into temporal chunks and adaptively
determines optimal frame rates based on information-theoretic content
complexity, and (2) A training-free adaptation mechanism that transforms
pretrained VAE architectures into a dynamic VAE that can process features with
variable frame rates. Our simple but effective DLFR-VAE can function as a
plug-and-play module, seamlessly integrating with existing video generation
models and accelerating the video generation process.


## Accurate Expert Predictions in MoE Inference via Cross-Layer Gate

>Authors: Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng

>2025-02-17

> http://arxiv.org/abs/2502.12224v1

Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, and their application in edge scenarios has attracted
significant attention. However, **sparse**-activated Mixture-of-Experts (MoE)
models, which are well suited for edge scenarios, have received relatively
little attention due to their high memory demands. Offload-based methods have
been proposed to address this challenge, but they face difficulties with expert
prediction. Inaccurate expert predictions can result in prolonged inference
delays. To promote the application of MoE models in edge scenarios, we propose
Fate, an offloading system designed for MoE models to enable efficient
inference in resource-constrained environments. The key insight behind Fate is
that gate inputs from adjacent layers can be effectively used for expert
prefetching, achieving high prediction accuracy without additional GPU
overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy
that increases the expert hit rate to 99\%. Additionally, Fate integrates
tailored **quantization** strategies for cache optimization and IO efficiency.
Experimental results show that, compared to Load on Demand and Expert
Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in
prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,
while maintaining inference quality. Moreover, Fate's performance improvements
are scalable across different memory budgets.


## HAAN A Holistic Approach for Accelerating Normalization Operations in Large Language Models

>Authors: Tianfan Peng, Jiajun Qin, Tianhua Xia, Sai Qian Zhang

>2025-02-17

> http://arxiv.org/abs/2502.11832v1

Large language models (LLMs) have revolutionized natural language processing
(NLP) tasks by achieving state-of-the-art performance across a range of
benchmarks. Central to the success of these models is the integration of
sophisticated architectural components aimed at improving training stability,
convergence speed, and generalization capabilities. Among these components,
normalization operation, such as layer normalization (LayerNorm), emerges as a
pivotal technique, offering substantial benefits to the overall model
performance. However, previous studies have indicated that normalization
operations can substantially elevate processing latency and energy usage. In
this work, we adopt the principles of algorithm and hardware co-design,
introducing a holistic normalization accelerating method named HAAN. The
evaluation results demonstrate that HAAN can achieve significantly better
hardware performance compared to state-of-the-art solutions.


## SFTs a scalable data-analysis framework for long-duration gravitational-wave signals

>Authors: Rodrigo Tenorio, Davide Gerosa

>2025-02-17

> http://arxiv.org/abs/2502.11823v1

We introduce a framework based on Short-time Fourier Transforms (SFTs) to
analyze long-duration gravitational wave signals from compact binaries.
Targeted systems include binary neutron stars observed by third-generation
ground-based detectors and massive black-hole binaries observed by the LISA
space mission. In short, ours is an extremely fast, scalable, and
parallelizable implementation of the gravitational-wave inner product, a core
operation of gravitational-wave matched filtering. By operating on disjoint
data segments, SFTs allow for efficient handling of noise non-stationarities,
data gaps, and detector-induced signal modulations. We present a pilot
application to early warning problems in both ground- and space-based
next-generation detectors. Overall, SFTs reduce the computing cost of
evaluating an inner product by three to five orders of magnitude, depending on
the specific application, with respect to a standard approach. We release
public tools to operate using the SFT framework, including a vectorized and
hardware-accelerated re-implementation of a time-domain waveform. The inner
product is the key building block of all gravitational-wave data treatments; by
speeding up this low-level element so massively, SFTs provide an extremely
promising solution for current and future gravitational-wave data-analysis
problem


## Exploring Translation Mechanism of Large Language Models

>Authors: Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Min Zhang

>2025-02-17

> http://arxiv.org/abs/2502.11806v1

Large language models (LLMs) have succeeded remarkably in multilingual
translation tasks. However, the inherent translation mechanisms of LLMs remain
poorly understood, largely due to sophisticated architectures and vast
parameter scales. In response to this issue, this study explores the
translation mechanism of LLM from the perspective of computational components
(e.g., attention heads and MLPs). Path patching is utilized to explore causal
relationships between components, detecting those crucial for translation tasks
and subsequently analyzing their behavioral patterns in human-interpretable
terms. Comprehensive analysis reveals that translation is predominantly
facilitated by a **sparse** subset of specialized attention heads (less than 5\%),
which extract source language, indicator, and positional features. MLPs
subsequently integrate and process these features by transiting towards
English-centric latent representations. Notably, building on the above
findings, targeted fine-tuning of only 64 heads achieves translation
improvement comparable to full-parameter tuning while preserving general
capabilities.


## JotlasNet Joint Tensor Low-Rank and Attention-based Sparse Unrolling Network for Accelerating Dynamic MRI

>Authors: Yinghao Zhang, Haiyan Gui, Ningdi Yang, Yue Hu

>2025-02-17

> http://arxiv.org/abs/2502.11749v1

Joint low-rank and **sparse** unrolling networks have shown superior performance
in dynamic MRI reconstruction. However, existing works mainly utilized matrix
low-rank priors, neglecting the tensor characteristics of dynamic MRI images,
and only a global threshold is applied for the **sparse** constraint to the
multi-channel data, limiting the flexibility of the network. Additionally, most
of them have inherently complex network structure, with intricate interactions
among variables. In this paper, we propose a novel deep unrolling network,
JotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank
and attention-based **sparse** priors. Specifically, we utilize tensor low-rank
prior to exploit the structural correlations in high-dimensional data.
Convolutional neural networks are used to adaptively learn the low-rank and
**sparse** transform domains. A novel attention-based soft thresholding operator is
proposed to assign a unique learnable threshold to each channel of the data in
the CNN-learned **sparse** domain. The network is unrolled from the elaborately
designed composite splitting algorithm and thus features a simple yet efficient
parallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)
demonstrate the superior performance of JotlasNet in dynamic MRI
reconstruction.


## Towards Reasoning Ability of Small Language Models

>Authors: Gaurav Srivastava, Shuxiang Cao, Xuan Wang

>2025-02-17

> http://arxiv.org/abs/2502.11569v1

Reasoning has long been viewed as an emergent property of large language
models (LLMs), appearing at or above a certain scale ($\sim$100B parameters).
However, recent studies challenge this assumption, showing that small language
models (SLMs) can also achieve competitive reasoning performance. SLMs are
increasingly favored for their efficiency and deployability. However, there is
a lack of systematic study on the reasoning abilities of diverse SLMs,
including those trained from scratch or derived from LLMs through **quantization**,
**pruning**, and distillation. This raises a critical question: Can SLMs achieve
reasoning abilities comparable to LLMs? In this work, we systematically survey,
benchmark, and analyze 72 SLMs from six model families across 14 reasoning
benchmarks. For reliable evaluation, we examine four evaluation methods and
compare four LLM judges against human evaluations on 800 data points. We repeat
all experiments three times to ensure a robust performance assessment.
Additionally, we analyze the impact of different prompting strategies in small
models. Beyond accuracy, we also evaluate model robustness under adversarial
conditions and intermediate reasoning steps. Our findings challenge the
assumption that scaling is the only way to achieve strong reasoning. Instead,
we foresee a future where SLMs with strong reasoning capabilities can be
developed through structured training or post-training compression. They can
serve as efficient alternatives to LLMs for reasoning-intensive tasks.


## Tactic Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs

>Authors: Kan Zhu, Tian Tang, Qinyu Xu, Yile Gu, Zhichen Zeng, Rohan Kadekodi, Liangyu Zhao, Ang Li, Arvind Krishnamurthy, Baris Kasikci

>2025-02-17

> http://arxiv.org/abs/2502.12216v1

Long-context models are essential for many applications but face
inefficiencies in loading large **KV** caches during decoding. Prior methods
enforce fixed token budgets for **sparse** attention, assuming a set number of
tokens can approximate full attention. However, these methods overlook
variations in the importance of attention across heads, layers, and contexts.
To address these limitations, we propose Tactic, a **sparsity**-adaptive and
calibration-free **sparse** attention mechanism that dynamically selects tokens
based on their cumulative attention scores rather than a fixed token budget. By
setting a target fraction of total attention scores, Tactic ensures that token
selection naturally adapts to variations in attention **sparsity**. To efficiently
approximate this selection, Tactic leverages clustering-based sorting and
distribution fitting, allowing it to accurately estimate token importance with
minimal computational overhead. We show that Tactic outperforms existing **sparse**
attention algorithms, achieving superior accuracy and up to 7.29x decode
attention speedup. This improvement translates to an overall 1.58x end-to-end
inference speedup, making Tactic a practical and effective solution for
long-context LLM inference in accuracy-sensitive applications.


## Token Pruning in Multimodal Large Language Models Are We Solving the Right Problem?

>Authors: Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang

>2025-02-17

> http://arxiv.org/abs/2502.11501v1

Multimodal large language models (MLLMs) have shown remarkable performance
for cross-modal understanding and generation, yet still suffer from severe
inference costs. Recently, abundant works have been proposed to solve this
problem with token **pruning**, which identifies the redundant tokens in MLLMs and
then prunes them to reduce the computation and **KV** storage costs, leading to
significant **acceleration** without training. While these methods claim efficiency
gains, critical questions about their fundamental design and evaluation remain
unanswered: Why do many existing approaches underperform even compared to naive
random token selection? Are attention-based scoring sufficient for reliably
identifying redundant tokens? Is language information really helpful during
token **pruning**? What makes a good trade-off between token importance and
duplication? Are current evaluation protocols comprehensive and unbiased? The
ignorance of previous research on these problems hinders the long-term
development of token **pruning**. In this paper, we answer these questions one by
one, providing insights into the design of future token **pruning** methods.


## Stop Looking for Important Tokens in Multimodal Language Models Duplication Matters More

>Authors: Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang

>2025-02-17

> http://arxiv.org/abs/2502.11494v1

Vision tokens in multimodal large language models often dominate huge
computational overhead due to their excessive length compared to linguistic
modality. Abundant recent methods aim to solve this problem with token **pruning**,
which first defines an importance criterion for tokens and then prunes the
unimportant vision tokens during inference. However, in this paper, we show
that the importance is not an ideal indicator to decide whether a token should
be pruned. Surprisingly, it usually results in inferior performance than random
token **pruning** and leading to incompatibility to efficient attention computation
operators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),
which prunes tokens based on its duplication with other tokens, leading to
significant and training-free **acceleration**. Concretely, DART selects a small
subset of pivot tokens and then retains the tokens with low duplication to the
pivots, ensuring minimal information loss during token **pruning**. Experiments
demonstrate that DART can prune 88.9% vision tokens while maintaining
comparable performance, leading to a 1.99$\times$ and 2.99$\times$ speed-up in
total time and prefilling stage, respectively, with good compatibility to
efficient attention operators. Our codes are available at
https://github.com/ZichenWen1/DART.


## Dictionary-Learning-Based Data Pruning for System Identification

>Authors: Tingna Wang, Sikai Zhang, Limin Sun

>2025-02-17

> http://arxiv.org/abs/2502.11484v1

System identification is normally involved in augmenting time series data by
time shifting and nonlinearisation (via polynomial basis), which introduce
redundancy both feature-wise and sample-wise. Many research works focus on
reducing redundancy feature-wise, while less attention is paid to sample-wise
redundancy. This paper proposes a novel data **pruning** method, called
(mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary
learning. Time series data is represented by some representative samples,
called atoms, via dictionary learning. The useful samples are selected based on
their correlation with the atoms. The method is tested on one simulated dataset
and two benchmark datasets. The R-squared between the coefficients of models
trained on the full and the coefficients of models trained on pruned datasets
is adopted to evaluate the performance of data **pruning** methods. It is found
that the proposed method significantly outperforms the random **pruning** method.


## Towards Efficient Pre-training Exploring FP4 Precision in Large Language Models

>Authors: Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang

>2025-02-17

> http://arxiv.org/abs/2502.11458v1

The burgeoning computational demands for training large language models
(LLMs) necessitate efficient methods, including **quantize**d training, which
leverages **low-bit** arithmetic operations to reduce costs. While FP8 precision
has shown potential, leveraging FP4 remains challenging due to inherent
**quantization** errors and limited representation capability. Based on the
Transformer architecture, we present an FP4 training scheme for LLMs,
overcoming these obstacles through mixed-precision **quantization** strategies
tailed for different modules and training stages. This allows us to apply the
precision level suitable to distinct components within the model, ensuring that
multi-head attention and linear layers are handled appropriately. Our
pretraining recipe ensures stability in backpropagation by incorporating
fine-grained **quantization** methods with a target precision training schedule.
Experimental results demonstrate that our FP4 training scheme achieves accuracy
comparable to BF16 and FP8, with smaller theoretical computational cost. With
the advent of next-generation hardware supporting FP4, our method sets the
foundation for efficient ultra-low precision training.


## Does RAG Really Perform Bad For Long-Context Processing?

>Authors: Kun Luo, Zheng Liu, Peitian Zhang, Hongjin Qian, Jun Zhao, Kang Liu

>2025-02-17

> http://arxiv.org/abs/2502.11444v1

The efficient processing of long context poses a serious challenge for large
language models (LLMs). Recently, retrieval-augmented generation (RAG) has
emerged as a promising strategy for this problem, as it enables LLMs to make
selective use of the long context for efficient computation. However, existing
RAG approaches lag behind other long-context processing methods due to inherent
limitations on inaccurate retrieval and fragmented contexts. To address these
challenges, we introduce RetroLM, a novel RAG framework for long-context
processing. Unlike traditional methods, RetroLM employs **KV**-level retrieval
augmentation, where it partitions the LLM's **KV** cache into contiguous pages and
retrieves the most crucial ones for efficient computation. This approach
enhances robustness to retrieval inaccuracy, facilitates effective utilization
of fragmented contexts, and saves the cost from repeated computation. Building
on this framework, we further develop a specialized retriever for precise
retrieval of critical pages and conduct unsupervised post-training to optimize
the model's ability to leverage retrieved information. We conduct comprehensive
evaluations with a variety of benchmarks, including LongBench, InfiniteBench,
and RULER, where RetroLM significantly outperforms existing long-context LLMs
and efficient long-context processing methods, particularly in tasks requiring
intensive reasoning or extremely long-context comprehension.


## Sparse Autoencoder Features for Classifications and Transferability

>Authors: Jack Gallifant, Shan Chen, Kuleen Sasse, Hugo Aerts, Thomas Hartvigsen, Danielle S. Bitterman

>2025-02-17

> http://arxiv.org/abs/2502.11367v1

Sparse Autoencoders (SAEs) provide potentials for uncovering structured,
human-interpretable representations in Large Language Models (LLMs), making
them a crucial tool for transparent and controllable AI systems. We
systematically analyze SAE for interpretable feature extraction from LLMs in
safety-critical classification tasks. Our framework evaluates (1) model-layer
selection and scaling properties, (2) SAE architectural configurations,
including width and pooling strategies, and (3) the effect of binarizing
continuous SAE activations. SAE-derived features achieve macro F1 > 0.8,
outperforming hidden-state and BoW baselines while demonstrating cross-model
transfer from Gemma 2 2B to 9B-IT models. These features generalize in a
zero-shot manner to cross-lingual toxicity detection and visual classification
tasks. Our analysis highlights the significant impact of pooling strategies and
binarization thresholds, showing that binarization offers an efficient
alternative to traditional feature selection while maintaining or improving
performance. These findings establish new best practices for SAE-based
interpretability and enable scalable, transparent deployment of LLMs in
real-world applications. Full repo: https://github.com/shan23chen/MOSAIC.


## Teleportation With Null Space Gradient Projection for Optimization Acceleration

>Authors: Zihao Wu, Juncheng Dong, Ahmed Aloui, Vahid Tarokh

>2025-02-17

> http://arxiv.org/abs/2502.11362v1

Optimization techniques have become increasingly critical due to the
ever-growing model complexity and data scale. In particular, teleportation has
emerged as a promising approach, which accelerates convergence of gradient
descent-based methods by navigating within the loss invariant level set to
identify parameters with advantageous geometric properties. Existing
teleportation algorithms have primarily demonstrated their effectiveness in
optimizing Multi-Layer Perceptrons (MLPs), but their extension to more advanced
architectures, such as Convolutional Neural Networks (CNNs) and Transformers,
remains challenging. Moreover, they often impose significant computational
demands, limiting their applicability to complex architectures. To this end, we
introduce an algorithm that projects the gradient of the teleportation
objective function onto the input null space, effectively preserving the
teleportation within the loss invariant level set and reducing computational
cost. Our approach is readily generalizable from MLPs to CNNs, transformers,
and potentially other advanced architectures. We validate the effectiveness of
our algorithm across various benchmark datasets and optimizers, demonstrating
its broad applicability.


## SAIF A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models

>Authors: Zirui He, Haiyan Zhao, Yiran Qiao, Fan Yang, Ali Payani, Jing Ma, Mengnan Du

>2025-02-17

> http://arxiv.org/abs/2502.11356v1

The ability of large language models (LLMs) to follow instructions is crucial
for their practical applications, yet the underlying mechanisms remain poorly
understood. This paper presents a novel framework that leverages **sparse**
autoencoders (SAE) to interpret how instruction following works in these
models. We demonstrate how the features we identify can effectively steer model
outputs to align with given instructions. Through analysis of SAE latent
activations, we identify specific latents responsible for instruction following
behavior. Our findings reveal that instruction following capabilities are
encoded by a distinct set of instruction-relevant SAE latents. These latents
both show semantic proximity to relevant instructions and demonstrate causal
effects on model behavior. Our research highlights several crucial factors for
achieving effective steering performance: precise feature identification, the
role of final layer, and optimal instruction positioning. Additionally, we
demonstrate that our methodology scales effectively across SAEs and LLMs of
varying sizes.


## Unveiling Environmental Impacts of Large Language Model Serving A Functional Unit View

>Authors: Yanran Wu, Inez Hua, Yi Ding

>2025-02-16

> http://arxiv.org/abs/2502.11256v1

Large language models (LLMs) offer powerful capabilities but come with
significant environmental costs, particularly in carbon emissions. Existing
studies benchmark these emissions but lack a standardized basis for comparison
across models. To address this, we introduce the concept of a functional unit
(FU) and develop FUEL, the first FU-based framework for evaluating LLM
serving's environmental impact. Through case studies on model size,
**quantization**, and hardware, we uncover key trade-offs in sustainability. Our
findings highlight the potential for reducing carbon emissions by optimizing
model selection, deployment strategies, and hardware choices, paving the way
for more sustainable AI infrastructure.


## RT-DEMT A hybrid real-time acupoint detection model combining mamba and transformer

>Authors: Shilong Yang, Qi Zang, Chulong Zhang, Lingfeng Huang, Yaoqin Xie

>2025-02-16

> http://arxiv.org/abs/2502.11179v1

Traditional Chinese acupuncture methods often face controversy in clinical
practice due to their high subjectivity. Additionally, current
intelligent-assisted acupuncture systems have two major limitations: slow
acupoint localization speed and low accuracy. To address these limitations, a
new method leverages the excellent inference efficiency of the state-space
model Mamba, while retaining the advantages of the attention mechanism in the
traditional DETR architecture, to achieve efficient global information
integration and provide high-quality feature information for acupoint
localization tasks. Furthermore, by employing the concept of residual
likelihood estimation, it eliminates the need for complex upsampling processes,
thereby accelerating the acupoint localization task. Our method achieved
state-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human
back, with an average Euclidean distance pixel error (EPE) of 7.792 and an
average time consumption of 10.05 milliseconds per localization task. Compared
to the second-best algorithm, our method improved both accuracy and speed by
approximately 14\%. This significant advancement not only enhances the efficacy
of acupuncture treatment but also demonstrates the commercial potential of
automated acupuncture robot systems. Access to our method is available at
https://github.com/Sohyu1/RT-DEMT


## Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity

>Authors: Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, Yizhou Shan

>2025-02-16

> http://arxiv.org/abs/2502.11147v1

Large Language Models (LLMs) have demonstrated strong capabilities across
various domains, with recent advancements in challenging reasoning tasks such
as mathematics and programming. However, solving reasoning tasks often requires
long decoding chains (of thoughts), which incur $O(N)$ time and memory
consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory
consumption, existing **sparsity**-based algorithms propose retaining only the most
critical token's intermediate data (i.e., key-value cache) and discarding the
rest. However, these existing algorithms struggle with the ``impossible
trinity'' of accuracy, time, and memory. For example, the state-of-the-art
algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory
($L$ is the cache budget, $L \ll N$). To address this issue, in this paper, we
identify a new attention pattern during the decode stage of reasoning tasks,
where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are
utilized, and then become unimportant afterward. Based on this pattern, we
propose a new algorithm named RaaS that identifies and retains milestone tokens
only until they are no longer needed, achieving high accuracy with $O(L)$ time
and $O(L)$ memory complexity.


## Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation

>Authors: Yu Cui, Hang Fu, Licheng Wang, Haibin Zhang

>2025-02-16

> http://arxiv.org/abs/2502.11110v1

Homomorphic encryption (HE) is a core building block in privacy-preserving
machine learning (PPML), but HE is also widely known as its efficiency
bottleneck. Therefore, many GPU-accelerated cryptographic schemes have been
proposed to improve the performance of HE. However, these methods often require
complex modifications tailored to specific algorithms and are tightly coupled
with specific GPU and operating systems. It is interesting to ask how to
generally offer more practical GPU-accelerated cryptographic algorithm
implementations. Given the powerful code generation capabilities of large
language models (LLMs), we aim to explore their potential to automatically
generate practical GPU-friendly algorithm code using CPU-friendly code. In this
paper, we focus on number theoretic transform (NTT) -- the core mechanism of
HE. We first develop and optimize a GPU-friendly NTT (GNTT) family that
exploits PyTorch's fast matrix computation and precomputation, achieving an
approximately 62x speedup -- a significant boost over existing ones. Then we
explore GPU-friendly code generation using various LLMs, including DeepSeek-R1,
OpenAI o1 and o3-mini. We discover many interesting findings throughout the
process. For instance, somewhat surprisingly, our experiments demonstrate that
DeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot
beat our optimized protocol. The findings provide valuable insights for
turbocharging PPML and enhancing code generation capabilities of LLMs. Codes
are available at: https://github.com/LMPC-Lab/GenGPUCrypto.


## CacheFocus Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation

>Authors: Kun-Hui Lee, Eunhwan Park, Donghoon Han, Seung-Hoon Na

>2025-02-16

> http://arxiv.org/abs/2502.11101v1

Large Language Models (LLMs) excel across a variety of language tasks yet are
constrained by limited input lengths and high computational costs. Existing
approaches\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)
and sliding window mechanisms\textemdash partially alleviate these issues but
often require additional training or suffer from performance degradation with
longer inputs. In this paper, we introduce \textbf{\textit{CacheFocus}}, a
method that enhances length normalization and reduces inference latency without
any further training. Our approach leverages query-independent, offline caching
to efficiently reuse a Context **KV** Cache Store. We address the amplification of
abnormal token distributions problem by re-positioning cached keys and
introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during
pre-filling. Additionally, our Adaptive Positional Allocation Strategy
dynamically reassigns cache positions to maximize the use of the available
positional encoding range. Experiments on the Natural Questions and TriviaQA
datasets demonstrate that CacheFocus outperforms alternative methods even when
inputs exceed the $4$K limit of the \texttt{LLaMA-2} model, emphasizing its
practical effectiveness for long-context LLMs. Moreover, even with large
maximum input length of \texttt{Qwen2}, the performance of CacheFocus shows
that it maintains consistent performance even as the number of documents
increases, effectively managing long-text generation without degradation.


## Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time

>Authors: Robert Dahlke, Henrik Klagges, Dan Zecha, Benjamin Merkel, Sven Rohr, Fabian Klemm

>2025-02-16

> http://arxiv.org/abs/2502.11096v1

We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the
Mixture-of-Experts architecture of Large Language Models (LLMs). Without
additional training, MoTE enables meaningful and focused behavior changes in
LLMs on-the-fly during inference time.
  By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub
'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using
prompts designed to elicit specific behavior (e.g., 'What happened
{time}{place}?') - we empirically identify distinctive experts associated with
behaviors like refusal responses.
  Using MoTE we are able to intervene and control such specific behavior. We
switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848
routed experts), achieving a 52% refusal reduction on sensitive reference
prompts without performance degradation on MT-Bench. Random expert deactivation
resulted in smaller behavioral shifts with increased noise, whereas forced
expert activation led to significantly higher refusal rates.
  Our approach shares similarities with **sparse** autoencoders (SAEs) in terms of
explainability and steerability. Unlike SAEs, MoTE does not require large
training efforts, as within MoEs with a vast number of experts, specialization
already emerged naturally during pretraining.
  Our findings suggest that significant functional mechanisms in
Mixture-of-Experts architectures can at least partially be localized in a small
number of specific experts, rather than being distributed throughout the
model's weights. Expert subgroups can be tuned to trigger significant behavior
variations, providing insights into the inner workings of LLMs.


## SyncSpeech Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer

>Authors: Zhengyan Sheng, Zhihao Du, Shiliang Zhang, Zhijie Yan, Yexin Yang, Zhenhua Ling

>2025-02-16

> http://arxiv.org/abs/2502.11094v1

This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech,
capable of receiving streaming text input from upstream models while
simultaneously generating streaming speech, facilitating seamless interaction
with large language models. SyncSpeech has the following advantages: Low
latency, as it begins generating streaming speech upon receiving the second
text token; High efficiency, as it decodes all speech tokens corresponding to
the each arrived text token in one step. To achieve this, we propose a temporal
masked transformer as the backbone of SyncSpeech, combined with token-level
duration prediction to predict speech tokens and the duration for the next
step. Additionally, we design a two-stage training strategy to improve training
efficiency and the quality of generated speech. We evaluated the SyncSpeech on
both English and Mandarin datasets. Compared to the recent dual-stream TTS
models, SyncSpeech significantly reduces the first packet delay of speech
tokens and accelerates the real-time factor. Moreover, with the same data
scale, SyncSpeech achieves performance comparable to that of traditional
autoregressive-based TTS models in terms of both speech quality and robustness.
Speech samples are available at
https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.


## Native Sparse Attention Hardware-Aligned and Natively Trainable Sparse Attention

>Authors: Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng

>2025-02-16

> http://arxiv.org/abs/2502.11089v1

Long-context modeling is crucial for next-generation language models, yet the
high computational cost of standard attention mechanisms poses significant
computational challenges. Sparse attention offers a promising direction for
improving efficiency while maintaining model capabilities. We present NSA, a
Natively trainable Sparse Attention mechanism that integrates algorithmic
innovations with hardware-aligned optimizations to achieve efficient
long-context modeling. NSA employs a dynamic hierarchical **sparse** strategy,
combining coarse-grained token compression with fine-grained token selection to
preserve both global context awareness and local precision. Our approach
advances **sparse** attention design with two key innovations: (1) We achieve
substantial speedups through arithmetic intensity-balanced algorithm design,
with implementation optimizations for modern hardware. (2) We enable end-to-end
training, reducing pretraining computation without sacrificing model
performance. As shown in Figure 1, experiments show the model pretrained with
NSA maintains or exceeds Full Attention models across general benchmarks,
long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves
substantial speedups over Full Attention on 64k-length sequences across
decoding, forward propagation, and backward propagation, validating its
efficiency throughout the model lifecycle.


## Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks

>Authors: Yuanjie Lyu, Chao Zhang, Yuhao Chen, Yong Chen, Tong Xu

>2025-02-16

> http://arxiv.org/abs/2502.11083v1

In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the
"Chain of Models" approach is widely used, where multiple specialized models
work sequentially on distinct sub-tasks. This approach is effective but
increases resource demands as each model must be deployed separately. Recent
advancements attempt to address this by applying prompt tuning, which allows a
shared base model to adapt to multiple tasks with minimal parameter changes.
However, a key challenge remains: intermediate outputs, passed between models
as plain text, require recomputation of hidden states (i.e., Key and Value (**KV**)
states in Transformers) during inference. In this paper, we introduce FTHSS, a
novel prompt-tuning method that enables models to share **KV** hidden states,
eliminating redundant forward passes and reducing **KV** cache storage. By
modifying input and attention masks during training, FTHSS allows models to
effectively utilize **KV** hidden states from prior models in both single- and
multi-round scenarios. Empirical results on four tasks show that FTHSS matches
the performance of traditional model chains while improving inference
efficiency.


## Accelerating Anchors via Specialization and Feature Transformation

>Authors: Haonan Yu, Junhao Liu, Xin Zhang

>2025-02-16

> http://arxiv.org/abs/2502.11068v1

Anchors is a popular local model-agnostic explanation technique whose
applicability is limited by its computational inefficiency. To address this
limitation, we propose a pre-training-based approach to accelerate Anchors
without compromising the explanation quality. Our approach leverages the
iterative nature of Anchors' algorithm which gradually refines an explanation
until it is precise enough for a given input by providing a general explanation
that is obtained through pre-training as Anchors' initial explanation.
Specifically, we develop a two-step rule transformation process: the horizontal
transformation adapts a pre-trained explanation to the current input by
replacing features, and the vertical transformation refines the general
explanation until it is precise enough for the input. We evaluate our method
across tabular, text, and image datasets, demonstrating that it significantly
reduces explanation generation time while maintaining fidelity and
interpretability, thereby enabling the practical adoption of Anchors in
time-sensitive applications.


## DreamDDP Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization

>Authors: Zhenheng Tang, Zichen Tang, Junlin Huang, Xinglin Pan, Rudan Yan, Yuxin Wang, Amelie Chi Zhou, Shaohuai Shi, Xiaowen Chu, Bo Li

>2025-02-16

> http://arxiv.org/abs/2502.11058v1

The growth of large language models (LLMs) increases challenges of
accelerating distributed training across multiple GPUs in different data
centers. Moreover, concerns about data privacy and data exhaustion have
heightened interest in geo-distributed data centers. Communication in
geo-distributed data parallel training (DDP) with stochastic gradient descent
(S-SGD) is the main bottleneck in low-bandwidth environments. Local SGD
mitigates communication overhead by reducing synchronization frequency, and
recent studies have successfully applied it to geo-distributedly pre-train
LLMs. However, we identify that its model synchronization mechanism prevents
overlapping communication and computation, which makes the system lose
opportunities to overlap communication and computation.
  To overcome this limitation, we expand the design space of local SGD by
layer-wisely decoupling model synchronization. In each iteration, only some
layers are synchronized instead of the entire model after a specific number of
iterations. Leveraging this methodology, we introduce DreamDDP, a training
framework to accelerate low-bandwidth distributed training with three key
innovations: (1) partial local SGD with theoretical assurances of convergence
rates comparable to S-SGD; (2) overlapping parameter synchronization with
computation without extra GPU memory occupation; (3) identifying and exploiting
three properties to schedule the communication and computation to reduce the
training time based on fine-grained profiling of layer-wise communication and
computation time. Empirical evaluations conducted on 32 GPUs using prominent
deep learning models, including ResNet-18, ResNet-50, GPT-2, and Llama-2,
demonstrate that DreamDDP enhances the convergence properties of Local SGD (and
Adam) and achieves speedups ranging from $1.49\times$ to $3.91\times$ over
leading baseline methods.


## GRIFFIN Effective Token Alignment for Faster Speculative Decoding

>Authors: Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou

>2025-02-16

> http://arxiv.org/abs/2502.11018v1

Speculative decoding accelerates inference in large language models (LLMs) by
generating multiple draft tokens simultaneously. However, existing methods
often struggle with token misalignment between the training and decoding
phases, limiting their performance. To address this, we propose GRIFFIN, a
novel framework that incorporates a token-alignable training strategy and a
token-alignable draft model to mitigate misalignment. The training strategy
employs a loss masking mechanism to exclude highly misaligned tokens during
training, preventing them from negatively impacting the draft model's
optimization. The token-alignable draft model introduces input tokens to
correct inconsistencies in generated features. Experiments on LLaMA-series and
Vicuna models demonstrate that GRIFFIN achieves an average acceptance length
improvement of over 7\% and a speedup ratio exceeding 8%, outperforming current
SoTAs as shown in Fig. 1 (a) and (b).


## GS-GVINS A Tightly-integrated GNSS-Visual-Inertial Navigation System Augmented by 3D Gaussian Splatting

>Authors: Zelin Zhou, Saurav Uprety, Shichuang Nie, Hongzhou Yang

>2025-02-16

> http://arxiv.org/abs/2502.10975v1

Recently, the emergence of 3D Gaussian Splatting (3DGS) has drawn significant
attention in the area of 3D map reconstruction and visual SLAM. While extensive
research has explored 3DGS for indoor trajectory tracking using visual sensor
alone or in combination with Light Detection and Ranging (LiDAR) and Inertial
Measurement Unit (IMU), its integration with GNSS for large-scale outdoor
navigation remains underexplored. To address these concerns, we proposed
GS-GVINS: a tightly-integrated GNSS-Visual-Inertial Navigation System augmented
by 3DGS. This system leverages 3D Gaussian as a continuous differentiable scene
representation in largescale outdoor environments, enhancing navigation
performance through the constructed 3D Gaussian map. Notably, GS-GVINS is the
first GNSS-Visual-Inertial navigation application that directly utilizes the
analytical jacobians of SE3 camera pose with respect to 3D Gaussians. To
maintain the quality of 3DGS rendering in extreme dynamic states, we introduce
a motionaware 3D Gaussian **pruning** mechanism, updating the map based on relative
pose translation and the accumulated opacity along the camera ray. For
validation, we test our system under different driving environments: open-sky,
sub-urban, and urban. Both self-collected and public datasets are used for
evaluation. The results demonstrate the effectiveness of GS-GVINS in enhancing
navigation accuracy across diverse driving environments.


## A recurrent vision transformer shows signatures of primate visual attention

>Authors: Jonathan Morgan, Badr Albanna, James P. Herman

>2025-02-16

> http://arxiv.org/abs/2502.10955v1

Attention is fundamental to both biological and artificial intelligence, yet
research on animal attention and AI self attention remains largely
disconnected. We propose a Recurrent Vision Transformer (Recurrent ViT) that
integrates self-attention with recurrent memory, allowing both current inputs
and stored information to guide attention allocation. Trained solely via **sparse**
reward feedback on a spatially cued orientation change detection task, a
paradigm used in primate studies, our model exhibits primate like signatures of
attention, including improved accuracy and faster responses for cued stimuli
that scale with cue validity. Analysis of self-attention maps reveals dynamic
spatial prioritization with reactivation prior to expected changes, and
targeted perturbations produce performance shifts similar to those observed in
primate frontal eye fields and superior colliculus. These findings demonstrate
that incorporating recurrent feedback into self attention can capture key
aspects of primate visual attention.


## Order-agnostic Identifier for Large Language Model-based Generative Recommendation

>Authors: Xinyu Lin, Haihan Shi, Wenjie Wang, Fuli Feng, Qifan Wang, See-Kiong Ng, Tat-Seng Chua

>2025-02-15

> http://arxiv.org/abs/2502.10833v1

Leveraging Large Language Models (LLMs) for generative recommendation has
attracted significant research interest, where item tokenization is a critical
step. It involves assigning item identifiers for LLMs to encode user history
and generate the next item. Existing approaches leverage either token-sequence
identifiers, representing items as discrete token sequences, or single-token
identifiers, using ID or semantic embeddings. Token-sequence identifiers face
issues such as the local optima problem in beam search and low generation
efficiency due to step-by-step generation. In contrast, single-token
identifiers fail to capture rich semantics or encode Collaborative Filtering
(CF) information, resulting in suboptimal performance.
  To address these issues, we propose two fundamental principles for item
identifier design: 1) integrating both CF and semantic information to fully
capture multi-dimensional item information, and 2) designing order-agnostic
identifiers without token dependency, mitigating the local optima issue and
achieving simultaneous generation for generation efficiency. Accordingly, we
introduce a novel set identifier paradigm for LLM-based generative
recommendation, representing each item as a set of order-agnostic tokens. To
implement this paradigm, we propose SETRec, which leverages CF and semantic
tokenizers to obtain order-agnostic multi-dimensional tokens. To eliminate
token dependency, SETRec uses a **sparse** attention mask for user history encoding
and a query-guided generation mechanism for simultaneous token generation. We
instantiate SETRec on T5 and Qwen (from 1.5B to 7B). Extensive experiments
demonstrate its effectiveness under various scenarios (e.g., full ranking,
warm- and cold-start ranking, and various item popularity groups). Moreover,
results validate SETRec's superior efficiency and show promising scalability on
cold-start items as model sizes increase.


## 1bit-Merging Dynamic Quantized Merging for Large Language Models

>Authors: Shuqi Liu, Han Wu, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Linqi Song

>2025-02-15

> http://arxiv.org/abs/2502.10743v1

Recent advances in large language models have led to specialized models
excelling in specific domains, creating a need for efficient model merging
techniques. While traditional merging approaches combine parameters into a
single static model, they often compromise task-specific performance. However,
task-specific routing methods maintain accuracy but introduce substantial
storage overhead. We present \texttt{1bit}-Merging, a novel framework that
integrates task-specific routing with 1-bit **quantize**d task vectors to balance
performance and storage efficiency. Our approach leverages the observation that
different task-specific models store knowledge in distinct layers-chat models
primarily in attention layers and math/code models in MLP layers-enabling
targeted compression strategies. Through extensive experiments with LLaMA2 and
Mistral model families across chat, mathematical reasoning, and code generation
tasks, we demonstrate that \texttt{1bit}-Merging achieves comparable or
superior performance to existing methods while significantly reducing storage
requirements. Our framework offers a practical solution for combining
specialized models while maintaining their individual strengths and addressing
the storage challenges of current approaches.


## OPTISHEAR Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization

>Authors: Shuqi Liu, Bowei He, Han Wu, Linqi Song

>2025-02-15

> http://arxiv.org/abs/2502.10735v1

Post-training **pruning** has emerged as a crucial optimization technique as
large language models (LLMs) continue to grow rapidly. However, the significant
variations in weight distributions across different LLMs make fixed **pruning**
strategies inadequate for multiple models. In this paper, we introduce
\textbf{\textsc{OptiShear}}, an efficient evolutionary optimization framework
for adaptive LLM **pruning**. Our framework features two key innovations: an
effective search space built on our Meta **pruning** metric to handle diverse
weight distributions, and a model-wise reconstruction error for rapid
evaluation during search trials. We employ Non-dominated Sorting Genetic
Algorithm III (NSGA-III) to optimize both **pruning** metrics and layerwise
**sparsity** ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models
(7B-70B) across multiple benchmarks, we demonstrate that our adaptive **pruning**
metrics consistently outperform existing methods. Additionally, our discovered
layerwise **sparsity** ratios enhance the effectiveness of other **pruning** metrics.
The framework exhibits strong cross-task and cross-model generalizability,
providing a cost-effective solution for model compression.


## Lagrangian formalism in the theory of relativistic vector fields

>Authors: Sergey G. Fedosin

>2025-02-15

> http://arxiv.org/abs/2502.12190v1

The Lagrangian formalism is used to derive covariant equations that are
suitable for use in continuously distributed matter in curved spacetime.
Special attention is given to theoretical representation, in which the
Lagrangian and its derivatives are directly involved. The obtained results,
including equation for metric, equation of motion, equations for fields, are
applied to purely vector fields. As a consequence, formulas are determined for
calculating the basic quantities necessary to describe physical systems. In
this case, not only the pressure field and the **acceleration** field are taken
into account, but also the electromagnetic and gravitational fields outside the
matter, which contribute to the four-momentum and to the four-dimensional
angular momentum pseudotensor of each system. It is shown that the canonical
representation of the angular momentum pseudotensor is its representation with
covariant indices. The radius-vector of the center of momentum of a physical
system is determined in covariant form.


## Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA

>Authors: Jindong Li, Tenglong Li, Guobin Shen, Dongcheng Zhao, Qian Zhang, Yi Zeng

>2025-02-15

> http://arxiv.org/abs/2502.10659v1

The extremely high computational and storage demands of large language models
have excluded most edge devices, which were widely used for efficient machine
learning, from being viable options. A typical edge device usually only has 4GB
of memory capacity and a bandwidth of less than 20GB/s, while a large language
model **quantize**d to 4-bit precision with 7B parameters already requires 3.5GB of
capacity, and its decoding process is purely bandwidth-bound. In this paper, we
aim to explore these limits by proposing a hardware accelerator for large
language model (LLM) inference on the Zynq-based **KV**260 platform, equipped with
4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,
achieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory
capacity and reaching 85% decoding speed of the theoretical memory bandwidth
limit. To fully reserve the memory capacity for model weights and key-value
cache, we develop the system in a bare-metal environment without an operating
system. To fully reserve the bandwidth for model weight transfers, we implement
a customized dataflow with an operator fusion pipeline and propose a data
arrangement format that can maximize the data transaction efficiency. This
research marks the first attempt to deploy a 7B level LLM on a standalone
embedded field programmable gate array (FPGA) device. It provides key insights
into efficient LLM inference on embedded FPGA devices and provides guidelines
for future architecture design.


## Time Parameterized Optimal Transport

>Authors: Kaiwen Shi

>2025-02-14

> http://arxiv.org/abs/2502.10607v1

Optimal transport has gained significant attention in recent years due to its
effectiveness in deep learning and computer vision. Its descendant metric, the
Wasserstein distance, has been particularly successful in measuring
distribution dissimilarities. While extensive research has focused on optimal
transport and its regularized variants (such as entropy, **sparsity**, and capacity
constraints) the role of time has been largely overlooked. However, time is a
critical factor in real world transport problems.
  In this work, we introduce a time parameterized formulation of the optimal
transport problem, incorporating a time variable t to represent sequential
steps and enforcing specific constraints at each step. We propose a systematic
method to solve a special subproblem and develop a heuristic search algorithm
that achieves nearly optimal solutions while significantly reducing
computational time.


## Weighted quantization using MMD From mean field to mean shift via gradient flows

>Authors: Ayoub Belhadji, Daniel Sharp, Youssef Marzouk

>2025-02-14

> http://arxiv.org/abs/2502.10600v1

Approximating a probability distribution using a set of particles is a
fundamental problem in machine learning and statistics, with applications
including clustering and **quantization**. Formally, we seek a finite weighted
mixture of Dirac measures that best approximates the target distribution. While
much existing work relies on the Wasserstein distance to quantify approximation
errors, maximum mean discrepancy (MMD) has received comparatively less
attention, especially when allowing for variable particle weights. We study the
**quantization** problem from the perspective of minimizing MMD via gradient flow
in the Wasserstein-Fisher-Rao (WFR) geometry. This gradient flow yields an ODE
system from which we further derive a fixed-point algorithm called mean shift
interacting particles (MSIP). We show that MSIP extends the (non-interacting)
mean shift algorithm, widely used for identifying modes in kernel density
estimates. Moreover, we show that MSIP can be interpreted as preconditioned
gradient descent, and that it acts as a relaxation of Lloyd's algorithm for
clustering. Our numerical experiments demonstrate that MSIP and the WFR ODEs
outperform other algorithms for **quantization** of multi-modal and
high-dimensional targets.


## Towards Watermarking of Open-Source LLMs

>Authors: Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev

>2025-02-14

> http://arxiv.org/abs/2502.10525v1

While watermarks for closed LLMs have matured and have been included in
large-scale deployments, these methods are not applicable to open-source
models, which allow users full control over the decoding process. This setting
is understudied yet critical, given the rising performance of open-source
models. In this work, we lay the foundation for systematic study of open-source
LLM watermarking. For the first time, we explicitly formulate key requirements,
including durability against common model modifications such as model merging,
**quantization**, or finetuning, and propose a concrete evaluation setup. Given the
prevalence of these modifications, durability is crucial for an open-source
watermark to be effective. We survey and evaluate existing methods, showing
that they are not durable. We also discuss potential ways to improve their
durability and highlight remaining challenges. We hope our work enables future
progress on this important problem.


## Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding

>Authors: Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu

>2025-02-14

> http://arxiv.org/abs/2502.10392v1

In this paper, we propose an efficient multi-level convolution architecture
for 3D visual grounding. Conventional methods are difficult to meet the
requirements of real-time inference due to the two-stage or point-based
architecture. Inspired by the success of multi-level fully **sparse** convolutional
architecture in 3D object detection, we aim to build a new 3D visual grounding
framework following this technical route. However, as in 3D visual grounding
task the 3D scene representation should be deeply interacted with text
features, **sparse** convolution-based architecture is inefficient for this
interaction due to the large amount of voxel features. To this end, we propose
text-guided **pruning** (TGP) and completion-based addition (CBA) to deeply fuse 3D
scene representation and text features in an efficient way by gradual region
**pruning** and target completion. Specifically, TGP iteratively sparsifies the 3D
scene representation and thus efficiently interacts the voxel features with
text features by cross-attention. To mitigate the affect of **pruning** on delicate
geometric information, CBA adaptively fixes the over-pruned region by voxel
completion with negligible computational overhead. Compared with previous
single-stage methods, our method achieves top inference speed and surpasses
previous fastest method by 100\% FPS. Our method also achieves state-of-the-art
accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on
ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code
is available at
\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.


## Region-Adaptive Sampling for Diffusion Transformers

>Authors: Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang

>2025-02-14

> http://arxiv.org/abs/2502.10389v1

Diffusion models (DMs) have become the leading choice for generative tasks
across diverse domains. However, their reliance on multiple sequential forward
passes significantly limits real-time performance. Previous **acceleration**
methods have primarily focused on reducing the number of sampling steps or
reusing intermediate results, failing to leverage variations across spatial
regions within the image due to the constraints of convolutional U-Net
structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in
handling variable number of tokens, we introduce RAS, a novel, training-free
sampling strategy that dynamically assigns different sampling ratios to regions
within an image based on the focus of the DiT model. Our key observation is
that during each sampling step, the model concentrates on semantically
meaningful regions, and these areas of focus exhibit strong continuity across
consecutive steps. Leveraging this insight, RAS updates only the regions
currently in focus, while other regions are updated using cached noise from the
previous step. The model's focus is determined based on the output from the
preceding step, capitalizing on the temporal consistency we observed. We
evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up
to 2.36x and 2.51x, respectively, with minimal degradation in generation
quality. Additionally, a user study reveals that RAS delivers comparable
qualities under human evaluation while achieving a 1.6x speedup. Our approach
makes a significant step towards more efficient diffusion transformers,
enhancing their potential for real-time applications.


## Agentic Verification for Ambiguous Query Disambiguation

>Authors: Youngwon Lee, Seung-won Hwang, Ruofan Wu, Feng Yan, Danmei Xu, Moutasem Akkad, Zhewei Yao, Yuxiong He

>2025-02-14

> http://arxiv.org/abs/2502.10352v1

In this work, we tackle the challenge of disambiguating queries in
retrieval-augmented generation (RAG) to diverse yet answerable interpretations.
State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse
interpretations are generated by an LLM, later used as search queries to
retrieve supporting passages. Such a process may introduce noise in either
interpretations or retrieval, particularly in enterprise settings, where LLMs
-- trained on static data -- may struggle with domain-specific disambiguations.
Thus, a post-hoc verification phase is introduced to prune noises. Our
distinction is to unify diversification with verification by incorporating
feedback from retriever and generator early on. This joint approach improves
both efficiency and robustness by reducing reliance on multiple retrieval and
inference steps, which are susceptible to cascading errors. We validate the
efficiency and effectiveness of our method, Verified-Diversification with
Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve
diverse yet verifiable interpretations. Empirical results show that VERDICT
improves grounding-aware F1 score by an average of 23% over the strongest
baseline across different backbone LLMs.


## Step-Video-T2V Technical Report The Practice, Challenges, and Future of Video Foundation Model

>Authors: Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang

>2025-02-14

> http://arxiv.org/abs/2502.10248v2

We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.


## Variational optical phase learning on a continuous-variable quantum compiler

>Authors: Matthew A. Feldman, Tyler Volkoff, Seongjin Hong, Claire E. Marvinney, Zoe Holmes, Raphael C. Pooser, Andrew Sornborger, Alberto M. Marino

>2025-02-14

> http://arxiv.org/abs/2502.10242v1

Quantum process learning is a fundamental primitive that draws inspiration
from machine learning with the goal of better studying the dynamics of quantum
systems. One approach to quantum process learning is quantum compilation,
whereby an analog quantum operation is digitized by compiling it into a series
of basic gates. While there has been significant focus on quantum compiling for
discrete-variable systems, the continuous-variable (CV) framework has received
comparatively less attention. We present an experimental implementation of a CV
quantum compiler that uses two mode-squeezed light to learn a Gaussian unitary
operation. We demonstrate the compiler by learning a parameterized linear phase
unitary through the use of target and control phase unitaries to demonstrate a
factor of 5.4 increase in the precision of the phase estimation and a 3.6-fold
**acceleration** in the time-to-solution metric when leveraging quantum resources.
Our results are enabled by the tunable control of our cost landscape via
variable squeezing, thus providing a critical framework to simultaneously
increase precision and reduce time-to-solution.


## Can Post-Training Quantization Benefit from an Additional QLoRA Integration?

>Authors: Xiliang Zhu, Elena Khasanova, Cheng Chen

>2025-02-14

> http://arxiv.org/abs/2502.10202v1

Large language models (LLMs) have transformed natural language processing but
pose significant challenges for real-world deployment. These models necessitate
considerable computing resources, which can be costly and frequently
unavailable. Model compression techniques such as **quantization** are often
leveraged to alleviate resource demand, but they may have a negative impact on
the generation quality. In this study, we explore the integration of 4-bit
Post-training Quantization (PTQ) with QLoRA to address these issues. We
demonstrate through extensive experiments that this integration outperforms
standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,
validated across proprietary and public datasets with different **quantization**
algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,
offering a viable solution for deploying powerful LLMs in resource-constrained
environments without compromising on performance.


## Janus Collaborative Vision Transformer Under Dynamic Network Environment

>Authors: Linyi Jiang, Silvery D. Fu, Yifei Zhu, Bo Li

>2025-02-14

> http://arxiv.org/abs/2502.10047v1

Vision Transformers (ViTs) have outperformed traditional Convolutional Neural
Network architectures and achieved state-of-the-art results in various computer
vision tasks. Since ViTs are computationally expensive, the models either have
to be pruned to run on resource-limited edge devices only or have to be
executed on remote cloud servers after receiving the raw data transmitted over
fluctuating networks. The resulting degraded performance or high latency all
hinder their widespread applications. In this paper, we present Janus, the
first framework for low-latency cloud-device collaborative Vision Transformer
inference over dynamic networks. Janus overcomes the intrinsic model
limitations of ViTs and realizes collaboratively executing ViT models on both
cloud and edge devices, achieving low latency, high accuracy, and low
communication overhead. Specifically, Janus judiciously combines token **pruning**
techniques with a carefully designed fine-to-coarse model splitting policy and
non-static mixed **pruning** policy. It attains a balance between accuracy and
latency by dynamically selecting the optimal **pruning** level and split point.
Experimental results across various tasks demonstrate that Janus enhances
throughput by up to 5.15 times and reduces latency violation ratios by up to
98.7% when compared with baseline approaches under various network
environments.


## Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts

>Authors: Shruti Joshi, Andrea Dittadi, Sébastien Lachapelle, Dhanya Sridhar

>2025-02-14

> http://arxiv.org/abs/2502.12179v1

Steering methods manipulate the representations of large language models
(LLMs) to induce responses that have desired properties, e.g., truthfulness,
offering a promising approach for LLM alignment without the need for
fine-tuning. Traditionally, steering has relied on supervision, such as from
contrastive pairs of prompts that vary in a single target concept, which is
costly to obtain and limits the speed of steering research. An appealing
alternative is to use unsupervised approaches such as **sparse** autoencoders
(SAEs) to map LLM embeddings to **sparse** representations that capture
human-interpretable concepts. However, without further assumptions, SAEs may
not be identifiable: they could learn latent dimensions that entangle multiple
concepts, leading to unintentional steering of unrelated properties. We
introduce Sparse Shift Autoencoders (SSAEs) that instead map the differences
between embeddings to **sparse** representations. Crucially, we show that SSAEs are
identifiable from paired observations that vary in \textit{multiple unknown
concepts}, leading to accurate steering of single concepts without the need for
supervision. We empirically demonstrate accurate steering across semi-synthetic
and real-world language datasets using Llama-3.1 embeddings.


## EmbBERT-Q Breaking Memory Barriers in Embedded NLP

>Authors: Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri

>2025-02-14

> http://arxiv.org/abs/2502.10001v1

Large Language Models (LLMs) have revolutionized natural language processing,
setting new standards across a wide range of applications. However, their
relevant memory and computational demands make them impractical for deployment
on technologically-constrained tiny devices such as wearable devices and
Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a
novel tiny language model specifically designed for tiny devices with stringent
memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in
Natural Language Processing tasks in this scenario, with a total memory
footprint (weights and activations) of just 781 kB, representing a 25x
reduction in size with respect to SotA models. By combining architectural
innovations with hardware-compatible 8-bit **quantization**, EmbBERT-Q consistently
outperforms several baseline models scaled down to a 2 MB memory budget (i.e.,
the maximum memory typically available in tiny devices), including heavily
compressed versions of BERT and MAMBA. Extensive experimental evaluations on
both a selected benchmark dataset, TinyNLP, specifically curated to evaluate
Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE
benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with
respect to existing approaches, achieving an unmatched balance between memory
and performance. To ensure the complete and immediate reproducibility of all
our results, we release all code, scripts, and model checkpoints at
https://github.com/RiccardoBravin/tiny-LLM.


## X-Boundary Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability

>Authors: Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao

>2025-02-14

> http://arxiv.org/abs/2502.09990v1

Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.


## Granite Vision a lightweight, open-source multimodal model for enterprise Intelligence

>Authors: Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek-koifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, Rogerio Feris

>2025-02-14

> http://arxiv.org/abs/2502.09927v1

We introduce Granite Vision, a lightweight large language model with vision
capabilities, specifically designed to excel in enterprise use cases,
particularly in visual document understanding. Our model is trained on a
comprehensive instruction-following dataset, including document-related tasks,
such as content extraction from tables, charts, diagrams, sketches, and
infographics, as well as general image tasks. The architecture of Granite
Vision is centered around visual modality alignment with a decoder-only, 2
billion parameter Granite large language model. Additionally, we introduce a
dedicated safety classification approach in test-time that leverages a **sparse**
set of attention vectors to identify potential harmful inputs. Despite its
lightweight architecture, Granite Vision achieves strong results in standard
benchmarks related to visual document understanding, as well as on the LiveXiv
benchmark, which is designed to avoid test set contamination by using a
constantly updated corpus of recently published Arxiv papers. We are releasing
the model under the Apache-2 license, allowing for both research and commercial
use, while offering complete visibility into the training data and other
relevant details. See https://huggingface.co/ibm-granite/ for model weights.


## INF^2 High-Throughput Generative Inference of Large Language Models using Near-Storage Processing

>Authors: Hongsun Jang, Siung Noh, Changmin Shin, Jaewon Jung, Jaeyong Song, Jinho Lee

>2025-02-14

> http://arxiv.org/abs/2502.09921v1

The growing memory and computational demands of large language models (LLMs)
for generative inference present significant challenges for practical
deployment. One promising solution to address these challenges is
offloading-based batched inference, which leverages host memory and disk as an
extended memory hierarchy for GPUs. While the approach cost-effectively enables
LLM inference, its performance is limited by substantial I/O overhead,
primarily due to the large key-value (**KV**) cache sizes, which increase with
batch size and LLM context window length.
  In this paper, we introduce INFerence-INFinity (INF^2), a framework that
boosts generative inference throughput using computational storage devices
(CSDs). The core of INF^2 is attention-near storage, which offloads
memory-intensive self-attention operations to near-storage accelerators,
significantly reducing traffic through the system interconnect. We also propose
delayed **KV** cache writeback to hide storage write latency by delaying newly
generated **KV** cache writes until the cache reaches sufficient size in system
memory. Additionally, we introduce cooperative X-cache, a technique designed to
further trade off the remaining memory capacity for storage bandwidth. Our
methods effectively minimize idle time for computation, improving the overall
throughput.
  To demonstrate the effectiveness of our approach, \thiswork has been
implemented on PyTorch and evaluated on a real system. Our experiments show
that INF^2 achieves up to 3.46$\times$ throughput improvement compared to
state-of-the-art baselines. We will open-source INF^2 to facilitate broader
adoption.


## An Efficient Large Recommendation Model Towards a Resource-Optimal Scaling Law

>Authors: Songpei Xu, Shijia Wang, Da Guo, Xianwen Guo, Qiang Xiao, Fangjian Li, Chuanjiang Luo

>2025-02-14

> http://arxiv.org/abs/2502.09888v1

The pursuit of scaling up recommendation models confronts intrinsic tensions
between expanding model capacity and preserving computational tractability.
While prior studies have explored scaling laws for recommendation systems,
their resource-intensive paradigms -- often requiring tens of thousands of A100
GPU hours -- remain impractical for most industrial applications. This work
addresses a critical gap: achieving sustainable model scaling under strict
computational budgets. We propose Climber, a resource-efficient recommendation
framework comprising two synergistic components: the ASTRO model architecture
for algorithmic innovation and the TURBO **acceleration** framework for engineering
optimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts
two core innovations: (1) multi-scale sequence partitioning that reduces
attention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,
enabling more efficient scaling with sequence length; (2) dynamic temperature
modulation that adaptively adjusts attention scores for multimodal
distributions arising from inherent multi-scenario and multi-behavior
interactions. Complemented by TURBO (Two-stage Unified Ranking with Batched
Output), a co-designed **acceleration** framework integrating gradient-aware
feature compression and memory-efficient Key-Value caching, Climber achieves
5.15x throughput gains without performance degradation. Comprehensive offline
experiments on multiple datasets validate that Climber exhibits a more ideal
scaling curve. To our knowledge, this is the first publicly documented
framework where controlled model scaling drives continuous online metric growth
(12.19% overall lift) without prohibitive resource costs. Climber has been
successfully deployed on Netease Cloud Music, one of China's largest music
streaming platforms, serving tens of millions of users daily.

