# 2025-01-03

# Table of Contents
* [Sparse chaos in cortical circuits](#Sparse-chaos-in-cortical-circuits)
* [Facilitating large language model Russian adaptation with Learned Embedding Propagation](#Facilitating-large-language-model-Russian-adaptation-with-Learned-Embedding-Propagation)
* [Exploring and Controlling Diversity in LLM-Agent Conversation](#Exploring-and-Controlling-Diversity-in-LLM-Agent-Conversation)
* [QAHAN A Quantum Annealing Hard Attention Network](#QAHAN-A-Quantum-Annealing-Hard-Attention-Network)
* [DoTA Weight-Decomposed Tensor Adaptation for Large Language Models](#DoTA-Weight-Decomposed-Tensor-Adaptation-for-Large-Language-Models)
* [KeyGS A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences](#KeyGS-A-Keyframe-Centric-Gaussian-Splatting-Method-for-Monocular-Image-Sequences)
* [Align Attention Heads Before Merging Them An Effective Way for Converting MHA to GQA](#Align-Attention-Heads-Before-Merging-Them-An-Effective-Way-for-Converting-MHA-to-GQA)
* [Topological invariant of non-Hermitian space-time modulated photonic crystals](#Topological-invariant-of-non-Hermitian-space-time-modulated-photonic-crystals)
* [ReTaKe Reducing Temporal and Knowledge Redundancy for Long Video Understanding](#ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding)
* [Open-Sora Democratizing Efficient Video Production for All](#Open-Sora-Democratizing-Efficient-Video-Production-for-All)
* [A market-based efficient matching mechanism for crowdsourced delivery systems with demand/supply elasticities](#A-market-based-efficient-matching-mechanism-for-crowdsourced-delivery-systems-with-demand/supply-elasticities)
* [GreenLLM Disaggregating Large Language Model Serving on Heterogeneous GPUs for Lower Carbon Emissions](#GreenLLM-Disaggregating-Large-Language-Model-Serving-on-Heterogeneous-GPUs-for-Lower-Carbon-Emissions)
* [TeLU Activation Function for Fast and Stable Deep Learning](#TeLU-Activation-Function-for-Fast-and-Stable-Deep-Learning)
* [IMSSA Deploying modern state-space models on memristive in-memory compute hardware](#IMSSA-Deploying-modern-state-space-models-on-memristive-in-memory-compute-hardware)
* [Pushing the Envelope of Low-Bit LLM via Dynamic Error Compensation](#Pushing-the-Envelope-of-Low-Bit-LLM-via-Dynamic-Error-Compensation)
* [LoL-PIM Long-Context LLM Decoding with Scalable DRAM-PIM System](#LoL-PIM-Long-Context-LLM-Decoding-with-Scalable-DRAM-PIM-System)
* [Topic-Aware Knowledge Graph with Large Language Models for Interoperability in Recommender Systems](#Topic-Aware-Knowledge-Graph-with-Large-Language-Models-for-Interoperability-in-Recommender-Systems)
* [ST$^3$ Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming](#ST$^3$-Accelerating-Multimodal-Large-Language-Model-by-Spatial-Temporal-Visual-Token-Trimming)
* [On Random Sampling of Diffused Graph Signals with Sparse Inputs on Vertex Domain](#On-Random-Sampling-of-Diffused-Graph-Signals-with-Sparse-Inputs-on-Vertex-Domain)
* [From Generalist to Specialist A Survey of Large Language Models for Chemistry](#From-Generalist-to-Specialist-A-Survey-of-Large-Language-Models-for-Chemistry)
* [Linear Shrinkage Convexification of Penalized Linear Regression With Missing Data](#Linear-Shrinkage-Convexification-of-Penalized-Linear-Regression-With-Missing-Data)
* [Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach](#Data-driven-tool-wear-prediction-in-milling,-based-on-a-process-integrated-single-sensor-approach)
* [HADES Hardware Accelerated Decoding for Efficient Speculation in Large Language Models](#HADES-Hardware-Accelerated-Decoding-for-Efficient-Speculation-in-Large-Language-Models)
* [Re-parameterization Invariance of FRW Model Supervariable and BRST Approaches](#Re-parameterization-Invariance-of-FRW-Model-Supervariable-and-BRST-Approaches)
* [IMTP Search-based Code Generation for In-memory Tensor Programs](#IMTP-Search-based-Code-Generation-for-In-memory-Tensor-Programs)
* [Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales](#Data-Free-Group-Wise-Fully-Quantized-Winograd-Convolution-via-Learnable-Scales)
* [MBQ Modality-Balanced Quantization for Large Vision-Language Models](#MBQ-Modality-Balanced-Quantization-for-Large-Vision-Language-Models)
* [A Survey on Large Language Model Acceleration based on KV Cache Management](#A-Survey-on-Large-Language-Model-Acceleration-based-on-KV-Cache-Management)
* [Revisiting PCA for time series reduction in temporal dimension](#Revisiting-PCA-for-time-series-reduction-in-temporal-dimension)
* [Multi-scale Latent Point Consistency Models for 3D Shape Generation](#Multi-scale-Latent-Point-Consistency-Models-for-3D-Shape-Generation)


## Sparse chaos in cortical circuits

>Authors: Rainer Engelken, Michael Monteforte, Fred Wolf

>2024-12-30

> http://arxiv.org/abs/2412.21188v1

Nerve impulses, the currency of information flow in the brain, are generated
by an instability of the neuronal membrane potential dynamics. Neuronal
circuits exhibit collective chaos that appears essential for learning, memory,
sensory processing, and motor control. However, the factors controlling the
nature and intensity of collective chaos in neuronal circuits are not well
understood. Here we use computational ergodic theory to demonstrate that basic
features of nerve impulse generation profoundly affect collective chaos in
neuronal circuits. Numerically exact calculations of Lyapunov spectra,
Kolmogorov-Sinai-entropy, and upper and lower bounds on attractor dimension
show that changes in nerve impulse generation in individual neurons moderately
impact information encoding rates but qualitatively transform phase space
structure. Specifically, we find a drastic reduction in the number of unstable
manifolds, Kolmogorov-Sinai entropy, and attractor dimension. Beyond a critical
point, marked by the simultaneous breakdown of the diffusion approximation, a
peak in the largest Lyapunov exponent, and a localization transition of the
leading covariant Lyapunov vector, networks exhibit **sparse** chaos: prolonged
periods of near stable dynamics interrupted by short bursts of intense chaos.
Analysis of large, more realistically structured networks supports the
generality of these findings. In cortical circuits, biophysical properties
appear tuned to this regime of **sparse** chaos. Our results reveal a close link
between fundamental aspects of single-neuron biophysics and the collective
dynamics of cortical circuits, suggesting that nerve impulse generation
mechanisms are adapted to enhance circuit controllability and information flow.


## Facilitating large language model Russian adaptation with Learned Embedding Propagation

>Authors: Mikhail Tikhomirov, Daniil Chernyshev

>2024-12-30

> http://arxiv.org/abs/2412.21140v1

Rapid advancements of large language model (LLM) technologies led to the
introduction of powerful open-source instruction-tuned LLMs that have the same
text generation quality as the state-of-the-art counterparts such as GPT-4.
While the emergence of such models accelerates the adoption of LLM technologies
in sensitive-information environments the authors of such models don not
disclose the training data necessary for replication of the results thus making
the achievements model-exclusive. Since those open-source models are also
multilingual this in turn reduces the benefits of training a language specific
LLMs as improved inference computation efficiency becomes the only guaranteed
advantage of such costly procedure. More cost-efficient options such as
vocabulary extension and subsequent continued pre-training are also inhibited
by the lack of access to high-quality instruction-tuning data since it is the
major factor behind the resulting LLM task-solving capabilities. To address the
limitations and cut the costs of the language adaptation pipeline we propose
Learned Embedding Propagation (LEP). Unlike existing approaches our method has
lower training data size requirements due to minimal impact on existing LLM
knowledge which we reinforce using novel ad-hoc embedding propagation procedure
that allows to skip the instruction-tuning step and instead implant the new
language knowledge directly into any existing instruct-tuned variant. We
evaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,
showing that LEP is competitive with traditional instruction-tuning methods,
achieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with
further improvements via self-calibration and continued tuning enhancing
task-solving capabilities.


## Exploring and Controlling Diversity in LLM-Agent Conversation

>Authors: KuanChao Chu, Yi-Pei Chen, Hideki Nakayama

>2024-12-30

> http://arxiv.org/abs/2412.21102v1

Diversity is a critical aspect of multi-agent communication. In this paper,
we focus on controlling and exploring diversity in the context of open-domain
multi-agent conversations, particularly for world simulation applications. We
propose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts
the content of the utterance generation prompt to control diversity using a
single parameter, lambda. Through extensive experiments, we show that APP
effectively controls the output diversity across models and datasets, with
**pruning** more information leading to more diverse output. We comprehensively
analyze the relationship between prompt content and conversational diversity.
Our findings reveal that information from all components of the prompt
generally constrains the diversity of the output, with the Memory block
exerting the most significant influence. APP is compatible with established
techniques like temperature sampling and top-p sampling, providing a versatile
tool for diversity management. To address the trade-offs of increased
diversity, such as inconsistencies with omitted information, we incorporate a
post-generation correction step, which effectively balances diversity
enhancement with output consistency. Additionally, we examine how prompt
structure, including component order and length, impacts diversity. This study
addresses key questions surrounding diversity in multi-agent world simulation,
offering insights into its control, influencing factors, and associated
trade-offs. Our contributions lay the foundation for systematically engineering
diversity in LLM-based multi-agent collaborations, advancing their
effectiveness in real-world applications.


## QAHAN A Quantum Annealing Hard Attention Network

>Authors: Ren-Xin Zhao

>2024-12-30

> http://arxiv.org/abs/2412.20930v1

Hard Attention Mechanisms (HAMs) effectively filter essential information
discretely and significantly boost the performance of machine learning models
on large datasets. Nevertheless, they confront the challenge of
non-differentiability, which raises the risk of convergence to a local optimum.
Quantum Annealing (QA) is expected to solve the above dilemma. We propose a
Quantum Annealing Hard Attention Mechanism (QAHAM) for faster convergence to
the global optimum without the need to compute gradients by exploiting the
quantum tunneling effect. Based on the above theory, we construct a Quantum
Annealing Hard Attention Network (QAHAN) on D-Wave and Pytorch platforms for
MNIST and CIFAR-10 multi-classification. Experimental results indicate that the
QAHAN converges faster, exhibits smoother accuracy and loss curves, and
demonstrates superior noise robustness compared to two traditional HAMs.
Predictably, our scheme accelerates the convergence between the fields of
quantum algorithms and machine learning, while advancing the field of quantum
machine vision.


## DoTA Weight-Decomposed Tensor Adaptation for Large Language Models

>Authors: Xiaolin Hu, Xiang Cheng, Peiyu Liu, Wei Liu, Jian Luan, Bin Wang, Yong Liu

>2024-12-30

> http://arxiv.org/abs/2412.20891v1

Low-rank adaptation (LoRA) reduces the computational and memory demands of
fine-tuning large language models (LLMs) by approximating updates with low-rank
matrices. However, low-rank approximation in two-dimensional space fails to
capture high-dimensional structures within the target matrix. Recently, tensor
decomposition methods have been explored for fine-tuning LLMs, leveraging their
ability to extract structured information. Yet, these approaches primarily rely
on random initialization, and the impact of initialization on tensor adaptation
remains underexplored. In this paper, we reveal that random initialization
significantly diverges from the validation loss achieved by full fine-tuning.
To address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which
leverages the Matrix Product Operator (MPO) decomposition of pre-trained
weights for effective initialization in fine-tuning LLMs. Additionally, we
introduce QDoTA, a **quantize**d version of DoTA designed for 4-bit **quantization**.
Experiments on commonsense and arithmetic reasoning tasks show that DoTA
outperforms random initialization methods with fewer parameters. QDoTA further
reduces memory consumption and achieves comparable performance to DoTA on
commonsense reasoning tasks. We will release our code to support future
research.


## KeyGS A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences

>Authors: Keng-Wei Chang, Zi-Ming Wang, Shang-Hong Lai

>2024-12-30

> http://arxiv.org/abs/2412.20767v1

Reconstructing high-quality 3D models from **sparse** 2D images has garnered
significant attention in computer vision. Recently, 3D Gaussian Splatting
(3DGS) has gained prominence due to its explicit representation with efficient
training speed and real-time rendering capabilities. However, existing methods
still heavily depend on accurate camera poses for reconstruction. Although some
recent approaches attempt to train 3DGS models without the
Structure-from-Motion (SfM) preprocessing from monocular video datasets, these
methods suffer from prolonged training times, making them impractical for many
applications.
  In this paper, we present an efficient framework that operates without any
depth or matching model. Our approach initially uses SfM to quickly obtain
rough camera poses within seconds, and then refines these poses by leveraging
the dense representation in 3DGS. This framework effectively addresses the
issue of long training times. Additionally, we integrate the densification
process with joint refinement and propose a coarse-to-fine frequency-aware
densification to reconstruct different levels of details. This approach
prevents camera pose estimation from being trapped in local minima or drifting
due to high-frequency signals. Our method significantly reduces training time
from hours to minutes while achieving more accurate novel view synthesis and
camera pose estimation compared to previous methods.


## Align Attention Heads Before Merging Them An Effective Way for Converting MHA to GQA

>Authors: Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin

>2024-12-30

> http://arxiv.org/abs/2412.20677v1

Large language models have been shown to perform well on a variety of natural
language processing problems. However, as the model size and the input
sequence's length increase, the rapid increase of **KV** Cache significantly slows
down inference speed. Therefore GQA model, as an alternative to MHA model, has
been widely introduced into LLMs. In this work, we propose a low-cost method
for **pruning** MHA models into GQA models with any compression ratio of key-value
heads. Our method is based on $\mathit{L_0}$ masks to gradually remove
redundant parameters. In addition, we apply orthogonal transformations to
attention heads without changing the model to increase similarity between
attention heads before **pruning** training, in order to further improve
performance of the model. Our method can be compatible with rotary position
embedding (RoPE), which means the model after training can be fully adapted to
the mainstream standard GQA framework. Experiments demonstrate that our
strategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model
without too much performance degradation, just achieved through supervised
fine-tuning.


## Topological invariant of non-Hermitian space-time modulated photonic crystals

>Authors: Xiaoke Gao, Xiaoyu Zhao, Jiawei Wang, Xikui Ma, Tianyu Dong

>2024-12-30

> http://arxiv.org/abs/2412.20636v1

We propose a medium transformation approach to formulate the adjoint system
of space-time modulated photonic crystals (STMPCs), essential for the
bi-orthogonal Berry connection when calculating the topological invariant. We
show that the non-Abelian Zak phase of STMPCs comprising stacked photonic time
crystals and dielectrics is **quantize**d to 0 or 1 for both the entangled and
isolated bands. We find that the eigenmodes at the center and edge of the
Brillouin zone differ in symmetry for the band with non-trivial Zak phases,
while they share the same symmetry for the trivial Zak phases. In addition,
topological phase transitions owing to band inversion are observed. Moreover, a
generalized Brillouin zone of the non-Hermitian STMPCs is established, which is
identical to the Hermitian counterpart, implicating that the non-Bloch band
theory is not required in this regard. The proposed medium transformation
method may serve as an alternative approach to exploring more intricate
topological phenomena in non-Hermitian systems when incorporating non-Bloch
band theory.


## ReTaKe Reducing Temporal and Knowledge Redundancy for Long Video Understanding

>Authors: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

>2024-12-29

> http://arxiv.org/abs/2412.20504v1

Video Large Language Models (VideoLLMs) have achieved remarkable progress in
video understanding. However, existing VideoLLMs often inherit the limitations
of their backbone LLMs in handling long sequences, leading to challenges for
long video understanding. Common solutions either simply uniformly sample
videos' frames or compress visual tokens, which focus primarily on low-level
temporal visual redundancy, overlooking high-level knowledge redundancy. This
limits the achievable compression rate with minimal loss. To this end. we
introduce a training-free method, $\textbf{ReTaKe}$, containing two novel
modules DPSelect and Pivot**KV**, to jointly model and reduce both temporal visual
redundancy and knowledge redundancy for long video understanding. Specifically,
DPSelect identifies keyframes with local maximum peak distance based on their
visual features, which are closely aligned with human video perception. Pivot**KV**
employs the obtained keyframes as pivots and conducts **KV**-Cache compression for
the non-pivot tokens with low attention scores, which are derived from the
learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and
LVBench, show that ReTaKe can support 4x longer video sequences with minimal
performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,
even surpassing or on par with much larger ones. Our code is available at
https://github.com/SCZwangxiao/video-ReTaKe


## Open-Sora Democratizing Efficient Video Production for All

>Authors: Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, Yang You

>2024-12-29

> http://arxiv.org/abs/2412.20404v1

Vision and language are the two foundational senses for humans, and they
build up our cognitive ability and intelligence. While significant
breakthroughs have been made in AI language ability, artificial visual
intelligence, especially the ability to generate and simulate the world we see,
is far lagging behind. To facilitate the development and accessibility of
artificial visual intelligence, we created Open-Sora, an open-source video
generation model designed to produce high-fidelity video content. Open-Sora
supports a wide spectrum of visual generation tasks, including text-to-image
generation, text-to-video generation, and image-to-video generation. The model
leverages advanced deep learning architectures and training/inference
techniques to enable flexible video synthesis, which could generate video
content of up to 15 seconds, up to 720p resolution, and arbitrary aspect
ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer
(STDiT), an efficient diffusion framework for videos that decouples spatial and
temporal attention. We also introduce a highly compressive 3D autoencoder to
make representations compact and further accelerate training with an ad hoc
training strategy. Through this initiative, we aim to foster innovation,
creativity, and inclusivity within the community of AI content creation. By
embracing the open-source principle, Open-Sora democratizes full access to all
the training/inference/data preparation codes as well as model weights. All
resources are publicly available at: https://github.com/hpcaitech/Open-Sora.


## A market-based efficient matching mechanism for crowdsourced delivery systems with demand/supply elasticities

>Authors: Yuki Oyama, Takashi Akamatsu

>2024-12-29

> http://arxiv.org/abs/2412.20395v1

Crowdsourced delivery (CSD) is an emerging business model that leverages the
underutilized or excess capacity of individual drivers to fulfill delivery
tasks. This paper presents a general formulation of a larege-scale two-sided
CSD matching problem, considering demand/supply elasticity, heterogeneous
preferences of both shippers and drivers, and task-bundling. We propose a set
of methodologies to solve this problem. First, we reveal that the
fluid-particle decomposition approach of Akamatsu and Oyama (2024) can be
extended to our general formulation. This approach decomposes the original
large-scale matching problem into a fluidly-approximated task partition problem
(master problem) and small-scale particle matching problems (sub-problems). We
propose to introduce a truthful auction mechanism to sub-problems, which
enables the observation of privately perceived costs for each shipper/driver.
Furthermore, by finding a theoretical link between auction problems and
parturbed utility theory, we succeed in accurately reflecting the information
collected from auctions to the master problem. This reduces the master problem
to a smooth convex optimization problem, theoretically guaranteeing the
computational efficiency and solution accuracy of the fluid approximation.
Second, we transform the master problem into a traffic assignment problem (TAP)
based on a task-chain network. This transformation overcomes the difficulty in
enumerating task bundles. Finally, we formulate the dual problem of the TAP,
whose decision variable is only a price/reward pattern at market equilibrium,
and develop an efficient accelerated gradient descent method. The numerical
experiments clarify that our approach drastically reduces the computational
cost of the matching problem (~700 times faster than a naive method) without
sacrificing accuracy of the optimal solution (mostly within 0.5% errors).


## GreenLLM Disaggregating Large Language Model Serving on Heterogeneous GPUs for Lower Carbon Emissions

>Authors: Tianyao Shi, Yanran Wu, Sihang Liu, Yi Ding

>2024-12-29

> http://arxiv.org/abs/2412.20322v1

LLMs have been widely adopted across many real-world applications. However,
their widespread use comes with significant environmental costs due to their
high computational intensity and resource demands. Specifically, this has
driven the development of new generations of high-performing GPUs, exacerbating
the problem of electronic waste and accelerating the premature disposal of
devices. To address this problem, this paper focuses on reducing the carbon
emissions of LLM serving by reusing older, low-performing GPUs. We present
GreenLLM, an SLO-aware LLM serving framework designed to minimize carbon
emissions by reusing older GPUs. GreenLLM builds on two identified use cases
that disaggregate specific computations onto older GPUs, reducing carbon
emissions while meeting performance goals. To deepen our understanding of the
potential carbon savings from disaggregation, we also provide a theoretical
analysis of its relationship with carbon intensity and GPU lifetime. Our
evaluations show that GreenLLM reduces carbon emissions by up to 40.6% compared
to running standard LLM serving on new GPU only, meeting latency SLOs for over
90% of requests across various applications, latency requirements, carbon
intensities, and GPU lifetimes.


## TeLU Activation Function for Fast and Stable Deep Learning

>Authors: Alfredo Fernandez, Ankur Mali

>2024-12-28

> http://arxiv.org/abs/2412.20269v2

We propose the Hyperbolic Tangent Exponential Linear Unit (TeLU), a neural
network hidden activation function defined as TeLU(x)=xtanh(exp(x)). TeLU's
design is grounded in the core principles of key activation functions,
achieving strong convergence by closely approximating the identity function in
its active region while effectively mitigating the vanishing gradient problem
in its saturating region. Its simple formulation enhances computational
efficiency, leading to improvements in scalability and convergence speed.
Unlike many modern activation functions, TeLU seamlessly combines the
simplicity and effectiveness of ReLU with the smoothness and analytic
properties essential for learning stability in deep neural networks. TeLU's
ability to mimic the behavior and optimal hyperparameter settings of ReLU,
while introducing the benefits of smoothness and curvature, makes it an ideal
drop-in replacement. Its analytic nature positions TeLU as a powerful universal
approximator, enhancing both robustness and generalization across a multitude
of experiments. We rigorously validate these claims through theoretical
analysis and experimental validation, demonstrating TeLU's performance across
challenging benchmarks; including ResNet18 on ImageNet, Dynamic-Pooling
Transformers on Text8, and Recurrent Neural Networks (RNNs) on the Penn
TreeBank dataset. These results highlight TeLU's potential to set a new
standard in activation functions, driving more efficient and stable learning in
deep neural networks, thereby accelerating scientific discoveries across
various fields.


## IMSSA Deploying modern state-space models on memristive in-memory compute hardware

>Authors: Sebastian Siegel, Ming-Jay Yang, John-Paul Strachan

>2024-12-28

> http://arxiv.org/abs/2412.20215v1

Processing long temporal sequences is a key challenge in deep learning. In
recent years, Transformers have become state-of-the-art for this task, but
suffer from excessive memory requirements due to the need to explicitly store
the sequences. To address this issue, structured state-space sequential (S4)
models recently emerged, offering a fixed memory state while still enabling the
processing of very long sequence contexts. The recurrent linear update of the
state in these models makes them highly efficient on modern graphics processing
units (GPU) by unrolling the recurrence into a convolution. However, this
approach demands significant memory and massively parallel computation, which
is only available on the latest GPUs. In this work, we aim to bring the power
of S4 models to edge hardware by significantly reducing the size and
computational demand of an S4D model through **quantization**-aware training, even
achieving ternary weights for a simple real-world task. To this end, we extend
conventional **quantization**-aware training to tailor it for analog in-memory
compute hardware. We then demonstrate the deployment of recurrent S4D kernels
on memrisitve crossbar arrays, enabling their computation in an in-memory
compute fashion. To our knowledge, this is the first implementation of S4
kernels on in-memory compute hardware.


## Pushing the Envelope of Low-Bit LLM via Dynamic Error Compensation

>Authors: Yeonhong Park, Jake Hyun, Hojoon Kim, Jae W. Lee

>2024-12-28

> http://arxiv.org/abs/2412.20185v1

Quantization of Large Language Models (LLMs) has recently gained popularity,
particularly for on-device settings with limited hardware resources. While
efficient, **quantization** inevitably degrades model quality, especially in
aggressive **low-bit** settings such as 3-bit and 4-bit precision. In this paper,
we propose QDEC, an inference scheme that improves the quality of **low-bit** LLMs
while preserving the key benefits of **quantization**: GPU memory savings and
inference latency reduction. QDEC stores the residual matrix -- the difference
between full-precision and **quantize**d weights -- in CPU, and dynamically fetches
the residuals for only a small portion of the weights. This portion corresponds
to the salient channels, marked by activation outliers, with the fetched
residuals helping to correct **quantization** errors in these channels. Salient
channels are identified dynamically at each decoding step by analyzing the
input activations -- this allows for the adaptation to the dynamic nature of
activation distribution, and thus maximizes the effectiveness of error
compensation. We demonstrate the effectiveness of QDEC by augmenting
state-of-the-art **quantization** methods. For example, QDEC reduces the perplexity
of a 3-bit Llama-3-8B-Instruct model from 10.15 to 9.12 -- outperforming its
3.5-bit counterpart -- while adding less than 0.0003\% to GPU memory usage and
incurring only a 1.7\% inference slowdown on NVIDIA RTX 4050 Mobile GPU. The
code will be publicly available soon.


## LoL-PIM Long-Context LLM Decoding with Scalable DRAM-PIM System

>Authors: Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi

>2024-12-28

> http://arxiv.org/abs/2412.20166v1

The expansion of large language models (LLMs) with hundreds of billions of
parameters presents significant challenges to computational resources,
particularly data movement and memory bandwidth. Long-context LLMs, which
process sequences of tens of thousands of tokens, further increase the demand
on the memory system as the complexity in attention layers and key-value cache
sizes is proportional to the context length. Processing-in-Memory (PIM)
maximizes memory bandwidth by moving compute to the data and can address the
memory bandwidth challenges; however, PIM is not necessarily scalable to
accelerate long-context LLM because of limited per-module memory capacity and
the inflexibility of fixed-functional unit PIM architecture and static memory
management. In this work, we propose LoL-PIM which is a multi-node PIM
architecture that accelerates long context LLM through hardware-software
co-design. In particular, we propose how pipeline parallelism can be exploited
across a multi-PIM module while a direct PIM access (DPA) controller (or DMA
for PIM) is proposed that enables dynamic PIM memory management and results in
efficient PIM utilization across a diverse range of context length. We
developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based
compiler where the software modifications were implemented and evaluated, while
the hardware changes were modeled in the simulator. Our evaluations demonstrate
that LoL-PIM significantly improves throughput and reduces latency for
long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems
(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient
deployment of LLMs in real-world applications.


## Topic-Aware Knowledge Graph with Large Language Models for Interoperability in Recommender Systems

>Authors: Minhye Jeon, Seokho Ahn, Young-Duk Seo

>2024-12-28

> http://arxiv.org/abs/2412.20163v1

The use of knowledge graphs in recommender systems has become one of the
common approaches to addressing data **sparsity** and cold start problems. Recent
advances in large language models (LLMs) offer new possibilities for processing
side and context information within knowledge graphs. However, consistent
integration across various systems remains challenging due to the need for
domain expert intervention and differences in system characteristics. To
address these issues, we propose a consistent approach that extracts both
general and specific topics from both side and context information using LLMs.
First, general topics are iteratively extracted and updated from side
information. Then, specific topics are extracted using context information.
Finally, to address synonymous topics generated during the specific topic
extraction process, a refining algorithm processes and resolves these issues
effectively. This approach allows general topics to capture broad knowledge
across diverse item characteristics, while specific topics emphasize detailed
attributes, providing a more comprehensive understanding of the semantic
features of items and the preferences of users. Experimental results
demonstrate significant improvements in recommendation performance across
diverse knowledge graphs.


## ST$^3$ Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming

>Authors: Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu

>2024-12-28

> http://arxiv.org/abs/2412.20105v1

Multimodal large language models (MLLMs) enhance their perceptual
capabilities by integrating visual and textual information. However, processing
the massive number of visual tokens incurs a significant computational cost.
Existing analysis of the MLLM attention mechanisms remains shallow, leading to
coarse-grain token **pruning** strategies that fail to effectively balance speed
and accuracy. In this paper, we conduct a comprehensive investigation of MLLM
attention mechanisms with LLaVA. We find that numerous visual tokens and
partial attention computations are redundant during the decoding process. Based
on this insight, we propose Spatial-Temporal Visual Token Trimming
($\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without
retraining. $\textbf{ST}^{3}$ consists of two primary components: 1)
Progressive Visual Token Pruning (\textbf{PVTP}), which eliminates inattentive
visual tokens across layers, and 2) Visual Token Annealing (\textbf{VTA}),
which dynamically reduces the number of visual tokens in each layer as the
generated tokens grow. Together, these techniques deliver around
$\mathbf{2\times}$ faster inference with only about $\mathbf{30\%}$ **KV** cache
memory compared to the original LLaVA, while maintaining consistent performance
across various datasets. Crucially, $\textbf{ST}^{3}$ can be seamlessly
integrated into existing pre-trained MLLMs, providing a plug-and-play solution
for efficient inference.


## On Random Sampling of Diffused Graph Signals with Sparse Inputs on Vertex Domain

>Authors: Yingcheng Lai, Li Chai, Jinming Xu

>2024-12-28

> http://arxiv.org/abs/2412.20041v1

The sampling of graph signals has recently drawn much attention due to the
wide applications of graph signal processing. While a lot of efficient methods
and interesting results have been reported to the sampling of band-limited or
smooth graph signals, few research has been devoted to non-smooth graph
signals, especially to **sparse** graph signals, which are also of importance in
many practical applications. This paper addresses the random sampling of
non-smooth graph signals generated by diffusion of **sparse** inputs. We aim to
present a solid theoretical analysis on the random sampling of diffused **sparse**
graph signals, which can be parallel to that of band-limited graph signals, and
thus present a sufficient condition to the number of samples ensuring the
unique recovery for uniform random sampling. Then, we focus on two classes of
widely used binary graph models, and give explicit and tighter estimations on
the sampling numbers ensuring unique recovery. We also propose an adaptive
variable-density sampling strategy to provide a better performance than uniform
random sampling. Finally, simulation experiments are presented to validate the
effectiveness of the theoretical results.


## From Generalist to Specialist A Survey of Large Language Models for Chemistry

>Authors: Yang Han, Ziping Wan, Lu Chen, Kai Yu, Xin Chen

>2024-12-28

> http://arxiv.org/abs/2412.19994v1

Large Language Models (LLMs) have significantly transformed our daily life
and established a new paradigm in natural language processing (NLP). However,
the predominant pretraining of LLMs on extensive web-based texts remains
insufficient for advanced scientific discovery, particularly in chemistry. The
scarcity of specialized chemistry data, coupled with the complexity of
multi-modal data such as 2D graph, 3D structure and spectrum, present distinct
challenges. Although several studies have reviewed Pretrained Language Models
(PLMs) in chemistry, there is a conspicuous absence of a systematic survey
specifically focused on chemistry-oriented LLMs. In this paper, we outline
methodologies for incorporating domain-specific chemistry knowledge and
multi-modal information into LLMs, we also conceptualize chemistry LLMs as
agents using chemistry tools and investigate their potential to accelerate
scientific research. Additionally, we conclude the existing benchmarks to
evaluate chemistry ability of LLMs. Finally, we critically examine the current
challenges and identify promising directions for future research. Through this
comprehensive survey, we aim to assist researchers in staying at the forefront
of developments in chemistry LLMs and to inspire innovative applications in the
field.


## Linear Shrinkage Convexification of Penalized Linear Regression With Missing Data

>Authors: Seongoh Park, Seongjin Lee, Nguyen Thi Hai Yen, Nguyen Phuoc Long, Johan Lim

>2024-12-28

> http://arxiv.org/abs/2412.19963v1

One of the common challenges faced by researchers in recent data analysis is
missing values. In the context of penalized linear regression, which has been
extensively explored over several decades, missing values introduce bias and
yield a non-positive definite covariance matrix of the covariates, rendering
the least square loss function non-convex. In this paper, we propose a novel
procedure called the linear shrinkage positive definite (LPD) modification to
address this issue. The LPD modification aims to modify the covariance matrix
of the covariates in order to ensure consistency and positive definiteness.
Employing the new covariance estimator, we are able to transform the penalized
regression problem into a convex one, thereby facilitating the identification
of **sparse** solutions. Notably, the LPD modification is computationally efficient
and can be expressed analytically. In the presence of missing values, we
establish the selection consistency and prove the convergence rate of the
$\ell_1$-penalized regression estimator with LPD, showing an $\ell_2$-error
convergence rate of square-root of $\log p$ over $n$ by a factor of
$(s_0)^{3/2}$ ($s_0$: the number of non-zero coefficients). To further evaluate
the effectiveness of our approach, we analyze real data from the Genomics of
Drug Sensitivity in Cancer (GDSC) dataset. This dataset provides incomplete
measurements of drug sensitivities of cell lines and their protein expressions.
We conduct a series of penalized linear regression models with each sensitivity
value serving as a response variable and protein expressions as explanatory
variables.


## Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach

>Authors: Eric Hirsch, Christian Friedrich

>2024-12-27

> http://arxiv.org/abs/2412.19950v1

Accurate tool wear prediction is essential for maintaining productivity and
minimizing costs in machining. However, the complex nature of the tool wear
process poses significant challenges to achieving reliable predictions. This
study explores data-driven methods, in particular deep learning, for tool wear
prediction. Traditional data-driven approaches often focus on a single process,
relying on multi-sensor setups and extensive data generation, which limits
generalization to new settings. Moreover, multi-sensor integration is often
impractical in industrial environments. To address these limitations, this
research investigates the transferability of predictive models using minimal
training data, validated across two processes. Furthermore, it uses a simple
setup with a single **acceleration** sensor to establish a low-cost data generation
approach that facilitates the generalization of models to other processes via
transfer learning. The study evaluates several machine learning models,
including convolutional neural networks (CNN), long short-term memory networks
(LSTM), support vector machines (SVM) and decision trees, trained on different
input formats such as feature vectors and short-time Fourier transform (STFT).
The performance of the models is evaluated on different amounts of training
data, including scenarios with significantly reduced datasets, providing
insight into their effectiveness under constrained data conditions. The results
demonstrate the potential of specific models and configurations for effective
tool wear prediction, contributing to the development of more adaptable and
efficient predictive maintenance strategies in machining. Notably, the ConvNeXt
model has an exceptional performance, achieving an 99.1% accuracy in
identifying tool wear using data from only four milling tools operated until
they are worn.


## HADES Hardware Accelerated Decoding for Efficient Speculation in Large Language Models

>Authors: Ze Yang, Yihong Jin, Xinhe Xu

>2024-12-27

> http://arxiv.org/abs/2412.19925v1

Large Language Models (LLMs) have revolutionized natural language processing
by understanding and generating human-like text. However, the increasing demand
for more sophisticated LLMs presents significant computational challenges due
to their scale and complexity. This paper introduces Hardware Accelerated
Decoding (HADES), a novel approach to enhance the performance and energy
efficiency of LLMs. We address the design of an LLM accelerator with
hardware-level speculative decoding support, a concept not previously explored
in existing literature. Our work demonstrates how speculative decoding can
significantly improve the efficiency of LLM operations, paving the way for more
advanced and practical applications of these models.


## Re-parameterization Invariance of FRW Model Supervariable and BRST Approaches

>Authors: B. Chauhan, R. Tripathi

>2024-12-27

> http://arxiv.org/abs/2412.19704v2

We perform the Becchi-Rouet-Stora-Tyutin BRST) **quantization** of a (0 +
1)-dimensional cosmological Friedmann-Robertson-Walker (FRW) model. This
**quantization** leverages the classical infinitesimal and continuous
re-parameterization symmetry transformations of the system. To derive the
nilpotent re-parameterization invariant BRST-anti-BRST symmetry transformations
for the scale factor and corresponding momentum variables present in the
cosmological FRW model, we employ the modified Bonora-Tonin supervariable
approach (MBTSA) to BRST formalism. Through this approach, we also establish
the BRST-anti-BRST invariant Curci-Ferrari (CF)-type restriction for this
cosmological re-parameterization invariant model. Further, we obtain the
off-shell nilpotent quantum BRST-anti-BRST symmetry transformations for other
variables within the model using the (anti-)chiral supervariable approach
(ACSA) to BRST formalism. Within the framework of ACSA, the CF-type restriction
is demonstrated through two key aspects: (i) the invariance of the coupled
Lagrangians under symmetry transformations, and (ii) the absolute
anti-commutativity of the conserved BRST-anti-BRST charges. Notably, applying
the MBTSA to a physical cosmological system, specifically a one-dimensional
one, constitutes a novel contribution to this work. Additionally, in the
application of ACSA, we restrict our analysis to (anti-)chiral super expansions
of supervariables, leading to the unique observation of the absolute
anti-commutativity of the conserved BRST-anti-BRST charges. Moreover, we
highlight that the CF-type restriction demonstrates a universal nature,
remaining consistent across any re-parameterization invariant models in general
D-dimensional spacetime.


## IMTP Search-based Code Generation for In-memory Tensor Programs

>Authors: Yongwon Shin, Dookyung Kang, Hyojin Sung

>2024-12-27

> http://arxiv.org/abs/2412.19630v1

Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for
accelerating memory-intensive operations in modern applications, such as Large
Language Models (LLMs). Despite its potential, current software stacks for
DRAM-PIM face significant challenges, including reliance on hand-tuned
libraries that hinder programmability, limited support for high-level
abstractions, and the lack of systematic optimization frameworks. To address
these limitations, we present IMTP, a search-based optimizing tensor compiler
for UPMEM. Key features of IMTP include: (1) automated searches of the joint
search space for host and kernel tensor programs, (2) PIM-aware optimizations
for efficiently handling boundary conditions, and (3) improved search
algorithms for the expanded search space of UPMEM systems. Our experimental
results on UPMEM hardware demonstrate performance gains of up to 8.21x for
various UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our
knowledge, IMTP is the first tensor compiler to provide fully automated,
autotuning-integrated code generation support for a DRAM-PIM system. By
bridging the gap between high-level tensor computation abstractions and
low-level hardware-specific requirements, IMTP establishes a foundation for
advancing DRAM-PIM programmability and enabling streamlined optimization.


## Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales

>Authors: Shuokai Pan, Gerti Tuzi, Sudarshan Sreeram, Dibakar Gope

>2024-12-27

> http://arxiv.org/abs/2412.19867v1

Despite the revolutionary breakthroughs of large-scale textto-image diffusion
models for complex vision and downstream tasks, their extremely high
computational and storage costs limit their usability. Quantization of
diffusion models has been explored in recent works to reduce compute costs and
memory bandwidth usage. To further improve inference time, fast convolution
algorithms such as Winograd can be used for convolution layers, which account
for a significant portion of computations in diffusion models. However, the
significant quality loss of fully **quantize**d Winograd using existing
coarser-grained post-training **quantization** methods, combined with the
complexity and cost of finetuning the Winograd transformation matrices for such
large models to recover quality, makes them unsuitable for large-scale
foundation models. Motivated by the presence of a large range of values in
them, we investigate the impact of finer-grained group-wise **quantization** in
quantizing diffusion models. While group-wise **quantization** can largely handle
the fully **quantize**d Winograd convolution, it struggles to deal with the large
distribution imbalance in a sizable portion of the Winograd domain computation.
To reduce range differences in the Winograd domain, we propose finetuning only
the scale parameters of the Winograd transform matrices without using any
domain-specific training data. Because our method does not depend on any
training data, the generalization performance of **quantize**d diffusion models is
safely guaranteed. For text-to-image generation task, the 8-bit fully-**quantize**d
diffusion model with Winograd provides near-lossless quality (FID and CLIP
scores) in comparison to the full-precision model. For image classification,
our method outperforms the state-of-the-art Winograd PTQ method by 1.62% and
2.56% in top-1 ImageNet accuracy on ResNet18 and ResNet-34, respectively, with
Winograd F(6, 3).


## MBQ Modality-Balanced Quantization for Large Vision-Language Models

>Authors: Shiyao Li, Yingchun Hu, Xuefei Ning, Xihui Liu, Ke Hong, Xiaotao Jia, Xiuhong Li, Yaqi Yan, Pei Ran, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang

>2024-12-27

> http://arxiv.org/abs/2412.19509v1

Vision-Language Models (VLMs) have enabled a variety of real-world
applications. The large parameter size of VLMs brings large memory and
computation overhead which poses significant challenges for deployment.
Post-Training Quantization (PTQ) is an effective technique to reduce the memory
and computation overhead. Existing PTQ methods mainly focus on large language
models (LLMs), without considering the differences across other modalities. In
this paper, we discover that there is a significant difference in sensitivity
between language and vision tokens in large VLMs. Therefore, treating tokens
from different modalities equally, as in existing PTQ methods, may
over-emphasize the insensitive modalities, leading to significant accuracy
loss. To deal with the above issue, we propose a simple yet effective method,
Modality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ
incorporates the different sensitivities across modalities during the
calibration process to minimize the reconstruction loss for better **quantization**
parameters. Extensive experiments show that MBQ can significantly improve task
accuracy by up to 4.4% and 11.6% under W3 and W4A8 **quantization** for 7B to 70B
VLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel
that fuses the de**quantization** and GEMV operators, achieving a 1.4x speedup on
LLaVA-onevision-7B on the RTX 4090. The code is available at
https://github.com/thu-nics/MBQ.


## A Survey on Large Language Model Acceleration based on KV Cache Management

>Authors: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen

>2024-12-27

> http://arxiv.org/abs/2412.19442v2

Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (**KV**) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of **KV** cache management strategies for LLM **acceleration**,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include **KV** cache selection, budget
allocation, merging, **quantization**, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance **KV** reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable **KV** cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for **KV** cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-**KV**-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-**KV**-Cache-Management}.


## Revisiting PCA for time series reduction in temporal dimension

>Authors: Jiaxin Gao, Wenbo Hu, Yuntian Chen

>2024-12-27

> http://arxiv.org/abs/2412.19423v1

Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.


## Multi-scale Latent Point Consistency Models for 3D Shape Generation

>Authors: Bi'an Du, Wei Hu, Renjie Liao

>2024-12-27

> http://arxiv.org/abs/2412.19413v1

Consistency Models (CMs) have significantly accelerated the sampling process
in diffusion models, yielding impressive results in synthesizing
high-resolution images. To explore and extend these advancements to
point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent
Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework
and introduces hierarchical levels of latent representations, ranging from
point-level to super-point levels, each corresponding to a different spatial
resolution. We design a multi-scale latent integration module along with 3D
spatial attention to effectively denoise the point-level latent representations
conditioned on those from multiple super-point levels. Additionally, we propose
a latent consistency model, learned through consistency distillation, that
compresses the prior into a one-step generator. This significantly improves
sampling efficiency while preserving the performance of the original teacher
model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol
demonstrate that MLPCM achieves a 100x speedup in the generation process, while
surpassing state-of-the-art diffusion models in terms of both shape quality and
diversity.

