# 2024-12-27

# Table of Contents
* [Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models](#Research-on-the-Proximity-Relationships-of-Psychosomatic-Disease-Knowledge-Graph-Modules-Extracted-by-Large-Language-Models)
* [RDPM Solve Diffusion Probabilistic Models via Recurrent Token Prediction](#RDPM-Solve-Diffusion-Probabilistic-Models-via-Recurrent-Token-Prediction)
* [Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization](#Improving-Multi-Step-Reasoning-Abilities-of-Large-Language-Models-with-Direct-Advantage-Policy-Optimization)
* [An Automatic Graph Construction Framework based on Large Language Models for Recommendation](#An-Automatic-Graph-Construction-Framework-based-on-Large-Language-Models-for-Recommendation)
* [Efficient Detection Framework Adaptation for Edge Computing A Plug-and-play Neural Network Toolbox Enabling Edge Deployment](#Efficient-Detection-Framework-Adaptation-for-Edge-Computing-A-Plug-and-play-Neural-Network-Toolbox-Enabling-Edge-Deployment)
* [GIMS Image Matching System Based on Adaptive Graph Construction and Graph Neural Network](#GIMS-Image-Matching-System-Based-on-Adaptive-Graph-Construction-and-Graph-Neural-Network)
* [LSAQ Layer-Specific Adaptive Quantization for Large Language Model Deployment](#LSAQ-Layer-Specific-Adaptive-Quantization-for-Large-Language-Model-Deployment)
* [SAR Despeckling via Log-Yeo-Johnson Transformation and Sparse Representation](#SAR-Despeckling-via-Log-Yeo-Johnson-Transformation-and-Sparse-Representation)
* [SlimGPT Layer-wise Structured Pruning for Large Language Models](#SlimGPT-Layer-wise-Structured-Pruning-for-Large-Language-Models)
* [Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels](#Tackling-the-Dynamicity-in-a-Production-LLM-Serving-System-with-SOTA-Optimizations-via-Hybrid-Prefill/Decode/Verify-Scheduling-on-Efficient-Meta-kernels)
* [LangYa Revolutionizing Cross-Spatiotemporal Ocean Forecasting](#LangYa-Revolutionizing-Cross-Spatiotemporal-Ocean-Forecasting)
* [AutoSculpt A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning](#AutoSculpt-A-Pattern-based-Model-Auto-pruning-Framework-Using-Reinforcement-Learning-and-Graph-Learning)
* [LayerDropBack A Universally Applicable Approach for Accelerating Training of Deep Networks](#LayerDropBack-A-Universally-Applicable-Approach-for-Accelerating-Training-of-Deep-Networks)
* [LLM-Driven Feedback for Enhancing Conceptual Design Learning in Database Systems Courses](#LLM-Driven-Feedback-for-Enhancing-Conceptual-Design-Learning-in-Database-Systems-Courses)
* [Deliberation in Latent Space via Differentiable Cache Augmentation](#Deliberation-in-Latent-Space-via-Differentiable-Cache-Augmentation)
* [LASE Learned Adjacency Spectral Embeddings](#LASE-Learned-Adjacency-Spectral-Embeddings)
* [A Reproducible Method for Mapping Electricity Transmission Infrastructure for Space Weather Risk Assessment](#A-Reproducible-Method-for-Mapping-Electricity-Transmission-Infrastructure-for-Space-Weather-Risk-Assessment)
* [Tracking the Feature Dynamics in LLM Training A Mechanistic Study](#Tracking-the-Feature-Dynamics-in-LLM-Training-A-Mechanistic-Study)
* [Emerging Security Challenges of Large Language Models](#Emerging-Security-Challenges-of-Large-Language-Models)
* [URoadNet Dual Sparse Attentive U-Net for Multiscale Road Network Extraction](#URoadNet-Dual-Sparse-Attentive-U-Net-for-Multiscale-Road-Network-Extraction)
* [GQSA Group Quantization and Sparsity for Accelerating Large Language Model Inference](#GQSA-Group-Quantization-and-Sparsity-for-Accelerating-Large-Language-Model-Inference)
* [CALLIC Content Adaptive Learning for Lossless Image Compression](#CALLIC-Content-Adaptive-Learning-for-Lossless-Image-Compression)
* [Learning Dynamic Local Context Representations for Infrared Small Target Detection](#Learning-Dynamic-Local-Context-Representations-for-Infrared-Small-Target-Detection)
* [ORIGAMI A generative transformer architecture for predictions from semi-structured data](#ORIGAMI-A-generative-transformer-architecture-for-predictions-from-semi-structured-data)
* [Understanding Dynamics in Coarse-Grained Models V. Extension of Coarse-Grained Dynamics Theory to Non-Hard Sphere Systems](#Understanding-Dynamics-in-Coarse-Grained-Models-V.-Extension-of-Coarse-Grained-Dynamics-Theory-to-Non-Hard-Sphere-Systems)
* [Half-form quantization of mixed toric polarizations and Hamiltonian flows in imaginary-time](#Half-form-quantization-of-mixed-toric-polarizations-and-Hamiltonian-flows-in-imaginary-time)
* [LLM Agent for Fire Dynamics Simulations](#LLM-Agent-for-Fire-Dynamics-Simulations)
* [Transformer-Based Model Predictive Path Integral Control](#Transformer-Based-Model-Predictive-Path-Integral-Control)
* [The HalluRAG Dataset Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM's Internal States](#The-HalluRAG-Dataset-Detecting-Closed-Domain-Hallucinations-in-RAG-Applications-Using-an-LLM's-Internal-States)
* [LLM-Powered User Simulator for Recommender System](#LLM-Powered-User-Simulator-for-Recommender-System)
* [A tensor network formulation of Lattice Gauge Theories based only on symmetric tensors](#A-tensor-network-formulation-of-Lattice-Gauge-Theories-based-only-on-symmetric-tensors)
* [TAR3D Creating High-Quality 3D Assets via Next-Part Prediction](#TAR3D-Creating-High-Quality-3D-Assets-via-Next-Part-Prediction)
* [GeoTexDensifier Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting](#GeoTexDensifier-Geometry-Texture-Aware-Densification-for-High-Quality-Photorealistic-3D-Gaussian-Splatting)
* [Large Language Models Compression via Low-Rank Feature Distillation](#Large-Language-Models-Compression-via-Low-Rank-Feature-Distillation)
* [Adaptive Elastic-Net estimation for sparse diffusion processes](#Adaptive-Elastic-Net-estimation-for-sparse-diffusion-processes)
* [V"Mean"ba Visual State Space Models only need 1 hidden dimension](#V"Mean"ba-Visual-State-Space-Models-only-need-1-hidden-dimension)
* [AIGCodeSet A New Annotated Dataset for AI Generated Code Detection](#AIGCodeSet-A-New-Annotated-Dataset-for-AI-Generated-Code-Detection)
* [Semantics Prompting Data-Free Quantization for Low-Bit Vision Transformers](#Semantics-Prompting-Data-Free-Quantization-for-Low-Bit-Vision-Transformers)
* [Accelerating Private Large Transformers Inference through Fine-grained Collaborative Computation](#Accelerating-Private-Large-Transformers-Inference-through-Fine-grained-Collaborative-Computation)
* [STKDRec Spatial-Temporal Knowledge Distillation for Takeaway Recommendation](#STKDRec-Spatial-Temporal-Knowledge-Distillation-for-Takeaway-Recommendation)
* [ImagePiece Content-aware Re-tokenization for Efficient Image Recognition](#ImagePiece-Content-aware-Re-tokenization-for-Efficient-Image-Recognition)
* [When Can Proxies Improve the Sample Complexity of Preference Learning?](#When-Can-Proxies-Improve-the-Sample-Complexity-of-Preference-Learning?)
* [Transducer-Llama Integrating LLMs into Streamable Transducer-based Speech Recognition](#Transducer-Llama-Integrating-LLMs-into-Streamable-Transducer-based-Speech-Recognition)
* [DFModel Design Space Optimization of Large-Scale Systems Exploiting Dataflow Mappings](#DFModel-Design-Space-Optimization-of-Large-Scale-Systems-Exploiting-Dataflow-Mappings)
* [Technical Report Small Language Model for Japanese Clinical and Medicine](#Technical-Report-Small-Language-Model-for-Japanese-Clinical-and-Medicine)
* [Analytical insights from a model of opinion formation based on Persuasive Argument Theory](#Analytical-insights-from-a-model-of-opinion-formation-based-on-Persuasive-Argument-Theory)
* [Offline Reinforcement Learning for LLM Multi-Step Reasoning](#Offline-Reinforcement-Learning-for-LLM-Multi-Step-Reasoning)
* [PruneVid Visual Token Pruning for Efficient Video Large Language Models](#PruneVid-Visual-Token-Pruning-for-Efficient-Video-Large-Language-Models)
* [CLEAR Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up](#CLEAR-Conv-Like-Linearization-Revs-Pre-Trained-Diffusion-Transformers-Up)
* [MetaScientist A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design](#MetaScientist-A-Human-AI-Synergistic-Framework-for-Automated-Mechanical-Metamaterial-Design)
* [Extraordinary oxidation behavior of W-Zr thin-film metallic glasses A route for tailoring functional properties of W-Zr-O films](#Extraordinary-oxidation-behavior-of-W-Zr-thin-film-metallic-glasses-A-route-for-tailoring-functional-properties-of-W-Zr-O-films)
* [Less is More Towards Green Code Large Language Models via Unified Structural Pruning](#Less-is-More-Towards-Green-Code-Large-Language-Models-via-Unified-Structural-Pruning)
* [WebLLM A High-Performance In-Browser LLM Inference Engine](#WebLLM-A-High-Performance-In-Browser-LLM-Inference-Engine)
* [Critique of Impure Reason Unveiling the reasoning behaviour of medical Large Language Models](#Critique-of-Impure-Reason-Unveiling-the-reasoning-behaviour-of-medical-Large-Language-Models)
* [Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking](#Exploiting-Multimodal-Spatial-temporal-Patterns-for-Video-Object-Tracking)
* [High-dimensional sliced inverse regression with endogeneity](#High-dimensional-sliced-inverse-regression-with-endogeneity)
* [SGTC Semantic-Guided Triplet Co-training for Sparsely Annotated Semi-Supervised Medical Image Segmentation](#SGTC-Semantic-Guided-Triplet-Co-training-for-Sparsely-Annotated-Semi-Supervised-Medical-Image-Segmentation)
* [PreNeT Leveraging Computational Features to Predict Deep Neural Network Training Time](#PreNeT-Leveraging-Computational-Features-to-Predict-Deep-Neural-Network-Training-Time)


## Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models

>Authors: Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen

>2024-12-24

> http://arxiv.org/abs/2412.18419v1

As social changes accelerate, the incidence of psychosomatic disorders has
significantly increased, becoming a major challenge in global health issues.
This necessitates an innovative knowledge system and analytical methods to aid
in diagnosis and treatment. Here, we establish the ontology model and entity
types, using the BERT model and LoRA-tuned LLM for named entity recognition,
constructing the knowledge graph with 9668 triples. Next, by analyzing the
network distances between disease, symptom, and drug modules, it was found that
closer network distances among diseases can predict greater similarities in
their clinical manifestations, treatment approaches, and psychological
mechanisms, and closer distances between symptoms indicate that they are more
likely to co-occur. Lastly, by comparing the proximity d and proximity z score,
it was shown that symptom-disease pairs in primary diagnostic relationships
have a stronger association and are of higher referential value than those in
diagnostic relationships. The research results revealed the potential
connections between diseases, co-occurring symptoms, and similarities in
treatment strategies, providing new perspectives for the diagnosis and
treatment of psychosomatic disorders and valuable information for future mental
health research and practice.


## RDPM Solve Diffusion Probabilistic Models via Recurrent Token Prediction

>Authors: Wu Xiaoping, Hu Jie, Wei Xiaoming

>2024-12-24

> http://arxiv.org/abs/2412.18390v1

Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach
for high-fidelity image synthesis, operating diffusion processes on continuous
VAE latent, which significantly differ from the text generation methods
employed by Large Language Models (LLMs). In this paper, we introduce a novel
generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which
enhances the diffusion process through a recurrent token prediction mechanism,
thereby pioneering the field of Discrete Diffusion. By progressively
introducing Gaussian noise into the latent representations of images and
encoding them into vector-**quantize**d tokens in a recurrent manner, RDPM
facilitates a unique diffusion process on discrete-value domains. This process
iteratively predicts the token codes for subsequent timesteps, transforming the
initial standard Gaussian noise into the source data distribution, aligning
with GPT-style models in terms of the loss function. RDPM demonstrates superior
performance while benefiting from the speed advantage of requiring only a few
inference steps. This model not only leverages the diffusion process to ensure
high-quality generation but also converts continuous signals into a series of
high-fidelity discrete tokens, thereby maintaining a unified optimization
strategy with other discrete tokens, such as text. We anticipate that this work
will contribute to the development of a unified model for multimodal
generation, specifically by integrating continuous signal domains such as
images, videos, and audio with text. We will release the code and model weights
to the open-source community.


## Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization

>Authors: Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou

>2024-12-24

> http://arxiv.org/abs/2412.18279v1

The role of reinforcement learning (RL) in enhancing the reasoning of large
language models (LLMs) is becoming increasingly significant. Despite the
success of RL in many scenarios, there are still many challenges in improving
the reasoning of LLMs. One challenge is the **sparse** reward, which makes
optimization difficult for RL and necessitates a large amount of data samples.
Another challenge stems from the inherent instability of RL, particularly when
using Actor-Critic (AC) methods to derive optimal policies, which often leads
to unstable training processes. To address these issues, we introduce Direct
Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.
Unlike standard alignment that rely solely outcome rewards to optimize policies
(such as DPO), DAPO employs a critic function to predict the reasoning accuracy
at each step, thereby generating dense signals to refine the generation
strategy. Additionally, the Actor and Critic components in DAPO are trained
independently, avoiding the co-training instability observed in standard AC
algorithms like PPO. We train DAPO on mathematical and code query datasets and
then evaluate its performance on multiple benchmarks. Our results show that
DAPO can effectively enhance the mathematical and code capabilities on both SFT
models and RL models, demonstrating the effectiveness of DAPO.


## An Automatic Graph Construction Framework based on Large Language Models for Recommendation

>Authors: Rong Shan, Jianghao Lin, Chenxu Zhu, Bo Chen, Menghui Zhu, Kangning Zhang, Jieming Zhu, Ruiming Tang, Yong Yu, Weinan Zhang

>2024-12-24

> http://arxiv.org/abs/2412.18241v1

Graph neural networks (GNNs) have emerged as state-of-the-art methods to
learn from graph-structured data for recommendation. However, most existing
GNN-based recommendation methods focus on the optimization of model structures
and learning strategies based on pre-defined graphs, neglecting the importance
of the graph construction stage. Earlier works for graph construction usually
rely on speciffic rules or crowdsourcing, which are either too simplistic or
too labor-intensive. Recent works start to utilize large language models (LLMs)
to automate the graph construction, in view of their abundant open-world
knowledge and remarkable reasoning capabilities. Nevertheless, they generally
suffer from two limitations: (1) invisibility of global view (e.g., overlooking
contextual information) and (2) construction inefficiency. To this end, we
introduce AutoGraph, an automatic graph construction framework based on LLMs
for recommendation. Specifically, we first use LLMs to infer the user
preference and item knowledge, which is encoded as semantic vectors. Next, we
employ vector **quantization** to extract the latent factors from the semantic
vectors. The latent factors are then incorporated as extra nodes to link the
user/item nodes, resulting in a graph with in-depth global-view semantics. We
further design metapath-based message aggregation to effectively aggregate the
semantic and collaborative information. The framework is model-agnostic and
compatible with different backbone models. Extensive experiments on three
real-world datasets demonstrate the efficacy and efffciency of AutoGraph
compared to existing baseline methods. We have deployed AutoGraph in Huawei
advertising platform, and gain a 2.69% improvement on RPM and a 7.31%
improvement on eCPM in the online A/B test. Currently AutoGraph has been used
as the main trafffc model, serving hundreds of millions of people.


## Efficient Detection Framework Adaptation for Edge Computing A Plug-and-play Neural Network Toolbox Enabling Edge Deployment

>Authors: Jiaqi Wu, Shihao Zhang, Simin Chen, Lixu Wang, Zehua Wang, Wei Chen, Fangyuan He, Zijian Tian, F. Richard Yu, Victor C. M. Leung

>2024-12-24

> http://arxiv.org/abs/2412.18230v1

Edge computing has emerged as a key paradigm for deploying deep
learning-based object detection in time-sensitive scenarios. However, existing
edge detection methods face challenges: 1) difficulty balancing detection
precision with lightweight models, 2) limited adaptability of generalized
deployment designs, and 3) insufficient real-world validation. To address these
issues, we propose the Edge Detection Toolbox (ED-TOOLBOX), which utilizes
generalizable plug-and-play components to adapt object detection models for
edge environments. Specifically, we introduce a lightweight Reparameterized
Dynamic Convolutional Network (Rep-DConvNet) featuring weighted multi-shape
convolutional branches to enhance detection performance. Additionally, we
design a Sparse Cross-Attention (SC-A) network with a
localized-mapping-assisted self-attention mechanism, enabling a well-crafted
joint module for adaptive feature transfer. For real-world applications, we
incorporate an Efficient Head into the YOLO framework to accelerate edge model
optimization. To demonstrate practical impact, we identify a gap in helmet
detection -- overlooking band fastening, a critical safety factor -- and create
the Helmet Band Detection Dataset (HBDD). Using ED-TOOLBOX-optimized models, we
address this real-world task. Extensive experiments validate the effectiveness
of ED-TOOLBOX, with edge detection models outperforming six state-of-the-art
methods in visual surveillance simulations, achieving real-time and accurate
performance. These results highlight ED-TOOLBOX as a superior solution for edge
object detection.


## GIMS Image Matching System Based on Adaptive Graph Construction and Graph Neural Network

>Authors: Xianfeng Song, Yi Zou, Zheng Shi, Zheng Liu

>2024-12-24

> http://arxiv.org/abs/2412.18221v1

Feature-based image matching has extensive applications in computer vision.
Keypoints detected in images can be naturally represented as graph structures,
and Graph Neural Networks (GNNs) have been shown to outperform traditional deep
learning techniques. Consequently, the paradigm of image matching via GNNs has
gained significant prominence in recent academic research. In this paper, we
first introduce an innovative adaptive graph construction method that utilizes
a filtering mechanism based on distance and dynamic threshold similarity. This
method dynamically adjusts the criteria for incorporating new vertices based on
the characteristics of existing vertices, allowing for the construction of more
precise and robust graph structures while avoiding redundancy. We further
combine the vertex processing capabilities of GNNs with the global awareness
capabilities of Transformers to enhance the model's representation of spatial
and feature information within graph structures. This hybrid model provides a
deeper understanding of the interrelationships between vertices and their
contributions to the matching process. Additionally, we employ the Sinkhorn
algorithm to iteratively solve for optimal matching results. Finally, we
validate our system using extensive image datasets and conduct comprehensive
comparative experiments. Experimental results demonstrate that our system
achieves an average improvement of 3.8x-40.3x in overall matching performance.
Additionally, the number of vertices and edges significantly impacts training
efficiency and memory usage; therefore, we employ multi-GPU technology to
accelerate the training process. Our code is available at
https://github.com/songxf1024/GIMS.


## LSAQ Layer-Specific Adaptive Quantization for Large Language Model Deployment

>Authors: Binrui Zeng, Bin Ji, Xiaodong Liu, Jie Yu, Shasha Li, Jun Ma, Xiaopeng Li, Shangwen Wang, Xinran Hong

>2024-12-24

> http://arxiv.org/abs/2412.18135v1

As large language models (LLMs) demonstrate exceptional performance across
various domains, the deployment of these models on edge devices has emerged as
a new trend. Quantization techniques, which reduce the size and memory
footprint of LLMs, are effective for enabling deployment on
resource-constrained edge devices. However, existing one-size-fits-all
**quantization** methods often fail to dynamically adjust the memory consumption of
LLMs based on specific hardware characteristics and usage scenarios. To address
this limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a
system for adaptive **quantization** and dynamic deployment of LLMs based on layer
importance. LSAQ evaluates layer importance by constructing top-k token sets
from the inputs and outputs of each layer and calculating their Jaccard
coefficient. Using this evaluation, the system adaptively adjusts **quantization**
strategies in real time according to the resource availability of edge devices,
assigning different precision levels to layers of varying importance. This
approach significantly reduces the storage requirements of LLMs while
maintaining model performance, enabling efficient deployment across diverse
hardware platforms and usage scenarios.


## SAR Despeckling via Log-Yeo-Johnson Transformation and Sparse Representation

>Authors: Xuran Hu, Mingzhe Zhu, Djordje Stanković, Zhenpeng Feng, Shouhan Mao, Ljubiša Stanković

>2024-12-24

> http://arxiv.org/abs/2412.18121v1

Synthetic Aperture Radar (SAR) images are widely used in remote sensing due
to their all-weather, all-day imaging capabilities. However, SAR images are
highly susceptible to noise, particularly speckle noise, caused by the coherent
imaging process, which severely degrades image quality. This has driven
increasing research interest in SAR despeckling. Sparse representation-based
denoising has been extensively applied in natural image processing, yet SAR
despeckling requires addressing non-Gaussian noise and ensuring **sparsity** in the
transform domain. In this work, we propose an innovative SAR despeckling
approach grounded in compressive sensing theory. By applying the
Log-Yeo-Johnson transformation, we convert gamma-distributed noise into an
approximate Gaussian distribution, facilitating **sparse** representation. The
method incorporates noise and **sparsity** priors, leveraging a non-local **sparse**
representation through auxiliary matrices: one capturing varying noise
characteristics across regions and the other encoding adaptive **sparsity**
information.


## SlimGPT Layer-wise Structured Pruning for Large Language Models

>Authors: Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu

>2024-12-24

> http://arxiv.org/abs/2412.18110v1

Large language models (LLMs) have garnered significant attention for their
remarkable capabilities across various domains, whose vast parameter scales
present challenges for practical deployment. Structured **pruning** is an effective
method to balance model performance with efficiency, but performance
restoration under computational resource constraints is a principal challenge
in **pruning** LLMs. Therefore, we present a low-cost and fast structured **pruning**
method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We
propose Batched Greedy Pruning for rapid and near-optimal **pruning**, which
enhances the accuracy of head-wise **pruning** error estimation through grouped
Cholesky decomposition and improves the **pruning** efficiency of FFN via Dynamic
Group Size, thereby achieving approximate local optimal **pruning** results within
one hour. Besides, we explore the limitations of layer-wise **pruning** from the
perspective of error accumulation and propose Incremental Pruning Ratio, a
non-uniform **pruning** strategy to reduce performance degradation. Experimental
results on the LLaMA benchmark show that SlimGPT outperforms other methods and
achieves state-of-the-art results.


## Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels

>Authors: Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long

>2024-12-24

> http://arxiv.org/abs/2412.18106v1

Meeting growing demands for low latency and cost efficiency in
production-grade large language model (LLM) serving systems requires
integrating advanced optimization techniques. However, dynamic and
unpredictable input-output lengths of LLM, compounded by these optimizations,
exacerbate the issues of workload variability, making it difficult to maintain
high efficiency on AI accelerators, especially DSAs with tile-based programming
models. To address this challenge, we introduce XY-Serve, a versatile, Ascend
native, end-to-end production LLM-serving system. The core idea is an
abstraction mechanism that smooths out the workload variability by decomposing
computations into unified, hardware-friendly, fine-grained meta primitives. For
attention, we propose a meta-kernel that computes the basic pattern of
matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we
introduce a virtual padding scheme that adapts to dynamic shape changes while
using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve
sits harmoniously with vLLM. Experimental results show up to 89% end-to-end
throughput improvement compared with current publicly available baselines on
Ascend NPUs. Additionally, our approach outperforms existing GEMM (average
14.6% faster) and attention (average 21.5% faster) kernels relative to existing
libraries. While the work is Ascend native, we believe the approach can be
readily applicable to SIMT architectures as well.


## LangYa Revolutionizing Cross-Spatiotemporal Ocean Forecasting

>Authors: Nan Yang, Chong Wang, Meihua Zhao, Zimeng Zhao, Huiling Zheng, Bin Zhang, Jianing Wang, Xiaofeng Li

>2024-12-24

> http://arxiv.org/abs/2412.18097v1

Ocean forecasting is crucial for both scientific research and societal
benefits. Currently, the most accurate forecasting systems are global ocean
forecasting systems (GOFSs), which represent the ocean state variables (OSVs)
as discrete grids and solve partial differential equations (PDEs) governing the
transitions of oceanic state variables using numerical methods. However, GOFSs
processes are computationally expensive and prone to cumulative errors.
Recently, large artificial intelligence (AI)-based models significantly boosted
forecasting speed and accuracy. Unfortunately, building a large AI ocean
forecasting system that can be considered cross-spatiotemporal and air-sea
coupled forecasts remains a significant challenge. Here, we introduce LangYa, a
cross-spatiotemporal and air-sea coupled ocean forecasting system. Results
demonstrate that the time embedding module in LangYa enables a single model to
make forecasts with lead times ranging from 1 to 7 days. The air-sea coupled
module effectively simulates air-sea interactions. The ocean self-attention
module improves network stability and accelerates convergence during training,
and the adaptive thermocline loss function improves the accuracy of thermocline
forecasting. Compared to existing numerical and AI-based ocean forecasting
systems, LangYa uses 27 years of global ocean data from the Global Ocean
Reanalysis and Simulation version 12 (GLORYS12) for training and achieves more
reliable deterministic forecasting results for OSVs. LangYa forecasting system
provides global ocean researchers with access to a powerful software tool for
accurate ocean forecasting and opens a new paradigm for ocean science.


## AutoSculpt A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning

>Authors: Lixian Jing, Jianpeng Qi, Junyu Dong, Yanwei Yu

>2024-12-24

> http://arxiv.org/abs/2412.18091v1

As deep neural networks (DNNs) are increasingly deployed on edge devices,
optimizing models for constrained computational resources is critical. Existing
auto-**pruning** methods face challenges due to the diversity of DNN models,
various operators (e.g., filters), and the difficulty in balancing **pruning**
granularity with model accuracy. To address these limitations, we introduce
AutoSculpt, a pattern-based automated **pruning** framework designed to enhance
efficiency and accuracy by leveraging graph learning and deep reinforcement
learning (DRL). AutoSculpt automatically identifies and prunes regular patterns
within DNN architectures that can be recognized by existing inference engines,
enabling runtime **acceleration**. Three key steps in AutoSculpt include: (1)
Constructing DNNs as graphs to encode their topology and parameter
dependencies, (2) embedding computationally efficient **pruning** patterns, and (3)
utilizing DRL to iteratively refine auto-**pruning** strategies until the optimal
balance between compression and accuracy is achieved. Experimental results
demonstrate the effectiveness of AutoSculpt across various architectures,
including ResNet, MobileNet, VGG, and Vision Transformer, achieving **pruning**
rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming
all baselines. The codes can be available at
https://anonymous.4open.science/r/AutoSculpt-DDA0


## LayerDropBack A Universally Applicable Approach for Accelerating Training of Deep Networks

>Authors: Evgeny Hershkovitch Neiterman, Gil Ben-Artzi

>2024-12-23

> http://arxiv.org/abs/2412.18027v1

Training very deep convolutional networks is challenging, requiring
significant computational resources and time. Existing **acceleration** methods
often depend on specific architectures or require network modifications. We
introduce LayerDropBack (LDB), a simple yet effective method to accelerate
training across a wide range of deep networks. LDB introduces randomness only
in the backward pass, maintaining the integrity of the forward pass,
guaranteeing that the same network is used during both training and inference.
LDB can be seamlessly integrated into the training process of any model without
altering its architecture, making it suitable for various network topologies.
Our extensive experiments across multiple architectures (ViT, Swin Transformer,
EfficientNet, DLA) and datasets (CIFAR-100, ImageNet) show significant training
time reductions of 16.93\% to 23.97\%, while preserving or even enhancing model
accuracy. Code is available at \url{https://github.com/neiterman21/LDB}.


## LLM-Driven Feedback for Enhancing Conceptual Design Learning in Database Systems Courses

>Authors: Sara Riazi, Pedram Rooshenas

>2024-12-23

> http://arxiv.org/abs/2412.17892v1

The integration of LLM-generated feedback into educational settings has shown
promise in enhancing student learning outcomes. This paper presents a novel
LLM-driven system that provides targeted feedback for conceptual designs in a
Database Systems course. The system converts student-created
entity-relationship diagrams (ERDs) into JSON format, allows the student to
prune the diagram by isolating a relationship, extracts relevant requirements
for the selected relationship, and utilizes a large language model (LLM) to
generate detailed feedback. Additionally, the system creates a tailored set of
questions and answers to further aid student understanding. Our pilot
implementation in a Database System course demonstrates effective feedback
generation that helped the students improve their design skills.


## Deliberation in Latent Space via Differentiable Cache Augmentation

>Authors: Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam

>2024-12-23

> http://arxiv.org/abs/2412.17747v1

Techniques enabling large language models (LLMs) to "think more" by
generating and attending to intermediate reasoning steps have shown promise in
solving complex problems. However, the standard approaches generate sequences
of discrete tokens immediately before responding, and so they can incur
significant latency costs and be challenging to optimize. In this work, we
demonstrate that a frozen LLM can be augmented with an offline coprocessor that
operates on the model's key-value (kv) cache. This coprocessor augments the
cache with a set of latent embeddings designed to improve the fidelity of
subsequent decoding. We train this coprocessor using the language modeling loss
from the decoder on standard pretraining data, while keeping the decoder itself
frozen. This approach enables the model to learn, in an end-to-end
differentiable fashion, how to distill additional computation into its
kv-cache. Because the decoder remains unchanged, the coprocessor can operate
offline and asynchronously, and the language model can function normally if the
coprocessor is unavailable or if a given cache is deemed not to require extra
computation. We show experimentally that when a cache is augmented, the decoder
achieves lower perplexity on numerous subsequent tokens. Furthermore, even
without any task-specific training, our experiments demonstrate that cache
augmentation consistently reduces perplexity and improves performance across a
range of reasoning-intensive tasks.


## LASE Learned Adjacency Spectral Embeddings

>Authors: Sofía Pérez Casulo, Marcelo Fiori, Federico Larroca, Gonzalo Mateos

>2024-12-23

> http://arxiv.org/abs/2412.17734v1

We put forth a principled design of a neural architecture to learn nodal
Adjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the
gradient descent (GD) method and leveraging the principle of algorithm
unrolling, we truncate and re-interpret each GD iteration as a layer in a graph
neural network (GNN) that is trained to approximate the ASE. Accordingly, we
call the resulting embeddings and our parametric model Learned ASE (LASE),
which is interpretable, parameter efficient, robust to inputs with unobserved
edges, and offers controllable complexity during inference. LASE layers combine
Graph Convolutional Network (GCN) and fully-connected Graph Attention Network
(GAT) modules, which is intuitively pleasing since GCN-based local aggregations
alone are insufficient to express the sought graph eigenvectors. We propose
several refinements to the unrolled LASE architecture (such as **sparse** attention
in the GAT module and decoupled layerwise parameters) that offer favorable
approximation error versus computation tradeoffs; even outperforming
heavily-optimized eigendecomposition routines from scientific computing
libraries. Because LASE is a differentiable function with respect to its
parameters as well as its graph input, we can seamlessly integrate it as a
trainable module within a larger (semi-)supervised graph representation
learning pipeline. The resulting end-to-end system effectively learns
``discriminative ASEs'' that exhibit competitive performance in supervised link
prediction and node classification tasks, outperforming a GNN even when the
latter is endowed with open loop, meaning task-agnostic, precomputed spectral
positional encodings.


## A Reproducible Method for Mapping Electricity Transmission Infrastructure for Space Weather Risk Assessment

>Authors: Edward J. Oughton, Evan Alexander Peters, Dennies Bor, Noah Rivera, C. Trevor Gaunt, Robert Weigel

>2024-12-23

> http://arxiv.org/abs/2412.17685v1

Space weather impact assessment is constrained by the lack of available asset
information to undertake modeling of Geomagnetically Induced Currents (GICs) in
Extra High Voltage electricity infrastructure networks. The U.S. National Space
Weather Strategy and Action Plan identifies underutilized data as a central
issue for improving risk assessment, motivating this research. Accurate GIC
prediction is generally not possible without information on the electrical
circuit, therefore we define a reproducible method based on open-source data,
which enables risk analysts to collect their own substation component data.
This process converts OpenStreetMap (OSM) substation locations to
high-resolution, component-level mapping of electricity transmission assets by
utilizing an innovative web-browser platform to facilitate component
annotation. As a case study example, we convert an initial 1,313 high-voltage
(>115 kV) substations to 52,273 substation components via Google Earth APIs
utilizing low-altitude, satellite, and Streetview imagery. We find that a total
of 41,642 substation components (79.6%) connect to the highest substation
voltage levels (>345 kV) and are possibly susceptible to GIC, with a total of
7,949 transformers identified. Compared to the initial OSM baseline, we provide
new detailed insights on voltage levels, line capacities, and substation
configurations. Two validation workshops were undertaken to align the method
and data with GIC assessment needs. The approach ensures consistency and rapid
scalability, enabling users to quickly count components via a flexible
web-browser application.


## Tracking the Feature Dynamics in LLM Training A Mechanistic Study

>Authors: Yang Xu, Yi Wang, Hao Wang

>2024-12-23

> http://arxiv.org/abs/2412.17626v1

Understanding training dynamics and feature evolution is crucial for the
mechanistic interpretability of large language models (LLMs). Although **sparse**
autoencoders (SAEs) have been used to identify features within LLMs, a clear
picture of how these features evolve during training remains elusive. In this
study, we: (1) introduce SAE-Track, a method to efficiently obtain a continual
series of SAEs; (2) formulate the process of feature formation and conduct a
mechanistic analysis; and (3) analyze and visualize feature drift during
training. Our work provides new insights into the dynamics of features in LLMs,
enhancing our understanding of training mechanisms and feature evolution.


## Emerging Security Challenges of Large Language Models

>Authors: Herve Debar, Sven Dietrich, Pavel Laskov, Emil C. Lupu, Eirini Ntoutsi

>2024-12-23

> http://arxiv.org/abs/2412.17614v1

Large language models (LLMs) have achieved record adoption in a short period
of time across many different sectors including high importance areas such as
education [4] and healthcare [23]. LLMs are open-ended models trained on
diverse data without being tailored for specific downstream tasks, enabling
broad applicability across various domains. They are commonly used for text
generation, but also widely used to assist with code generation [3], and even
analysis of security information, as Microsoft Security Copilot demonstrates
[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial
attacks [9]. So the concerns on the potential security implications of such
wide scale adoption of LLMs have led to the creation of this working group on
the security of LLMs. During the Dagstuhl seminar on "Network Attack Detection
and Defense - AI-Powered Threats and Responses", the working group discussions
focused on the vulnerability of LLMs to adversarial attacks, rather than their
potential use in generating malware or enabling cyberattacks. Although we note
the potential threat represented by the latter, the role of the LLMs in such
uses is mostly as an accelerator for development, similar to what it is in
benign use. To make the analysis more specific, the working group employed
ChatGPT as a concrete example of an LLM and addressed the following points,
which also form the structure of this report: 1. How do LLMs differ in
vulnerabilities from traditional ML models? 2. What are the attack objectives
in LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities
of LLMs? 4. What is the supply chain in LLMs, how data flow in and out of
systems and what are the security implications? We conclude with an overview of
open challenges and outlook.


## URoadNet Dual Sparse Attentive U-Net for Multiscale Road Network Extraction

>Authors: Jie Song, Yue Sun, Ziyun Cai, Liang Xiao, Yawen Huang, Yefeng Zheng

>2024-12-23

> http://arxiv.org/abs/2412.17573v1

The challenges of road network segmentation demand an algorithm capable of
adapting to the **sparse** and irregular shapes, as well as the diverse context,
which often leads traditional encoding-decoding methods and simple Transformer
embeddings to failure. We introduce a computationally efficient and powerful
framework for elegant road-aware segmentation. Our method, called URoadNet,
effectively encodes fine-grained local road connectivity and holistic global
topological semantics while decoding multiscale road network information.
URoadNet offers a novel alternative to the U-Net architecture by integrating
connectivity attention, which can exploit intra-road interactions across
multi-level sampling features with reduced computational complexity. This local
interaction serves as valuable prior information for learning global
interactions between road networks and the background through another
integrality attention mechanism. The two forms of **sparse** attention are arranged
alternatively and complementarily, and trained jointly, resulting in
performance improvements without significant increases in computational
complexity. Extensive experiments on various datasets with different
resolutions, including Massachusetts, DeepGlobe, SpaceNet, and Large-Scale
remote sensing images, demonstrate that URoadNet outperforms state-of-the-art
techniques. Our approach represents a significant advancement in the field of
road network extraction, providing a computationally feasible solution that
achieves high-quality segmentation results.


## GQSA Group Quantization and Sparsity for Accelerating Large Language Model Inference

>Authors: Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, Lean Fu

>2024-12-23

> http://arxiv.org/abs/2412.17560v1

With the rapid growth in the scale and complexity of large language models
(LLMs), the costs of training and inference have risen substantially. Model
compression has emerged as a mainstream solution to reduce memory usage and
computational overhead. This paper presents Group Quantization and Sparse
Acceleration (\textbf{GQSA}), a novel compression technique tailored for LLMs.
Traditional methods typically focus exclusively on either **quantization** or
sparsification, but relying on a single strategy often results in significant
performance loss at high compression rates. In contrast, GQSA integrates
**quantization** and sparsification in a tightly coupled manner, leveraging
GPU-friendly structured group **sparsity** and **quantization** for efficient
**acceleration**. The proposed method consists of three key steps. First, GQSA
applies group structured **pruning** to adhere to GPU-friendly **sparse** pattern
constraints. Second, a two-stage **sparsity**-aware training process is employed to
maximize performance retention after compression. Finally, the framework adopts
the Block Sparse Row (BSR) format to enable practical deployment and efficient
execution. Experimental results on the LLaMA model family show that GQSA
achieves an excellent balance between model speed and accuracy. Furthermore, on
the latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM
compression techniques significantly.


## CALLIC Content Adaptive Learning for Lossless Image Compression

>Authors: Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu, Wen Gao

>2024-12-23

> http://arxiv.org/abs/2412.17464v1

Learned lossless image compression has achieved significant advancements in
recent years. However, existing methods often rely on training amortized
generative models on massive datasets, resulting in sub-optimal probability
distribution estimation for specific testing images during encoding process. To
address this challenge, we explore the connection between the Minimum
Description Length (MDL) principle and Parameter-Efficient Transfer Learning
(PETL), leading to the development of a novel content-adaptive approach for
learned lossless image compression, dubbed CALLIC. Specifically, we first
propose a content-aware autoregressive self-attention mechanism by leveraging
convolutional gating operations, termed Masked Gated ConvFormer (MGCF), and
pretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed
to accelerate the coding process. During encoding, we decompose pre-trained
layers, including depth-wise convolutions, using low-rank matrices and then
adapt the incremental weights on testing image by Rate-guided Progressive
Fine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are
sorted in descending order by estimated entropy, optimizing learning process
and reducing adaptation time. Extensive experiments across diverse datasets
demonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless
image compression.


## Learning Dynamic Local Context Representations for Infrared Small Target Detection

>Authors: Guoyi Zhang, Guangsheng Xu, Han Wang, Siyang Chen, Yunxiao Shan, Xiaohu Zhang

>2024-12-23

> http://arxiv.org/abs/2412.17401v1

Infrared small target detection (ISTD) is challenging due to complex
backgrounds, low signal-to-clutter ratios, and varying target sizes and shapes.
Effective detection relies on capturing local contextual information at the
appropriate scale. However, small-kernel CNNs have limited receptive fields,
leading to false alarms, while transformer models, with global receptive
fields, often treat small targets as noise, resulting in miss-detections.
Hybrid models struggle to bridge the semantic gap between CNNs and
transformers, causing high complexity.To address these challenges, we propose
LCRNet, a novel method that learns dynamic local context representations for
ISTD. The model consists of three components: (1) C2FBlock, inspired by PDE
solvers, for efficient small target information capture; (2) DLC-Attention, a
large-kernel attention mechanism that dynamically builds context and reduces
feature redundancy; and (3) HLKConv, a hierarchical convolution operator based
on large-kernel decomposition that preserves **sparsity** and mitigates the
drawbacks of dilated convolutions. Despite its simplicity, with only 1.65M
parameters, LCRNet achieves state-of-the-art (SOTA) performance.Experiments on
multiple datasets, comparing LCRNet with 33 SOTA methods, demonstrate its
superior performance and efficiency.


## ORIGAMI A generative transformer architecture for predictions from semi-structured data

>Authors: Thomas Rückstieß, Alana Huang, Robin Vujanic

>2024-12-23

> http://arxiv.org/abs/2412.17348v1

Despite the popularity and widespread use of semi-structured data formats
such as JSON, end-to-end supervised learning applied directly to such data
remains underexplored. We present ORIGAMI (Object RepresentatIon via Generative
Autoregressive ModellIng), a transformer-based architecture that directly
processes nested key/value pairs while preserving their hierarchical semantics.
Our key technical contributions include: (1) a structure-preserving tokenizer,
(2) a novel key/value position encoding scheme, and (3) a grammar-constrained
training and inference framework that ensures valid outputs and accelerates
training convergence. These enhancements enable efficient end-to-end modeling
of semi-structured data. By reformulating classification as next-token
prediction, ORIGAMI naturally handles both single-label and multi-label tasks
without architectural modifications. Empirical evaluation across diverse
domains demonstrates ORIGAMI's effectiveness: On standard tabular benchmarks
converted to JSON, ORIGAMI remains competitive with classical and
state-of-the-art approaches. On native JSON datasets, we outperform baselines
on multi-label classification and specialized models such as convolutional and
graph neural networks on a code classification task. Through extensive ablation
studies, we validate the impact of each architectural component and establish
ORIGAMI as a robust framework for end-to-end learning on semi-structured data.


## Understanding Dynamics in Coarse-Grained Models V. Extension of Coarse-Grained Dynamics Theory to Non-Hard Sphere Systems

>Authors: Jaehyeok Jin, Gregory A. Voth

>2024-12-22

> http://arxiv.org/abs/2412.17186v1

Coarse-grained (CG) modeling has gained significant attention in recent years
due to its wide applicability in enhancing the spatiotemporal scales of
molecular simulations. While CG simulations, often performed with Hamiltonian
mechanics, faithfully recapitulate structural correlations at equilibrium, they
lead to ambiguously accelerated dynamics. In the first paper of this series [J.
Chem. Phys. 158, 034103 (2023)], we proposed the excess entropy scaling
relationship to understand the CG dynamics. Then, in the following companion
paper [J. Chem. Phys. 158, 034104 (2023)], we developed a theory to map the CG
system into a dynamically-consistent hard sphere system to analytically derive
an expression for fast CG dynamics. However, many chemical and physical systems
do not exhibit hard sphere-like behavior, limiting the extensibility of the
developed theory. In this paper, we aim to generalize the theory to the
non-hard sphere system based on the Weeks-Chandler-Andersen perturbation
theory. Since non-hard sphere-like CG interactions affect the excess entropy
term as it deviates from the hard sphere description, we explicitly account for
the extra entropy to correct the non-hard sphere nature of the system. This
approach is demonstrated for two different types of interactions seen in
liquids, and we further provide a generalized description for any CG models
using the generalized Gaussian CG models using Gaussian basis sets. Altogether,
this work allows for extending the range and applicability of the hard sphere
CG dynamics theory to a myriad of CG liquids.


## Half-form quantization of mixed toric polarizations and Hamiltonian flows in imaginary-time

>Authors: José M. Mourão, João P. Nunes, Augusto Pereira, Dan Wang

>2024-12-22

> http://arxiv.org/abs/2412.17157v1

We consider the half-form corrected geometric **quantization** of symplectic
toric manifolds with respect to mixed toric polarizations
$\mathcal{P}_{\infty}$. These polarizations are obtained at infinite geodesic
time along Mabuchi rays of toric K\"ahler polarizations generated by the norm
square of the moment map of a torus subgroup.
  The geodesic rays are lifted to the quantum bundle via generalized coherent
state transforms (gCST) and define equivariant isomorphisms between Hilbert
spaces for the K\"ahler polarizations and the Hilbert space for the mixed
polarization.


## LLM Agent for Fire Dynamics Simulations

>Authors: Leidong Xu, Danyal Mohaddes, Yi Wang

>2024-12-22

> http://arxiv.org/abs/2412.17146v1

Significant advances have been achieved in leveraging foundation models, such
as large language models (LLMs), to accelerate complex scientific workflows. In
this work we introduce FoamPilot, a proof-of-concept LLM agent designed to
enhance the usability of FireFOAM, a specialized solver for fire dynamics and
fire suppression simulations built using OpenFOAM, a popular open-source
toolbox for computational fluid dynamics (CFD). FoamPilot provides three core
functionalities: code insight, case configuration and simulation evaluation.
Code insight is an alternative to traditional keyword searching leveraging
retrieval-augmented generation (RAG) and aims to enable efficient navigation
and summarization of the FireFOAM source code for developers and experienced
users. For case configuration, the agent interprets user requests in natural
language and aims to modify existing simulation setups accordingly to support
intermediate users. FoamPilot's job execution functionality seeks to manage the
submission and execution of simulations in high-performance computing (HPC)
environments and provide preliminary analysis of simulation results to support
less experienced users. Promising results were achieved for each functionality,
particularly for simple tasks, and opportunities were identified for
significant further improvement for more complex tasks. The integration of
these functionalities into a single LLM agent is a step aimed at accelerating
the simulation workflow for engineers and scientists employing FireFOAM for
complex simulations critical for improving fire safety.


## Transformer-Based Model Predictive Path Integral Control

>Authors: Shrenik Zinage, Vrushabh Zinage, Efstathios Bakolas

>2024-12-22

> http://arxiv.org/abs/2412.17118v1

This paper presents a novel approach to improve the Model Predictive Path
Integral (MPPI) control by using a transformer to initialize the mean control
sequence. Traditional MPPI methods often struggle with sample efficiency and
computational costs due to suboptimal initial rollouts. We propose
TransformerMPPI, which uses a transformer trained on historical control data to
generate informed initial mean control sequences. TransformerMPPI combines the
strengths of the attention mechanism in transformers and sampling-based
control, leading to improved computational performance and sample efficiency.
The ability of the transformer to capture long-horizon patterns in optimal
control sequences allows TransformerMPPI to start from a more informed control
sequence, reducing the number of samples required, and accelerating convergence
to optimal control sequence. We evaluate our method on various control tasks,
including avoidance of collisions in a 2D environment and autonomous racing in
the presence of static and dynamic obstacles. Numerical simulations demonstrate
that TransformerMPPI consistently outperforms traditional MPPI algorithms in
terms of overall average cost, sample efficiency, and computational speed in
the presence of static and dynamic obstacles.


## The HalluRAG Dataset Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM's Internal States

>Authors: Fabian Ridder, Malte Schilling

>2024-12-22

> http://arxiv.org/abs/2412.17056v1

Detecting hallucinations in large language models (LLMs) is critical for
enhancing their reliability and trustworthiness. Most research focuses on
hallucinations as deviations from information seen during training. However,
the opaque nature of an LLM's parametric knowledge complicates the
understanding of why generated texts appear ungrounded: The LLM might not have
picked up the necessary knowledge from large and often inaccessible datasets,
or the information might have been changed or contradicted during further
training. Our focus is on hallucinations involving information not used in
training, which we determine by using recency to ensure the information emerged
after a cut-off date. This study investigates these hallucinations by detecting
them at sentence level using different internal states of various LLMs. We
present HalluRAG, a dataset designed to train classifiers on these
hallucinations. Depending on the model and **quantization**, MLPs trained on
HalluRAG detect hallucinations with test accuracies ranging up to 75 %, with
Mistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results
show that IAVs detect hallucinations as effectively as CEVs and reveal that
answerable and unanswerable prompts are encoded differently as separate
classifiers for these categories improved accuracy. However, HalluRAG showed
some limited generalizability, advocating for more diversity in datasets on
hallucinations.


## LLM-Powered User Simulator for Recommender System

>Authors: Zijian Zhang, Shuchang Liu, Ziru Liu, Rui Zhong, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Qidong Liu, Peng Jiang

>2024-12-22

> http://arxiv.org/abs/2412.16984v1

User simulators can rapidly generate a large volume of timely user behavior
data, providing a testing platform for reinforcement learning-based recommender
systems, thus accelerating their iteration and optimization. However, prevalent
user simulators generally suffer from significant limitations, including the
opacity of user preference modeling and the incapability of evaluating
simulation accuracy. In this paper, we introduce an LLM-powered user simulator
to simulate user engagement with items in an explicit manner, thereby enhancing
the efficiency and effectiveness of reinforcement learning-based recommender
systems training. Specifically, we identify the explicit logic of user
preferences, leverage LLMs to analyze item characteristics and distill user
sentiments, and design a logical model to imitate real human engagement. By
integrating a statistical model, we further enhance the reliability of the
simulation, proposing an ensemble model that synergizes logical and statistical
insights for user interaction simulations. Capitalizing on the extensive
knowledge and semantic generation capabilities of LLMs, our user simulator
faithfully emulates user behaviors and preferences, yielding high-fidelity
training data that enrich the training of recommendation algorithms. We
establish quantifying and qualifying experiments on five datasets to validate
the simulator's effectiveness and stability across various recommendation
scenarios.


## A tensor network formulation of Lattice Gauge Theories based only on symmetric tensors

>Authors: Manu Canals, Natalia Chepiga, Luca Tagliacozzo

>2024-12-22

> http://arxiv.org/abs/2412.16961v1

The Lattice Gauge Theory Hilbert space is divided into gauge-invariant
sectors selected by the background charges. Such a projector can be directly
embedded in a tensor network ansatz for gauge-invariant states as originally
discussed in [Phys. Rev. B 83, 115127 (2011)] and in [Phys. Rev. X 4, 041024
(2014)] in the context of PEPS. The original ansatz is based on **sparse** tensors,
though parts of them are not explicitly symmetric, and thus their actual
implementation in numerical simulations has been hindered by the complexity of
developing ad hoc libraries. Here we provide a new PEPS tensor network
formulation of gauge-invariant theories purely based on symmetric elementary
tensors. The new formulation can be implemented in numerical simulation using
available state-of-the-art tensor network libraries but also holds interest
from a purely theoretical perspective since it requires embedding the original
gauge theory with gauge symmetry G into an enlarged globally symmetric theory
with symmetry GxG. By revisiting the original ansatz in the modern landscape of
i) duality transformations between gauge and spin systems, ii) finite depth
quantum circuits followed by measurements that allow generating topologically
ordered states, and iii) Clifford enhanced tensor networks, we show that such a
new formulation provides a novel duality transformation between lattice gauge
theories and specific sectors of globally invariant systems.


## TAR3D Creating High-Quality 3D Assets via Next-Part Prediction

>Authors: Xuying Zhang, Yutong Liu, Yangguang Li, Renrui Zhang, Yufei Liu, Kai Wang, Wanli Ouyang, Zhiwei Xiong, Peng Gao, Qibin Hou, Ming-Ming Cheng

>2024-12-22

> http://arxiv.org/abs/2412.16919v1

We present TAR3D, a novel framework that consists of a 3D-aware Vector
Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained
Transformer (GPT) to generate high-quality 3D assets. The core insight of this
work is to migrate the multimodal unification and promising learning
capabilities of the next-token prediction paradigm to conditional 3D object
generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D
shapes into a compact triplane latent space and utilizes a set of discrete
representations from a trainable codebook to reconstruct fine-grained
geometries under the supervision of query point occupancy. Then, the 3D GPT,
equipped with a custom triplane position embedding called TriPE, predicts the
codebook index sequence with prefilling prompt tokens in an autoregressive
manner so that the composition of 3D geometries can be modeled part by part.
Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can
achieve superior generation quality over existing methods in text-to-3D and
image-to-3D tasks


## GeoTexDensifier Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting

>Authors: Hanqing Jiang, Xiaojun Xiang, Han Sun, Hongjie Li, Liyang Zhou, Xiaoyu Zhang, Guofeng Zhang

>2024-12-22

> http://arxiv.org/abs/2412.16809v1

3D Gaussian Splatting (3DGS) has recently attracted wide attentions in
various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation,
due to its photorealistic and efficient rendering performance. High-quality
reconstrution of 3DGS relies on sufficient splats and a reasonable distribution
of these splats to fit real geometric surface and texture details, which turns
out to be a challenging problem. We present GeoTexDensifier, a novel
geometry-texture-aware densification strategy to reconstruct high-quality
Gaussian splats which better comply with the geometric structure and texture
richness of the scene. Specifically, our GeoTexDensifier framework carries out
an auxiliary texture-aware densification method to produce a denser
distribution of splats in fully textured areas, while keeping **sparsity** in
low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile,
a geometry-aware splitting strategy takes depth and normal priors to guide the
splitting sampling and filter out the noisy splats whose initial positions are
far from the actual geometric surfaces they aim to fit, under a Validation of
Depth Ratio Change checking. With the help of relative monocular depth prior,
such geometry-aware validation can effectively reduce the influence of
scattered Gaussians to the final rendering quality, especially in regions with
weak textures or without sufficient training views. The texture-aware
densification and geometry-aware splitting strategies are fully combined to
obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier
framework on various datasets and compare our Novel View Synthesis results to
other state-of-the-art 3DGS approaches, with detailed quantitative and
qualitative evaluations to demonstrate the effectiveness of our method in
producing more photorealistic 3DGS models.


## Large Language Models Compression via Low-Rank Feature Distillation

>Authors: Yaya Sy, Christophe Cerisara, Irina Illina

>2024-12-21

> http://arxiv.org/abs/2412.16719v1

Current LLM structured **pruning** methods involve two steps: (1) compressing
with calibration data and (2) continued pretraining on billions of tokens to
recover the lost performance. This costly second step is needed as the first
step significantly impacts performance. Previous studies have found that
pretrained Transformer weights aren't inherently low-rank, unlike their
activations, which may explain this performance drop. Based on this
observation, we introduce a one-shot compression method that locally distills
low-rank weights. We accelerate convergence by initializing the low-rank
weights with SVD and using a joint loss that combines teacher and student
activations. We reduce memory requirements by applying local gradient updates
only. Our approach can compress Mixtral-8x7B within minutes on a single A100
GPU, removing 10 billion parameters while maintaining over 95% of the original
performance. Phi-2 3B can be compressed by 40% using only 13 million
calibration tokens into a small model that competes with recent models of
similar size. We show our method generalizes well to non-transformer
architectures: Mamba-3B can be compressed by 20% while maintaining 99% of its
performance.


## Adaptive Elastic-Net estimation for sparse diffusion processes

>Authors: Alessandro De Gregorio, Dario Frisardi, Francesco Iafrate, Stefano Iacus

>2024-12-21

> http://arxiv.org/abs/2412.16659v1

Penalized estimation methods for diffusion processes and dependent data have
recently gained significant attention due to their effectiveness in handling
high-dimensional stochastic systems. In this work, we introduce an adaptive
Elastic-Net estimator for ergodic diffusion processes observed under
high-frequency sampling schemes. Our method combines the least squares
approximation of the quasi-likelihood with adaptive $\ell_1$ and $\ell_2$
regularization. This approach allows to enhance prediction accuracy and
interpretability while effectively recovering the **sparse** underlying structure
of the model.
  In the spirit of analyzing high-dimensional scenarios, we provide
finite-sample guarantees for the (block-diagonal) estimator's performance by
deriving high-probability non-asymptotic bounds for the $\ell_2$ estimation
error. These results complement the established oracle properties in the
high-frequency asymptotic regime with mixed convergence rates, ensuring
consistent selection of the relevant interactions and achieving optimal rates
of convergence. Furthermore, we utilize our results to analyze one-step-ahead
predictions, offering non-asymptotic control over the $\ell_1$ prediction
error.
  The performance of our method is evaluated through simulations and real data
applications, demonstrating its effectiveness, particularly in scenarios with
strongly correlated variables.


## V"Mean"ba Visual State Space Models only need 1 hidden dimension

>Authors: Tien-Yu Chi, Hung-Yueh Chiang, Chi-Chih Chang, Ning-Chi Huang, Kai-Chiang Wu

>2024-12-21

> http://arxiv.org/abs/2412.16602v1

Vision transformers dominate image processing tasks due to their superior
performance. However, the quadratic complexity of self-attention limits the
scalability of these systems and their deployment on resource-constrained
devices. State Space Models (SSMs) have emerged as a solution by introducing a
linear recurrence mechanism, which reduces the complexity of sequence modeling
from quadratic to linear. Recently, SSMs have been extended to high-resolution
vision tasks. Nonetheless, the linear recurrence mechanism struggles to fully
utilize matrix multiplication units on modern hardware, resulting in a
computational bottleneck. We address this issue by introducing
\textit{VMeanba}, a training-free compression method that eliminates the
channel dimension in SSMs using mean operations. Our key observation is that
the output activations of SSM blocks exhibit low variances across channels. Our
\textit{VMeanba} leverages this property to optimize computation by averaging
activation maps across the channel to reduce the computational overhead without
compromising accuracy. Evaluations on image classification and semantic
segmentation tasks demonstrate that \textit{VMeanba} achieves up to a 1.12x
speedup with less than a 3\% accuracy loss. When combined with 40\%
unstructured **pruning**, the accuracy drop remains under 3\%.


## AIGCodeSet A New Annotated Dataset for AI Generated Code Detection

>Authors: Basak Demirok, Mucahid Kutlu

>2024-12-21

> http://arxiv.org/abs/2412.16594v1

With the rapid advancement of LLM models, they have become widely useful in
various fields. While these AI systems can be used for code generation,
significantly simplifying and accelerating the tasks of developers, their use
for students to do assignments has raised ethical questions in the field of
education. In this context, determining the author of a particular code becomes
important. In this study, we introduce AIGCodeSet, a dataset for AI-generated
code detection tasks, specifically for the Python programming language. We
obtain the problem descriptions and human-written codes from the CodeNet
dataset. Using the problem descriptions, we generate AI-written codes with
CodeLlama 34B, Codestral 22B, and Gemini 1.5 Flash models in three approaches:
i) generating code from the problem description alone, ii) generating code
using the description along with human-written source code containing runtime
errors, and iii) generating code using the problem description and
human-written code that resulted in wrong answers. Lastly, we conducted a
post-processing step to eliminate LLM output irrelevant to code snippets.
Overall, AIGCodeSet consists of 2,828 AI-generated and 4,755 human-written code
snippets. We share our code with the research community to support studies on
this important topic and provide performance results for baseline AI-generated
code detection methods.


## Semantics Prompting Data-Free Quantization for Low-Bit Vision Transformers

>Authors: Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Shen Li, Yong Li, Fei Chao, Zhanpeng Zeng, Rongrong Ji

>2024-12-21

> http://arxiv.org/abs/2412.16553v1

Data-free **quantization** (DFQ), which facilitates model **quantization** without
real data to address increasing concerns about data security, has garnered
significant attention within the model compression community. Recently, the
unique architecture of vision transformers (ViTs) has driven the development of
specialized DFQ techniques. However, we observe that the synthetic images from
existing methods suffer from the deficient semantics issue compared to real
images, thereby compromising performance. Motivated by this, we propose SPDFQ,
a Semantics Prompting Data-Free Quantization method for ViTs. First, SPDFQ
incorporates Attention Priors Alignment (APA), which uses randomly generated
attention priors to enhance the semantics of synthetic images. Second, SPDFQ
introduces Multi-Semantic Reinforcement (MSR), which utilizes localized patch
optimization to prompt efficient parameterization and diverse semantics in
synthetic images. Finally, SPDFQ employs Softlabel Learning (SL), where soft
learning targets are adapted to encourage more complex semantics and
accommodate images augmented by MSR. Experimental results demonstrate that
SPDFQ significantly outperforms existing methods. For instance, SPDFQ achieves
a 15.52% increase in top-1 accuracy on ImageNet for W4A4 ViT-B


## Accelerating Private Large Transformers Inference through Fine-grained Collaborative Computation

>Authors: Yuntian Chen, Zhanyong Tang, Tianpei Lu, Bingsheng Zhang, Zhiying Shi, Zheng Wang

>2024-12-21

> http://arxiv.org/abs/2412.16537v1

Homomorphic encryption (HE) and secret sharing (SS) enable computations on
encrypted data, providing significant privacy benefits for large
transformer-based models (TBM) in sensitive sectors like medicine and finance.
However, private TBM inference incurs significant costs due to the
coarse-grained application of HE and SS. We present FASTLMPI, a new approach to
accelerate private TBM inference through fine-grained computation optimization.
Specifically, through the fine-grained co-design of homomorphic encryption and
secret sharing, FASTLMPI achieves efficient protocols for matrix
multiplication, SoftMax, LayerNorm, and GeLU. In addition, FASTLMPI introduces
a precise segmented approximation technique for differentiable non-linear,
improving its fitting accuracy while maintaining a low polynomial degree.
Compared to solution BOLT (S\&P'24), \SystemName shows a remarkable 54\% to
64\% decrease in runtime and an impressive 72.2\% reduction in communication
costs.


## STKDRec Spatial-Temporal Knowledge Distillation for Takeaway Recommendation

>Authors: Shuyuan Zhao, Wei Chen, Boyan Shi, Liyong Zhou, Shuohao Lin, Huaiyu Wan

>2024-12-21

> http://arxiv.org/abs/2412.16502v1

The takeaway recommendation system is designed to recommend users' future
takeaway purchases based on their historical purchase behaviors, thereby
improving user satisfaction and increasing merchant sales. Existing methods
focus on incorporating auxiliary information or leveraging knowledge graphs to
alleviate the **sparsity** issue of user purchase sequence data. However, two main
challenges limit the performance of these approaches: (1) how to capture
dynamic user preferences on complex geospatial information and (2) how to
efficiently integrate spatial-temporal knowledge from graphs and sequence data
with low calculation costs. In this paper, we propose a novel spatial-temporal
knowledge distillation for takeaway recommendation model (STKDRec) based on the
two-stage training process. Specifically, during the first pre-training stage,
a spatial-temporal knowledge graph (STKG) encoder is pre-trained to extract the
high-order spatial-temporal and collaborative associations within the STKG.
During the second STKD stage, a spatial-temporal Transformer is employed to
comprehensively model dynamic user preferences on various types of fine-grained
geospatial information from a sequence perspective. Furthermore, the STKD
strategy is introduced to adaptively fuse the rich spatial-temporal knowledge
from the pre-trained STKG encoder and the spatial-temporal transformer while
reducing the cost of model training. Extensive experiments on three real-world
datasets show that our STKDRec significantly outperforms the state-of-the-art
baselines. Our code is available at:https://github.com/Zhaoshuyuan0246/STKDRec.


## ImagePiece Content-aware Re-tokenization for Efficient Image Recognition

>Authors: Seungdong Yoa, Seungjun Lee, Hyeseung Cho, Bumsoo Kim, Woohyung Lim

>2024-12-21

> http://arxiv.org/abs/2412.16491v1

Vision Transformers (ViTs) have achieved remarkable success in various
computer vision tasks. However, ViTs have a huge computational cost due to
their inherent reliance on multi-head self-attention (MHSA), prompting efforts
to accelerate ViTs for practical applications. To this end, recent works aim to
reduce the number of tokens, mainly focusing on how to effectively prune or
merge them. Nevertheless, since ViT tokens are generated from non-overlapping
grid patches, they usually do not convey sufficient semantics, making it
incompatible with efficient ViTs. To address this, we propose ImagePiece, a
novel re-tokenization strategy for Vision Transformers. Following the MaxMatch
strategy of NLP tokenization, ImagePiece groups semantically insufficient yet
locally coherent tokens until they convey meaning. This simple retokenization
is highly compatible with previous token reduction methods, being able to
drastically narrow down relevant tokens, enhancing the inference speed of
DeiT-S by 54% (nearly 1.5$\times$ faster) while achieving a 0.39% improvement
in ImageNet classification accuracy. For hyper-speed inference scenarios (with
251% **acceleration**), our approach surpasses other baselines by an accuracy over
8%.


## When Can Proxies Improve the Sample Complexity of Preference Learning?

>Authors: Yuchen Zhu, Daniel Augusto de Souza, Zhengyan Shi, Mengyue Yang, Pasquale Minervini, Alexander D'Amour, Matt J. Kusner

>2024-12-21

> http://arxiv.org/abs/2412.16475v1

We address the problem of reward hacking, where maximising a proxy reward
does not necessarily increase the true reward. This is a key concern for Large
Language Models (LLMs), as they are often fine-tuned on human preferences that
may not accurately reflect a true objective. Existing work uses various tricks
such as regularisation, tweaks to the reward model, and reward hacking
detectors, to limit the influence that such proxy preferences have on a model.
Luckily, in many contexts such as medicine, education, and law, a **sparse** amount
of expert data is often available. In these cases, it is often unclear whether
the addition of proxy data can improve policy learning. We outline a set of
sufficient conditions on proxy feedback that, if satisfied, indicate that proxy
data can provably improve the sample complexity of learning the ground truth
policy. These conditions can inform the data collection process for specific
tasks. The result implies a parameterisation for LLMs that achieves this
improved sample complexity. We detail how one can adapt existing architectures
to yield this improved sample complexity.


## Transducer-Llama Integrating LLMs into Streamable Transducer-based Speech Recognition

>Authors: Keqi Deng, Jinxi Guo, Yingyi Ma, Niko Moritz, Philip C. Woodland, Ozlem Kalinli, Mike Seltzer

>2024-12-21

> http://arxiv.org/abs/2412.16464v1

While large language models (LLMs) have been applied to automatic speech
recognition (ASR), the task of making the model streamable remains a challenge.
This paper proposes a novel model architecture, Transducer-Llama, that
integrates LLMs into a Factorized Transducer (FT) model, naturally enabling
streaming capabilities. Furthermore, given that the large vocabulary of LLMs
can cause data **sparsity** issue and increased training costs for spoken language
systems, this paper introduces an efficient vocabulary adaptation technique to
align LLMs with speech system vocabularies. The results show that directly
optimizing the FT model with a strong pre-trained LLM-based predictor using the
RNN-T loss yields some but limited improvements over a smaller pre-trained LM
predictor. Therefore, this paper proposes a weak-to-strong LM swap strategy,
using a weak LM predictor during RNN-T loss training and then replacing it with
a strong LLM. After LM replacement, the minimum word error rate (MWER) loss is
employed to finetune the integration of the LLM predictor with the
Transducer-Llama model. Experiments on the LibriSpeech and large-scale
multi-lingual LibriSpeech corpora show that the proposed streaming
Transducer-Llama approach gave a 17% relative WER reduction (WERR) over a
strong FT baseline and a 32% WERR over an RNN-T baseline.


## DFModel Design Space Optimization of Large-Scale Systems Exploiting Dataflow Mappings

>Authors: Sho Ko, Nathan Zhang, Olivia Hsu, Ardavan Pedram, Kunle Olukotun

>2024-12-21

> http://arxiv.org/abs/2412.16432v1

We propose DFModel, a modeling framework for mapping dataflow computation
graphs onto large-scale systems. Mapping a workload to a system requires
optimizing dataflow mappings at various levels, including the inter-chip
(between chips) level and the intra-chip (within a chip) level. DFModel is, to
the best of our knowledge, the first framework to perform the optimization at
multiple levels of the memory hierarchy and the interconnection network
hierarchy. We use DFModel to explore a wide range of workloads on a variety of
systems. Evaluated workloads include two state-of-the-art machine learning
applications (Large Language Models and Deep Learning Recommendation Models)
and two high-performance computing applications (High Performance LINPACK and
Fast Fourier Transform). System parameters investigated span the combination of
dataflow and traditional accelerator architectures, memory technologies (DDR,
HBM), interconnect technologies (PCIe, NVLink), and interconnection network
topologies (torus, DGX, dragonfly). For a variety of workloads on a wide range
of systems, the DFModel provided a mapping that predicts an average of 1.25X
better performance compared to the ones measured on real systems. DFModel shows
that for large language model training, dataflow architectures achieve 1.52X
higher performance, 1.59X better cost efficiency, and 1.6X better power
efficiency compared to non-dataflow architectures. On an industrial system with
dataflow architectures, the DFModel-optimized dataflow mapping achieves a
speedup of 6.13X compared to non-dataflow mappings from previous performance
models such as Calculon, and 1.52X compared to a vendor provided dataflow
mapping.


## Technical Report Small Language Model for Japanese Clinical and Medicine

>Authors: Shogo Watanabe

>2024-12-21

> http://arxiv.org/abs/2412.16423v1

This report presents a small language model (SLM) for Japanese clinical and
medicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese
text classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with
respect to clinical and medicine content that includes the variety of diseases,
drugs, and examinations. Using a carefully designed pre-processing, a
specialized morphological analyzer and tokenizer, this small and light-weight
model performed not only to generate text but also indicated the feasibility of
understanding clinical and medicine text. In comparison to other large language
models, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of
total 8 on JMED-LLM. According to this result, SLM indicated the feasibility of
performing several downstream tasks in the field of clinical and medicine.
Hopefully, NCVC-slm-1 will be contributed to develop and accelerate the field
of clinical and medicine for a bright future.


## Analytical insights from a model of opinion formation based on Persuasive Argument Theory

>Authors: Lucía Pedraza, Nicolas Saintier, Juan Pablo Pinasco, Pablo Balenzuela, Celia Anteneodo

>2024-12-20

> http://arxiv.org/abs/2412.16397v1

In recent years, numerous mathematical models of opinion formation have been
developed, incorporating diverse interaction mechanisms such as imitation and
majority rule. However, limited attention has been given to models grounded in
persuasive arguments theory (PAT), which describes how individuals may alter
their opinions through the exchange of arguments during discussions. Moreover,
analytical investigations of PAT-based models remain **sparse**. In this study, we
propose an analytical model rooted in PAT, demonstrating that a group of agents
can exhibit two distinct collective dynamics: quasi-consensus and
bipolarization. Specifically, we explore various scenarios characterized by the
number of arguments and the degree of homophily, revealing that bipolarization
arises within this framework only in the presence of homophily.


## Offline Reinforcement Learning for LLM Multi-Step Reasoning

>Authors: Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu

>2024-12-20

> http://arxiv.org/abs/2412.16145v1

Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with **sparse** reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.


## PruneVid Visual Token Pruning for Efficient Video Large Language Models

>Authors: Xiaohu Huang, Hao Zhou, Kai Han

>2024-12-20

> http://arxiv.org/abs/2412.16117v1

In this paper, we introduce PruneVid, a visual token **pruning** method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing **pruning** methods. Code:
https://github.com/Visual-AI/PruneVid.


## CLEAR Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up

>Authors: Songhua Liu, Zhenxiong Tan, Xinchao Wang

>2024-12-20

> http://arxiv.org/abs/2412.16112v1

Diffusion Transformers (DiT) have become a leading architecture in image
generation. However, the quadratic complexity of attention mechanisms, which
are responsible for modeling token-wise relationships, results in significant
latency when generating high-resolution images. To address this issue, we aim
at a linear attention mechanism in this paper that reduces the complexity of
pre-trained DiTs to linear. We begin our exploration with a comprehensive
summary of existing efficient attention mechanisms and identify four key
factors crucial for successful linearization of pre-trained DiTs: locality,
formulation consistency, high-rank attention maps, and feature integrity. Based
on these insights, we introduce a convolution-like local attention strategy
termed CLEAR, which limits feature interactions to a local window around each
query token, and thus achieves linear complexity. Our experiments indicate
that, by fine-tuning the attention layer on merely 10K self-generated samples
for 10K iterations, we can effectively transfer knowledge from a pre-trained
DiT to a student model with linear complexity, yielding results comparable to
the teacher model. Simultaneously, it reduces attention computations by 99.5%
and accelerates generation by 6.3 times for generating 8K-resolution images.
Furthermore, we investigate favorable properties in the distilled attention
layers, such as zero-shot generalization cross various models and plugins, and
improved support for multi-GPU parallel inference. Models and codes are
available here: https://github.com/Huage001/CLEAR.


## MetaScientist A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design

>Authors: Jingyuan Qi, Zian Jia, Minqian Liu, Wangzhi Zhan, Junkai Zhang, Xiaofei Wen, Jingru Gan, Jianpeng Chen, Qin Liu, Mingyu Derek Ma, Bangzheng Li, Haohui Wang, Adithya Kulkarni, Muhao Chen, Dawei Zhou, Ling Li, Wei Wang, Lifu Huang

>2024-12-20

> http://arxiv.org/abs/2412.16270v1

The discovery of novel mechanical metamaterials, whose properties are
dominated by their engineered structures rather than chemical composition, is a
knowledge-intensive and resource-demanding process. To accelerate the design of
novel metamaterials, we present MetaScientist, a human-in-the-loop system that
integrates advanced AI capabilities with expert oversight with two primary
phases: (1) hypothesis generation, where the system performs complex reasoning
to generate novel and scientifically sound hypotheses, supported with
domain-specific foundation models and inductive biases retrieved from existing
literature; (2) 3D structure synthesis, where a 3D structure is synthesized
with a novel 3D diffusion model based on the textual hypothesis and refined it
with a LLM-based refinement model to achieve better structure properties. At
each phase, domain experts iteratively validate the system outputs, and provide
feedback and supplementary materials to ensure the alignment of the outputs
with scientific principles and human preferences. Through extensive evaluation
from human scientists, MetaScientist is able to deliver novel and valid
mechanical metamaterial designs that have the potential to be highly impactful
in the metamaterial field.


## Extraordinary oxidation behavior of W-Zr thin-film metallic glasses A route for tailoring functional properties of W-Zr-O films

>Authors: Petr Zeman, Michaela Červená, Jiří Houška, Stanislav Haviar, Jiří Rezek, Šárka Zuzjaková

>2024-12-20

> http://arxiv.org/abs/2412.15943v1

The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.


## Less is More Towards Green Code Large Language Models via Unified Structural Pruning

>Authors: Guang Yang, Yu Zhou, Xiangyu Zhang, Wei Cheng, Ke Liu, Xiang Chen, Terry Yue Zhuo, Taolue Chen

>2024-12-20

> http://arxiv.org/abs/2412.15921v1

The extensive application of Large Language Models (LLMs) in generative
coding tasks has raised concerns due to their high computational demands and
energy consumption. Unlike previous structural **pruning** methods designed for
classification models that deal with lowdimensional classification logits,
generative Code LLMs produce high-dimensional token logit sequences, making
traditional **pruning** objectives inherently limited. Moreover, existing single
component **pruning** approaches further constrain the effectiveness when applied
to generative Code LLMs. In response, we propose Flab-Pruner, an innovative
unified structural **pruning** method that combines vocabulary, layer, and
Feed-Forward Network (FFN) **pruning**. This approach effectively reduces model
parameters while maintaining performance. Additionally, we introduce a
customized code instruction data strategy for coding tasks to enhance the
performance recovery efficiency of the pruned model. Through extensive
evaluations on three state-of-the-art Code LLMs across multiple generative
coding tasks, the results demonstrate that Flab-Pruner retains 97% of the
original performance after **pruning** 22% of the parameters and achieves the same
or even better performance after post-training. The pruned models exhibit
significant improvements in storage, GPU usage, computational efficiency, and
environmental impact, while maintaining well robustness. Our research provides
a sustainable solution for green software engineering and promotes the
efficient deployment of LLMs in real-world generative coding intelligence
applications.


## WebLLM A High-Performance In-Browser LLM Inference Engine

>Authors: Charlie F. Ruan, Yucheng Qin, Xun Zhou, Ruihang Lai, Hongyi Jin, Yixin Dong, Bohan Hou, Meng-Shiun Yu, Yiyan Zhai, Sudeep Agarwal, Hangrui Cao, Siyuan Feng, Tianqi Chen

>2024-12-20

> http://arxiv.org/abs/2412.15803v1

Advancements in large language models (LLMs) have unlocked remarkable
capabilities. While deploying these models typically requires server-grade GPUs
and cloud-based inference, the recent emergence of smaller open-source models
and increasingly powerful consumer devices have made on-device deployment
practical. The web browser as a platform for on-device deployment is
universally accessible, provides a natural agentic environment, and
conveniently abstracts out the different backends from diverse device vendors.
To address this opportunity, we introduce WebLLM, an open-source JavaScript
framework that enables high-performance LLM inference entirely within web
browsers. WebLLM provides an OpenAI-style API for seamless integration into web
applications, and leverages WebGPU for efficient local GPU **acceleration** and
WebAssembly for performant CPU computation. With machine learning compilers
MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming
the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM
can retain up to 80% native performance on the same device, with room to
further close the gap. WebLLM paves the way for universally accessible,
privacy-preserving, personalized, and locally powered LLM applications in web
browsers. The code is available at: https://github.com/mlc-ai/web-llm.


## Critique of Impure Reason Unveiling the reasoning behaviour of medical Large Language Models

>Authors: Shamus Sim, Tyrone Chen

>2024-12-20

> http://arxiv.org/abs/2412.15748v1

Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, we define the concept of
reasoning behaviour in the specific context of medical LLMs. We then categorise
and discuss the current state of the art of methods which evaluate reasoning
behaviour in medical LLMs. Finally, we propose theoretical frameworks which can
empower medical professionals or machine learning engineers to gain insight
into the low-level reasoning operations of these previously obscure models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole


## Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking

>Authors: Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, Jian Yang

>2024-12-20

> http://arxiv.org/abs/2412.15691v1

Multimodal tracking has garnered widespread attention as a result of its
ability to effectively address the inherent limitations of traditional RGB
tracking. However, existing multimodal trackers mainly focus on the fusion and
enhancement of spatial features or merely leverage the **sparse** temporal
relationships between video frames. These approaches do not fully exploit the
temporal correlations in multimodal videos, making it difficult to capture the
dynamic changes and motion information of targets in complex scenarios. To
alleviate this problem, we propose a unified multimodal spatial-temporal
tracking approach named STTrack. In contrast to previous paradigms that solely
relied on updating reference information, we introduced a temporal state
generator (TSG) that continuously generates a sequence of tokens containing
multimodal temporal information. These temporal information tokens are used to
guide the localization of the target in the next time state, establish
long-range contextual relationships between video frames, and capture the
temporal trajectory of the target. Furthermore, at the spatial level, we
introduced the mamba fusion and background suppression interactive (BSI)
modules. These modules establish a dual-stage mechanism for coordinating
information interaction and fusion between modalities. Extensive comparisons on
five benchmark datasets illustrate that STTrack achieves state-of-the-art
performance across various multimodal tracking scenarios. Code is available at:
https://github.com/NJU-PCALab/STTrack.


## High-dimensional sliced inverse regression with endogeneity

>Authors: Linh H. Nghiem, Francis. K. C. Hui, Samuel Muller, A. H. Welsh

>2024-12-20

> http://arxiv.org/abs/2412.15530v1

Sliced inverse regression (SIR) is a popular sufficient dimension reduction
method that identifies a few linear transformations of the covariates without
losing regression information with the response. In high-dimensional settings,
SIR can be combined with **sparsity** penalties to achieve sufficient dimension
reduction and variable selection simultaneously. Nevertheless, both classical
and **sparse** estimators assume the covariates are exogenous. However, endogeneity
can arise in a variety of situations, such as when variables are omitted or are
measured with error. In this article, we show such endogeneity invalidates SIR
estimators, leading to inconsistent estimation of the true central subspace. To
address this challenge, we propose a two-stage Lasso SIR estimator, which first
constructs a **sparse** high-dimensional instrumental variables model to obtain
fitted values of the covariates spanned by the instruments, and then applies
SIR augmented with a Lasso penalty on these fitted values. We establish
theoretical bounds for the estimation and selection consistency of the true
central subspace for the proposed estimators, allowing the number of covariates
and instruments to grow exponentially with the sample size. Simulation studies
and applications to two real-world datasets in nutrition and genetics
illustrate the superior empirical performance of the two-stage Lasso SIR
estimator compared with existing methods that disregard endogeneity and/or
nonlinearity in the outcome model.


## SGTC Semantic-Guided Triplet Co-training for Sparsely Annotated Semi-Supervised Medical Image Segmentation

>Authors: Ke Yan, Qing Cai, Fan Zhang, Ziyan Cao, Zhi Liu

>2024-12-20

> http://arxiv.org/abs/2412.15526v1

Although semi-supervised learning has made significant advances in the field
of medical image segmentation, fully annotating a volumetric sample slice by
slice remains a costly and time-consuming task. Even worse, most of the
existing approaches pay much attention to image-level information and ignore
semantic features, resulting in the inability to perceive weak boundaries. To
address these issues, we propose a novel Semantic-Guided Triplet Co-training
(SGTC) framework, which achieves high-end medical image segmentation by only
annotating three orthogonal slices of a few volumetric samples, significantly
alleviating the burden of radiologists. Our method consist of two main
components. Specifically, to enable semantic-aware, fine-granular segmentation
and enhance the quality of pseudo-labels, a novel semantic-guided auxiliary
learning mechanism is proposed based on the pretrained CLIP. In addition,
focusing on a more challenging but clinically realistic scenario, a new
triple-view disparity training strategy is proposed, which uses **sparse**
annotations (i.e., only three labeled slices of a few volumes) to perform
co-training between three sub-networks, significantly improving the robustness.
Extensive experiments on three public medical datasets demonstrate that our
method outperforms most state-of-the-art semi-supervised counterparts under
**sparse** annotation settings. The source code is available at
https://github.com/xmeimeimei/SGTC.


## PreNeT Leveraging Computational Features to Predict Deep Neural Network Training Time

>Authors: Alireza Pourali, Arian Boukani, Hamzeh Khazaei

>2024-12-20

> http://arxiv.org/abs/2412.15519v1

Training deep learning models, particularly Transformer-based architectures
such as Large Language Models (LLMs), demands substantial computational
resources and extended training periods. While optimal configuration and
infrastructure selection can significantly reduce associated costs, this
optimization requires preliminary analysis tools. This paper introduces PreNeT,
a novel predictive framework designed to address this optimization challenge.
PreNeT facilitates training optimization by integrating comprehensive
computational metrics, including layer-specific parameters, arithmetic
operations and memory utilization. A key feature of PreNeT is its capacity to
accurately predict training duration on previously unexamined hardware
infrastructures, including novel accelerator architectures. This framework
employs a sophisticated approach to capture and analyze the distinct
characteristics of various neural network layers, thereby enhancing existing
prediction methodologies. Through proactive implementation of PreNeT,
researchers and practitioners can determine optimal configurations, parameter
settings, and hardware specifications to maximize cost-efficiency and minimize
training duration. Experimental results demonstrate that PreNeT achieves up to
72% improvement in prediction accuracy compared to contemporary
state-of-the-art frameworks.

