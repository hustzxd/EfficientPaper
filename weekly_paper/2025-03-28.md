# 2025-03-28

# Table of Contents
* [Vision as LoRA](#Vision-as-LoRA)
* [SaViD Spectravista Aesthetic Vision Integration for Robust and Discerning 3D Object Detection in Challenging Environments](#SaViD-Spectravista-Aesthetic-Vision-Integration-for-Robust-and-Discerning-3D-Object-Detection-in-Challenging-Environments)
* [Beyond Intermediate States Explaining Visual Redundancy through Language](#Beyond-Intermediate-States-Explaining-Visual-Redundancy-through-Language)
* [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](#Accelerate-Parallelizable-Reasoning-via-Parallel-Decoding-within-One-Sequence)
* [MAR-3D Progressive Masked Auto-regressor for High-Resolution 3D Generation](#MAR-3D-Progressive-Masked-Auto-regressor-for-High-Resolution-3D-Generation)
* [Lipschitz Constant Meets Condition Number Learning Robust and Compact Deep Neural Networks](#Lipschitz-Constant-Meets-Condition-Number-Learning-Robust-and-Compact-Deep-Neural-Networks)
* [Latent Beam Diffusion Models for Decoding Image Sequences](#Latent-Beam-Diffusion-Models-for-Decoding-Image-Sequences)
* [Comparative analysis and evaluation of ageing forecasting methods for semiconductor devices in online health monitoring](#Comparative-analysis-and-evaluation-of-ageing-forecasting-methods-for-semiconductor-devices-in-online-health-monitoring)
* [FastFT Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies](#FastFT-Accelerating-Reinforced-Feature-Transformation-via-Advanced-Exploration-Strategies)
* [MoLe-VLA Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation](#MoLe-VLA-Dynamic-Layer-skipping-Vision-Language-Action-Model-via-Mixture-of-Layers-for-Efficient-Robot-Manipulation)
* [Self-ReS Self-Reflection in Large Vision-Language Models for Long Video Understanding](#Self-ReS-Self-Reflection-in-Large-Vision-Language-Models-for-Long-Video-Understanding)
* [Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization](#Bridging-Evolutionary-Multiobjective-Optimization-and-GPU-Acceleration-via-Tensorization)
* [Design of Macroscale Optical Systems with Metaoptics Using Transformer-Based Neural Networks](#Design-of-Macroscale-Optical-Systems-with-Metaoptics-Using-Transformer-Based-Neural-Networks)
* [RxRx3-core Benchmarking drug-target interactions in High-Content Microscopy](#RxRx3-core-Benchmarking-drug-target-interactions-in-High-Content-Microscopy)
* [Innovative LSGTime Model for Crime Spatiotemporal Prediction Based on MindSpore Framework](#Innovative-LSGTime-Model-for-Crime-Spatiotemporal-Prediction-Based-on-MindSpore-Framework)
* [Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations](#Can-We-Make-Code-Green?-Understanding-Trade-Offs-in-LLMs-vs.-Human-Code-Optimizations)
* [Developing a Complete AI-Accelerated Workflow for Superconductor Discovery](#Developing-a-Complete-AI-Accelerated-Workflow-for-Superconductor-Discovery)
* [Scaling Vision Pre-Training to 4K Resolution](#Scaling-Vision-Pre-Training-to-4K-Resolution)
* [A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design](#A-Multi-Agent-Framework-Integrating-Large-Language-Models-and-Generative-AI-for-Accelerated-Metamaterial-Design)
* [GENIUS A Generative Framework for Universal Multimodal Search](#GENIUS-A-Generative-Framework-for-Universal-Multimodal-Search)
* [LogQuant Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation](#LogQuant-Log-Distributed-2-Bit-Quantization-of-KV-Cache-with-Superior-Accuracy-Preservation)
* [Gemma 3 Technical Report](#Gemma-3-Technical-Report)
* [An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators](#An-Efficient-Data-Reuse-with-Tile-Based-Adaptive-Stationary-for-Transformer-Accelerators)
* [SINR Sparsity Driven Compressed Implicit Neural Representations](#SINR-Sparsity-Driven-Compressed-Implicit-Neural-Representations)
* [KSHSeek Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models](#KSHSeek-Data-Driven-Approaches-to-Mitigating-and-Detecting-Knowledge-Shortcut-Hallucinations-in-Generative-Models)
* [QUAD Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition](#QUAD-Quantization-and-Parameter-Efficient-Tuning-of-LLM-with-Activation-Decomposition)
* [Membership Inference Attacks on Large-Scale Models A Survey](#Membership-Inference-Attacks-on-Large-Scale-Models-A-Survey)
* [AI-Driven Defect Engineering for Advanced Thermoelectric Materials](#AI-Driven-Defect-Engineering-for-Advanced-Thermoelectric-Materials)
* [Joint Sparse Graph for Enhanced MIMO-AFDM Receiver Design](#Joint-Sparse-Graph-for-Enhanced-MIMO-AFDM-Receiver-Design)
* [Numerical evaluation of the integrals of motion in particle accelerator tracking codes](#Numerical-evaluation-of-the-integrals-of-motion-in-particle-accelerator-tracking-codes)
* [Rank-Based Modeling for Universal Packets Compression in Multi-Modal Communications](#Rank-Based-Modeling-for-Universal-Packets-Compression-in-Multi-Modal-Communications)
* [Detecting Arbitrary Planted Subgraphs in Random Graphs](#Detecting-Arbitrary-Planted-Subgraphs-in-Random-Graphs)
* [Video-T1 Test-Time Scaling for Video Generation](#Video-T1-Test-Time-Scaling-for-Video-Generation)
* [Training-free Diffusion Acceleration with Bottleneck Sampling](#Training-free-Diffusion-Acceleration-with-Bottleneck-Sampling)
* [Trajectory Balance with Asynchrony Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training](#Trajectory-Balance-with-Asynchrony-Decoupling-Exploration-and-Learning-for-Fast,-Scalable-LLM-Post-Training)
* [FFN Fusion Rethinking Sequential Computation in Large Language Models](#FFN-Fusion-Rethinking-Sequential-Computation-in-Large-Language-Models)
* [xKV Cross-Layer SVD for KV-Cache Compression](#xKV-Cross-Layer-SVD-for-KV-Cache-Compression)
* [I Have Covered All the Bases Here Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](#I-Have-Covered-All-the-Bases-Here-Interpreting-Reasoning-Features-in-Large-Language-Models-via-Sparse-Autoencoders)
* [Reimagining Memory Access for LLM Inference Compression-Aware Memory Controller Design](#Reimagining-Memory-Access-for-LLM-Inference-Compression-Aware-Memory-Controller-Design)
* [Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation](#Exploring-the-Integration-of-Key-Value-Attention-Into-Pure-and-Hybrid-Transformers-for-Semantic-Segmentation)
* [Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction](#Dual-domain-Multi-path-Self-supervised-Diffusion-Model-for-Accelerated-MRI-Reconstruction)
* [BitDecoding Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache](#BitDecoding-Unlocking-Tensor-Cores-for-Long-Context-LLMs-Decoding-with-Low-Bit-KV-Cache)
* [Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings](#Boosting-Resolution-Generalization-of-Diffusion-Transformers-with-Randomized-Positional-Encodings)
* [Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology](#Revisiting-Automatic-Data-Curation-for-Vision-Foundation-Models-in-Digital-Pathology)
* [A Comprehensive Review on Hashtag Recommendation From Traditional to Deep Learning and Beyond](#A-Comprehensive-Review-on-Hashtag-Recommendation-From-Traditional-to-Deep-Learning-and-Beyond)
* [Oaken Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization](#Oaken-Fast-and-Efficient-LLM-Serving-with-Online-Offline-Hybrid-KV-Cache-Quantization)
* [AMD-Hummingbird Towards an Efficient Text-to-Video Model](#AMD-Hummingbird-Towards-an-Efficient-Text-to-Video-Model)
* [U-REPA Aligning Diffusion U-Nets to ViTs](#U-REPA-Aligning-Diffusion-U-Nets-to-ViTs)
* [Maximum Redundancy Pruning A Principle-Driven Layerwise Sparsity Allocation for LLMs](#Maximum-Redundancy-Pruning-A-Principle-Driven-Layerwise-Sparsity-Allocation-for-LLMs)
* [TopV Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](#TopV-Compatible-Token-Pruning-with-Inference-Time-Optimization-for-Fast-and-Low-Memory-Multimodal-Vision-Language-Model)
* [Adaptive Rank Allocation Speeding Up Modern Transformers with RaNA Adapters](#Adaptive-Rank-Allocation-Speeding-Up-Modern-Transformers-with-RaNA-Adapters)
* [Can news and social media attention reduce the influence of problematic research?](#Can-news-and-social-media-attention-reduce-the-influence-of-problematic-research?)
* [AgentRxiv Towards Collaborative Autonomous Research](#AgentRxiv-Towards-Collaborative-Autonomous-Research)
* [Temporal Relation Extraction in Clinical Texts A Span-based Graph Transformer Approach](#Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach)
* [Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension](#Investigating-Recent-Large-Language-Models-for-Vietnamese-Machine-Reading-Comprehension)
* [WindowKV Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference](#WindowKV-Task-Adaptive-Group-Wise-KV-Cache-Window-Selection-for-Efficient-LLM-Inference)
* [Accelerating and enhancing thermodynamic simulations of electrochemical interfaces](#Accelerating-and-enhancing-thermodynamic-simulations-of-electrochemical-interfaces)
* [Feather-SQL A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models](#Feather-SQL-A-Lightweight-NL2SQL-Framework-with-Dual-Model-Collaboration-Paradigm-for-Small-Language-Models)
* [Machine Learning - Driven Materials Discovery Unlocking Next-Generation Functional Materials - A minireview](#Machine-Learning---Driven-Materials-Discovery-Unlocking-Next-Generation-Functional-Materials---A-minireview)
* [Energy-Aware LLMs A step towards sustainable AI for downstream applications](#Energy-Aware-LLMs-A-step-towards-sustainable-AI-for-downstream-applications)
* [Normalized Matching Transformer](#Normalized-Matching-Transformer)
* [Causal Inference based Transfer Learning with LLMs An Efficient Framework for Industrial RUL Prediction](#Causal-Inference-based-Transfer-Learning-with-LLMs-An-Efficient-Framework-for-Industrial-RUL-Prediction)
* [Autonomous Radiotherapy Treatment Planning Using DOLA A Privacy-Preserving, LLM-Based Optimization Agent](#Autonomous-Radiotherapy-Treatment-Planning-Using-DOLA-A-Privacy-Preserving,-LLM-Based-Optimization-Agent)
* [Improving Quantization with Post-Training Model Expansion](#Improving-Quantization-with-Post-Training-Model-Expansion)
* [Variance Control via Weight Rescaling in LLM Pre-training](#Variance-Control-via-Weight-Rescaling-in-LLM-Pre-training)
* [Efficient Knowledge Distillation via Curriculum Extraction](#Efficient-Knowledge-Distillation-via-Curriculum-Extraction)
* [Pow3R Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors](#Pow3R-Empowering-Unconstrained-3D-Reconstruction-with-Camera-and-Scene-Priors)
* [Bugdar AI-Augmented Secure Code Review for GitHub Pull Requests](#Bugdar-AI-Augmented-Secure-Code-Review-for-GitHub-Pull-Requests)
* [Token Dynamics Towards Efficient and Dynamic Video Token Representation for Video Large Language Models](#Token-Dynamics-Towards-Efficient-and-Dynamic-Video-Token-Representation-for-Video-Large-Language-Models)
* [Rankformer A Graph Transformer for Recommendation based on Ranking Objective](#Rankformer-A-Graph-Transformer-for-Recommendation-based-on-Ranking-Objective)
* [Design of 3D Non-Cartesian Trajectories for Fast Volumetric MRI via Analytic Coordinate Discretization](#Design-of-3D-Non-Cartesian-Trajectories-for-Fast-Volumetric-MRI-via-Analytic-Coordinate-Discretization)
* [Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation](#Federated-Cross-Domain-Click-Through-Rate-Prediction-With-Large-Language-Model-Augmentation)
* [Towards LLM Guardrails via Sparse Representation Steering](#Towards-LLM-Guardrails-via-Sparse-Representation-Steering)
* [Multi-property directed generative design of inorganic materials through Wyckoff-augmented transfer learning](#Multi-property-directed-generative-design-of-inorganic-materials-through-Wyckoff-augmented-transfer-learning)


## Vision as LoRA

>Authors: Han Wang, Yongjie Ye, Bingru Li, Yuxiang Nie, Jinghui Lu, Jingqun Tang, Yanjie Wang, Can Huang

>2025-03-26

> http://arxiv.org/abs/2503.20680v1

We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM
into an MLLM. Unlike prevalent MLLM architectures that rely on external vision
modules for vision encoding, VoRA internalizes visual capabilities by
integrating vision-specific LoRA layers directly into the LLM. This design
allows the added parameters to be seamlessly merged into the LLM during
inference, eliminating structural complexity and minimizing computational
overhead. Moreover, inheriting the LLM's ability of handling flexible context,
VoRA can process inputs at arbitrary resolutions.
  To further strengthen VoRA's visual capabilities, we introduce a block-wise
distillation method that transfers visual priors from a pre-trained ViT into
the LoRA layers, effectively accelerating training by injecting visual
knowledge. Additionally, we apply bi-directional attention masks to better
capture the context information of an image. We successfully demonstrate that
with additional pre-training data, VoRA can perform comparably with
conventional encode-based MLLMs. All training data, codes, and model weights
will be released at https://github.com/Hon-Wong/VoRA.


## SaViD Spectravista Aesthetic Vision Integration for Robust and Discerning 3D Object Detection in Challenging Environments

>Authors: Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith, Aniruddha Maiti, Supriyo Chakraborty, Mir Feroskhan

>2025-03-26

> http://arxiv.org/abs/2503.20614v1

The fusion of LiDAR and camera sensors has demonstrated significant
effectiveness in achieving accurate detection for short-range tasks in
autonomous driving. However, this fusion approach could face challenges when
dealing with long-range detection scenarios due to disparity between **sparsity**
of LiDAR and high-resolution camera data. Moreover, sensor corruption
introduces complexities that affect the ability to maintain robustness, despite
the growing adoption of sensor fusion in this domain. We present SaViD, a novel
framework comprised of a three-stage fusion alignment mechanism designed to
address long-range detection challenges in the presence of natural corruption.
The SaViD framework consists of three key elements: the Global Memory Attention
Network (GMAN), which enhances the extraction of image features through
offering a deeper understanding of global patterns; the Attentional Sparse
Memory Network (ASMN), which enhances the integration of LiDAR and image
features; and the KNNnectivity Graph Fusion (KGF), which enables the entire
fusion of spatial information. SaViD achieves superior performance on the
long-range detection Argoverse-2 (AV2) dataset with a performance improvement
of 9.87% in AP value and an improvement of 2.39% in mAPH for L2 difficulties on
the Waymo Open dataset (WOD). Comprehensive experiments are carried out to
showcase its robustness against 14 natural sensor corruptions. SaViD exhibits a
robust performance improvement of 31.43% for AV2 and 16.13% for WOD in RCE
value compared to other existing fusion-based methods while considering all the
corruptions for both datasets. Our code is available at
\href{https://github.com/sanjay-810/SAVID}


## Beyond Intermediate States Explaining Visual Redundancy through Language

>Authors: Dingchen Yang, Bowen Cao, Anran Zhang, Weibo Gu, Winston Hu, Guang Chen

>2025-03-26

> http://arxiv.org/abs/2503.20540v1

Multi-modal Large Langue Models (MLLMs) often process thousands of visual
tokens, which consume a significant portion of the context window and impose a
substantial computational burden. Prior work has empirically explored visual
token **pruning** methods based on MLLMs' intermediate states (e.g., attention
scores). However, they have limitations in precisely defining visual redundancy
due to their inability to capture the influence of visual tokens on MLLMs'
visual understanding (i.e., the predicted probabilities for textual token
candidates). To address this issue, we manipulate the visual input and
investigate variations in the textual output from both token-centric and
context-centric perspectives, achieving intuitive and comprehensive analysis.
Experimental results reveal that visual tokens with low ViT-[cls] association
and low text-to-image attention scores can contain recognizable information and
significantly contribute to images' overall information. To develop a more
reliable method for identifying and **pruning** redundant visual tokens, we
integrate these two perspectives and introduce a context-independent condition
to identify redundant prototypes from training images, which probes the
redundancy of each visual token during inference. Extensive experiments on
single-image, multi-image and video comprehension tasks demonstrate the
effectiveness of our method, notably achieving 90% to 110% of the performance
while **pruning** 80% to 90% of visual tokens.


## Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence

>Authors: Yijiong Yu

>2025-03-26

> http://arxiv.org/abs/2503.20533v1

Recent advances in reasoning models have demonstrated significant
improvements in accuracy, particularly for complex tasks such as mathematical
reasoning, by employing detailed and comprehensive reasoning processes.
However, generating these lengthy reasoning sequences is computationally
expensive and time-consuming. To address this inefficiency, we leverage the
inherent parallelizability of certain tasks to accelerate the reasoning
process. Specifically, when multiple parallel reasoning branches exist, we
decode multiple tokens per step using a specialized attention mask, processing
them within a single sequence. Experimental results show that our method
achieves over 100% speedup in decoding time while basically maintaining
accuracy.


## MAR-3D Progressive Masked Auto-regressor for High-Resolution 3D Generation

>Authors: Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, Gim Hee Lee

>2025-03-26

> http://arxiv.org/abs/2503.20519v2

Recent advances in auto-regressive transformers have revolutionized
generative modeling across different domains, from language processing to
visual generation, demonstrating remarkable capabilities. However, applying
these advances to 3D generation presents three key challenges: the unordered
nature of 3D data conflicts with sequential next-token prediction paradigm,
conventional vector **quantization** approaches incur substantial compression loss
when applied to 3D meshes, and the lack of efficient scaling strategies for
higher resolution latent prediction. To address these challenges, we introduce
MAR-3D, which integrates a pyramid variational autoencoder with a cascaded
masked auto-regressive transformer (Cascaded MAR) for progressive latent
upscaling in the continuous space. Our architecture employs random masking
during training and auto-regressive denoising in random order during inference,
naturally accommodating the unordered property of 3D latent tokens.
Additionally, we propose a cascaded training strategy with condition
augmentation that enables efficiently up-scale the latent token resolution with
fast convergence. Extensive experiments demonstrate that MAR-3D not only
achieves superior performance and generalization capabilities compared to
existing methods but also exhibits enhanced scaling capabilities compared to
joint distribution modeling approaches (e.g., diffusion transformers).


## Lipschitz Constant Meets Condition Number Learning Robust and Compact Deep Neural Networks

>Authors: Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, Xian Wei

>2025-03-26

> http://arxiv.org/abs/2503.20454v1

Recent research has revealed that high compression of Deep Neural Networks
(DNNs), e.g., massive **pruning** of the weight matrix of a DNN, leads to a severe
drop in accuracy and susceptibility to adversarial attacks. Integration of
network **pruning** into an adversarial training framework has been proposed to
promote adversarial robustness. It has been observed that a highly pruned
weight matrix tends to be ill-conditioned, i.e., increasing the condition
number of the weight matrix. This phenomenon aggravates the vulnerability of a
DNN to input noise. Although a highly pruned weight matrix is considered to be
able to lower the upper bound of the local Lipschitz constant to tolerate large
distortion, the ill-conditionedness of such a weight matrix results in a
non-robust DNN model. To overcome this challenge, this work develops novel
joint constraints to adjust the weight distribution of networks, namely, the
Transformed Sparse Constraint joint with Condition Number Constraint (TSCNC),
which copes with smoothing distribution and differentiable constraint functions
to reduce condition number and thus avoid the ill-conditionedness of weight
matrices. Furthermore, our theoretical analyses unveil the relevance between
the condition number and the local Lipschitz constant of the weight matrix,
namely, the sharply increasing condition number becomes the dominant factor
that restricts the robustness of over-sparsified models. Extensive experiments
are conducted on several public datasets, and the results show that the
proposed constraints significantly improve the robustness of a DNN with high
**pruning** rates.


## Latent Beam Diffusion Models for Decoding Image Sequences

>Authors: Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, João Magalhães

>2025-03-26

> http://arxiv.org/abs/2503.20429v1

While diffusion models excel at generating high-quality images from text
prompts, they struggle with visual consistency in image sequences. Existing
methods generate each image independently, leading to disjointed narratives - a
challenge further exacerbated in non-linear storytelling, where scenes must
connect beyond adjacent frames. We introduce a novel beam search strategy for
latent space exploration, enabling conditional generation of full image
sequences with beam search decoding. Unlike prior approaches that use fixed
latent priors, our method dynamically searches for an optimal sequence of
latent representations, ensuring coherent visual transitions. To address beam
search's quadratic complexity, we integrate a cross-attention mechanism that
efficiently scores search paths and enables **pruning**, prioritizing alignment
with both textual prompts and visual context. Human evaluations confirm that
our approach outperforms baseline methods, producing full sequences with
superior coherence, visual continuity, and textual alignment. By bridging
advances in search optimization and latent space refinement, this work sets a
new standard for structured image sequence generation.


## Comparative analysis and evaluation of ageing forecasting methods for semiconductor devices in online health monitoring

>Authors: Adrian Villalobos, Iban Barrutia, Rafael Pena-Alzola, Tomislav Dragicevic, Jose I. Aizpurua

>2025-03-26

> http://arxiv.org/abs/2503.20403v1

Semiconductor devices, especially MOSFETs (Metal-oxide-semiconductor
field-effect transistor), are crucial in power electronics, but their
reliability is affected by aging processes influenced by cycling and
temperature. The primary aging mechanism in discrete semiconductors and power
modules is the bond wire lift-off, caused by crack growth due to thermal
fatigue. The process is empirically characterized by exponential growth and an
abrupt end of life, making long-term aging forecasts challenging. This research
presents a comprehensive comparative assessment of different forecasting
methods for MOSFET failure forecasting applications. Classical tracking,
statistical forecasting and Neural Network (NN) based forecasting models are
implemented along with novel Temporal Fusion Transformers (TFTs). A
comprehensive comparison is performed assessing their MOSFET ageing forecasting
ability for different forecasting horizons. For short-term predictions, all
algorithms result in acceptable results, with the best results produced by
classical NN forecasting models at the expense of higher computations. For
long-term forecasting, only the TFT is able to produce valid outcomes owing to
the ability to integrate covariates from the expected future conditions.
Additionally, TFT attention points identify key ageing turning points, which
indicate new failure modes or accelerated ageing phases.


## FastFT Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies

>Authors: Tianqi He, Xiaohan Huang, Yi Du, Qingqing Long, Ziyue Qiao, Min Wu, Yanjie Fu, Yuanchun Zhou, Meng Xiao

>2025-03-26

> http://arxiv.org/abs/2503.20394v1

Feature Transformation is crucial for classic machine learning that aims to
generate feature combinations to enhance the performance of downstream tasks
from a data-centric perspective. Current methodologies, such as manual
expert-driven processes, iterative-feedback techniques, and
exploration-generative tactics, have shown promise in automating such data
engineering workflow by minimizing human involvement. However, three challenges
remain in those frameworks: (1) It predominantly depends on downstream task
performance metrics, as assessment is time-consuming, especially for large
datasets. (2) The diversity of feature combinations will hardly be guaranteed
after random exploration ends. (3) Rare significant transformations lead to
**sparse** valuable feedback that hinders the learning processes or leads to less
effective results. In response to these challenges, we introduce FastFT, an
innovative framework that leverages a trio of advanced strategies.We first
decouple the feature transformation evaluation from the outcomes of the
generated datasets via the performance predictor. To address the issue of
reward **sparsity**, we developed a method to evaluate the novelty of generated
transformation sequences. Incorporating this novelty into the reward function
accelerates the model's exploration of effective transformations, thereby
improving the search productivity. Additionally, we combine novelty and
performance to create a prioritized memory buffer, ensuring that essential
experiences are effectively revisited during exploration. Our extensive
experimental evaluations validate the performance, efficiency, and traceability
of our proposed framework, showcasing its superiority in handling complex
feature transformation tasks.


## MoLe-VLA Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation

>Authors: Rongyu Zhang, Menghang Dong, Yuan Zhang, Liang Heng, Xiaowei Chi, Gaole Dai, Li Du, Dan Wang, Yuan Du, Shanghang Zhang

>2025-03-26

> http://arxiv.org/abs/2503.20384v1

Multimodal Large Language Models (MLLMs) excel in understanding complex
language and visual data, enabling generalist robotic systems to interpret
instructions and perform embodied tasks. Nevertheless, their real-world
deployment is hindered by substantial computational and storage demands. Recent
insights into the homogeneous patterns in the LLM layer have inspired
sparsification techniques to address these challenges, such as early exit and
token **pruning**. However, these methods often neglect the critical role of the
final layers that encode the semantic information most relevant to downstream
robotic tasks. Aligning with the recent breakthrough of the Shallow Brain
Hypothesis (SBH) in neuroscience and the mixture of experts in model
sparsification, we conceptualize each LLM layer as an expert and propose a
Mixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)
architecture for dynamic LLM layer activation. We introduce a Spatial-Temporal
Aware Router (STAR) for MoLe to selectively activate only parts of the layers
based on the robot's current state, mimicking the brain's distinct signal
pathways specialized for cognition and causal reasoning. Additionally, to
compensate for the cognitive ability of LLMs lost in MoLe, we devise a
Cognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the
understanding of task demands and improves the generation of task-relevant
action sequences by leveraging cognitive features. Extensive experiments
conducted in both RLBench simulation and real-world environments demonstrate
the superiority of MoLe-VLA in both efficiency and performance. Specifically,
MoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks
while reducing computational costs by up to x5.6 compared to standard LLMs.


## Self-ReS Self-Reflection in Large Vision-Language Models for Long Video Understanding

>Authors: Joao Pereira, Vasco Lopes, David Semedo, Joao Neves

>2025-03-26

> http://arxiv.org/abs/2503.20362v1

Large Vision-Language Models (LVLMs) demonstrate remarkable performance in
short-video tasks such as video question answering, but struggle in long-video
understanding. The linear frame sampling strategy, conventionally used by
LVLMs, fails to account for the non-linear distribution of key events in video
data, often introducing redundant or irrelevant information in longer contexts
while risking the omission of critical events in shorter ones. To address this,
we propose SelfReS, a non-linear spatiotemporal self-reflective sampling method
that dynamically selects key video fragments based on user prompts. Unlike
prior approaches, SelfReS leverages the inherently **sparse** attention maps of
LVLMs to define reflection tokens, enabling relevance-aware token selection
without requiring additional training or external modules. Experiments
demonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,
improving long-video task accuracy and achieving up to 46% faster inference
speed within the same GPU memory budget.


## Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization

>Authors: Zhenyu Liang, Hao Li, Naiwei Yu, Kebin Sun, Ran Cheng

>2025-03-26

> http://arxiv.org/abs/2503.20286v2

Evolutionary multiobjective optimization (EMO) has made significant strides
over the past two decades. However, as problem scales and complexities
increase, traditional EMO algorithms face substantial performance limitations
due to insufficient parallelism and scalability. While most work has focused on
algorithm design to address these challenges, little attention has been given
to hardware **acceleration**, thereby leaving a clear gap between EMO algorithms
and advanced computing devices, such as GPUs. To bridge the gap, we propose to
parallelize EMO algorithms on GPUs via the tensorization methodology. By
employing tensorization, the data structures and operations of EMO algorithms
are transformed into concise tensor representations, which seamlessly enables
automatic utilization of GPU computing. We demonstrate the effectiveness of our
approach by applying it to three representative EMO algorithms: NSGA-III,
MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a
multiobjective robot control benchmark using a GPU-accelerated physics engine.
Our experiments show that the tensorized EMO algorithms achieve speedups of up
to 1113x compared to their CPU-based counterparts, while maintaining solution
quality and effectively scaling population sizes to hundreds of thousands.
Furthermore, the tensorized EMO algorithms efficiently tackle complex
multiobjective robot control tasks, producing high-quality solutions with
diverse behaviors. Source codes are available at
https://github.com/EMI-Group/evomo.


## Design of Macroscale Optical Systems with Metaoptics Using Transformer-Based Neural Networks

>Authors: Ryan C. Ng, Stéphane Larouche, Peter Y. Schneider, Aditi Munshi, Robert Bedford, Philip W. Hon, Katherine T. Fountaine

>2025-03-26

> http://arxiv.org/abs/2503.20159v1

Metaoptics are thin, planar surfaces consisting of many subwavelength optical
resonators that can be designed to simultaneously control the amplitude, phase,
and polarization to arbitrarily shape an optical wavefront much in the same
manner as a traditional lens but with a much smaller form factor. The
incorporation of metaoptics into a conventional optical system spans multiple
length scales between that of the individual metaoptic elements (< {\lambda})
and that of the entire size of the optic (>> {\lambda}), making computational
techniques that accurately simulate the optical response of metaoptics
computationally intractable, while more efficient techniques utilizing various
approximations suffer from inaccuracies in their prediction of the optical
response. To overcome the trade between speed and accuracy, we implement a
transformer-based neural network solver to calculate the optical response of
metaoptics and combine it with commercial ray optics software incorporating
Fourier propagation methods to simulate an entire optical system. We
demonstrate that this neural net method is more than 3 orders of magnitude
faster than a traditional finite-difference time domain method, with only a
0.47 % deviation in total irradiance when compared with a full wave simulation,
which is nearly 2 orders of magnitude more accurate than standard approximation
methods for metaoptics. The ability to accurately and efficiently predict the
optical response of a metaoptic could enable their optimization, further
accelerating and facilitating their application.


## RxRx3-core Benchmarking drug-target interactions in High-Content Microscopy

>Authors: Oren Kraus, Federico Comitani, John Urbanik, Kian Kenyon-Dean, Lakshmanan Arumugam, Saber Saberian, Cas Wognum, Safiye Celik, Imran S. Haque

>2025-03-26

> http://arxiv.org/abs/2503.20158v1

High Content Screening (HCS) microscopy datasets have transformed the ability
to profile cellular responses to genetic and chemical perturbations, enabling
cell-based inference of drug-target interactions (DTI). However, the adoption
of representation learning methods for HCS data has been hindered by the lack
of accessible datasets and robust benchmarks. To address this gap, we present
RxRx3-core, a curated and compressed subset of the RxRx3 dataset, and an
associated DTI benchmarking task. At just 18GB, RxRx3-core significantly
reduces the size barrier associated with large-scale HCS datasets while
preserving critical data necessary for benchmarking representation learning
models against a zero-shot DTI prediction task. RxRx3-core includes 222,601
microscopy images spanning 736 CRISPR knockouts and 1,674 compounds at 8
concentrations. RxRx3-core is available on HuggingFace and Polaris, along with
pre-trained embeddings and benchmarking code, ensuring accessibility for the
research community. By providing a compact dataset and robust benchmarks, we
aim to accelerate innovation in representation learning methods for HCS data
and support the discovery of novel biological insights.


## Innovative LSGTime Model for Crime Spatiotemporal Prediction Based on MindSpore Framework

>Authors: Zhenkai Qin, Weibao Zhong, Caifeng Gao

>2025-03-26

> http://arxiv.org/abs/2503.20136v1

With the **acceleration** of urbanization, the spatiotemporal characteristics of
criminal activities have become increasingly complex. Accurate prediction of
crime distribution is crucial for optimizing the allocation of police resources
and preventing crime. This paper proposes LGSTime, a crime spatiotemporal
prediction model that integrates Long Short-Term Memory (LSTM), Gated Recurrent
Unit (GRU), and the Multi-head Sparse Self-attention mechanism. LSTM and GRU
capture long-term dependencies in crime time series, such as seasonality and
periodicity, through their unique gating mechanisms. The Multi-head Sparse
Self-attention mechanism, on the other hand, focuses on both temporal and
spatial features of criminal events simultaneously through parallel processing
and sparsification techniques, significantly improving computational efficiency
and prediction accuracy. The integrated model leverages the strengths of each
technique to better handle complex spatiotemporal data. Experimental findings
demonstrate that the model attains optimal performance across four real - world
crime datasets. In comparison to the CNN model, it exhibits performance
enhancements of 2.8\%, 1.9\%, and 1.4\% in the Mean Squared Error (MSE), Mean
Absolute Error (MAE), and Root Mean Squared Error (RMSE) metrics respectively.
These results offer a valuable reference for tackling the challenges in crime
prediction.


## Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations

>Authors: Pooja Rani, Jan-Andrea Bard, June Sallou, Alexander Boll, Timo Kehrer, Alberto Bacchelli

>2025-03-26

> http://arxiv.org/abs/2503.20126v1

The rapid technological evolution has accelerated software development for
various domains and use cases, contributing to a growing share of global carbon
emissions. While recent large language models (LLMs) claim to assist developers
in optimizing code for performance and energy efficiency, their efficacy in
real-world scenarios remains under exploration. In this work, we explore the
effectiveness of LLMs in reducing the environmental footprint of real-world
projects, focusing on software written in Matlab-widely used in both academia
and industry for scientific and engineering applications. We analyze
energy-focused optimization on 400 scripts across 100 top GitHub repositories.
We examine potential 2,176 optimizations recommended by leading LLMs, such as
GPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy
consumption, memory usage, execution time consumption, and code correctness.
The developer serves as a real-world baseline for comparing typical human and
LLM-generated optimizations.
  Mapping these optimizations to 13 high-level themes, we found that LLMs
propose a broad spectrum of improvements--beyond energy efficiency--including
improving code readability and maintainability, memory management, error
handling while the developer overlooked some parallel processing, error
handling etc. However, our statistical tests reveal that the energy-focused
optimizations unexpectedly negatively impacted memory usage, with no clear
benefits regarding execution time or energy consumption. Our qualitative
analysis of energy-time trade-offs revealed that some themes, such as
vectorization preallocation, were among the common themes shaping these
trade-offs. With LLMs becoming ubiquitous in modern software development, our
study serves as a call to action: prioritizing the evaluation of common coding
practices to identify the green ones.


## Developing a Complete AI-Accelerated Workflow for Superconductor Discovery

>Authors: Jason B. Gibson, Ajinkya C. Hire, Philip M. Dee, Benjamin Geisler, Jung Soo Kim, Zhongwei Li, James J. Hamlin, Gregory R. Stewart, P. J. Hirschfeld, Richard G. Hennig

>2025-03-25

> http://arxiv.org/abs/2503.20005v1

The evolution of materials discovery has continually transformed, progressing
from empirical experimentation to virtual high-throughput screening, which
leverages computational techniques to fully characterize a material before
synthesis. Despite the successes of these approaches, significant bottlenecks
remain due to the high computational cost of density functional theory (DFT)
calculations required to determine the thermodynamic and dynamic stability of a
material and its functional properties. In particular, discovering new
superconductors is massively impeded by the cost of computing the
electron-phonon spectral functions, which limits the feasible materials space.
Recent advances in machine learning offer an opportunity to accelerate the
superconductor discovery workflow by developing machine-learning-based
surrogates for DFT. Here we present a Bootstrapped Ensemble of Equivariant
Graph Neural Networks (BEE-NET), a ML model that predicts the Eliashberg
spectral function achieves a test mean absolute error of 0.87 K for the
superconducting critical temperature ($T_c$), relative to predictions of the
Allen-Dynes equation using DFT-derived spectral functions. BEE-NET
simultaneously identifies candidate structures with $T_c > 5$ K at a precision
of 86% and a true negative rate of 99.4%. Combined with elemental substitution
and ML interatomic potentials, this models puts us in position to develop a
complete AI-accelerated workflow to identify novel superconductors. The
workflow achieved 87% precision, narrowing 1.3 million candidates to 741 stable
compounds with DFT-confirmed $T_c > 5$ K. We report the prediction and
successful experimental synthesis, characterization, and verification of two
novel superconductors. This work exemplifies the potential of integrating
machine learning, computational methods, and experimental techniques to
revolutionize the field of materials discovery.


## Scaling Vision Pre-Training to 4K Resolution

>Authors: Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, Hongxu Yin

>2025-03-25

> http://arxiv.org/abs/2503.19903v1

High-resolution perception of visual details is crucial for daily tasks.
Current vision pre-training, however, is still limited to low resolutions
(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.
We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution
with a near-constant cost. Instead of contrastive learning on global image
representation, PS3 is pre-trained by selectively processing local regions and
contrasting them with local detailed captions, enabling high-resolution
representation learning with greatly reduced computational overhead. The
pre-trained PS3 is able to both encode the global image at low resolution and
selectively process local high-resolution regions based on their saliency or
relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the
resulting model, named VILA-HD, significantly improves high-resolution visual
perception compared to baselines without high-resolution vision pre-training
such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks
appealing scaling properties of VILA-HD, including scaling up resolution for
free and scaling up test-time compute for better performance. Compared to state
of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL
across multiple benchmarks and achieves better efficiency than latest token
**pruning** approaches. Finally, we find current benchmarks do not require
4K-resolution perception, which motivates us to propose 4KPro, a new benchmark
of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,
including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x
speedup over Qwen2-VL.


## A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design

>Authors: Jie Tian, Martin Taylor Sobczak, Dhanush Patil, Jixin Hou, Lin Pang, Arunachalam Ramanathan, Libin Yang, Xianyan Chen, Yuval Golan, Hongyue Sun, Kenan Song, Xianqiao Wang

>2025-03-25

> http://arxiv.org/abs/2503.19889v1

Metamaterials, renowned for their exceptional mechanical, electromagnetic,
and thermal properties, hold transformative potential across diverse
applications, yet their design remains constrained by labor-intensive
trial-and-error methods and limited data interoperability. Here, we introduce
CrossMatAgent--a novel multi-agent framework that synergistically integrates
large language models with state-of-the-art generative AI to revolutionize
metamaterial design. By orchestrating a hierarchical team of agents--each
specializing in tasks such as pattern analysis, architectural synthesis, prompt
engineering, and supervisory feedback--our system leverages the multimodal
reasoning of GPT-4o alongside the generative precision of DALL-E 3 and a
fine-tuned Stable Diffusion XL model. This integrated approach automates data
augmentation, enhances design fidelity, and produces simulation- and 3D
printing-ready metamaterial patterns. Comprehensive evaluations, including
CLIP-based alignment, SHAP interpretability analyses, and mechanical
simulations under varied load conditions, demonstrate the framework's ability
to generate diverse, reproducible, and application-ready designs. CrossMatAgent
thus establishes a scalable, AI-driven paradigm that bridges the gap between
conceptual innovation and practical realization, paving the way for accelerated
metamaterial development.


## GENIUS A Generative Framework for Universal Multimodal Search

>Authors: Sungyeon Kim, Xinliang Zhu, Xiaofan Lin, Muhammet Bastan, Douglas Gray, Suha Kwak

>2025-03-25

> http://arxiv.org/abs/2503.19868v1

Generative retrieval is an emerging approach in information retrieval that
generates identifiers (IDs) of target data based on a query, providing an
efficient alternative to traditional embedding-based retrieval methods.
However, existing models are task-specific and fall short of embedding-based
retrieval in performance. This paper proposes GENIUS, a universal generative
retrieval framework supporting diverse tasks across multiple modalities and
domains. At its core, GENIUS introduces modality-decoupled semantic
**quantization**, transforming multimodal data into discrete IDs encoding both
modality and semantics. Moreover, to enhance generalization, we propose a query
augmentation that interpolates between a query and its target, allowing GENIUS
to adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses
prior generative methods by a clear margin. Unlike embedding-based retrieval,
GENIUS consistently maintains high retrieval speed across database size, with
competitive performance across multiple benchmarks. With additional re-ranking,
GENIUS often achieves results close to those of embedding-based methods while
preserving efficiency.


## LogQuant Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation

>Authors: Han Chen, Zicong Jiang, Zining Zhang, Bingsheng He, Pingyi Luo, Mian Lu, Yuqiang Chen

>2025-03-25

> http://arxiv.org/abs/2503.19950v1

We introduce LogQuant, a groundbreaking 2-bit **quantization** technique for **KV**
Cache in large language model (LLM) inference, delivering substantial memory
savings while preserving superior performance. Previous methods either assume
that later tokens are more important or attempt to predict important tokens
based on earlier attention patterns. Both approaches, however, can result in
performance bottlenecks or frequent mispredictions.
  LogQuant takes a different approach. By applying a log-based filtering
mechanism, it selectively compresses the **KV** Cache across the entire context,
achieving better performance with the same or even reduced memory footprint
compared to existing methods. In benchmark tests, it enhances throughput by 25%
and boosts batch size by 60% without increasing memory consumption. For
challenging tasks such as Math and Code Completion, LogQuant improves accuracy
by 40% to 200% at the same compression ratio, outperforming comparable
techniques.LogQuant integrates effortlessly with popular inference frameworks
like Python's transformers library. Implementation can be available in
https://github.com/Concyclics/LogQuant**KV**.


## Gemma 3 Technical Report

>Authors: Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, Léonard Hussenot

>2025-03-25

> http://arxiv.org/abs/2503.19786v1

We introduce Gemma 3, a multimodal addition to the Gemma family of
lightweight open models, ranging in scale from 1 to 27 billion parameters. This
version introduces vision understanding abilities, a wider coverage of
languages and longer context - at least 128K tokens. We also change the
architecture of the model to reduce the **KV**-cache memory that tends to explode
with long context. This is achieved by increasing the ratio of local to global
attention layers, and keeping the span on local attention short. The Gemma 3
models are trained with distillation and achieve superior performance to Gemma
2 for both pre-trained and instruction finetuned versions. In particular, our
novel post-training recipe significantly improves the math, chat,
instruction-following and multilingual abilities, making Gemma3-4B-IT
competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro
across benchmarks. We release all our models to the community.


## An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators

>Authors: Tseng-Jen Li, Tian-Sheuan Chang

>2025-03-25

> http://arxiv.org/abs/2503.19640v1

Transformer-based models have become the \textit{de facto} backbone across
many fields, such as computer vision and natural language processing. However,
as these models scale in size, external memory access (EMA) for weight and
activations becomes a critical bottleneck due to its significantly higher
energy consumption compared to internal computations. While most prior work has
focused on optimizing the self-attention mechanism, little attention has been
given to optimizing data transfer during linear projections, where EMA costs
are equally important. In this paper, we propose the Tile-based Adaptive
Stationary (TAS) scheme that selects the input or weight stationary in a tile
granularity, based on the input sequence length. Our experimental results
demonstrate that TAS can significantly reduce EMA by more than 97\% compared to
traditional stationary schemes, while being compatible with various attention
optimization techniques and hardware accelerators.


## SINR Sparsity Driven Compressed Implicit Neural Representations

>Authors: Dhananjaya Jayasundara, Sudarshan Rajagopalan, Yasiru Ranasinghe, Trac D. Tran, Vishal M. Patel

>2025-03-25

> http://arxiv.org/abs/2503.19576v1

Implicit Neural Representations (INRs) are increasingly recognized as a
versatile data modality for representing discretized signals, offering benefits
such as infinite query resolution and reduced storage requirements. Existing
signal compression approaches for INRs typically employ one of two strategies:
1. direct **quantization** with entropy coding of the trained INR; 2. deriving a
latent code on top of the INR through a learnable transformation. Thus, their
performance is heavily dependent on the **quantization** and entropy coding schemes
employed. In this paper, we introduce SINR, an innovative compression algorithm
that leverages the patterns in the vector spaces formed by weights of INRs. We
compress these vector spaces using a high-dimensional **sparse** code within a
dictionary. Further analysis reveals that the atoms of the dictionary used to
generate the **sparse** code do not need to be learned or transmitted to
successfully recover the INR weights. We demonstrate that the proposed approach
can be integrated with any existing INR-based signal compression technique. Our
results indicate that SINR achieves substantial reductions in storage
requirements for INRs across various configurations, outperforming conventional
INR-based compression baselines. Furthermore, SINR maintains high-quality
decoding across diverse data modalities, including images, occupancy fields,
and Neural Radiance Fields.


## KSHSeek Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models

>Authors: Zhiwei Wang, Zhongxin Liu, Ying Li, Hongyu Sun, Meng Xu, Yuqing Zhang

>2025-03-25

> http://arxiv.org/abs/2503.19482v1

The emergence of large language models (LLMs) has significantly advanced the
development of natural language processing (NLP), especially in text generation
tasks like question answering. However, model hallucinations remain a major
challenge in natural language generation (NLG) tasks due to their complex
causes. We systematically expand on the causes of factual hallucinations from
the perspective of knowledge shortcuts, analyzing hallucinations arising from
correct and defect-free data and demonstrating that knowledge-shortcut
hallucinations are prevalent in generative models. To mitigate this issue, we
propose a high similarity **pruning** algorithm at the data preprocessing level to
reduce spurious correlations in the data. Additionally, we design a specific
detection method for knowledge-shortcut hallucinations to evaluate the
effectiveness of our mitigation strategy. Experimental results show that our
approach effectively reduces knowledge-shortcut hallucinations, particularly in
fine-tuning tasks, without negatively impacting model performance in question
answering. This work introduces a new paradigm for mitigating specific
hallucination issues in generative models, enhancing their robustness and
reliability in real-world applications.


## QUAD Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition

>Authors: Yuxuan Hu, Xiaodong Chen, Cuiping Li, Hong Chen, Jing Zhang

>2025-03-25

> http://arxiv.org/abs/2503.19353v1

Large Language Models (LLMs) excel in diverse applications but suffer
inefficiency due to massive scale. While **quantization** reduces computational
costs, existing methods degrade accuracy in medium-sized LLMs (e.g.,
Llama-3-8B) due to activation outliers. To address this, we propose QUAD
(Quantization with Activation Decomposition), a framework leveraging Singular
Value Decomposition (SVD) to suppress activation outliers for effective 4-bit
**quantization**. QUAD estimates activation singular vectors offline using
calibration data to construct an orthogonal transformation matrix P, shifting
outliers to additional dimensions in full precision while quantizing rest
components to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning
via adaptable full-precision outlier weights, narrowing the accuracy gap
between **quantize**d and full-precision models. Experiments demonstrate that QUAD
achieves 94% ~ 96% accuracy under W4A4 **quantization** and 98% accuracy with
W4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.
Our code is available at \href{https://github.com/hyx1999/Quad}{repository}.


## Membership Inference Attacks on Large-Scale Models A Survey

>Authors: Hengyu Wu, Yang Cao

>2025-03-25

> http://arxiv.org/abs/2503.19338v1

The adoption of the Large Language Model (LLM) has accelerated dramatically
since the ChatGPT from OpenAI went online in November 2022. Recent advances in
Large Multimodal Models (LMMs), which process diverse data types and enable
interaction through various channels, have expanded beyond the text-to-text
limitations of early LLMs, attracting significant and concurrent attention from
both researchers and industry. While LLMs and LMMs are starting to spread
widely, concerns about their privacy risks are increasing as well. Membership
Inference Attacks (MIAs), techniques used to determine whether a particular
data point was part of a model's training set, serve as a key metric for
assessing the privacy vulnerabilities of machine learning models. Hu et al.
show that various machine learning algorithms are vulnerable to MIA. Despite
extensive studies on MIAs in traditional models, there remains a lack of
systematic surveys addressing their effectiveness and implications in modern
large-scale models like LLMs and LMMs. In this paper, we systematically
reviewed recent studies of MIA against LLMs and LMMs. We analyzed and
categorized each attack based on their methodology and scenario and discussed
the limitations in existing research. Additionally, we examine privacy concerns
associated with the fine-tuning process. Finally, we provided some suggestions
for future research in this direction.


## AI-Driven Defect Engineering for Advanced Thermoelectric Materials

>Authors: Chu-Liang Fu, Mouyang Cheng, Nguyen Tuan Hung, Eunbi Rha, Zhantao Chen, Ryotaro Okabe, Denisse Córdova Carrizales, Manasi Mandal, Yongqiang Cheng, Mingda Li

>2025-03-24

> http://arxiv.org/abs/2503.19148v1

Thermoelectric materials offer a promising pathway to directly convert waste
heat to electricity. However, achieving high performance remains challenging
due to intrinsic trade-offs between electrical conductivity, the Seebeck
coefficient, and thermal conductivity, which are further complicated by the
presence of defects. This review explores how artificial intelligence (AI) and
machine learning (ML) are transforming thermoelectric materials design.
Advanced ML approaches including deep neural networks, graph-based models, and
transformer architectures, integrated with high-throughput simulations and
growing databases, effectively capture structure-property relationships in a
complex multiscale defect space and overcome the curse of dimensionality. This
review discusses AI-enhanced defect engineering strategies such as composition
optimization, entropy and dislocation engineering, and grain boundary design,
along with emerging inverse design techniques for generating materials with
targeted properties. Finally, it outlines future opportunities in novel physics
mechanisms and sustainability, highlighting the critical role of AI in
accelerating the discovery of thermoelectric materials.


## Joint Sparse Graph for Enhanced MIMO-AFDM Receiver Design

>Authors: Qu Luo, Jing Zhu, Zilong Liu, Yanqun Tang, Pei Xiao, Gaojie Chen, Jia Shi

>2025-03-24

> http://arxiv.org/abs/2503.19143v1

Affine frequency division multiplexing (AFDM) is a promising chirp-assisted
multicarrier waveform for future high-mobility communications. This paper is
devoted to enhanced receiver design for multiple input and multiple output AFDM
(MIMO-AFDM) systems. Firstly, we introduce a unified variational inference (VI)
approach to approximate the target posterior distribution, under which the
belief propagation (BP) and expectation propagation (EP)-based algorithms are
derived. As both VI-based detection and low-density parity-check (LDPC)
decoding can be expressed by bipartite graphs in MIMO-AFDM systems, we
construct a joint **sparse** graph (JSG) by merging the graphs of these two for
low-complexity receiver design. Then, based on this graph model, we present the
detailed message propagation of the proposed JSG. Additionally, we propose an
enhanced JSG (E-JSG) receiver based on the linear constellation encoding model.
The proposed E-JSG eliminates the need for interleavers, de-interleavers, and
log-likelihood ratio transformations, thus leading to concurrent detection and
decoding over the integrated **sparse** graph. To further reduce detection
complexity, we introduce a **sparse** channel method by approaximating multiple
graph edges with insignificant channel coefficients into a single edge on the
VI graph. Simulation results show the superiority of the proposed receivers in
terms of computational complexity, detection and decoding latency, and error
rate performance compared to the conventional ones.


## Numerical evaluation of the integrals of motion in particle accelerator tracking codes

>Authors: Philippe Belanger, Guido Sterbini

>2025-03-24

> http://arxiv.org/abs/2503.19122v1

Particle tracking codes are one of the fundamental tools used in the design
and the study of complex magnetic lattices in accelerator physics. For most
practical applications, non-linear lenses are included and the Courant-Snyder
formalism falls short of a complete description of the motion. Likewise, when
the longitudinal motion is added, synchro-betatron coupling complicates the
dynamics and different formalisms are typically needed to explain the motion.
In this paper, a revised formalism is proposed based on the Fourier expansion
of the trajectory -- known to be foundational in the KAM theorem -- which
naturally describes non-linear motion in 2D, 4D and 6D. After extracting the
fundamental frequencies and the Fourier coefficients from tracking data, it is
shown that an approximate energy manifold (an invariant torus) can be
constructed from the single-particle motion. This cornerstone allows to
visualize and compute the areas of the torus projections in all conjugate
planes, conserved under symplectic transformations. These are the integrals of
motion, ultimately expressed in terms of the Fourier coefficients. As a
numerical demonstration of this formalism, the case of the 6-dimensional Large
Hadron Collider (LHC) is studied. Examples from the 2D and 4D H\'enon map are
also provided. Even for heavily smeared and intricate non-linear motion, it is
shown that invariant tori accurately describe the motion of single particles
for a large region of the phase space, as suggested by the KAM theorem.


## Rank-Based Modeling for Universal Packets Compression in Multi-Modal Communications

>Authors: Xuanhao Luo, Zhiyuan Peng, Zhouyu Li, Ruozhou Yu, Yuchen Liu

>2025-03-24

> http://arxiv.org/abs/2503.19097v1

The rapid increase in networked systems and data transmission requires
advanced data compression solutions to optimize bandwidth utilization and
enhance network performance. This study introduces a novel byte-level
predictive model using Transformer architecture, capable of handling the
redundancy and diversity of data types in network traffic as byte sequences.
Unlike traditional methods that require separate compressors for different data
types, this unified approach sets new benchmarks and simplifies predictive
modeling across various data modalities such as video, audio, images, and text,
by processing them at the byte level. This is achieved by predicting subsequent
byte probability distributions, encoding them into a **sparse** rank sequence using
lossless entropy coding, and significantly reducing both data size and entropy.
Experimental results show that our model achieves compression ratios below 50%,
while offering models of various sizes tailored for different communication
devices. Additionally, we successfully deploy these models on a range of edge
devices and servers, demonstrating their practical applicability and
effectiveness in real-world network scenarios. This approach significantly
enhances data throughput and reduces bandwidth demands, making it particularly
valuable in resource-constrained environments like the Internet of Things
sensor networks.


## Detecting Arbitrary Planted Subgraphs in Random Graphs

>Authors: Dor Elimelech, Wasim Huleihel

>2025-03-24

> http://arxiv.org/abs/2503.19069v1

The problems of detecting and recovering planted structures/subgraphs in
Erd\H{o}s-R\'{e}nyi random graphs, have received significant attention over the
past three decades, leading to many exciting results and mathematical
techniques. However, prior work has largely focused on specific ad hoc planted
structures and inferential settings, while a general theory has remained
elusive. In this paper, we bridge this gap by investigating the detection of an
\emph{arbitrary} planted subgraph $\Gamma = \Gamma_n$ in an Erd\H{o}s-R\'{e}nyi
random graph $\mathcal{G}(n, q_n)$, where the edge probability within $\Gamma$
is $p_n$. We examine both the statistical and computational aspects of this
problem and establish the following results. In the dense regime, where the
edge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the
information-theoretic and computational thresholds for detecting $\Gamma$, and
provide conditions under which a computational-statistical gap arises. Most
notably, these thresholds depend on $\Gamma$ only through its number of edges,
maximum degree, and maximum subgraph density. Our lower and upper bounds are
general and apply to any value of $p_n$ and $q_n$ as functions of $n$.
Accordingly, we also analyze the **sparse** regime where $q_n =
\Theta(n^{-\alpha})$ and $p_n-q_n =\Theta(q_n)$, with $\alpha\in[0,2]$, as well
as the critical regime where $p_n=1-o(1)$ and $q_n = \Theta(n^{-\alpha})$, both
of which have been widely studied, for specific choices of $\Gamma$. For these
regimes, we show that our bounds are tight for all planted subgraphs
investigated in the literature thus far\textemdash{}and many more. Finally, we
identify conditions under which detection undergoes sharp phase transition,
where the boundaries at which algorithms succeed or fail shift abruptly as a
function of $q_n$.


## Video-T1 Test-Time Scaling for Video Generation

>Authors: Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan

>2025-03-24

> http://arxiv.org/abs/2503.18942v1

With the scale capability of increasing training data, model size, and
computational cost, video generation has achieved impressive results in digital
creation, enabling users to express creativity across various domains.
Recently, researchers in Large Language Models (LLMs) have expanded the scaling
to test-time, which can significantly improve LLM performance by using more
inference-time computation. Instead of scaling up video foundation models
through expensive training costs, we explore the power of Test-Time Scaling
(TTS) in video generation, aiming to answer the question: if a video generation
model is allowed to use non-trivial amount of inference-time compute, how much
can it improve generation quality given a challenging text prompt. In this
work, we reinterpret the test-time scaling of video generation as a searching
problem to sample better trajectories from Gaussian noise space to the target
video distribution. Specifically, we build the search space with test-time
verifiers to provide feedback and heuristic algorithms to guide searching
process. Given a text prompt, we first explore an intuitive linear search
strategy by increasing noise candidates at inference time. As full-step
denoising all frames simultaneously requires heavy test-time computation costs,
we further design a more efficient TTS method for video generation called
Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an
autoregressive manner. Extensive experiments on text-conditioned video
generation benchmarks demonstrate that increasing test-time compute
consistently leads to significant improvements in the quality of videos.
Project page: https://liuff19.github.io/Video-T1


## Training-free Diffusion Acceleration with Bottleneck Sampling

>Authors: Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, Bin Cui

>2025-03-24

> http://arxiv.org/abs/2503.18940v2

Diffusion models have demonstrated remarkable capabilities in visual content
generation but remain challenging to deploy due to their high computational
cost during inference. This computational burden primarily arises from the
quadratic complexity of self-attention with respect to image or video
resolution. While existing **acceleration** methods often compromise output quality
or necessitate costly retraining, we observe that most diffusion models are
pre-trained at lower resolutions, presenting an opportunity to exploit these
low-resolution priors for more efficient inference without degrading
performance. In this work, we introduce Bottleneck Sampling, a training-free
framework that leverages low-resolution priors to reduce computational overhead
while preserving output fidelity. Bottleneck Sampling follows a high-low-high
denoising workflow: it performs high-resolution denoising in the initial and
final stages while operating at lower resolutions in intermediate steps. To
mitigate aliasing and blurring artifacts, we further refine the resolution
transition points and adaptively shift the denoising timesteps at each stage.
We evaluate Bottleneck Sampling on both image and video generation tasks, where
extensive experiments demonstrate that it accelerates inference by up to
3$\times$ for image generation and 2.5$\times$ for video generation, all while
maintaining output quality comparable to the standard full-resolution sampling
process across multiple evaluation metrics.


## Trajectory Balance with Asynchrony Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training

>Authors: Brian R. Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, Bhavya Kailkhura

>2025-03-24

> http://arxiv.org/abs/2503.18929v1

Reinforcement learning (RL) is a critical component of large language model
(LLM) post-training. However, existing on-policy algorithms used for
post-training are inherently incompatible with the use of experience replay
buffers, which can be populated scalably by distributed off-policy actors to
enhance exploration as compute increases. We propose efficiently obtaining this
benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a
massively scalable LLM RL system. In contrast to existing approaches, TBA uses
a larger fraction of compute on search, constantly generating off-policy data
for a central replay buffer. A training node simultaneously samples data from
this buffer based on reward or recency to update the policy using Trajectory
Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA
offers three key advantages: (1) decoupled training and search, speeding up
training wall-clock time by 4x or more; (2) improved diversity through
large-scale off-policy sampling; and (3) scalable search for **sparse** reward
settings. On mathematical reasoning, preference-tuning, and automated
red-teaming (diverse and representative post-training tasks), TBA produces
speed and performance improvements over strong baselines.


## FFN Fusion Rethinking Sequential Computation in Large Language Models

>Authors: Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv

>2025-03-24

> http://arxiv.org/abs/2503.18908v1

We introduce FFN Fusion, an architectural optimization technique that reduces
sequential computation in large language models by identifying and exploiting
natural opportunities for parallelization. Our key insight is that sequences of
Feed-Forward Network (FFN) layers, particularly those remaining after the
removal of specific attention layers, can often be parallelized with minimal
accuracy impact. We develop a principled methodology for identifying and fusing
such sequences, transforming them into parallel operations that significantly
reduce inference latency while preserving model behavior. Applying these
techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base
(Ultra-253B-Base), an efficient and soon-to-be publicly available model that
achieves a 1.71X speedup in inference latency and 35X lower per-token cost
while maintaining strong performance across benchmarks. Through extensive
experiments on models from 49B to 253B parameters, we demonstrate that FFN
Fusion becomes increasingly effective at larger scales and can complement
existing optimization techniques like **quantization** and **pruning**. Most
intriguingly, we find that even full transformer blocks containing both
attention and FFN layers can sometimes be parallelized, suggesting new
directions for neural architecture design.


## xKV Cross-Layer SVD for KV-Cache Compression

>Authors: Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, Mohamed S. Abdelfattah

>2025-03-24

> http://arxiv.org/abs/2503.18893v1

Large Language Models (LLMs) with long context windows enable powerful
applications but come at the cost of high memory consumption to store the Key
and Value states (**KV**-Cache). Recent studies attempted to merge **KV**-cache from
multiple layers into shared representations, yet these approaches either
require expensive pretraining or rely on assumptions of high per-token cosine
similarity across layers which generally does not hold in practice. We find
that the dominant singular vectors are remarkably well-aligned across multiple
layers of the **KV**-Cache. Exploiting this insight, we propose x**KV**, a simple
post-training method that applies Singular Value Decomposition (SVD) on the
**KV**-Cache of grouped layers. x**KV** consolidates the **KV**-Cache of multiple layers
into a shared low-rank subspace, significantly reducing **KV**-Cache sizes. Through
extensive evaluations on the RULER long-context benchmark with widely-used LLMs
(e.g., Llama-3.1 and Qwen2.5), x**KV** achieves up to 6.8x higher compression rates
than state-of-the-art inter-layer technique while improving accuracy by 2.7%.
Moreover, x**KV** is compatible with the emerging Multi-Head Latent Attention (MLA)
(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding
tasks without performance degradation. These results highlight x**KV**'s strong
capability and versatility in addressing memory bottlenecks for long-context
LLM inference. Our code is publicly available at:
https://github.com/abdelfattah-lab/x**KV**.


## I Have Covered All the Bases Here Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders

>Authors: Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets

>2025-03-24

> http://arxiv.org/abs/2503.18878v1

Large Language Models (LLMs) have achieved remarkable success in natural
language processing. Recent advances have led to the developing of a new class
of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved
state-of-the-art performance by integrating deep thinking and complex
reasoning. Despite these impressive capabilities, the internal reasoning
mechanisms of such models remain unexplored. In this work, we employ Sparse
Autoencoders (SAEs), a method to learn a **sparse** decomposition of latent
representations of a neural network into interpretable features, to identify
features that drive reasoning in the DeepSeek-R1 series of models. First, we
propose an approach to extract candidate ''reasoning features'' from SAE
representations. We validate these features through empirical analysis and
interpretability methods, demonstrating their direct correlation with the
model's reasoning abilities. Crucially, we demonstrate that steering these
features systematically enhances reasoning performance, offering the first
mechanistic account of reasoning in LLMs. Code available at
https://github.com/AIRI-Institute/SAE-Reasoning


## Reimagining Memory Access for LLM Inference Compression-Aware Memory Controller Design

>Authors: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

>2025-03-24

> http://arxiv.org/abs/2503.18869v2

The efficiency of Large Language Model~(LLM) inference is often constrained
by substantial memory bandwidth and capacity demands. Existing techniques, such
as **pruning**, **quantization**, and mixture of experts/depth, reduce memory capacity
and/or bandwidth consumption at the cost of slight degradation in inference
quality. This paper introduces a design solution that further alleviates memory
bottlenecks by enhancing the on-chip memory controller in AI accelerators to
achieve two main objectives: (1) significantly reducing memory capacity and
bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of
model weights and key-value (**KV**) cache without compromising inference quality,
and (2) enabling memory bandwidth and energy consumption to scale
proportionally with context-dependent dynamic **quantization**. These goals are
accomplished by equipping the on-chip memory controller with mechanisms to
improve fine-grained bit-level accessibility and compressibility of weights and
**KV** cache through LLM-aware configuration of in-memory placement and
representation. Experimental results on publicly available LLMs demonstrate the
effectiveness of this approach, showing memory footprint reductions of 25.2\%
for model weights and 46.9\% for **KV** cache. In addition, our hardware prototype
at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area
overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware
memory control as a key to efficient large-scale inference.


## Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation

>Authors: DeShin Hwa, Tobias Holmes, Klaus Drechsler

>2025-03-24

> http://arxiv.org/abs/2503.18862v1

While CNNs were long considered state of the art for image processing, the
introduction of Transformer architectures has challenged this position. While
achieving excellent results in image classification and segmentation,
Transformers remain inherently reliant on large training datasets and remain
computationally expensive. A newly introduced Transformer derivative named **KV**
Transformer shows promising results in synthetic, NLP, and image classification
tasks, while reducing complexity and memory usage. This is especially conducive
to use cases where local inference is required, such as medical screening
applications. We endeavoured to further evaluate the merit of **KV** Transformers
on semantic segmentation tasks, specifically in the domain of medical imaging.
By directly comparing traditional and **KV** variants of the same base
architectures, we provide further insight into the practical tradeoffs of
reduced model complexity. We observe a notable reduction in parameter count and
multiply accumulate operations, while achieving similar performance from most
of the **KV** variant models when directly compared to their Q**KV** implementation.


## Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction

>Authors: Yuxuan Zhang, Jinkui Hao, Bo Zhou

>2025-03-24

> http://arxiv.org/abs/2503.18836v1

Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its
inherently long acquisition times reduce clinical efficiency and patient
comfort. Recent advancements in deep learning, particularly diffusion models,
have improved accelerated MRI reconstruction. However, existing diffusion
models' training often relies on fully sampled data, models incur high
computational costs, and often lack uncertainty estimation, limiting their
clinical applicability. To overcome these challenges, we propose a novel
framework, called Dual-domain Multi-path Self-supervised Diffusion Model
(DMSM), that integrates a self-supervised dual-domain diffusion model training
scheme, a lightweight hybrid attention network for the reconstruction diffusion
model, and a multi-path inference strategy, to enhance reconstruction accuracy,
efficiency, and explainability. Unlike traditional diffusion-based models, DMSM
eliminates the dependency on training from fully sampled data, making it more
practical for real-world clinical settings. We evaluated DMSM on two human MRI
datasets, demonstrating that it achieves favorable performance over several
supervised and self-supervised baselines, particularly in preserving fine
anatomical structures and suppressing artifacts under high **acceleration**
factors. Additionally, our model generates uncertainty maps that correlate
reasonably well with reconstruction errors, offering valuable clinically
interpretable guidance and potentially enhancing diagnostic confidence.


## BitDecoding Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache

>Authors: Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang

>2025-03-24

> http://arxiv.org/abs/2503.18773v1

The growing adoption of long-context Large Language Models (LLMs) has
introduced significant memory and computational challenges in autoregressive
decoding due to the expanding Key-Value (**KV**) cache. **KV** cache **quantization** has
emerged as a promising solution, with prior work showing that 4-bit or even
2-bit **quantization** can maintain model accuracy while reducing memory costs.
However, despite these benefits, preliminary implementations for the **low-bit** **KV**
cache struggle to deliver the expected speedup due to **quantization** and
de**quantization** overheads and the lack of Tensor Cores utilization. In this
work, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor
Cores for efficient decoding with **low-bit** **KV** cache. Efficiently leveraging
Tensor Cores for **low-bit** **KV** cache is challenging due to the dynamic nature of
**KV** cache generation at each decoding step. BitDecoding addresses these
challenges with a Tensor Cores-Centric BitFusion Scheme that ensures data
layout compatibility to enable high utilization of Tensor Cores. Additionally,
BitDecoding incorporates a warp-efficient parallel decoding kernel and a
fine-grained asynchronous pipeline, minimizing de**quantization** overhead and
improving computational efficiency. Experiments show that BitDecoding achieves
up to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to
FP16 FlashDecoding-v2. It also outperforms the state-of-the-art **low-bit** **KV**
cache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K
sequence length, BitDecoding reduces single-batch decoding latency by 3x,
demonstrating its effectiveness in long-context generation scenarios. The code
is available at https://github.com/DD-DuDa/BitDecoding.


## Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings

>Authors: Cong Liu, Liang Hou, Mingwu Zheng, Xin Tao, Pengfei Wan, Di Zhang, Kun Gai

>2025-03-24

> http://arxiv.org/abs/2503.18719v1

Resolution generalization in image generation tasks enables the production of
higher-resolution images with lower training resolution overhead. However, a
significant challenge in resolution generalization, particularly in the widely
used Diffusion Transformers, lies in the mismatch between the positional
encodings encountered during testing and those used during training. While
existing methods have employed techniques such as interpolation, extrapolation,
or their combinations, none have fully resolved this issue. In this paper, we
propose a novel two-dimensional randomized positional encodings (RPE-2D)
framework that focuses on learning positional order of image patches instead of
the specific distances between them, enabling seamless high- and low-resolution
image generation without requiring high- and low-resolution image training.
Specifically, RPE-2D independently selects positions over a broader range along
both the horizontal and vertical axes, ensuring that all position encodings are
trained during the inference phase, thus improving resolution generalization.
Additionally, we propose a random data augmentation technique to enhance the
modeling of position order. To address the issue of image cropping caused by
the augmentation, we introduce corresponding micro-conditioning to enable the
model to perceive the specific cropping patterns. On the ImageNet dataset, our
proposed RPE-2D achieves state-of-the-art resolution generalization
performance, outperforming existing competitive methods when trained at a
resolution of $256 \times 256$ and inferred at $384 \times 384$ and $512 \times
512$, as well as when scaling from $512 \times 512$ to $768 \times 768$ and
$1024 \times 1024$. And it also exhibits outstanding capabilities in
low-resolution image generation, multi-stage training **acceleration** and
multi-resolution inheritance.


## Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology

>Authors: Boqi Chen, Cédric Vincent-Cuaz, Lydia A. Schoenpflug, Manuel Madeira, Lisa Fournier, Vaishnavi Subramanian, Sonali Andani, Samuel Ruiperez-Campillo, Julia E. Vogt, Raphaëlle Luisier, Dorina Thanou, Viktor H. Koelzer, Pascal Frossard, Gabriele Campanella, Gunnar Rätsch

>2025-03-24

> http://arxiv.org/abs/2503.18709v1

Vision foundation models (FMs) are accelerating the development of digital
pathology algorithms and transforming biomedical research. These models learn,
in a self-supervised manner, to represent histological features in highly
heterogeneous tiles extracted from whole-slide images (WSIs) of real-world
patient samples. The performance of these FMs is significantly influenced by
the size, diversity, and balance of the pre-training data. However, data
selection has been primarily guided by expert knowledge at the WSI level,
focusing on factors such as disease classification and tissue types, while
largely overlooking the granular details available at the tile level. In this
paper, we investigate the potential of unsupervised automatic data curation at
the tile-level, taking into account 350 million tiles. Specifically, we apply
hierarchical clustering trees to pre-extracted tile embeddings, allowing us to
sample balanced datasets uniformly across the embedding space of the pretrained
FM. We further identify these datasets are subject to a trade-off between size
and balance, potentially compromising the quality of representations learned by
FMs, and propose tailored batch sampling strategies to mitigate this effect. We
demonstrate the effectiveness of our method through improved performance on a
diverse range of clinically relevant downstream tasks.


## A Comprehensive Review on Hashtag Recommendation From Traditional to Deep Learning and Beyond

>Authors: Shubhi Bansal, Kushaan Gowda, Anupama Sureshbabu K, Chirag Kothari, Nagendra Kumar

>2025-03-24

> http://arxiv.org/abs/2503.18669v2

The exponential growth of user-generated content on social media platforms
has precipitated significant challenges in information management, particularly
in content organization, retrieval, and discovery. Hashtags, as a fundamental
categorization mechanism, play a pivotal role in enhancing content visibility
and user engagement. However, the development of accurate and robust hashtag
recommendation systems remains a complex and evolving research challenge.
Existing surveys in this domain are limited in scope and recency, focusing
narrowly on specific platforms, methodologies, or timeframes. To address this
gap, this review article conducts a systematic analysis of hashtag
recommendation systems, comprehensively examining recent advancements across
several dimensions. We investigate unimodal versus multimodal methodologies,
diverse problem formulations, filtering strategies, methodological evolution
from traditional frequency-based models to advanced deep learning
architectures. Furthermore, we critically evaluate performance assessment
paradigms, including quantitative metrics, qualitative analyses, and hybrid
evaluation frameworks. Our analysis underscores a paradigm shift toward
transformer-based deep learning models, which harness contextual and semantic
features to achieve superior recommendation accuracy. Key challenges such as
data **sparsity**, cold-start scenarios, polysemy, and model explainability are
rigorously discussed, alongside practical applications in tweet classification,
sentiment analysis, and content popularity prediction. By synthesizing insights
from diverse methodological and platform-specific perspectives, this survey
provides a structured taxonomy of current research, identifies unresolved gaps,
and proposes future directions for developing adaptive, user-centric
recommendation systems.


## Oaken Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization

>Authors: Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park

>2025-03-24

> http://arxiv.org/abs/2503.18599v1

Modern Large Language Model serving system batches multiple requests to
achieve high throughput, while batching attention operations is challenging,
rendering memory bandwidth a critical bottleneck. The community relies on
high-end GPUs with multiple high-bandwidth memory channels. Unfortunately,
HBM's high bandwidth often comes at the expense of limited memory capacity,
which reduces core utilization and increases costs. Recent advancements
enabling longer contexts for LLMs have substantially increased the key-value
cache size, further intensifying the pressures on memory capacity. The
literature has explored **KV** cache **quantization** techniques, which commonly use
low bitwidth for most values, selectively using higher bitwidth for outlier
values. While this approach helps achieve high accuracy and low bitwidth
simultaneously, it comes with the limitation that cost for online outlier
detection is excessively high, negating the advantages. We propose Oaken, an
**acceleration** solution that achieves high accuracy and high performance
simultaneously through co-designing algorithm and hardware. To effectively find
a sweet spot in the accuracy-performance trade-off space of **KV** cache
**quantization**, Oaken employs an online-offline hybrid approach, setting outlier
thresholds offline, which are then used to determine the **quantization** scale
online. To translate the proposed algorithmic technique into tangible
performance gains, Oaken also comes with custom **quantization** engines and memory
management units that can be integrated with any LLM accelerators. We built an
Oaken accelerator on top of an LLM accelerator, LPU, and conducted a
comprehensive evaluation. Our experiments show that for a batch size of 256,
Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,
incurring a minimal accuracy loss of only 0.54\% on average, compared to
state-of-the-art **KV** cache **quantization** techniques.


## AMD-Hummingbird Towards an Efficient Text-to-Video Model

>Authors: Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, Emad Barsoum

>2025-03-24

> http://arxiv.org/abs/2503.18559v2

Text-to-Video (T2V) generation has attracted significant attention for its
ability to synthesize realistic videos from textual descriptions. However,
existing models struggle to balance computational efficiency and high visual
quality, particularly on resource-limited devices, e.g.,iGPUs and mobile
phones. Most prior work prioritizes visual fidelity while overlooking the need
for smaller, more efficient models suitable for real-world deployment. To
address this challenge, we propose a lightweight T2V framework, termed
Hummingbird, which prunes existing models and enhances visual quality through
visual feedback learning. Our approach reduces the size of the U-Net from 1.4
billion to 0.7 billion parameters, significantly improving efficiency while
preserving high-quality video generation. Additionally, we introduce a novel
data processing pipeline that leverages Large Language Models (LLMs) and Video
Quality Assessment (VQA) models to enhance the quality of both text prompts and
video data. To support user-driven training and style customization, we
publicly release the full training code, including data processing and model
training. Extensive experiments show that our method achieves a 31X speedup
compared to state-of-the-art models such as VideoCrafter2, while also attaining
the highest overall score on VBench. Moreover, our method supports the
generation of videos with up to 26 frames, addressing the limitations of
existing U-Net-based methods in long video generation. Notably, the entire
training process requires only four GPUs, yet delivers performance competitive
with existing leading methods. Hummingbird presents a practical and efficient
solution for T2V generation, combining high performance, scalability, and
flexibility for real-world applications.


## U-REPA Aligning Diffusion U-Nets to ViTs

>Authors: Yuchuan Tian, Hanting Chen, Mengyu Zheng, Yuchen Liang, Chao Xu, Yunhe Wang

>2025-03-24

> http://arxiv.org/abs/2503.18414v1

Representation Alignment (REPA) that aligns Diffusion Transformer (DiT)
hidden-states with ViT visual encoders has proven highly effective in DiT
training, demonstrating superior convergence properties, but it has not been
validated on the canonical diffusion U-Net architecture that shows faster
convergence compared to DiTs. However, adapting REPA to U-Net architectures
presents unique challenges: (1) different block functionalities necessitate
revised alignment strategies; (2) spatial-dimension inconsistencies emerge from
U-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT
hinder the effectiveness of tokenwise alignment. To encounter these challenges,
we propose U-REPA, a representation alignment paradigm that bridges U-Net
hidden states and ViT features as follows: Firstly, we propose via observation
that due to skip connection, the middle stage of U-Net is the best alignment
option. Secondly, we propose upsampling of U-Net features after passing them
through MLPs. Thirdly, we observe difficulty when performing tokenwise
similarity alignment, and further introduces a manifold loss that regularizes
the relative similarity between samples. Experiments indicate that the
resulting U-REPA could achieve excellent generation quality and greatly
accelerates the convergence speed. With CFG guidance interval, U-REPA could
reach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\times$ 256,
and needs only half the total epochs to perform better than REPA. Codes are
available at https://github.com/YuchuanTian/U-REPA.


## Maximum Redundancy Pruning A Principle-Driven Layerwise Sparsity Allocation for LLMs

>Authors: Chang Gao, Kang Zhao, Jianfei Chen, Liping Jing

>2025-03-24

> http://arxiv.org/abs/2503.18377v1

Large language models (LLMs) have demonstrated impressive capabilities, but
their enormous size poses significant challenges for deployment in real-world
applications. To address this issue, researchers have sought to apply network
**pruning** techniques to LLMs. A critical challenge in **pruning** is allocation the
**sparsity** for each layer. Recent **sparsity** allocation methods is often based on
heuristics or search that can easily lead to suboptimal performance. In this
paper, we conducted an extensive investigation into various LLMs and revealed
three significant discoveries: (1) the layerwise **pruning** sensitivity (LPS) of
LLMs is highly non-uniform, (2) the choice of **pruning** metric affects LPS, and
(3) the performance of a **sparse** model is related to the uniformity of its
layerwise redundancy level. Based on these observations, we propose that the
layerwise **sparsity** of LLMs should adhere to three principles:
\emph{non-uniformity}, \emph{**pruning** metric dependency}, and \emph{uniform
layerwise redundancy level} in the pruned model. To this end, we proposed
Maximum Redundancy Pruning (MRP), an iterative **pruning** algorithm that prunes in
the most redundant layers (\emph{i.e.}, those with the highest non-outlier
ratio) at each iteration. The achieved layerwise **sparsity** aligns with the
outlined principles. We conducted extensive experiments on publicly available
LLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental
results validate the effectiveness of MRP, demonstrating its superiority over
previous methods.


## TopV Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model

>Authors: Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan

>2025-03-24

> http://arxiv.org/abs/2503.18278v1

Vision-Language Models (VLMs) demand substantial computational resources
during inference, largely due to the extensive visual input tokens for
representing visual information. Previous studies have noted that visual tokens
tend to receive less attention than text tokens, suggesting their lower
importance during inference and potential for **pruning**. However, their methods
encounter several challenges: reliance on greedy heuristic criteria for token
importance and incompatibility with FlashAttention and **KV** cache. To address
these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken
\textbf{P}runing with inference Time Optimization for fast and low-memory
\textbf{V}LM, achieving efficient **pruning** without additional training or
fine-tuning. Instead of relying on attention scores, we formulate token **pruning**
as an optimization problem, accurately identifying important visual tokens
while remaining compatible with FlashAttention. Additionally, since we only
perform this **pruning** once during the prefilling stage, it effectively reduces
**KV** cache size. Our optimization framework incorporates a visual-aware cost
function considering factors such as Feature Similarity, Relative Spatial
Distance, and Absolute Central Distance, to measure the importance of each
source visual token, enabling effective **pruning** of low-importance tokens.
Extensive experiments demonstrate that our method outperforms previous token
**pruning** methods, validating the effectiveness and efficiency of our approach.


## Adaptive Rank Allocation Speeding Up Modern Transformers with RaNA Adapters

>Authors: Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu

>2025-03-23

> http://arxiv.org/abs/2503.18216v1

Large Language Models (LLMs) are computationally intensive, particularly
during inference. Neuron-adaptive techniques, which selectively activate
neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer
from limitations in modern Transformers. These include reliance on **sparse**
activations, incompatibility with attention layers, and the use of costly
neuron masking techniques. To address these issues, we propose the Adaptive
Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)
adapter. RaNA adapters leverage rank adapters, which operate on linear layers
by applying both low-rank matrix decompositions and adaptive masking to
efficiently allocate compute without depending on activation **sparsity**. This
enables RaNA to be generally applied to MLPs and linear components of attention
modules, while eliminating the need for expensive maskers found in
neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA
improves perplexity by up to 7 points and increases accuracy by up to 8
percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art
Transformer architectures. These results position RaNA as a robust solution for
improving inference efficiency in modern Transformer architectures.


## Can news and social media attention reduce the influence of problematic research?

>Authors: Er-Te Zheng, Hui-Zhen Fu, Xiaorui Jiang, Zhichao Fang, Mike Thelwall

>2025-03-23

> http://arxiv.org/abs/2503.18215v1

News and social media are widely used to disseminate science, but do they
also help raise awareness of problems in research? This study investigates
whether high levels of news and social media attention might accelerate the
retraction process and increase the visibility of retracted articles. To
explore this, we analyzed 15,642 news mentions, 6,588 blog mentions, and
404,082 X mentions related to 15,461 retracted articles. Articles receiving
high levels of news and X mentions were retracted more quickly than
non-mentioned articles in the same broad field and with comparable publication
years, author impact, and journal impact. However, this effect was not
statistically signicant for articles with high levels of blog mentions.
Notably, articles frequently mentioned in the news experienced a significant
increase in annual citation rates after their retraction, possibly because
media exposure enhances the visibility of retracted articles, making them more
likely to be cited. These findings suggest that increased public scrutiny can
improve the efficiency of scientific self-correction, although mitigating the
influence of retracted articles remains a gradual process.


## AgentRxiv Towards Collaborative Autonomous Research

>Authors: Samuel Schmidgall, Michael Moor

>2025-03-23

> http://arxiv.org/abs/2503.18102v1

Progress in scientific discovery is rarely the result of a single "Eureka"
moment, but is rather the product of hundreds of scientists incrementally
working together toward a common goal. While existing agent workflows are
capable of producing research autonomously, they do so in isolation, without
the ability to continuously improve upon prior research results. To address
these challenges, we introduce AgentRxiv-a framework that lets LLM agent
laboratories upload and retrieve reports from a shared preprint server in order
to collaborate, share insights, and iteratively build on each other's research.
We task agent laboratories to develop new reasoning and prompting techniques
and find that agents with access to their prior research achieve higher
performance improvements compared to agents operating in isolation (11.4%
relative improvement over baseline on MATH-500). We find that the best
performing strategy generalizes to benchmarks in other domains (improving on
average by 3.3%). Multiple agent laboratories sharing research through
AgentRxiv are able to work together towards a common goal, progressing more
rapidly than isolated laboratories, achieving higher overall accuracy (13.7%
relative improvement over baseline on MATH-500). These findings suggest that
autonomous agents may play a role in designing future AI systems alongside
humans. We hope that AgentRxiv allows agents to collaborate toward research
goals and enables researchers to accelerate discovery.


## Temporal Relation Extraction in Clinical Texts A Span-based Graph Transformer Approach

>Authors: Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio

>2025-03-23

> http://arxiv.org/abs/2503.18085v1

Temporal information extraction from unstructured text is essential for
contextualizing events and deriving actionable insights, particularly in the
medical domain. We address the task of extracting clinical events and their
temporal relations using the well-studied I2B2 2012 Temporal Relations
Challenge corpus. This task is inherently challenging due to complex clinical
language, long documents, and **sparse** annotations. We introduce GRAPHTREX, a
novel method integrating span-based entity-relation extraction, clinical large
pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)
to capture local and global dependencies. Our HGT component facilitates
information propagation across the document through innovative global landmarks
that bridge distant entities. Our method improves the state-of-the-art with
5.5% improvement in the tempeval $F_1$ score over the previous best and up to
8.9% improvement on long-range relations, which presents a formidable
challenge. This work not only advances temporal information extraction but also
lays the groundwork for improved diagnostic and prognostic models through
enhanced temporal reasoning.


## Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension

>Authors: Anh Duc Nguyen, Hieu Minh Phi, Anh Viet Ngo, Long Hai Trieu, Thai Phuong Nguyen

>2025-03-23

> http://arxiv.org/abs/2503.18062v1

Large Language Models (LLMs) have shown remarkable proficiency in Machine
Reading Comprehension (MRC) tasks; however, their effectiveness for
low-resource languages like Vietnamese remains largely unexplored. In this
paper, we fine-tune and evaluate two state-of-the-art LLMs: Llama 3 (8B
parameters) and Gemma (7B parameters), on ViMMRC, a Vietnamese MRC dataset. By
utilizing Quantized Low-Rank Adaptation (QLoRA), we efficiently fine-tune these
models and compare their performance against powerful LLM-based baselines.
Although our fine-tuned models are smaller than GPT-3 and GPT-3.5, they
outperform both traditional BERT-based approaches and these larger models. This
demonstrates the effectiveness of our fine-tuning process, showcasing how
modern LLMs can surpass the capabilities of older models like BERT while still
being suitable for deployment in resource-constrained environments. Through
intensive analyses, we explore various aspects of model performance, providing
valuable insights into adapting LLMs for low-resource languages like
Vietnamese. Our study contributes to the advancement of natural language
processing in low-resource languages, and we make our fine-tuned models
publicly available at: https://huggingface.co/iaiuet.


## WindowKV Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference

>Authors: Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song

>2025-03-23

> http://arxiv.org/abs/2503.17922v2

With the advancements in long-context inference capabilities of large
language models (LLMs), the **KV** cache has become one of the foundational
components. However, its substantial GPU memory consumption makes **KV** cache
compression a key technique for enabling efficient LLM inference in industrial
scenarios. While recent studies have focused on optimizing the memory occupied
by the **KV** cache, they overlook two critical factors: preserving semantic
coherence and considering task-specific characteristic during compression. To
address these limitations, we propose a novel task-adaptive **KV** cache window
selection method, Window**KV**. Window**KV** dynamically selects local semantic windows
consisting of consecutive tokens, according to task-specific characteristics,
ensuring the retained **KV** cache captures continuous, essential context.
Additionally, we introduce an intra-group layer **KV** cache indices sharing
strategy to reduce computational overhead, achieving a balance between
performance and efficiency. We rigorously evaluate Window**KV** on the LongBench
benchmark, and the results demonstrate that it maintains a performance
comparable to full **KV** cache retention while using only 12% of the original **KV**
cache, significantly reducing memory requirements. Furthermore, our method also
achieves state-of-the-art results in the Needle-in-a-Haystack evaluation,
highlighting its effectiveness and robustness.


## Accelerating and enhancing thermodynamic simulations of electrochemical interfaces

>Authors: Xiaochen Du, Mengren Liu, Jiayu Peng, Hoje Chun, Alexander Hoffman, Bilge Yildiz, Lin Li, Martin Z. Bazant, Rafael Gómez-Bombarelli

>2025-03-22

> http://arxiv.org/abs/2503.17870v1

Electrochemical interfaces are crucial in catalysis, energy storage, and
corrosion, where their stability and reactivity depend on complex interactions
between the electrode, adsorbates, and electrolyte. Predicting stable surface
structures remains challenging, as traditional surface Pourbaix diagrams tend
to either rely on expert knowledge or costly $\textit{ab initio}$ sampling, and
neglect thermodynamic equilibration with the environment. Machine learning (ML)
potentials can accelerate static modeling but often overlook dynamic surface
transformations. Here, we extend the Virtual Surface Site Relaxation-Monte
Carlo (VSSR-MC) method to autonomously sample surface reconstructions modeled
under aqueous electrochemical conditions. Through fine-tuning foundational ML
force fields, we accurately and efficiently predict surface energetics,
recovering known Pt(111) phases and revealing new LaMnO$_\mathrm{3}$(001)
surface reconstructions. By explicitly accounting for bulk-electrolyte
equilibria, our framework enhances electrochemical stability predictions,
offering a scalable approach to understanding and designing materials for
electrochemical applications.


## Feather-SQL A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models

>Authors: Wenqi Pei, Hailing Xu, Hengyuan Zhao, Shizheng Hou, Han Chen, Zining Zhang, Pingyi Luo, Bingsheng He

>2025-03-22

> http://arxiv.org/abs/2503.17811v1

Natural Language to SQL (NL2SQL) has seen significant advancements with large
language models (LLMs). However, these models often depend on closed-source
systems and high computational resources, posing challenges in data privacy and
deployment. In contrast, small language models (SLMs) struggle with NL2SQL
tasks, exhibiting poor performance and incompatibility with existing
frameworks. To address these issues, we introduce Feather-SQL, a new
lightweight framework tailored for SLMs. Feather-SQL improves SQL executability
and accuracy through 1) schema **pruning** and linking, 2) multi-path and
multi-candidate generation. Additionally, we introduce the 1+1 Model
Collaboration Paradigm, which pairs a strong general-purpose chat model with a
fine-tuned SQL specialist, combining strong analytical reasoning with
high-precision SQL generation. Experimental results on BIRD demonstrate that
Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for
models without fine-tuning. The proposed paradigm raises the accuracy ceiling
of SLMs to 54.76%, highlighting its effectiveness.


## Machine Learning - Driven Materials Discovery Unlocking Next-Generation Functional Materials - A minireview

>Authors: Dilshod Nematov, Mirabbos Hojamberdiev

>2025-03-22

> http://arxiv.org/abs/2503.18975v1

The rapid advancement of machine learning and artificial intelligence
(AI)-driven techniques is revolutionizing materials discovery, property
prediction, and material design by minimizing human intervention and
accelerating scientific progress. This review provides a comprehensive overview
of smart, machine learning (ML)-driven approaches, emphasizing their role in
predicting material properties, discovering novel compounds, and optimizing
material structures. Key methodologies ranging from deep learning, graph neural
networks, and Bayesian optimization to automated generative models, such as
generative adversarial networks (GANs) and variational autoencoders (VAEs)
enable the autonomous design of materials with tailored functionalities. By
leveraging AutoML frameworks (e.g., AutoGluon, TPOT, and H2O.ai), researchers
can automate the model selection, hyperparameter tuning, and feature
engineering, significantly improving the efficiency of materials informatics.
Furthermore, the integration of AI-driven robotic laboratories and
high-throughput computing has established a fully automated pipeline for rapid
synthesis and experimental validation, drastically reducing the time and cost
of material discovery. This review highlights real-world applications of
automated ML-driven approaches in predicting mechanical, thermal, electrical,
and optical properties of materials, demonstrating successful cases in
superconductors, catalysts, photovoltaics, and energy storage systems. We also
address key challenges, such as data quality, interpretability, and the
integration of AutoML with quantum computing, which are essential for future
advancements. Ultimately, the synergy between AI, automated experimentation,
and computational modeling transforms the way the materials are discovered,
optimized, and designed, paving the way for next-generation innovations in
energy, electronics, and nanotechnology.


## Energy-Aware LLMs A step towards sustainable AI for downstream applications

>Authors: Nguyen Phuc Tran, Brigitte Jaumard, Oscar Delgado

>2025-03-22

> http://arxiv.org/abs/2503.17783v1

Advanced Large Language Models (LLMs) have revolutionized various fields,
including communication networks, sparking an innovation wave that has led to
new applications and services, and significantly enhanced solution schemes.
Despite all these impressive developments, most LLMs typically require huge
computational resources, resulting in terribly high energy consumption. Thus,
this research study proposes an end-to-end pipeline that investigates the
trade-off between energy efficiency and model performance for an LLM during
fault ticket analysis in communication networks. It further evaluates the
pipeline performance using two real-world datasets for the tasks of root cause
analysis and response feedback in a communication network. Our results show
that an appropriate combination of **quantization** and **pruning** techniques is able
to reduce energy consumption while significantly improving model performance.


## Normalized Matching Transformer

>Authors: Abtin Pourhadi, Paul Swoboda

>2025-03-22

> http://arxiv.org/abs/2503.17715v1

We present a new state of the art approach for **sparse** keypoint matching
between pairs of images. Our method consists of a fully deep learning based
approach combining a visual backbone coupled with a SplineCNN graph neural
network for feature processing and a normalized transformer decoder for
decoding keypoint correspondences together with the Sinkhorn algorithm. Our
method is trained using a contrastive and a hyperspherical loss for better
feature representations. We additionally use data augmentation during training.
This comparatively simple architecture combining extensive normalization and
advanced losses outperforms current state of the art approaches on PascalVOC
and SPair-71k datasets by $5.1\%$ and $2.2\%$ respectively compared to BBGM,
ASAR, COMMON and GMTR while training for at least $1.7x$ fewer epochs.


## Causal Inference based Transfer Learning with LLMs An Efficient Framework for Industrial RUL Prediction

>Authors: Yan Chen, Cheng Liu

>2025-03-22

> http://arxiv.org/abs/2503.17686v1

Accurate prediction of Remaining Useful Life (RUL) for complex industrial
machinery is critical for the reliability and maintenance of mechatronic
systems, but it is challenged by high-dimensional, noisy sensor data. We
propose the Causal-Informed Data Pruning Framework (CIDPF), which pioneers the
use of causal inference to identify sensor signals with robust causal
relationships to RUL through PCMCI-based stability analysis, while a Gaussian
Mixture Model (GMM) screens for anomalies. By training on only 10% of the
pruned data, CIDPF fine-tunes pre-trained Large Language Models (LLMs) using
parameter-efficient strategies, reducing training time by 90% compared to
traditional approaches. Experiments on the N-CMAPSS dataset demonstrate that
CIDPF achieves a 26% lower RMSE than existing methods and a 25% improvement
over full-data baselines, showcasing superior accuracy and computational
efficiency in industrial mechatronic systems. The framework's adaptability to
multi-condition scenarios further underscores its practicality for industrial
deployment.


## Autonomous Radiotherapy Treatment Planning Using DOLA A Privacy-Preserving, LLM-Based Optimization Agent

>Authors: Humza Nusrat, Bing Luo, Ryan Hall, Joshua Kim, Hassan Bagher-Ebadian, Anthony Doemer, Benjamin Movsas, Kundan Thind

>2025-03-21

> http://arxiv.org/abs/2503.17553v1

Radiotherapy treatment planning is a complex and time-intensive process,
often impacted by inter-planner variability and subjective decision-making. To
address these challenges, we introduce Dose Optimization Language Agent (DOLA),
an autonomous large language model (LLM)-based agent designed for optimizing
radiotherapy treatment plans while rigorously protecting patient privacy. DOLA
integrates the LLaMa3.1 LLM directly with a commercial treatment planning
system, utilizing chain-of-thought prompting, retrieval-augmented generation
(RAG), and reinforcement learning (RL). Operating entirely within secure local
infrastructure, this agent eliminates external data sharing. We evaluated DOLA
using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in
20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and
optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.
The 70B model demonstrated significantly improved performance, achieving
approximately 16.4% higher final scores than the 8B model. The RAG approach
outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated
convergence, highlighting the synergy of retrieval-based memory and
reinforcement learning. Optimal temperature hyperparameter analysis identified
0.4 as providing the best balance between exploration and exploitation. This
proof of concept study represents the first successful deployment of locally
hosted LLM agents for autonomous optimization of treatment plans within a
commercial radiotherapy planning system. By extending human-machine interaction
through interpretable natural language reasoning, DOLA offers a scalable and
privacy-conscious framework, with significant potential for clinical
implementation and workflow improvement.


## Improving Quantization with Post-Training Model Expansion

>Authors: Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott

>2025-03-21

> http://arxiv.org/abs/2503.17513v1

The size of a model has been a strong predictor of its quality, as well as
its cost. As such, the trade-off between model cost and quality has been
well-studied. Post-training optimizations like **quantization** and **pruning** have
typically focused on reducing the overall volume of pre-trained models to
reduce inference costs while maintaining model quality. However, recent
advancements have introduced optimization techniques that, interestingly,
expand models post-training, increasing model size to improve quality when
reducing volume. For instance, to enable 4-bit weight and activation
**quantization**, incoherence processing often necessitates inserting online
Hadamard rotations in the compute graph, and preserving highly sensitive
weights often calls for additional higher precision computations. However, if
application requirements cannot be met, the prevailing solution is to relax
**quantization** constraints. In contrast, we demonstrate post-training model
expansion is a viable strategy to improve model quality within a **quantization**
co-design space, and provide theoretical justification. We show it is possible
to progressively and selectively expand the size of a pre-trained large
language model (LLM) to improve model quality without end-to-end retraining. In
particular, when quantizing the weights and activations to 4 bits for Llama3
1B, we reduce the zero-shot accuracy gap to full precision by an average of 3%
relative to both QuaRot and SpinQuant with only 5% more parameters, which is
still a 3.8% reduction in volume relative to a BF16 reference model.


## Variance Control via Weight Rescaling in LLM Pre-training

>Authors: Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian Güra

>2025-03-21

> http://arxiv.org/abs/2503.17500v1

The outcome of Large Language Model (LLM) pre-training strongly depends on
weight initialization and variance control strategies. Although the importance
of initial variance control has been well documented in neural networks in
general, the literature on initialization and management of its growth during
LLM pre-training, specifically, is somewhat **sparse**. In this paper, we introduce
the Layer Index Rescaling (LIR) weight initialization scheme, and the Target
Variance Rescaling (TVR) variance control strategy. Experiments on a 1B
parameter LLaMA model demonstrate that better variance management using these
techniques yields substantial improvements in downstream task performance (up
to 4.6% on common pre-training benchmarks) and reduces extreme activation
values, thus mitigating challenges associated with **quantization** and
low-precision training. Our code is available at:
https://github.com/bluorion-com/weight_rescaling.


## Efficient Knowledge Distillation via Curriculum Extraction

>Authors: Shivam Gupta, Sushrut Karmalkar

>2025-03-21

> http://arxiv.org/abs/2503.17494v1

Knowledge distillation is a technique used to train a small student network
using the output generated by a large teacher network, and has many empirical
advantages~\citep{Hinton2015DistillingTK}. While the standard one-shot approach
to distillation only uses the output of the final teacher network, recent
work~\citep{panigrahi2024progressive} has shown that using intermediate
checkpoints from the teacher's training process as an implicit ``curriculum''
for progressive distillation can significantly speed up training. However, such
schemes require storing these checkpoints, and often require careful selection
of the intermediate checkpoints to train on, which can be impractical for
large-scale training.
  In this paper, we show that a curriculum can be \emph{extracted} from just
the fully trained teacher network, and that this extracted curriculum can give
similar efficiency benefits to those of progressive distillation. Our
extraction scheme is natural; we use a random projection of the hidden
representations of the teacher network to progressively train the student
network, before training using the output of the full network. We show that our
scheme significantly outperforms one-shot distillation and achieves a
performance similar to that of progressive distillation for learning **sparse**
parities with two-layer networks, and provide theoretical guarantees for this
setting. Additionally, we show that our method outperforms one-shot
distillation even when using transformer-based architectures, both for
**sparse**-parity learning, and language modeling tasks.


## Pow3R Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors

>Authors: Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud

>2025-03-21

> http://arxiv.org/abs/2503.17316v1

We present Pow3r, a novel large 3D vision regression model that is highly
versatile in the input modalities it accepts. Unlike previous feed-forward
models that lack any mechanism to exploit known camera or scene priors at test
time, Pow3r incorporates any combination of auxiliary information such as
intrinsics, relative pose, dense or **sparse** depth, alongside input images,
within a single network. Building upon the recent DUSt3R paradigm, a
transformer-based architecture that leverages powerful pre-training, our
lightweight and versatile conditioning acts as additional guidance for the
network to predict more accurate estimates when auxiliary information is
available. During training we feed the model with random subsets of modalities
at each iteration, which enables the model to operate under different levels of
known priors at test time. This in turn opens up new capabilities, such as
performing inference in native image resolution, or point-cloud completion. Our
experiments on 3D reconstruction, depth completion, multi-view depth
prediction, multi-view stereo, and multi-view pose estimation tasks yield
state-of-the-art results and confirm the effectiveness of Pow3r at exploiting
all available information. The project webpage is
https://europe.naverlabs.com/pow3r.


## Bugdar AI-Augmented Secure Code Review for GitHub Pull Requests

>Authors: John Naulty, Eason Chen, Joy Wang, George Digkas, Kostas Chalkias

>2025-03-21

> http://arxiv.org/abs/2503.17302v1

As software systems grow increasingly complex, ensuring security during
development poses significant challenges. Traditional manual code audits are
often expensive, time-intensive, and ill-suited for fast-paced workflows, while
automated tools frequently suffer from high false-positive rates, limiting
their reliability. To address these issues, we introduce Bugdar, an
AI-augmented code review system that integrates seamlessly into GitHub pull
requests, providing near real-time, context-aware vulnerability analysis.
Bugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval
Augmented Generation (RAGs) to deliver project-specific, actionable feedback
that aligns with each codebase's unique requirements and developer practices.
Supporting multiple programming languages, including Solidity, Move, Rust, and
Python, Bugdar demonstrates exceptional efficiency, processing an average of
56.4 seconds per pull request or 30 lines of code per second. This is
significantly faster than manual reviews, which could take hours per pull
request. By facilitating a proactive approach to secure coding, Bugdar reduces
the reliance on manual reviews, accelerates development cycles, and enhances
the security posture of software systems without compromising productivity.


## Token Dynamics Towards Efficient and Dynamic Video Token Representation for Video Large Language Models

>Authors: Haichao Zhang, Zhuowei Li, Dimitris Metaxas, Yun Fu

>2025-03-21

> http://arxiv.org/abs/2503.16980v1

Token-based video representation has emerged as a promising approach for
enabling large language models to interpret video content. However, existing
token reduction techniques, such as token **pruning** and token merging, often
disrupt essential spatial-temporal positional embeddings, failing to adequately
balance computational efficiency with fewer tokens. Consequently, these methods
result in relatively lengthy token sequences, limiting their applicability in
scenarios requiring extreme token compression, such as video large language
models. In this paper, we introduce the novel task of extreme short token
reduction, aiming to represent extensive video sequences with a minimal number
of tokens. To address this challenge, we propose Token Dynamics, a new video
representation framework that dynamically reduces token count while preserving
spatial-temporal coherence. Specifically, we disentangle video representations
by separating visual embeddings from grid-level motion information, structuring
them into: 1. a concise token base, created by clustering tokens that describe
object-level content; 2. a token dynamics map, capturing detailed
spatial-temporal motion patterns across grids. Furthermore, we introduce a
cross-dynamics attention mechanism that integrates motion features into the
token base without increasing token length, thereby maintaining compactness and
spatial-temporal integrity. The experiments demonstrate a reduction of token
count to merely 0.07% of the original tokens, with only a minor performance
drop of 1.13%. Additionally, we propose two novel subtasks within extreme token
reduction (fixed-length and adaptive-length compression), both effectively
representing long token sequences for video-language tasks. Our method offers
significantly lower theoretical complexity, fewer tokens, and enhanced
throughput, thus providing an efficient solution for video LLMs.


## Rankformer A Graph Transformer for Recommendation based on Ranking Objective

>Authors: Sirui Chen, Shen Han, Jiawei Chen, Binbin Hu, Sheng Zhou, Gang Wang, Yan Feng, Chun Chen, Can Wang

>2025-03-21

> http://arxiv.org/abs/2503.16927v1

Recommender Systems (RS) aim to generate personalized ranked lists for each
user and are evaluated using ranking metrics. Although personalized ranking is
a fundamental aspect of RS, this critical property is often overlooked in the
design of model architectures. To address this issue, we propose Rankformer, a
ranking-inspired recommendation model. The architecture of Rankformer is
inspired by the gradient of the ranking objective, embodying a unique (graph)
transformer architecture -- it leverages global information from all users and
items to produce more informative representations and employs specific
attention weights to guide the evolution of embeddings towards improved ranking
performance. We further develop an **acceleration** algorithm for Rankformer,
reducing its complexity to a linear level with respect to the number of
positive instances. Extensive experimental results demonstrate that Rankformer
outperforms state-of-the-art methods. The code is available at
https://github.com/StupidThree/Rankformer.


## Design of 3D Non-Cartesian Trajectories for Fast Volumetric MRI via Analytic Coordinate Discretization

>Authors: Kwang Eun Jang, Dwight G. Nishimura

>2025-03-21

> http://arxiv.org/abs/2503.16918v1

3D non-Cartesian trajectories offer several advantages over rectilinear
trajectories for rapid volumetric imaging, including improved sampling
efficiency and greater robustness to motion, flow, and aliasing artifacts. In
this paper, we present a unified framework for designing three widely used
non-Cartesian trajectories: 3D Radial, 3D Cones, and Stack-of-Spirals. Our
approach is based on the idea that a non-Cartesian trajectory can be
interpreted as a discretized version of an analytic coordinate defined by a set
of template trajectories. Equivalently, the analytic coordinate is
conceptualized as a non-Cartesian trajectory composed of an infinite number of
copies of a set of template trajectories. The discretization is accomplished by
constructing a continuous spiral path on a surface and sampling points along
this path at unit intervals, leaving only the essential spokes/interleaves,
thereby yielding the practical non-Cartesian trajectory from the analytic
coordinate. One of the advantages of our approach is that the analytic density
compensation factor can be readily derived using Jacobian determinants, which
quantify changes in unit areas due to the transformation from the analytic
coordinate to the Cartesian grid. Additionally, the proposed approach derives
analytic formulae to compute the number of readouts based on prescribed
parameters, allowing us to specify the trajectory's **acceleration** factor for a
given total scan time. Furthermore, variable-density sampling can be easily
incorporated, and spokes/interleaves are smoothly distributed in k-space along
the derived spiral path, even for a small number of readouts. In a preliminary
phantom study, the proposed method demonstrated improved sampling efficiency
and image quality compared to the conventional approach.


## Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation

>Authors: Jiangcheng Qin, Xueyuan Zhang, Baisong Liu, Jiangbo Qian, Yangyang Wang

>2025-03-21

> http://arxiv.org/abs/2503.16875v1

Accurately predicting click-through rates (CTR) under stringent privacy
constraints poses profound challenges, particularly when user-item interactions
are **sparse** and fragmented across domains. Conventional cross-domain CTR (CCTR)
methods frequently assume homogeneous feature spaces and rely on centralized
data sharing, neglecting complex inter-domain discrepancies and the subtle
trade-offs imposed by privacy-preserving protocols. Here, we present Federated
Cross-Domain CTR Prediction with Large Language Model Augmentation
(FedCCTR-LM), a federated framework engineered to address these limitations by
synchronizing data augmentation, representation disentanglement, and adaptive
privacy protection. Our approach integrates three core innovations. First, the
Privacy-Preserving Augmentation Network (PrivAugNet) employs large language
models to enrich user and item representations and expand interaction
sequences, mitigating data **sparsity** and feature incompleteness. Second, the
Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL)
module disentangles domain-specific and shared user preferences, employing
intra-domain representation alignment (IDRA) and crossdomain representation
disentanglement (CDRD) to refine the learned embeddings and enhance knowledge
transfer across domains. Finally, the Adaptive Local Differential Privacy
(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal
balance between rigorous privacy guarantees and predictive accuracy. Empirical
evaluations on four real-world datasets demonstrate that FedCCTR-LM
substantially outperforms existing baselines, offering robust,
privacy-preserving, and generalizable cross-domain CTR prediction in
heterogeneous, federated environments.


## Towards LLM Guardrails via Sparse Representation Steering

>Authors: Zeqing He, Zhibo Wang, Huiyu Xu, Kui Ren

>2025-03-21

> http://arxiv.org/abs/2503.16851v1

Large Language Models (LLMs) have demonstrated remarkable performance in
natural language generation tasks, yet their uncontrolled outputs pose
significant ethical and safety risks. Recently, representation engineering
methods have shown promising results in steering model behavior by modifying
the rich semantic information encoded in activation vectors. However, due to
the difficulty of precisely disentangling semantic directions within
high-dimensional representation space, existing approaches suffer from three
major limitations: lack of fine-grained control, quality degradation of
generated content, and poor interpretability. To address these challenges, we
propose a **sparse** encoding-based representation engineering method, named SRE,
which decomposes polysemantic activations into a structured, monosemantic
feature space. By leveraging **sparse** autoencoding, our approach isolates and
adjusts only task-specific **sparse** feature dimensions, enabling precise and
interpretable steering of model behavior while preserving content quality. We
validate our method on three critical domains, i.e., safety, fairness, and
truthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show
that SRE achieves superior controllability while maintaining the overall
quality of generated content (i.e., controllability and quality), demonstrating
its effectiveness as a fine-grained and interpretable activation steering
framework.


## Multi-property directed generative design of inorganic materials through Wyckoff-augmented transfer learning

>Authors: Shuya Yamazaki, Wei Nong, Ruiming Zhu, Kostya S. Novoselov, Andrey Ustyuzhanin, Kedar Hippalgaonkar

>2025-03-21

> http://arxiv.org/abs/2503.16784v1

Accelerated materials discovery is an urgent demand to drive advancements in
fields such as energy conversion, storage, and catalysis. Property-directed
generative design has emerged as a transformative approach for rapidly
discovering new functional inorganic materials with multiple desired properties
within vast and complex search spaces. However, this approach faces two primary
challenges: data scarcity for functional properties and the multi-objective
optimization required to balance competing tasks. Here, we present a
multi-property-directed generative framework designed to overcome these
limitations and enhance site symmetry-compliant crystal generation beyond P1
(translational) symmetry. By incorporating Wyckoff-position-based data
augmentation and transfer learning, our framework effectively handles **sparse**
and small functional datasets, enabling the generation of new stable materials
simultaneously conditioned on targeted space group, band gap, and formation
energy. Using this approach, we identified previously unknown thermodynamically
and lattice-dynamically stable semiconductors in tetragonal, trigonal, and
cubic systems, with bandgaps ranging from 0.13 to 2.20 eV, as validated by
density functional theory (DFT) calculations. Additionally, we assessed their
thermoelectric descriptors using DFT, indicating their potential suitability
for thermoelectric applications. We believe our integrated framework represents
a significant step forward in generative design of inorganic materials.

