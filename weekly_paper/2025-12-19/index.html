<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>2025-12-19 - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2025-12-19";
        var mkdocs_page_input_path = "weekly_paper/2025-12-19.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-07/">2025-11-07</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-14/">2025-11-14</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-21/">2025-11-21</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-28/">2025-11-28</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-05/">2025-12-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-12/">2025-12-12</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">2025-12-19</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-26/">2025-12-26</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Legacy</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-07-25/">2025-07-25</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-01/">2025-08-01</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-08/">2025-08-08</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-15/">2025-08-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-22/">2025-08-22</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-29/">2025-08-29</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-05/">2025-09-05</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-15/">2025-09-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-19/">2025-09-19</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-26/">2025-09-26</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-28/">2025-09-28</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-09/">2025-10-09</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-17/">2025-10-17</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-24/">2025-10-24</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-31/">2025-10-31</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Weekly Paper</li>
      <li class="breadcrumb-item active">2025-12-19</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="2025-12-19">2025-12-19</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#plausibility-as-failure-how-llms-and-humans-co-construct-epistemic-error">Plausibility as Failure How LLMs and Humans Co-Construct Epistemic Error</a></li>
<li><a href="#treenet-a-light-weight-model-for-low-bitrate-image-compression">TreeNet A Light Weight Model for Low Bitrate Image Compression</a></li>
<li><a href="#yuan-tecswin-a-text-conditioned-diffusion-model-with-swin-transformer-blocks">Yuan-TecSwin A text conditioned Diffusion model with Swin-transformer blocks</a></li>
<li><a href="#stagevar-stage-aware-acceleration-for-visual-autoregressive-models">StageVAR Stage-Aware Acceleration for Visual Autoregressive Models</a></li>
<li><a href="#efficient-cpu-gpu-collaborative-inference-for-moe-based-llms-on-memory-limited-systems">Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems</a></li>
<li><a href="#SoK-Reviewing-Two-Decades-of-Security,-Privacy,-Accessibility,-and-Usability-Studies-on-Internet-of-Things-for-Older-Adults">SoK Reviewing Two Decades of Security, Privacy, Accessibility, and Usability Studies on Internet of Things for Older Adults</a></li>
<li><a href="#kascade-a-practical-sparse-attention-method-for-long-context-llm-inference">Kascade A Practical Sparse Attention Method for Long-Context LLM Inference</a></li>
<li><a href="#gmodiff-one-step-gain-map-refinement-with-diffusion-priors-for-hdr-reconstruction">GMODiff One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction</a></li>
<li><a href="#feature-selective-representation-misdirection-for-machine-unlearning">Feature-Selective Representation Misdirection for Machine Unlearning</a></li>
<li><a href="#Ein-Typenrad-auf-der-Überholspur-Die-Kult-Schreibmaschine-&quot;Erika&quot;-trifft-KI">Ein Typenrad auf der Überholspur Die Kult-Schreibmaschine "Erika" trifft KI</a></li>
<li><a href="#cka-guided-modular-quantization-beyond-bit-width-to-algorithmic-diversity">CKA-Guided Modular Quantization Beyond Bit-Width to Algorithmic Diversity</a></li>
<li><a href="#fast-collaborative-inference-via-distributed-speculative-decoding">Fast Collaborative Inference via Distributed Speculative Decoding</a></li>
<li><a href="#alignmerge---alignment-preserving-large-language-model-merging-via-fisher-guided-geometric-constraints">AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints</a></li>
<li><a href="#trustworthy-and-controllable-professional-knowledge-utilization-in-large-language-models-with-tee-gpu-execution">Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution</a></li>
<li><a href="#lopa-scaling-dllm-inference-via-lookahead-parallel-decoding">LoPA Scaling dLLM Inference via Lookahead Parallel Decoding</a></li>
<li><a href="#seggraph-leveraging-graphs-of-sam-segments-for-few-shot-3d-part-segmentation">SegGraph Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</a></li>
<li><a href="#LLM4Perf-Large-Language-Models-Are-Effective-Samplers-for-Multi-Objective-Performance-Modeling-(Copy)">LLM4Perf Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)</a></li>
<li><a href="#multipath-transfer-engine-breaking-gpu-and-host-memory-bandwidth-bottlenecks-in-llm-services">MultiPath Transfer Engine Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</a></li>
<li><a href="#enhancing-line-density-plots-with-outlier-control-and-bin-based-illumination">Enhancing Line Density Plots with Outlier Control and Bin-based Illumination</a></li>
<li><a href="#hierarchical-neural-surfaces-for-3d-mesh-compression">Hierarchical Neural Surfaces for 3D Mesh Compression</a></li>
<li><a href="#aie4ml-an-end-to-end-framework-for-compiling-neural-networks-for-the-next-generation-of-amd-ai-engines">AIE4ML An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</a></li>
<li><a href="#salve-sparse-autoencoder-latent-vector-editing-for-mechanistic-control-of-neural-networks">SALVE Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</a></li>
<li><a href="#dynamic-rebatching-for-efficient-early-exit-inference-with-drex">Dynamic Rebatching for Efficient Early-Exit Inference with DREX</a></li>
<li><a href="#multi-modal-semantic-communication">Multi-Modal Semantic Communication</a></li>
<li><a href="#the-longest-known-tails-of-ram-pressure-stripped-star-forming-galaxies-are-caused-by-an-icm-shock-in-abell-1367">The longest known tails of ram-pressure stripped star-forming galaxies are caused by an ICM shock in Abell 1367</a></li>
<li><a href="#VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression?">VTCBench Can Vision-Language Models Understand Long Context with Vision-Text Compression?</a></li>
<li><a href="#ic-effect-precise-and-efficient-video-effects-editing-via-in-context-learning">IC-Effect Precise and Efficient Video Effects Editing via In-Context Learning</a></li>
<li><a href="#note-on-bulk-viscosity-as-an-alternative-to-dark-energy">Note on bulk viscosity as an alternative to dark energy</a></li>
<li><a href="#reducing-pilots-in-channel-estimation-with-predictive-foundation-models">Reducing Pilots in Channel Estimation With Predictive Foundation Models</a></li>
<li><a href="#ctkvr-kv-cache-retrieval-for-long-context-llms-via-centroid-then-token-indexing">CTkvr KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing</a></li>
<li><a href="#attention-in-motion-secure-platooning-via-transformer-based-misbehavior-detection">Attention in Motion Secure Platooning via Transformer-based Misbehavior Detection</a></li>
<li><a href="#genai-enabled-residual-motion-estimation-for-energy-efficient-semantic-video-communication">GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication</a></li>
<li><a href="#randomized-orthogonalization-and-krylov-subspace-methods-principles-and-algorithms">Randomized orthogonalization and Krylov subspace methods principles and algorithms</a></li>
<li><a href="#three-dimensional-radio-localization-a-channel-charting-based-approach">Three-Dimensional Radio Localization A Channel Charting-Based Approach</a></li>
<li><a href="#emotion-recognition-in-signers">Emotion Recognition in Signers</a></li>
<li><a href="#adversarial-versification-in-portuguese-as-a-jailbreak-operator-in-llms">Adversarial versification in portuguese as a jailbreak operator in LLMs</a></li>
<li><a href="#llmq-efficient-lower-precision-pretraining-for-consumer-gpus">LLMQ Efficient Lower-Precision Pretraining for Consumer GPUs</a></li>
<li><a href="#keep-the-core-adversarial-priors-for-significance-preserving-brain-mri-segmentation">Keep the Core Adversarial Priors for Significance-Preserving Brain MRI Segmentation</a></li>
<li><a href="#defect-tolerance-and-local-structural-response-to-3d-transition-metal-substitution-in-cspbi3">Defect Tolerance and Local Structural Response to 3d Transition-Metal Substitution in CsPbI3</a></li>
<li><a href="#distillation-guided-structural-transfer-for-continual-learning-beyond-sparse-distributed-memory">Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</a></li>
<li><a href="#audio-visual-cross-modal-compression-for-generative-face-video-coding">Audio-Visual Cross-Modal Compression for Generative Face Video Coding</a></li>
<li><a href="#the-moralization-corpus-frame-based-annotation-and-analysis-of-moralizing-speech-acts-across-diverse-text-genres">The Moralization Corpus Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres</a></li>
<li><a href="#magnetised-turbulent-plasmas-as-high-energy-particle-accelerators">Magnetised turbulent plasmas as high-energy particle accelerators</a></li>
<li><a href="#DEER-Draft-with-Diffusion,-Verify-with-Autoregressive-Models">DEER Draft with Diffusion, Verify with Autoregressive Models</a></li>
<li><a href="#beyond-majority-voting-towards-fine-grained-and-more-reliable-reward-signal-for-test-time-reinforcement-learning">Beyond Majority Voting Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning</a></li>
<li><a href="#tracking-spatial-temporal-details-in-ultrasound-long-video-via-wavelet-analysis-and-memory-bank">Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</a></li>
</ul>
<h2 id="plausibility-as-failure-how-llms-and-humans-co-construct-epistemic-error">Plausibility as Failure How LLMs and Humans Co-Construct Epistemic Error</h2>
<blockquote>
<p>Authors: Claudia Vale Oliveira, Nelson Zagalo, Filipe Silva, Anabela Brandao, Syeda Faryal Hussain Khurrum, Joaquim Santos</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16750v1">http://arxiv.org/abs/2512.16750v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> assessment, digital literacy, and the design of trustworthy human AI <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>.</p>
<h2 id="treenet-a-light-weight-model-for-low-bitrate-image-compression">TreeNet A Light Weight Model for Low Bitrate Image Compression</h2>
<blockquote>
<p>Authors: Mahadev Prasad Panda, Purnachandra Rao Makkena, Srivatsa Prativadibhayankaram, Siegfried Fößel, André Kaup</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16743v1">http://arxiv.org/abs/2512.16743v1</a></p>
</blockquote>
<p>Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> techniques. In this work, we propose TreeNet, a novel low-complexity image <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> model that leverages a binary tree-structured encoder-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.</p>
<h2 id="yuan-tecswin-a-text-conditioned-diffusion-model-with-swin-transformer-blocks">Yuan-TecSwin A text conditioned Diffusion model with Swin-transformer blocks</h2>
<blockquote>
<p>Authors: Shaohua Wu, Tong Yu, Shenling Wang, Xudong Zhao</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16586v1">http://arxiv.org/abs/2512.16586v1</a></p>
</blockquote>
<p>Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> in this work. The Swin-<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> blocks take the place of CNN blocks in the encoder and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.</p>
<h2 id="stagevar-stage-aware-acceleration-for-visual-autoregressive-models">StageVAR Stage-Aware Acceleration for Visual Autoregressive Models</h2>
<blockquote>
<p>Authors: Senmao Li, Kai Wang, Salman Khan, Fahad Shahbaz Khan, Jian Yang, Yaxing Wang</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16483v1">http://arxiv.org/abs/2512.16483v1</a></p>
</blockquote>
<p>Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> framework for VAR models. Our analysis shows that early steps are critical for pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>. Building on these insights, StageVAR introduces a plug-and-play <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.</p>
<h2 id="efficient-cpu-gpu-collaborative-inference-for-moe-based-llms-on-memory-limited-systems">Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems</h2>
<blockquote>
<p>Authors: En-Ming Huang, Li-Shang Lin, Chun-Yi Lee</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16473v1">http://arxiv.org/abs/2512.16473v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> hits. Computations are offloaded to CPU for efficient <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.</p>
<h2 id="sok-reviewing-two-decades-of-security-privacy-accessibility-and-usability-studies-on-internet-of-things-for-older-adults">SoK Reviewing Two Decades of Security, Privacy, Accessibility, and Usability Studies on Internet of Things for Older Adults</h2>
<blockquote>
<p>Authors: Suleiman Saka, Sanchari Das</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16394v1">http://arxiv.org/abs/2512.16394v1</a></p>
</blockquote>
<p>The Internet of Things (IoT) has the potential to enhance older adults' independence and quality of life, but it also exposes them to security, privacy, accessibility, and usability (SPAU) risks. We conducted a systematic review of 44 peer-reviewed studies published between 2004 and 2024 using a five-phase screening pipeline. From each study, we extracted data on study design, IoT type, SPAU measures, and identified research gaps. We introduce the SPAU-IoT Framework, which comprises 27 criteria across four dimensions: security (e.g., resilience to cyber threats, secure authentication, encrypted <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, secure-by-default settings, and guardianship features), privacy (e.g., data minimization, explicit consent, and privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> analytics), accessibility (e.g., compliance with ADA/WCAG standards and assistive-technology compatibility), and usability (e.g., guided interaction, integrated assistance, and progressive learning). Applying this framework revealed that more than 70% of studies implemented authentication and encryption mechanisms, whereas fewer than 50% addressed accessibility or usability concerns. We further developed a threat model that maps IoT assets, networks, and backend servers to exploit vectors such as phishing, caregiver exploitation, and weak-password attacks, explicitly accounting for age-related vulnerabilities including cognitive decline and sensory impairment. Our results expose a systemic lack of integrated SPAU approaches in existing IoT research and translate these gaps into actionable, standards-aligned design guidelines for IoT systems designed for older adults.</p>
<h2 id="kascade-a-practical-sparse-attention-method-for-long-context-llm-inference">Kascade A Practical Sparse Attention Method for Long-Context LLM Inference</h2>
<blockquote>
<p>Authors: Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16391v1">http://arxiv.org/abs/2512.16391v1</a></p>
</blockquote>
<p>Attention is the dominant source of latency during long-context <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention method that leverages known observations such as 1) post-softmax attention is intrinsically <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> attention and 2.2x speedup in <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.</p>
<h2 id="gmodiff-one-step-gain-map-refinement-with-diffusion-priors-for-hdr-reconstruction">GMODiff One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction</h2>
<blockquote>
<p>Authors: Tao Hu, Weiyu Zhou, Yanjie Tu, Peng Wu, Wei Dong, Qingsen Yan, Yanning Zhang</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16357v1">http://arxiv.org/abs/2512.16357v1</a></p>
</blockquote>
<p>Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> of the LDM, suppressing hallucinations while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.</p>
<h2 id="feature-selective-representation-misdirection-for-machine-unlearning">Feature-Selective Representation Misdirection for Machine Unlearning</h2>
<blockquote>
<p>Authors: Taozhao Chen, Linghan Huang, Kim-Kwang Raymond Choo, Huaming Chen</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16297v1">http://arxiv.org/abs/2512.16297v1</a></p>
</blockquote>
<p>As large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.</p>
<h2 id="ein-typenrad-auf-der-uberholspur-die-kult-schreibmaschine-erika-trifft-ki">Ein Typenrad auf der Überholspur Die Kult-Schreibmaschine "Erika" trifft KI</h2>
<blockquote>
<p>Authors: Karola Köpferl, Albrecht Kurze</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16293v1">http://arxiv.org/abs/2512.16293v1</a></p>
</blockquote>
<p>In the 15th century, printing revolutionized the dissemination of information. Innovations such as typewriters and computers have increased the speed and volume of information flows over time. More recent developments in large language models such as ChatGPT enable text to be generated in a matter of seconds. However, many people do not understand how this works and what the long-term implications are. That is why we have "hacked" an old typewriter so that users can interact with an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> chatbot, which over 1,200 participants have now been able to experience. It helps to understand the possibilities and limitations of AI. It gives us researchers insights into participants' concepts of AI as well as their expectations and concerns. It raises questions about these technological developments and stimulates discussions about the social impact of the intensification and <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> of information and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> flows.</p>
<h2 id="cka-guided-modular-quantization-beyond-bit-width-to-algorithmic-diversity">CKA-Guided Modular Quantization Beyond Bit-Width to Algorithmic Diversity</h2>
<blockquote>
<p>Authors: Jinhao Zhang, Yunquan Zhang, Daning Chen</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16282v1">http://arxiv.org/abs/2512.16282v1</a></p>
</blockquote>
<p>Current mainstream post-training <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> methods for large language models typically apply a uniform <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> strategy per layer. The individually optimized strategies are then integrated to construct a hybrid <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d model. Experiments demonstrate that our approach consistently outperforms both uniform <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> baselines and state-of-the-art mixed-precision methods across mainstream <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.</p>
<h2 id="fast-collaborative-inference-via-distributed-speculative-decoding">Fast Collaborative Inference via Distributed Speculative Decoding</h2>
<blockquote>
<p>Authors: Ce Zheng, Ke Zhang, Sun Chen, Wenqi Zhang, Qiong Liu, Angesom Ataklity Tesfay</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16273v1">http://arxiv.org/abs/2512.16273v1</a></p>
</blockquote>
<p>Speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> accelerates large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference by allowing a small draft model to predict multiple future tokens for verification by a larger target model. In AI-native radio access networks (AI-RAN), this enables device-edge collaborative inference but introduces significant uplink overhead, as existing distributed speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> schemes transmit full vocabulary logits at every step. We propose a sparsify-then-sample strategy, Truncated Sparse Logits Transmission (TSLT), which transmits only the logits and indices of a truncated candidate set. We provide theoretical guarantees showing that the acceptance rate is preserved under TSLT. TSLT is further extended to multi-candidate case, where multiple draft candidates per step increase acceptance probability. Experiments show that TSLT significantly reduces uplink <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> while maintaining end-to-end inference latency and model quality, demonstrating its effectiveness for scalable, <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>-efficient distributed <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference in future AI-RAN systems.</p>
<h2 id="alignmerge-alignment-preserving-large-language-model-merging-via-fisher-guided-geometric-constraints">AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints</h2>
<blockquote>
<p>Authors: Aniruddha Roy, Jyoti Patel, Aman Chadha, Vinija Jain, Amitava Das</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16245v1">http://arxiv.org/abs/2512.16245v1</a></p>
</blockquote>
<p>Merging large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.</p>
<h2 id="trustworthy-and-controllable-professional-knowledge-utilization-in-large-language-models-with-tee-gpu-execution">Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution</h2>
<blockquote>
<p>Authors: Yifeng Cai, Zhida An, Yuhan Meng, Houqian Liu, Pengli Wang, Yao Guo, Ding Li</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16238v1">http://arxiv.org/abs/2512.16238v1</a></p>
</blockquote>
<p>Future improvements in large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> backbone.
  In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, multi-provider aggregation, and split-execution scheduling together make this design practical at <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.</p>
<h2 id="lopa-scaling-dllm-inference-via-lookahead-parallel-decoding">LoPA Scaling dLLM Inference via Lookahead Parallel Decoding</h2>
<blockquote>
<p>Authors: Chenkai Xu, Yijie Jin, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, Zhijie Deng</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16229v1">http://arxiv.org/abs/2512.16229v1</a></p>
</blockquote>
<p>Diffusion Large Language Models (d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have demonstrated significant potential for high-speed inference. However, current confidence-driven <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.</p>
<h2 id="seggraph-leveraging-graphs-of-sam-segments-for-few-shot-3d-part-segmentation">SegGraph Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</h2>
<blockquote>
<p>Authors: Yueyang Hu, Haiyong Jiang, Haoxuan Song, Jun Xiao, Hao Pan</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16143v1">http://arxiv.org/abs/2512.16143v1</a></p>
</blockquote>
<p>This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> and adjacency between segments while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (<a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.</p>
<h2 id="llm4perf-large-language-models-are-effective-samplers-for-multi-objective-performance-modeling-copy">LLM4Perf Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)</h2>
<blockquote>
<p>Authors: Xin Wang, Zhenhao Li, Zishuo Ding</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16070v1">http://arxiv.org/abs/2512.16070v1</a></p>
</blockquote>
<p>The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) motivates the central question of this work: Can <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-driven sampling. We design and implement <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>4Perf, a feedback-based framework, and use it to systematically evaluate the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-guided sampling process across four highly configurable, real-world systems. Our study reveals that the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-guided approach outperforms traditional baselines in most cases. Quantitatively, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s dual capabilities of configuration space <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> and feedback-driven strategy refinement. The effectiveness of this <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> choices for each component and hyperparameters within <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in performance engineering and offers concrete insights into the mechanisms that drive their success.</p>
<h2 id="multipath-transfer-engine-breaking-gpu-and-host-memory-bandwidth-bottlenecks-in-llm-services">MultiPath Transfer Engine Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</h2>
<blockquote>
<p>Authors: Lingfeng Tang, Daoping Zhang, Junjie Chen, Peihao Huang, Feng Jin, Chengguang Xu, Yuxin Chen, Feiqiang Sun, Guo Chen</p>
<p>2025-12-18</p>
<p><a href="http://arxiv.org/abs/2512.16056v1">http://arxiv.org/abs/2512.16056v1</a></p>
</blockquote>
<p>The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) performance, such as prefix <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> by 1.14x to 2.38x and decreases model-switching latency in v<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s sleep mode by 1.12x to 2.48x.</p>
<h2 id="enhancing-line-density-plots-with-outlier-control-and-bin-based-illumination">Enhancing Line Density Plots with Outlier Control and Bin-based Illumination</h2>
<blockquote>
<p>Authors: Yumeng Xue, Bin Chen, Patrick Paetzold, Yunhai Wang, Christophe Hurter, Oliver Deussen</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.16017v1">http://arxiv.org/abs/2512.16017v1</a></p>
</blockquote>
<p>Density plots effectively summarize large numbers of points, which would otherwise lead to severe overplotting in, for example, a scatter plot. However, when applied to line-based datasets, such as trajectories or time series, density plots alone are insufficient, as they disrupt path continuity, obscuring smooth trends and rare anomalies. We propose a bin-based illumination model that decouples structure from density to enhance flow and reveal <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> outliers while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the original colormap. We introduce a bin-based outlierness metric to rank trajectories. Guided by this ranking, we construct a structural normal map and apply locally-adaptive lighting in the luminance channel to highlight chosen patterns -- from dominant trends to atypical paths -- with acceptable color distortion. Our interactive method enables analysts to prioritize main trends, focus on outliers, or strike a balance between the two. We demonstrate our method on several real-world datasets, showing it reveals details missed by simpler alternatives, achieves significantly lower CIEDE2000 color distortion than standard shading, and supports interactive updates for up to 10,000 lines.</p>
<h2 id="hierarchical-neural-surfaces-for-3d-mesh-compression">Hierarchical Neural Surfaces for 3D Mesh Compression</h2>
<blockquote>
<p>Authors: Sai Karthikey Pentapati, Gregoire Phillips, Alan Bovik</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15985v1">http://arxiv.org/abs/2512.15985v1</a></p>
</blockquote>
<p>Implicit Neural Representations (INRs) have been demonstrated to achieve state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> of a broad range of modalities such as images, videos, 3D surfaces, and audio. Most studies have focused on building neural counterparts of traditional implicit representations of 3D geometries, such as signed distance functions. However, the triangle mesh-based representation of geometry remains the most widely used representation in the industry, while building INRs capable of generating them has been <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly studied. In this paper, we present a method for building compact INRs of zero-genus 3D manifolds. Our method relies on creating a spherical parameterization of a given 3D mesh - mapping the surface of a mesh to that of a unit sphere - then constructing an INR that encodes the displacement vector field defined continuously on its surface that regenerates the original shape. The compactness of our representation can be attributed to its hierarchical structure, wherein it first recovers the coarse structure of the encoded surface before adding high-frequency details to it. Once the INR is computed, 3D meshes of arbitrary resolution/connectivity can be <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>d from it. The <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> can be performed in real time while achieving a state-of-the-art trade-off between reconstruction quality and the size of the compressed representations.</p>
<h2 id="aie4ml-an-end-to-end-framework-for-compiling-neural-networks-for-the-next-generation-of-amd-ai-engines">AIE4ML An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</h2>
<blockquote>
<p>Authors: Dimitrios Danopoulos, Enrico Lupi, Chang Sun, Sebastian Dittmeier, Michael Kagan, Vladimir Loncar, Maurizio Pierini</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15946v1">http://arxiv.org/abs/2512.15946v1</a></p>
</blockquote>
<p>Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d models imported from high-level tools such as hls4ml or PyTorch while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.</p>
<h2 id="salve-sparse-autoencoder-latent-vector-editing-for-mechanistic-control-of-neural-networks">SALVE Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</h2>
<blockquote>
<p>Authors: Vegard Flovik</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15938v1">http://arxiv.org/abs/2512.15938v1</a></p>
</blockquote>
<p>Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an <script type="math/tex">\ell_1</script>-regularized autoencoder, we learn a <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, <script type="math/tex">α_{crit}</script>, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.</p>
<h2 id="dynamic-rebatching-for-efficient-early-exit-inference-with-drex">Dynamic Rebatching for Efficient Early-Exit Inference with DREX</h2>
<blockquote>
<p>Authors: Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15705v1">http://arxiv.org/abs/2512.15705v1</a></p>
</blockquote>
<p>Early-Exit (EE) is a Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the output quality intended by the EE model.</p>
<h2 id="multi-modal-semantic-communication">Multi-Modal Semantic Communication</h2>
<blockquote>
<p>Authors: Matin Mortaheb, Erciyes Karakaya, Sennur Ulukus</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15691v1">http://arxiv.org/abs/2512.15691v1</a></p>
</blockquote>
<p>Semantic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> aims to transmit information most relevant to a task rather than raw data, offering significant gains in <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> in complex and bandwidth-constrained environments.</p>
<h2 id="the-longest-known-tails-of-ram-pressure-stripped-star-forming-galaxies-are-caused-by-an-icm-shock-in-abell-1367">The longest known tails of ram-pressure stripped star-forming galaxies are caused by an ICM shock in Abell 1367</h2>
<blockquote>
<p>Authors: H. W. Edler, M. Hoeft, S. Bhagat, A. Basu, A. Drabent, K. Rajpurohit, M. Sun, F. de Gasperin, A. Botteon, M. Brüggen, A. Ignesti, I. D. Roberts, R. van Weeren</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15660v1">http://arxiv.org/abs/2512.15660v1</a></p>
</blockquote>
<p>The environment plays an important role in shaping the evolution of cluster galaxies through mechanisms such as ram pressure stripping (RPS), whose effect may be enhanced in merging clusters. We investigate a complex of three galaxies UGC 6697, CGCG 097-073 and CGCG 097-079, that are currently undergoing extreme RPS, as evident from their multi-wavelength-detected tails. The galaxies are members of the nearby (<script type="math/tex">d=92</script> Mpc) merging cluster Abell 1367 and are located in proximity to an intracluster medium (ICM) shock that is traced by X-ray observations and the presence of a radio relic. We analyze LOFAR and MeerKAT observations at frequencies of 54, 144, 817 and 1270 MHz to perform a detailed spectral analysis of the tails. We find that all three tails are significantly more extended than in previous radio studies, with lengths <script type="math/tex">\geq70</script> kpc. For UGC 6697, we detected a tail of 300 kpc, making it the longest known RPS tail of a star-forming galaxy at any wavelength. The length and spectral variations of the tail cannot be explained purely by the spectral aging of stripped cosmic rays. We construct a model of the tail that includes <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> and re-<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> due to the encounter with the nearby ICM shock, which can plausibly account for the extreme RPS as well as the length and spectral variation of the tail. We further discover a radio plume at the leading edge of UGC 6697 that connects to a narrow filament. These sources exhibit extremely steep (<script type="math/tex">α\approx-1.7</script>) and highly curved spectra. We speculate that this emission arises from cosmic rays re-energized by UGC 6697's rapid infall which propagate along magnetic filaments in the cluster center. Our findings represent direct evidence of a cluster merger shock impacting the evolution of member galaxies. Furthermore, we report the first tentative detection of particle <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> at the leading edge of an infalling galaxy.</p>
<h2 id="vtcbench-can-vision-language-models-understand-long-context-with-vision-text-compression">VTCBench Can Vision-Language Models Understand Long Context with Vision-Text Compression?</h2>
<blockquote>
<p>Authors: Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15649v1">http://arxiv.org/abs/2512.15649v1</a></p>
</blockquote>
<p>The computational and memory overheads associated with expanding the context window of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s severely limit their scalability. A noteworthy solution is vision-text <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.</p>
<h2 id="ic-effect-precise-and-efficient-video-effects-editing-via-in-context-learning">IC-Effect Precise and Efficient Video Effects Editing via In-Context Learning</h2>
<blockquote>
<p>Authors: Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15635v1">http://arxiv.org/abs/2512.15635v1</a></p>
</blockquote>
<p>We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning <script type="math/tex">15</script> high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.</p>
<h2 id="note-on-bulk-viscosity-as-an-alternative-to-dark-energy">Note on bulk viscosity as an alternative to dark energy</h2>
<blockquote>
<p>Authors: P. P. Avelino, A. R. Gomes, D. A. Tamayo</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15633v1">http://arxiv.org/abs/2512.15633v1</a></p>
</blockquote>
<p>Bulk viscosity, which characterizes the irreversible dissipative resistance of a fluid to volume changes, has been proposed as a potential mechanism for explaining both early- and late-time accelerated expansion of the Universe. In this work, we investigate two distinct physical scenarios for the origin of bulk viscosity: (1) nonminimal interactions between two fluids, and (2) elastic collisions in an ideal gas. In both cases, we demonstrate that while the associated energy-momentum exchange can significantly influence fluid dynamics, overall energy-momentum conservation precludes such exchange from having any direct gravitational effect in the context of General Relativity. In case (1), we show that the standard bulk viscous energy-momentum tensor can be obtained for the two-fluid system only at the cost of the violation of all classical energy conditions: null, weak, dominant, and strong. In case (2), we consider a single fluid composed of point particles undergoing instantaneous, energy- and momentum-con<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> collisions, and find that the proper pressure remains strictly non-negative, with the equation-of-state parameter confined to the interval <script type="math/tex">[0,1/3]</script>. In both scenarios, achieving a sufficiently negative effective pressure to drive cosmic <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> requires assumptions that compromise the physical viability of the model. Our results highlight some of the key physical challenges involved in modeling dark energy through bulk viscous effects.</p>
<h2 id="reducing-pilots-in-channel-estimation-with-predictive-foundation-models">Reducing Pilots in Channel Estimation With Predictive Foundation Models</h2>
<blockquote>
<p>Authors: Xingyu Zhou, Le Liang, Hao Ye, Jing Zhang, Chao-Kai Wen, Shi Jin</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15562v1">http://arxiv.org/abs/2512.15562v1</a></p>
</blockquote>
<p>Accurate channel state information (CSI) acquisition is essential for modern wireless systems, which becomes increasingly difficult under large antenna arrays, strict pilot overhead constraints, and diverse deployment environments. Existing artificial intelligence-based solutions often lack robustness and fail to generalize across scenarios. To address this limitation, this paper introduces a predictive-foundation-model-based channel estimation framework that enables accurate, low-overhead, and generalizable CSI acquisition. The proposed framework employs a predictive foundation model trained on large-scale cross-domain CSI data to extract universal channel representations and provide predictive priors with strong cross-scenario transferability. A pilot processing network based on a vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture is further designed to capture spatial, temporal, and frequency correlations from pilot observations. An efficient fusion mechanism integrates predictive priors with real-time measurements, enabling reliable CSI reconstruction even under <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> or noisy conditions. Extensive evaluations across diverse configurations demonstrate that the proposed estimator significantly outperforms both classical and data-driven baselines in accuracy, robustness, and generalization capability.</p>
<h2 id="ctkvr-kv-cache-retrieval-for-long-context-llms-via-centroid-then-token-indexing">CTkvr KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing</h2>
<blockquote>
<p>Authors: Kuan Lu, Shuhang Lin, Sai Wu, Yichen Yao, Junhan Yang, Huan Li, Wei Chu, Xu Yinghui, Yuan Qi, Gang Chen</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15550v1">http://arxiv.org/abs/2512.15550v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> and increased latency due to excessive memory accesses. Recent methods for dynamic <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CT<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>R, a novel centroid-then-token <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> retrieval scheme that addresses these limitations. CT<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>R leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> entries. Based on this insight, CT<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>R employs a two-stage retrieval strategy: lightweight centroids are precomputed during <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a>ing for centroid-grained indexing, followed by token-level refinement for precise <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CT<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>R achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CT<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>R delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.</p>
<h2 id="attention-in-motion-secure-platooning-via-transformer-based-misbehavior-detection">Attention in Motion Secure Platooning via Transformer-based Misbehavior Detection</h2>
<blockquote>
<p>Authors: Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15503v1">http://arxiv.org/abs/2512.15503v1</a></p>
</blockquote>
<p>Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance (<script type="math/tex">\geq</script> 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.</p>
<h2 id="genai-enabled-residual-motion-estimation-for-energy-efficient-semantic-video-communication">GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication</h2>
<blockquote>
<p>Authors: Shavbo Salehi, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid Bavand, Yigit Ozcan, Melike Erol-Kantarci</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15481v1">http://arxiv.org/abs/2512.15481v1</a></p>
</blockquote>
<p>Semantic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for con<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, hybrid, and adaptive bitrate semantic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.</p>
<h2 id="randomized-orthogonalization-and-krylov-subspace-methods-principles-and-algorithms">Randomized orthogonalization and Krylov subspace methods principles and algorithms</h2>
<blockquote>
<p>Authors: Jean-Guillaume de Damas, Laura Grigori, Igor Simunec, Edouard Timsit</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15455v1">http://arxiv.org/abs/2512.15455v1</a></p>
</blockquote>
<p>We present an overview of randomized orthogonalization techniques that construct a well-conditioned basis whose sketch is orthonormal. Randomized orthogonalization has recently emerged as a powerful paradigm for reducing the computational and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> cost of state-of-the-art orthogonalization procedures on parallel architectures, while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a>, and in some cases improving, their numerical stability. This approach can be employed within Krylov subspace methods to mitigate the cost of orthogonalization, yielding a randomized Arnoldi relation. We review the main variants of the randomized Gram--Schmidt and Householder QR algorithms, and discuss their application to Krylov methods for the solution of large-scale linear algebra problems, such as linear systems of equations, eigenvalue problems, the evaluation of matrix functions, and matrix equations.</p>
<h2 id="three-dimensional-radio-localization-a-channel-charting-based-approach">Three-Dimensional Radio Localization A Channel Charting-Based Approach</h2>
<blockquote>
<p>Authors: Phillip Stephan, Florian Euchner, Stephan ten Brink</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15399v1">http://arxiv.org/abs/2512.15399v1</a></p>
</blockquote>
<p>Channel charting creates a low-dimensional representation of the radio environment in a self-supervised manner using manifold learning. Pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> relative spatial distances in the latent space, channel charting is well suited to support user localization. While prior work on channel charting has mainly focused on two-dimensional scenarios, real-world environments are inherently three-dimensional. In this work, we investigate two distinct three-dimensional indoor localization scenarios using simulated, but realistic ray tracing-based datasets: a factory hall with a three-dimensional spatial distribution of datapoints, and a multistory building where each floor exhibits a two-dimensional datapoint distribution. For the first scenario, we apply the concept of augmented channel charting, which combines classical localization and channel charting, to a three-dimensional setting. For the second scenario, we introduce multistory channel charting, a two-stage approach consisting of floor classification via clustering followed by the training of a dedicated expert neural network for channel charting on each individual floor, thereby enhancing the channel charting performance. In addition, we propose a novel feature engineering method designed to extract <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> features from the beamspace channel state information that are suitable for localization.</p>
<h2 id="emotion-recognition-in-signers">Emotion Recognition in Signers</h2>
<blockquote>
<p>Authors: Kotaro Funakoshi, Yaoxiong Zhu</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15376v1">http://arxiv.org/abs/2512.15376v1</a></p>
</blockquote>
<p>Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="adversarial-versification-in-portuguese-as-a-jailbreak-operator-in-llms">Adversarial versification in portuguese as a jailbreak operator in LLMs</h2>
<blockquote>
<p>Authors: Joao Queiroz</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15353v1">http://arxiv.org/abs/2512.15353v1</a></p>
</blockquote>
<p>Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.</p>
<h2 id="llmq-efficient-lower-precision-pretraining-for-consumer-gpus">LLMQ Efficient Lower-Precision Pretraining for Consumer GPUs</h2>
<blockquote>
<p>Authors: Erik Schultheis, Dan Alistarh</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15306v1">http://arxiv.org/abs/2512.15306v1</a></p>
</blockquote>
<p>We present <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>Q, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>Q is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>Q rivals that of production-scale systems on much more expensive cloud-grade GPUs.</p>
<h2 id="keep-the-core-adversarial-priors-for-significance-preserving-brain-mri-segmentation">Keep the Core Adversarial Priors for Significance-Preserving Brain MRI Segmentation</h2>
<blockquote>
<p>Authors: Feifei Zhang, Zhenhong Jia, Sensen Song, Fei Shi, Aoxue Chen, Dayong Ren</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15811v1">http://arxiv.org/abs/2512.15811v1</a></p>
</blockquote>
<p>Medical image segmentation is constrained by <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce "Keep the Core," a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map <script type="math/tex">W</script> by solving an adversarial optimization problem to maximally degrade performance, while an <script type="math/tex">\ell_1</script>
<a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \&amp; Preservation) module uses <script type="math/tex">W</script> for a two-pronged augmentation strategy: (1) Semantic-Pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an <script type="math/tex">\text{MAE}</script>-style reconstruction, forcing the model to learn robust representations from preserved critical features. "Keep the Core" is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets.</p>
<h2 id="defect-tolerance-and-local-structural-response-to-3d-transition-metal-substitution-in-cspbi3">Defect Tolerance and Local Structural Response to 3d Transition-Metal Substitution in CsPbI3</h2>
<blockquote>
<p>Authors: Misbah Shaheen, Sheharyar Pervez</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15280v1">http://arxiv.org/abs/2512.15280v1</a></p>
</blockquote>
<p>We present a systematic first-principles study of substitutional 3d transition-metal (TM) defects in CsPbI3 using the spin-polarized GGA+U framework. TM incorporation is generally energetically favorable and induces lattice distortions that are strongly localized around the defect site, pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the overall structural integrity of the host. Analysis of defect formation energies and electronic structure shows that, with the exception of Sc and Ti, CsPbI3 exhibits a strong resistance to deep trap formation. Most TM substitutions instead introduce resonant states that hybridize with the band edges, consistent with the defect-tolerant nature of the material. While these states can modify the band gap, they do not generate isolated mid-gap traps. The observed distortions arise from strain-driven Van Vleck modes governed by ionic-radius mismatch, electronegativity differences, and TM-I orbital <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>, with amplitudes that decay rapidly away from the defect. Spin-polarized calculations reveal significant TM-induced spin polarization on the ligands and, in some cases, on neighboring Pb atoms, reflecting variations in covalency and hybridization across the 3d series. Together, these results establish a unified picture in which local structural response, electronic hybridization, and spin polarization jointly control the stability and electronic impact of TM defects in CsPbI3 , identifying dopants that are electronically benign or detrimental.</p>
<h2 id="distillation-guided-structural-transfer-for-continual-learning-beyond-sparse-distributed-memory">Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</h2>
<blockquote>
<p>Authors: Huiyan Xue, Xuming Ran, Yaxin Li, Qi Xu, Enhui Li, Yi Xu, Qiang Zhang</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15267v1">http://arxiv.org/abs/2512.15267v1</a></p>
</blockquote>
<p>Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> continual learning.</p>
<h2 id="audio-visual-cross-modal-compression-for-generative-face-video-coding">Audio-Visual Cross-Modal Compression for Generative Face Video Coding</h2>
<blockquote>
<p>Authors: Youmin Xu, Mengxi Guo, Shijie Zhao, Weiqi Li, Junlin Li, Li Zhang, Jian Zhang</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15262v1">http://arxiv.org/abs/2512.15262v1</a></p>
</blockquote>
<p>Generative face video coding (GFVC) is vital for modern applications like video conferencing, yet existing methods primarily focus on video motion while neglecting the significant bitrate contribution of audio. Despite the well-established correlation between audio and lip movements, this cross-modal coherence has not been systematically exploited for <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>. To address this, we propose an Audio-Visual Cross-Modal Compression (AVCC) framework that jointly compresses audio and video streams. Our framework extracts motion information from video and tokenizes audio features, then aligns them through a unified audio-video diffusion process. This allows synchronized reconstruction of both modalities from a shared representation. In extremely low-rate scenarios, AVCC can even reconstruct one modality from the other. Experiments show that AVCC significantly outperforms the Versatile Video Coding (VVC) standard and state-of-the-art GFVC schemes in rate-distortion performance, paving the way for more efficient multimodal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> systems.</p>
<h2 id="the-moralization-corpus-frame-based-annotation-and-analysis-of-moralizing-speech-acts-across-diverse-text-genres">The Moralization Corpus Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres</h2>
<blockquote>
<p>Authors: Maria Becker, Mirko Sommer, Lars Tapken, Yi Wan Teh, Bruno Brocai</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15248v1">http://arxiv.org/abs/2512.15248v1</a></p>
</blockquote>
<p>Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.</p>
<h2 id="magnetised-turbulent-plasmas-as-high-energy-particle-accelerators">Magnetised turbulent plasmas as high-energy particle accelerators</h2>
<blockquote>
<p>Authors: M. Lemoine</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15239v1">http://arxiv.org/abs/2512.15239v1</a></p>
</blockquote>
<p>This proceedings paper reports on the theoretical modelling of particle <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> in magnetised turbulent plasmas. It briefly reviews some recent findings obtained from fully kinetic numerical simulations of large-amplitude, semi to fully relativistic turbulence. The paper then argues that these findings can be understood within the framework of a ``generalised Fermi'' picture of stochastic <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>, which it summarises. The dominant contributions to <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> appear to arise from particle interactions with sharp, dynamic bends of the magnetic field lines and regions of velocity <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>. Interestingly, the <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> rate is spatially inhomogeneous and its probability distribution follows a broken power law extending up to large values. This makes relativistic, large-amplitude turbulence an extreme particle accelerator. Some implications for particle transport and the shape of the particle energy spectrum in the presence of radiative losses and over long timescales are also discussed.</p>
<h2 id="deer-draft-with-diffusion-verify-with-autoregressive-models">DEER Draft with Diffusion, Verify with Autoregressive Models</h2>
<blockquote>
<p>Authors: Zicong Cheng, Guo-Wei Yang, Jia Li, Zhijie Deng, Meng-Hao Guo, Shi-Min Hu</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15176v1">http://arxiv.org/abs/2512.15176v1</a></p>
</blockquote>
<p>Efficiency, as a critical practical challenge for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>. Speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> strategy. Building on this insight, we introduce DEER, an efficient speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based drafters with the target AR model, and further adopts single-step <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/</p>
<h2 id="beyond-majority-voting-towards-fine-grained-and-more-reliable-reward-signal-for-test-time-reinforcement-learning">Beyond Majority Voting Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning</h2>
<blockquote>
<p>Authors: Weiqin Wang, Yile Wang, Kehao Chen, Hui Huang</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15146v2">http://arxiv.org/abs/2512.15146v2</a></p>
</blockquote>
<p>Test-time reinforcement learning mitigates the reliance on annotated data by using majority voting results as pseudo-labels, emerging as a complementary direction to reinforcement learning with verifiable rewards (RLVR) for improving reasoning ability of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, this voting strategy often induces confirmation bias and suffers from <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> rewards, limiting the overall performance. In this work, we propose subgroup-specific step-wise confidence-weighted pseudo-label estimation (SCOPE), a framework integrating model confidence and dynamic subgroup partitioning to address these issues. Specifically, SCOPE integrates the proposed step-wise confidence into pseudo label deduction, prioritizing high-quality reasoning paths over simple frequency count. Furthermore, it dynamically partitions the candidate outputs pool into independent subgroups by balancing reasoning quality against exploration diversity. By deriving local consensus via repeat sampling for each sub group, SCOPE provides diverse supervision targets to encourage broader exploration. We conduct experiments across various models and benchmarks, experimental results show that SCOPE consistently outperforms recent baselines. Notably, SCOPE achieving relative improvements of 13.1% on challenging AIME 2025 and 8.1% on AMC. The code is released at https://github.com/szu-tera/SCOPE.</p>
<h2 id="tracking-spatial-temporal-details-in-ultrasound-long-video-via-wavelet-analysis-and-memory-bank">Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</h2>
<blockquote>
<p>Authors: Chenxiao Zhang, Runshi Zhang, Junchen Wang</p>
<p>2025-12-17</p>
<p><a href="http://arxiv.org/abs/2512.15066v1">http://arxiv.org/abs/2512.15066v1</a></p>
</blockquote>
<p>Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.</p>
              
  <!-- Giscus 评论系统 - 只在 notes 文件夹下显示 -->
<script>
  // 使用 JavaScript 来判断 URL 路径
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../2025-12-12/" class="btn btn-neutral float-left" title="2025-12-12"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../2025-12-26/" class="btn btn-neutral float-right" title="2025-12-26">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../2025-12-12/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../2025-12-26/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../js/prism-prototxt.js"></script>
      <script src="../../js/preview.js"></script>
      <script src="../../js/back-to-top.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
