# 2025-06-13

# Table of Contents
* [Beyond Attention or Similarity Maximizing Conditional Diversity for Token Pruning in MLLMs](#Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs)
* [SWE-Factory Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](#SWE-Factory-Your-Automated-Factory-for-Issue-Resolution-Training-Data-and-Evaluation-Benchmarks)
* [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](#Decomposing-MLP-Activations-into-Interpretable-Features-via-Semi-Nonnegative-Matrix-Factorization)
* [Accelerating Diffusion Large Language Models with SlowFast The Three Golden Principles](#Accelerating-Diffusion-Large-Language-Models-with-SlowFast-The-Three-Golden-Principles)
* [Constructing and Evaluating Declarative RAG Pipelines in PyTerrier](#Constructing-and-Evaluating-Declarative-RAG-Pipelines-in-PyTerrier)
* [Transformer IMU Calibrator Dynamic On-body IMU Calibration for Inertial Motion Capture](#Transformer-IMU-Calibrator-Dynamic-On-body-IMU-Calibration-for-Inertial-Motion-Capture)
* [Lambert's problem in orbital dynamics a self--contained introduction](#Lambert's-problem-in-orbital-dynamics-a-self--contained-introduction)
* [MNN-LLM A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices](#MNN-LLM-A-Generic-Inference-Engine-for-Fast-Large-Language-Model-Deployment-on-Mobile-Devices)
* [Broadband Tunable Deep-UV Emission from AI-Optimized Nonlinear Metasurface Architectures](#Broadband-Tunable-Deep-UV-Emission-from-AI-Optimized-Nonlinear-Metasurface-Architectures)
* [DART Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba](#DART-Differentiable-Dynamic-Adaptive-Region-Tokenizer-for-Vision-Transformer-and-Mamba)
* [TreeLoRA Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree](#TreeLoRA-Efficient-Continual-Learning-via-Layer-Wise-LoRAs-Guided-by-a-Hierarchical-Gradient-Similarity-Tree)
* [Heterogeneous-IRS-Assisted MIMO Systems Channel Estimation and Beamforming](#Heterogeneous-IRS-Assisted-MIMO-Systems-Channel-Estimation-and-Beamforming)
* [LightKG Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture](#LightKG-Efficient-Knowledge-Aware-Recommendations-with-Simplified-GNN-Architecture)
* [PointGS Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](#PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting)
* ["Check My Work?" Measuring Sycophancy in a Simulated Educational Context](#"Check-My-Work?"-Measuring-Sycophancy-in-a-Simulated-Educational-Context)
* [Discrete Audio Tokens More Than a Survey!](#Discrete-Audio-Tokens-More-Than-a-Survey!)
* [AWP Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent](#AWP-Activation-Aware-Weight-Pruning-and-Quantization-with-Projected-Gradient-Descent)
* [Prompt Variability Effects On LLM Code Generation](#Prompt-Variability-Effects-On-LLM-Code-Generation)
* [Formalizing Neuromorphic Control Systems A General Proposal and A Rhythmic Case Study](#Formalizing-Neuromorphic-Control-Systems-A-General-Proposal-and-A-Rhythmic-Case-Study)
* [Attention on flow control transformer-based reinforcement learning for lift regulation in highly disturbed flows](#Attention-on-flow-control-transformer-based-reinforcement-learning-for-lift-regulation-in-highly-disturbed-flows)
* [EfficientVLA Training-Free Acceleration and Compression for Vision-Language-Action Models](#EfficientVLA-Training-Free-Acceleration-and-Compression-for-Vision-Language-Action-Models)
* [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](#Reward-Models-Enable-Scalable-Code-Verification-by-Trading-Accuracy-for-Throughput)
* [HadaNorm Diffusion Transformer Quantization through Mean-Centered Transformations](#HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations)
* [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](#Causal-Sufficiency-and-Necessity-Improves-Chain-of-Thought-Reasoning)
* [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](#Do-LLMs-Give-Psychometrically-Plausible-Responses-in-Educational-Assessments?)
* [Q-SAM2 Accurate Quantization for Segment Anything Model 2](#Q-SAM2-Accurate-Quantization-for-Segment-Anything-Model-2)
* [Auto-Compressing Networks](#Auto-Compressing-Networks)
* [Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks](#Multi-Level-Damage-Aware-Graph-Learning-for-Resilient-UAV-Swarm-Networks)
* [SparseSSM Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](#SparseSSM-Efficient-Selective-Structured-State-Space-Models-Can-Be-Pruned-in-One-Shot)
* [Unmasking real-world audio deepfakes A data-centric approach](#Unmasking-real-world-audio-deepfakes-A-data-centric-approach)
* [Attention-Based Map Encoding for Learning Generalized Legged Locomotion](#Attention-Based-Map-Encoding-for-Learning-Generalized-Legged-Locomotion)
* [Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities](#Integrating-Quantized-LLMs-into-Robotics-Systems-as-Edge-AI-to-Leverage-their-Natural-Language-Processing-Capabilities)
* [Understanding the Performance and Power of LLM Inferencing on Edge Accelerators](#Understanding-the-Performance-and-Power-of-LLM-Inferencing-on-Edge-Accelerators)
* [HAIF-GS Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](#HAIF-GS-Hierarchical-and-Induced-Flow-Guided-Gaussian-Splatting-for-Dynamic-Scene)
* [TinySplat Feedforward Approach for Generating Compact 3D Scene Representation](#TinySplat-Feedforward-Approach-for-Generating-Compact-3D-Scene-Representation)
* [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](#Attention-Bayesian-Hybrid-Approach-to-Modular-Multiple-Particle-Tracking)
* [ODG Occupancy Prediction Using Dual Gaussians](#ODG-Occupancy-Prediction-Using-Dual-Gaussians)
* [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](#Synergizing-Reinforcement-Learning-and-Genetic-Algorithms-for-Neural-Combinatorial-Optimization)
* [SLED A Speculative LLM Decoding Framework for Efficient Edge Serving](#SLED-A-Speculative-LLM-Decoding-Framework-for-Efficient-Edge-Serving)
* [A new approach for image segmentation based on diffeomorphic registration and gradient fields](#A-new-approach-for-image-segmentation-based-on-diffeomorphic-registration-and-gradient-fields)
* [DIVE into MoE Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](#DIVE-into-MoE-Diversity-Enhanced-Reconstruction-of-Large-Language-Models-from-Dense-into-Mixture-of-Experts)
* [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](#Autoregressive-Adversarial-Post-Training-for-Real-Time-Interactive-Video-Generation)
* [Latent Multi-Head Attention for Small Language Models](#Latent-Multi-Head-Attention-for-Small-Language-Models)
* [Quantum Algorithm Software for Condensed Matter Physics](#Quantum-Algorithm-Software-for-Condensed-Matter-Physics)
* [ScalableHD Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](#ScalableHD-Scalable-and-High-Throughput-Hyperdimensional-Computing-Inference-on-Multi-Core-CPUs)
* [FastFLUX Pruning FLUX with Block-wise Replacement and Sandwich Training](#FastFLUX-Pruning-FLUX-with-Block-wise-Replacement-and-Sandwich-Training)
* [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](#Robust-Noise-Attenuation-via-Adaptive-Pooling-of-Transformer-Outputs)
* [The Quantum Paldus Transform Efficient Circuits with Applications](#The-Quantum-Paldus-Transform-Efficient-Circuits-with-Applications)
* [Understanding Task Vectors in In-Context Learning Emergence, Functionality, and Limitations](#Understanding-Task-Vectors-in-In-Context-Learning-Emergence,-Functionality,-and-Limitations)
* [FZOO Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](#FZOO-Fast-Zeroth-Order-Optimizer-for-Fine-Tuning-Large-Language-Models-towards-Adam-Scale-Speed)
* [CodeBrain Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](#CodeBrain-Bridging-Decoupled-Tokenizer-and-Multi-Scale-Architecture-for-EEG-Foundation-Model)
* [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](#Unifying-Block-wise-PTQ-and-Distillation-based-QAT-for-Progressive-Quantization-toward-2-bit-Instruction-Tuned-LLMs)
* [Hardware Limitations and Optimization Approach in 1-Bit RIS Design at 28 GHz](#Hardware-Limitations-and-Optimization-Approach-in-1-Bit-RIS-Design-at-28-GHz)
* [PlantBert An Open Source Language Model for Plant Science](#PlantBert-An-Open-Source-Language-Model-for-Plant-Science)
* [SeerAttention-R Sparse Attention Adaptation for Long Reasoning](#SeerAttention-R-Sparse-Attention-Adaptation-for-Long-Reasoning)
* [HunyuanVideo-HOMA Generic Human-Object Interaction in Multimodal Driven Human Animation](#HunyuanVideo-HOMA-Generic-Human-Object-Interaction-in-Multimodal-Driven-Human-Animation)
* [POLARON Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](#POLARON-Precision-aware-On-device-Learning-and-Adaptive-Runtime-cONfigurable-AI-acceleration)
* [sparseGeoHOPCA A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](#sparseGeoHOPCA-A-Geometric-Solution-to-Sparse-Higher-Order-PCA-Without-Covariance-Estimation)
* [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](#Diversity-Guided-MLP-Reduction-for-Efficient-Large-Vision-Transformers)
* [k-Planar and Fan-Crossing Drawings and Transductions of Planar Graphs](#k-Planar-and-Fan-Crossing-Drawings-and-Transductions-of-Planar-Graphs)
* [The Geometries of Truth Are Orthogonal Across Tasks](#The-Geometries-of-Truth-Are-Orthogonal-Across-Tasks)
* [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](#Plug-and-Play-Linear-Attention-for-Pre-trained-Image-and-Video-Restoration-Models)
* [CoQMoE Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA](#CoQMoE-Co-Designed-Quantization-and-Computation-Orchestration-for-Mixture-of-Experts-Vision-Transformer-on-FPGA)
* [Fairness is Not Silence Unmasking Vacuous Neutrality in Small Language Models](#Fairness-is-Not-Silence-Unmasking-Vacuous-Neutrality-in-Small-Language-Models)
* [Olica Efficient Structured Pruning of Large Language Models without Retraining](#Olica-Efficient-Structured-Pruning-of-Large-Language-Models-without-Retraining)
* [Draft-based Approximate Inference for LLMs](#Draft-based-Approximate-Inference-for-LLMs)
* [DEAL Disentangling Transformer Head Activations for LLM Steering](#DEAL-Disentangling-Transformer-Head-Activations-for-LLM-Steering)
* [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](#Hyperspectral-Image-Classification-via-Transformer-based-Spectral-Spatial-Attention-Decoupling-and-Adaptive-Gating)
* [An efficient Fourier spectral algorithm for the Bogoliubov-de Gennes excitation eigenvalue problem](#An-efficient-Fourier-spectral-algorithm-for-the-Bogoliubov-de-Gennes-excitation-eigenvalue-problem)
* [AutoSDT Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](#AutoSDT-Scaling-Data-Driven-Discovery-Tasks-Toward-Open-Co-Scientists)
* [Vision Transformers Don't Need Trained Registers](#Vision-Transformers-Don't-Need-Trained-Registers)
* [Aligning Text, Images, and 3D Structure Token-by-Token](#Aligning-Text,-Images,-and-3D-Structure-Token-by-Token)
* [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](#A-Generative-Physics-Informed-Reinforcement-Learning-Based-Approach-for-Construction-of-Representative-Drive-Cycle)
* [Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model](#Efficient-Seismic-Data-Interpolation-via-Sparse-Attention-Transformer-and-Diffusion-Model)
* [Speedy Deformable 3D Gaussian Splatting Fast Rendering and Compression of Dynamic Scenes](#Speedy-Deformable-3D-Gaussian-Splatting-Fast-Rendering-and-Compression-of-Dynamic-Scenes)
* [MiniCPM4 Ultra-Efficient LLMs on End Devices](#MiniCPM4-Ultra-Efficient-LLMs-on-End-Devices)
* [Evaluating Large Language Models on the Frame and Symbol Grounding Problems A Zero-shot Benchmark](#Evaluating-Large-Language-Models-on-the-Frame-and-Symbol-Grounding-Problems-A-Zero-shot-Benchmark)
* [CrosswalkNet An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing](#CrosswalkNet-An-Optimized-Deep-Learning-Framework-for-Pedestrian-Crosswalk-Detection-in-Aerial-Images-with-High-Performance-Computing)
* [Teaching special relativity in elementary physics or upper high school courses](#Teaching-special-relativity-in-elementary-physics-or-upper-high-school-courses)
* [Learning to Focus Causal Attention Distillation via Gradient-Guided Token Pruning](#Learning-to-Focus-Causal-Attention-Distillation-via-Gradient-Guided-Token-Pruning)
* [Training Superior Sparse Autoencoders for Instruct Models](#Training-Superior-Sparse-Autoencoders-for-Instruct-Models)
* [MCPWorld A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](#MCPWorld-A-Unified-Benchmarking-Testbed-for-API,-GUI,-and-Hybrid-Computer-Use-Agents)
* [ProSplat Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views](#ProSplat-Improved-Feed-Forward-3D-Gaussian-Splatting-for-Wide-Baseline-Sparse-Views)
* [Event-Priori-Based Vision-Language Model for Efficient Visual Understanding](#Event-Priori-Based-Vision-Language-Model-for-Efficient-Visual-Understanding)
* [IDENT Review Recent Advances in Identification of Differential Equations from Noisy Data](#IDENT-Review-Recent-Advances-in-Identification-of-Differential-Equations-from-Noisy-Data)
* [TwinBreak Jailbreaking LLM Security Alignments based on Twin Prompts](#TwinBreak-Jailbreaking-LLM-Security-Alignments-based-on-Twin-Prompts)
* [MoQAE Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts](#MoQAE-Mixed-Precision-Quantization-for-Long-Context-LLM-Inference-via-Mixture-of-Quantization-Aware-Experts)
* [STREAMINGGS Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](#STREAMINGGS-Voxel-Based-Streaming-3D-Gaussian-Splatting-with-Memory-Optimization-and-Architectural-Support)
* [Graph-of-Causal Evolution Challenging Chain-of-Model for Reasoning](#Graph-of-Causal-Evolution-Challenging-Chain-of-Model-for-Reasoning)
* [LiteVLM A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](#LiteVLM-A-Low-Latency-Vision-Language-Model-Inference-Pipeline-for-Resource-Constrained-Environments)
* [MapBERT Bitwise Masked Modeling for Real-Time Semantic Mapping Generation](#MapBERT-Bitwise-Masked-Modeling-for-Real-Time-Semantic-Mapping-Generation)
* [Improving LLM Reasoning through Interpretable Role-Playing Steering](#Improving-LLM-Reasoning-through-Interpretable-Role-Playing-Steering)
* [Graph-KV Breaking Sequence via Injecting Structural Biases into Large Language Models](#Graph-KV-Breaking-Sequence-via-Injecting-Structural-Biases-into-Large-Language-Models)
* [Paged Attention Meets FlexAttention Unlocking Long-Context Efficiency in Deployed Inference](#Paged-Attention-Meets-FlexAttention-Unlocking-Long-Context-Efficiency-in-Deployed-Inference)
* [Exploring the Impact of Temperature on Large Language ModelsHot or Cold?](#Exploring-the-Impact-of-Temperature-on-Large-Language-ModelsHot-or-Cold?)
* [ReStNet A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](#ReStNet-A-Reusable-&-Stitchable-Network-for-Dynamic-Adaptation-on-IoT-Devices)
* [GGBall Graph Generative Model on Poincaré Ball](#GGBall-Graph-Generative-Model-on-Poincaré-Ball)
* [Learning Compact Vision Tokens for Efficient Large Multimodal Models](#Learning-Compact-Vision-Tokens-for-Efficient-Large-Multimodal-Models)
* [MAGNet A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](#MAGNet-A-Multi-Scale-Attention-Guided-Graph-Fusion-Network-for-DRC-Violation-Detection)
* [Accelerating Two-Dimensional Materials Research via a Universal Interatomic Potential and Large Language Model Agent](#Accelerating-Two-Dimensional-Materials-Research-via-a-Universal-Interatomic-Potential-and-Large-Language-Model-Agent)
* [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](#Auditing-Black-Box-LLM-APIs-with-a-Rank-Based-Uniformity-Test)
* [Towards AI-Native Fronthaul Neural Compression for NextG Cloud RAN](#Towards-AI-Native-Fronthaul-Neural-Compression-for-NextG-Cloud-RAN)
* [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](#High-Fidelity-Scientific-Simulation-Surrogates-via-Adaptive-Implicit-Neural-Representations)
* [An Efficient Digital Watermarking Technique for Small Scale devices](#An-Efficient-Digital-Watermarking-Technique-for-Small-Scale-devices)
* [Spark Transformer Reactivating Sparsity in FFN and Attention](#Spark-Transformer-Reactivating-Sparsity-in-FFN-and-Attention)
* [GELD A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](#GELD-A-Unified-Neural-Model-for-Efficiently-Solving-Traveling-Salesman-Problems-Across-Different-Scales)
* [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](#Training-Free-Tokenizer-Transplantation-via-Orthogonal-Matching-Pursuit)
* [Stacey Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](#Stacey-Promoting-Stochastic-Steepest-Descent-via-Accelerated-$\ell_p$-Smooth-Nonconvex-Optimization)
* [Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data](#Enhancing-Robot-Safety-via-MLLM-Based-Semantic-Interpretation-of-Failure-Data)
* [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](#Hierarchical-and-Collaborative-LLM-Based-Control-for-Multi-UAV-Motion-and-Communication-in-Integrated-Terrestrial-and-Non-Terrestrial-Networks)
* [Lecture Notes in Loop Quantum Gravity. LN3 Boundary equations for Ashtekar-Barbero-Immirzi model](#Lecture-Notes-in-Loop-Quantum-Gravity.-LN3-Boundary-equations-for-Ashtekar-Barbero-Immirzi-model)
* [Towards Infant Sleep-Optimized Driving Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](#Towards-Infant-Sleep-Optimized-Driving-Synergizing-Wearable-and-Vehicle-Sensing-in-Intelligent-Cruise-Control)
* [Saffron-1 Towards an Inference Scaling Paradigm for LLM Safety Assurance](#Saffron-1-Towards-an-Inference-Scaling-Paradigm-for-LLM-Safety-Assurance)
* [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](#Eigenspectrum-Analysis-of-Neural-Networks-without-Aspect-Ratio-Bias)
* [RecGPT A Foundation Model for Sequential Recommendation](#RecGPT-A-Foundation-Model-for-Sequential-Recommendation)
* [Can Theoretical Physics Research Benefit from Language Agents?](#Can-Theoretical-Physics-Research-Benefit-from-Language-Agents?)
* [Reusing Trajectories in Policy Gradients Enables Fast Convergence](#Reusing-Trajectories-in-Policy-Gradients-Enables-Fast-Convergence)
* [Venus Cloud Research Progress and Perspectives](#Venus-Cloud-Research-Progress-and-Perspectives)
* [Bidirectional Image-Event Guided Low-Light Image Enhancement](#Bidirectional-Image-Event-Guided-Low-Light-Image-Enhancement)
* [Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](#Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU)
* [BEAST Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning](#BEAST-Efficient-Tokenization-of-B-Splines-Encoded-Action-Sequences-for-Imitation-Learning)
* [Efficient Memory Tiering in a Virtual Machine](#Efficient-Memory-Tiering-in-a-Virtual-Machine)
* [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](#Mitigating-Catastrophic-Forgetting-with-Adaptive-Transformer-Block-Expansion-in-Federated-Fine-Tuning)
* [AQUATIC-Diff Additive Quantization for Truly Tiny Compressed Diffusion Models](#AQUATIC-Diff-Additive-Quantization-for-Truly-Tiny-Compressed-Diffusion-Models)
* [MOGO Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation](#MOGO-Residual-Quantized-Hierarchical-Causal-Transformer-for-High-Quality-and-Real-Time-3D-Human-Motion-Generation)
* [MoA Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](#MoA-Heterogeneous-Mixture-of-Adapters-for-Parameter-Efficient-Fine-Tuning-of-Large-Language-Models)
* [Token Transforming A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration](#Token-Transforming-A-Unified-and-Training-Free-Token-Compression-Framework-for-Vision-Transformer-Acceleration)
* [Bridging the Modality Gap Softly Discretizing Audio Representation for LLM-based Automatic Speech Recognition](#Bridging-the-Modality-Gap-Softly-Discretizing-Audio-Representation-for-LLM-based-Automatic-Speech-Recognition)
* [Simulation Everywhere An Evolutionary Expansion of Discrete-Event Modeling and Simulation research and practice](#Simulation-Everywhere-An-Evolutionary-Expansion-of-Discrete-Event-Modeling-and-Simulation-research-and-practice)
* [EdgeProfiler A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](#EdgeProfiler-A-Fast-Profiling-Framework-for-Lightweight-LLMs-on-Edge-Using-Analytical-Model)
* [BAQ Efficient Bit Allocation Quantization for Large Language Models](#BAQ-Efficient-Bit-Allocation-Quantization-for-Large-Language-Models)
* [FedShield-LLM A Secure and Scalable Federated Fine-Tuned Large Language Model](#FedShield-LLM-A-Secure-and-Scalable-Federated-Fine-Tuned-Large-Language-Model)


## Beyond Attention or Similarity Maximizing Conditional Diversity for Token Pruning in MLLMs

>Authors: Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang

>2025-06-12

> http://arxiv.org/abs/2506.10967v1

In multimodal large language models (MLLMs), the length of input visual
tokens is often significantly greater than that of their textual counterparts,
leading to a high inference cost. Many works aim to address this issue by
removing redundant visual tokens. However, current approaches either rely on
attention-based **pruning**, which retains numerous duplicate tokens, or use
similarity-based **pruning**, overlooking the instruction relevance, consequently
causing suboptimal performance. In this paper, we go beyond attention or
similarity by proposing a novel visual token **pruning** method named CDPruner,
which maximizes the conditional diversity of retained tokens. We first define
the conditional similarity between visual tokens conditioned on the
instruction, and then reformulate the token **pruning** problem with determinantal
point process (DPP) to maximize the conditional diversity of the selected
subset. The proposed CDPruner is training-free and model-agnostic, allowing
easy application to various MLLMs. Extensive experiments across diverse MLLMs
show that CDPruner establishes new state-of-the-art on various vision-language
benchmarks. By maximizing conditional diversity through DPP, the selected
subset better represents the input images while closely adhering to user
instructions, thereby preserving strong performance even with high reduction
ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\% and CUDA latency
by 78\%, while maintaining 94\% of the original accuracy. Our code is available
at https://github.com/Theia-4869/CDPruner.


## SWE-Factory Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks

>Authors: Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, Zibin Zheng

>2025-06-12

> http://arxiv.org/abs/2506.10954v1

Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.


## Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization

>Authors: Or Shafran, Atticus Geiger, Mor Geva

>2025-06-12

> http://arxiv.org/abs/2506.10920v1

A central goal for mechanistic interpretability has been to identify the
right units of analysis in large language models (LLMs) that causally explain
their outputs. While early work focused on individual neurons, evidence that
neurons often encode multiple concepts has motivated a shift toward analyzing
directions in activation space. A key question is how to find directions that
capture interpretable features in an unsupervised manner. Current methods rely
on dictionary learning with **sparse** autoencoders (SAEs), commonly trained over
residual stream activations to learn directions from scratch. However, SAEs
often struggle in causal evaluations and lack intrinsic interpretability, as
their learning is not explicitly tied to the computations of the model. Here,
we tackle these limitations by directly decomposing MLP activations with
semi-nonnegative matrix factorization (SNMF), such that the learned features
are (a) **sparse** linear combinations of co-activated neurons, and (b) mapped to
their activating inputs, making them directly interpretable. Experiments on
Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs
and a strong supervised baseline (difference-in-means) on causal steering,
while aligning with human-interpretable concepts. Further analysis reveals that
specific neuron combinations are reused across semantically-related features,
exposing a hierarchical structure in the MLP's activation space. Together,
these results position SNMF as a simple and effective tool for identifying
interpretable features and dissecting concept representations in LLMs.


## Accelerating Diffusion Large Language Models with SlowFast The Three Golden Principles

>Authors: Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang

>2025-06-12

> http://arxiv.org/abs/2506.10848v1

Diffusion-based language models (dLLMs) have emerged as a promising
alternative to traditional autoregressive LLMs by enabling parallel token
generation and significantly reducing inference latency. However, existing
sampling strategies for dLLMs, such as confidence-based or semi-autoregressive
decoding, often suffer from static behavior, leading to suboptimal efficiency
and limited flexibility. In this paper, we propose SlowFast Sampling, a novel
dynamic sampling strategy that adaptively alternates between exploratory and
accelerated decoding stages. Our method is guided by three golden principles:
certainty principle, convergence principle, and positional principle, which
govern when and where tokens can be confidently and efficiently decoded. We
further integrate our strategy with dLLM-Cache to reduce redundant computation.
Extensive experiments across benchmarks and models show that SlowFast Sampling
achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and
up to 34.22$\times$ when combined with caching. Notably, our approach
outperforms strong autoregressive baselines like LLaMA3 8B in throughput,
demonstrating that well-designed sampling can unlock the full potential of
dLLMs for fast and high-quality generation.


## Constructing and Evaluating Declarative RAG Pipelines in PyTerrier

>Authors: Craig Macdonald, Jinyuan Fang, Andrew Parry, Zaiqiao Meng

>2025-06-12

> http://arxiv.org/abs/2506.10802v1

Search engines often follow a pipeline architecture, where complex but
effective reranking components are used to refine the results of an initial
retrieval. Retrieval augmented generation (RAG) is an exciting application of
the pipeline architecture, where the final component generates a coherent
answer for the users from the retrieved documents. In this demo paper, we
describe how such RAG pipelines can be formulated in the declarative PyTerrier
architecture, and the advantages of doing so. Our PyTerrier-RAG extension for
PyTerrier provides easy access to standard RAG datasets and evaluation
measures, state-of-the-art LLM readers, and using PyTerrier's unique operator
notation, easy-to-build pipelines. We demonstrate the succinctness of indexing
and RAG pipelines on standard datasets (including Natural Questions) and how to
build on the larger PyTerrier ecosystem with state-of-the-art **sparse**,
learned-**sparse**, and dense retrievers, and other neural rankers.


## Transformer IMU Calibrator Dynamic On-body IMU Calibration for Inertial Motion Capture

>Authors: Chengxu Zuo, Jiawei Huang, Xiao Jiang, Yuan Yao, Xiangren Shi, Rui Cao, Xinyu Yi, Feng Xu, Shihui Guo, Yipeng Qin

>2025-06-12

> http://arxiv.org/abs/2506.10580v1

In this paper, we propose a novel dynamic calibration method for **sparse**
inertial motion capture systems, which is the first to break the restrictive
absolute static assumption in IMU calibration, i.e., the coordinate drift RG'G
and measurement offset RBS remain constant during the entire motion, thereby
significantly expanding their application scenarios. Specifically, we achieve
real-time estimation of RG'G and RBS under two relaxed assumptions: i) the
matrices change negligibly in a short time window; ii) the human movements/IMU
readings are diverse in such a time window. Intuitively, the first assumption
reduces the number of candidate matrices, and the second assumption provides
diverse constraints, which greatly reduces the solution space and allows for
accurate estimation of RG'G and RBS from a short history of IMU readings in
real time. To achieve this, we created synthetic datasets of paired RG'G, RBS
matrices and IMU readings, and learned their mappings using a Transformer-based
model. We also designed a calibration trigger based on the diversity of IMU
readings to ensure that assumption ii) is met before applying our method. To
our knowledge, we are the first to achieve implicit IMU calibration (i.e.,
seamlessly putting IMUs into use without the need for an explicit calibration
process), as well as the first to enable long-term and accurate motion capture
using **sparse** IMUs. The code and dataset are available at
https://github.com/ZuoCX1996/TIC.


## Lambert's problem in orbital dynamics a self--contained introduction

>Authors: Lenox Helene Baloglou, Parneet Gill, Tonatiuh Sánchez-Vizuet

>2025-06-12

> http://arxiv.org/abs/2506.10556v1

Lambert's problem is a classical boundary value problem in analytical
mechanics. It arises when trying to determine the energy required to place a
particle, subject to a central gravitational potential, in a free fall
trajectory connecting two given points on a desired travel time. Due to its
mathematical beauty and its relevance in aerospace engineering, it has been and
remains the object of attention of countless engineers, mathematicians (pure
and applied), and physicists seeking to produce efficient solution algorithms.
In this expository article, didactic in nature, we present a unified and
comprehensive derivation that assumes only a minimal background in physics and
mathematics. We focus on the simplest two--body case and carefully develop the
argument for elliptical trajectories. The goal is to provide a single reference
that can serve as an accelerated introduction for students and researchers
interested in a quick introduction to the subject.


## MNN-LLM A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices

>Authors: Zhaode Wang, Jingbang Yang, Xinyu Qian, Shiwen Xing, Xiaotang Jiang, Chengfei Lv, Shengyu Zhang

>2025-06-12

> http://arxiv.org/abs/2506.10443v1

Large language models (LLMs) have demonstrated exceptional performance across
a variety of tasks. However, their substantial scale leads to significant
computational resource consumption during inference, resulting in high costs.
Consequently, edge device inference presents a promising solution. The primary
challenges of edge inference include memory usage and inference speed. This
paper introduces MNN-LLM, a framework specifically designed to accelerate the
deployment of large language models on mobile devices. MNN-LLM addresses the
runtime characteristics of LLMs through model **quantization** and DRAM-Flash
hybrid storage, effectively reducing memory usage. It rearranges weights and
inputs based on mobile CPU instruction sets and GPU characteristics while
employing strategies such as multicore load balancing, mixed-precision
floating-point operations, and geometric computations to enhance performance.
Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current
mainstream LLM-specific frameworks.


## Broadband Tunable Deep-UV Emission from AI-Optimized Nonlinear Metasurface Architectures

>Authors: Omar A. M. Abdelraouf

>2025-06-12

> http://arxiv.org/abs/2506.10442v1

Metasurfaces represent a pivotal advancement in nonlinear optics, leveraging
high-Q resonant cavities to enhance harmonic generation. Multi-layer
metasurfaces (MLMs) further amplify this potential by intensifying light-matter
interactions within individual meta-atoms at the nanoscale. However, maximizing
nonlinear efficiency demands extreme field confinement through optimized
designs of large geometric and material parameters, which exceed traditional
simulation's computational ability. To overcome this, we introduce
NanoPhotoNet-NL, an AI-driven design tool employing a hybrid deep neural
network (DNN) that synergizes convolutional neural networks (CNNs) and Long
Short-Term Memory (LSTM) models. This framework accelerates nonlinear MLM
design speed by four orders of magnitude while maintaining over 98.3%
prediction accuracy relative to physical simulators. The optimized MLMs achieve
quality factors exceeding 50, enabling broadband third-harmonic generation
(THG) in the deep ultraviolet (DUV) from wavelengths 200 nm to 260 nm via
parametric sweeps. Furthermore, NanoPhotoNet-NL facilitates dynamically tunable
DUV nanolight sources with 20 nm spectral coverage in the UVC band using
low-loss nonlinear phase change materials. This work marks a transformative
leap in nonlinear metasurface engineering, unlocking high-performance,
reconfigurable platforms for nonlinear and quantum optical nanodevices.


## DART Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba

>Authors: Shicheng Yin, Kaixuan Yin, Yang Liu, Weixing Chen, Liang Lin

>2025-06-12

> http://arxiv.org/abs/2506.10390v1

Recently, non-convolutional models such as the Vision Transformer (ViT) and
Vision Mamba (Vim) have achieved remarkable performance in computer vision
tasks. However, their reliance on fixed-size patches often results in excessive
encoding of background regions and omission of critical local details,
especially when informative objects are **sparse**ly distributed. To address this,
we introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),
which adaptively partitions images into content-dependent patches of varying
sizes. DART combines learnable region scores with piecewise differentiable
quantile operations to allocate denser tokens to information-rich areas.
Despite introducing only approximately 1 million (1M) additional parameters,
DART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that
uniformly increase token density to capture fine-grained details, DART offers a
more efficient alternative, achieving 45% FLOPs reduction with superior
performance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that
DART consistently enhances accuracy while incurring minimal or even reduced
computational overhead. Code is available at
https://github.com/HCPLab-SYSU/DART.


## TreeLoRA Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree

>Authors: Yu-Yang Qian, Yuan-Ze Xu, Zhen-Yu Zhang, Peng Zhao, Zhi-Hua Zhou

>2025-06-12

> http://arxiv.org/abs/2506.10355v1

Many real-world applications collect data in a streaming environment, where
learning tasks are encountered sequentially. This necessitates continual
learning (CL) to update models online, enabling adaptation to new tasks while
preserving past knowledge to prevent catastrophic forgetting. Nowadays, with
the flourish of large pre-trained models (LPMs), efficiency has become
increasingly critical for CL, due to their substantial computational demands
and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of
Low-Rank Adapters), a novel approach that constructs layer-wise adapters by
leveraging hierarchical gradient similarity to enable efficient CL,
particularly for LPMs. To reduce the computational burden of task similarity
estimation, we employ bandit techniques to develop an algorithm based on lower
confidence bounds to efficiently explore the task structure. Furthermore, we
use **sparse** gradient updates to facilitate parameter optimization, making the
approach better suited for LPMs. Theoretical analysis is provided to justify
the rationale behind our approach, and experiments on both vision transformers
(ViTs) and large language models (LLMs) demonstrate the effectiveness and
efficiency of our approach across various domains, including vision and natural
language processing tasks.


## Heterogeneous-IRS-Assisted MIMO Systems Channel Estimation and Beamforming

>Authors: Weibiao Zhao, Qiucen Wu, Yuanqi Tang, Yu Zhu

>2025-06-12

> http://arxiv.org/abs/2506.10350v1

Intelligent reflecting surface (IRS) has gained great attention for its
ability to create favorable propagation environments. However, the power
consumption of conventional IRSs cannot be ignored due to the large number of
reflecting elements and control circuits. To balance performance and power
consumption, we previously proposed a heterogeneous-IRS (HE-IRS), a green IRS
structure integrating dynamically tunable elements (DTEs) and statically
tunable elements (STEs). Compared to conventional IRSs with only DTEs, the
unique DTE-STE integrated structure introduces new challenges in both channel
estimation and beamforming. In this paper, we investigate the channel
estimation and beamforming problems in HE-IRS-assisted multi-user
multiple-input multiple-output systems. Unlike the overall cascaded channel
estimated in conventional IRSs, we show that the HE-IRS channel to be estimated
is decomposed into a DTE-based cascaded channel and an STE-based equivalent
channel. Leveraging it along with the inherent **sparsity** of DTE- and STE-based
channels and manifold optimization, we propose an efficient channel estimation
scheme. To address the rank mismatch problem in the imperfect channel **sparsity**
information, a robust rank selection rule is developed. For beamforming, we
propose an offline algorithm to optimize the STE phase shifts for wide beam
coverage, and an online algorithm to optimize the BS precoder and the DTE phase
shifts using the estimated HE-IRS channel. Simulation results show that the
HE-IRS requires less pilot overhead than conventional IRSs with the same number
of elements. With the proposed channel estimation and beamforming schemes, the
green HE-IRS achieves competitive sum rate performance with significantly
reduced power consumption.


## LightKG Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture

>Authors: Yanhui Li, Dongxia Wang, Zhu Sun, Haonan Zhang, Huizhong Guo

>2025-06-12

> http://arxiv.org/abs/2506.10347v1

Recently, Graph Neural Networks (GNNs) have become the dominant approach for
Knowledge Graph-aware Recommender Systems (KGRSs) due to their proven
effectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL)
has been incorporated to address the sparity issue, leading to longer training
time. However, through extensive experiments, we reveal that: (1)compared to
other KGRSs, the existing GNN-based KGRSs fail to keep their superior
performance under **sparse** interactions even with SSL. (2) More complex models
tend to perform worse in **sparse** interaction scenarios and complex mechanisms,
like attention mechanism, can be detrimental as they often increase learning
difficulty. Inspired by these findings, we propose LightKG, a simple yet
powerful GNN-based KGRS to address **sparsity** issues. LightKG includes a
simplified GNN layer that encodes directed relations as scalar pairs rather
than dense embeddings and employs a linear aggregation framework, greatly
reducing the complexity of GNNs. Additionally, LightKG incorporates an
efficient contrastive layer to implement SSL. It directly minimizes the node
similarity in original graph, avoiding the time-consuming subgraph generation
and comparison required in previous SSL methods. Experiments on four benchmark
datasets show that LightKG outperforms 12 competitive KGRSs in both **sparse** and
dense scenarios while significantly reducing training time. Specifically, it
surpasses the best baselines by an average of 5.8\% in recommendation accuracy
and saves 84.3\% of training time compared to KGRSs with SSL. Our code is
available at https://github.com/1371149/LightKG.


## PointGS Point Attention-Aware Sparse View Synthesis with Gaussian Splatting

>Authors: Lintao Xiang, Hongpei Zheng, Yating Huang, Qijun Yang, Hujun Yin

>2025-06-12

> http://arxiv.org/abs/2506.10335v1

3D Gaussian splatting (3DGS) is an innovative rendering technique that
surpasses the neural radiance field (NeRF) in both rendering speed and visual
quality by leveraging an explicit 3D scene representation. Existing 3DGS
approaches require a large number of calibrated views to generate a consistent
and complete scene representation. When input views are limited, 3DGS tends to
overfit the training views, leading to noticeable degradation in rendering
quality. To address this limitation, we propose a Point-wise Feature-Aware
Gaussian Splatting framework that enables real-time, high-quality rendering
from **sparse** training views. Specifically, we first employ the latest stereo
foundation model to estimate accurate camera poses and reconstruct a dense
point cloud for Gaussian initialization. We then encode the colour attributes
of each 3D Gaussian by sampling and aggregating multiscale 2D appearance
features from **sparse** inputs. To enhance point-wise appearance representation,
we design a point interaction network based on a self-attention mechanism,
allowing each Gaussian point to interact with its nearest neighbors. These
enriched features are subsequently decoded into Gaussian parameters through two
lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive
experiments on diverse benchmarks demonstrate that our method significantly
outperforms NeRF-based approaches and achieves competitive performance under
few-shot settings compared to the state-of-the-art 3DGS methods.


## "Check My Work?" Measuring Sycophancy in a Simulated Educational Context

>Authors: Chuck Arvin

>2025-06-12

> http://arxiv.org/abs/2506.10297v1

This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.


## Discrete Audio Tokens More Than a Survey!

>Authors: Pooneh Mousavi, Gallil Maimon, Adel Moumen, Darius Petermann, Jiatong Shi, Haibin Wu, Haici Yang, Anastasia Kuznetsova, Artem Ploujnikov, Ricard Marxer, Bhuvana Ramabhadran, Benjamin Elizalde, Loren Lugosch, Jinyu Li, Cem Subakan, Phil Woodland, Minje Kim, Hung-yi Lee, Shinji Watanabe, Yossi Adi, Mirco Ravanelli

>2025-06-12

> http://arxiv.org/abs/2506.10274v1

Discrete audio tokens are compact representations that aim to preserve
perceptual quality, phonetic content, and speaker characteristics while
enabling efficient storage and inference, as well as competitive performance
across diverse downstream tasks.They provide a practical alternative to
continuous features, enabling the integration of speech and audio into modern
large language models (LLMs). As interest in token-based audio processing
grows, various tokenization methods have emerged, and several surveys have
reviewed the latest progress in the field. However, existing studies often
focus on specific domains or tasks and lack a unified comparison across various
benchmarks. This paper presents a systematic review and benchmark of discrete
audio tokenizers, covering three domains: speech, music, and general audio. We
propose a taxonomy of tokenization approaches based on encoder-decoder,
**quantization** techniques, training paradigm, streamability, and application
domains. We evaluate tokenizers on multiple benchmarks for reconstruction,
downstream performance, and acoustic language modeling, and analyze trade-offs
through controlled ablation studies. Our findings highlight key limitations,
practical considerations, and open challenges, providing insight and guidance
for future research in this rapidly evolving area. For more information,
including our main results and tokenizer database, please refer to our website:
https://poonehmousavi.github.io/dates-website/.


## AWP Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent

>Authors: Jing Liu, Toshiaki Koike-Akino, Ye Wang, Hassan Mansour, Matthew Brand

>2025-06-11

> http://arxiv.org/abs/2506.10205v1

To address the enormous size of Large Language Models (LLMs), model
compression methods, such as **quantization** and **pruning**, are often deployed,
especially on edge devices. In this work, we focus on layer-wise post-training
**quantization** and **pruning**. Drawing connections between activation-aware weight
**pruning** and **sparse** approximation problems, and motivated by the success of
Iterative Hard Thresholding (IHT), we propose a unified method for
Activation-aware Weight **pruning** and **quantization** via Projected gradient descent
(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM
**pruning** and **quantization** methods. Theoretical convergence guarantees of the
proposed method for **pruning** are also provided.


## Prompt Variability Effects On LLM Code Generation

>Authors: Andrei Paleyes, Radzim Sendyka, Diana Robinson, Christian Cabrera, Neil D. Lawrence

>2025-06-11

> http://arxiv.org/abs/2506.10204v1

Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.


## Formalizing Neuromorphic Control Systems A General Proposal and A Rhythmic Case Study

>Authors: Taisia Medvedeva, Alessio Franci, Fernando Castaños

>2025-06-11

> http://arxiv.org/abs/2506.10203v1

Neuromorphic control is receiving growing attention due to the multifaceted
advantages it brings over more classical control approaches, including: **sparse**
and on-demand sensing, information transmission, and actuation;
energy-efficient designs and realizations in neuromorphic hardware; event-based
signal processing and control signal computation. However, a general
control-theoretical formalization of what "neuromorphic control systems" are
and how we can rigorously analyze, design, and control them is still largely
missing. In this note, we suggest a possible path toward formalizing
neuromorphic control systems. We apply the proposed framework to a rhythmic
control case study and rigorously show how it has the potential to make
neuromorphic control systems analysis and design amenable to mature control
theoretical approaches like describing function analysis and harmonic balance,
fast-slow analysis, discrete and hybrid systems, and robust optimization.


## Attention on flow control transformer-based reinforcement learning for lift regulation in highly disturbed flows

>Authors: Zhecheng Liu, Jeff D. Eldredge

>2025-06-11

> http://arxiv.org/abs/2506.10153v1

A linear flow control strategy designed for weak disturbances may not remain
effective in sequences of strong disturbances due to nonlinear interactions,
but it is sensible to leverage it for developing a better strategy. In the
present study, we propose a transformer-based reinforcement learning (RL)
framework to learn an effective control strategy for regulating aerodynamic
lift in gust sequences via pitch control. The transformer addresses the
challenge of partial observability from limited surface pressure sensors. We
demonstrate that the training can be accelerated with two techniques --
pretraining with an expert policy (here, linear control) and task-level
transfer learning (here, extending a policy trained on isolated gusts to
multiple gusts). We show that the learned strategy outperforms the best
proportional control, with the performance gap widening as the number of gusts
increases. The control strategy learned in an environment with a small number
of successive gusts is shown to effectively generalize to an environment with
an arbitrarily long sequence of gusts. We investigate the pivot configuration
and show that quarter-chord pitching control can achieve superior lift
regulation with substantially less control effort compared to mid-chord
pitching control. Through a decomposition of the lift, we attribute this
advantage to the dominant added-mass contribution accessible via quarter-chord
pitching. The success on multiple configurations shows the generalizability of
the proposed transformer-based RL framework, which offers a promising approach
to solve more computationally demanding flow control problems when combined
with the proposed **acceleration** techniques.


## EfficientVLA Training-Free Acceleration and Compression for Vision-Language-Action Models

>Authors: Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang

>2025-06-11

> http://arxiv.org/abs/2506.10100v1

Vision-Language-Action (VLA) models, particularly diffusion-based
architectures, demonstrate transformative potential for embodied intelligence
but are severely hampered by high computational and memory demands stemming
from extensive inherent and inference-time redundancies. While existing
**acceleration** efforts often target isolated inefficiencies, such piecemeal
solutions typically fail to holistically address the varied computational and
memory bottlenecks across the entire VLA pipeline, thereby limiting practical
deployability. We introduce EfficientVLA, a structured and training-free
inference **acceleration** framework that systematically eliminates these barriers
by cohesively exploiting multifaceted redundancies. EfficientVLA
synergistically integrates three targeted strategies: (1) **pruning** of
functionally inconsequential layers from the language module, guided by an
analysis of inter-layer redundancies; (2) optimizing the visual processing
pathway through a task-aware strategy that selects a compact, diverse set of
visual tokens, balancing task-criticality with informational coverage; and (3)
alleviating temporal computational redundancy within the iterative
diffusion-based action head by strategically caching and reusing key
intermediate features. We apply our method to a standard VLA model CogACT,
yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%
success rate drop in the SIMPLER benchmark.


## Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput

>Authors: Gabriel Orlanski, Nicholas Roberts, Aws Albarghouthi, Frederic Sala

>2025-06-11

> http://arxiv.org/abs/2506.10056v1

The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.


## HadaNorm Diffusion Transformer Quantization through Mean-Centered Transformations

>Authors: Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel

>2025-06-11

> http://arxiv.org/abs/2506.09932v1

Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before **quantization**. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation **quantization**. We demonstrate that HadaNorm consistently
reduces **quantization** error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.


## Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning

>Authors: Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang

>2025-06-11

> http://arxiv.org/abs/2506.09853v1

Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the **pruning** of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.


## Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?

>Authors: Andreas Säuberli, Diego Frassinelli, Barbara Plank

>2025-06-11

> http://arxiv.org/abs/2506.09796v1

Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.


## Q-SAM2 Accurate Quantization for Segment Anything Model 2

>Authors: Nicola Farronato, Florian Scheidegger, Mattia Rigotti, Cristiano Malossi, Michele Magno, Haotong Qin

>2025-06-11

> http://arxiv.org/abs/2506.09782v1

The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate **low-bit** **quantization** method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during **quantization**, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for **low-bit** initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved **quantization**.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to **quantization**
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general **quantization** schemes, especially for
ultra-low 2-bit **quantization**. While designed for **quantization**-aware training,
our proposed calibration technique also proves effective in post-training
**quantization**, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.


## Auto-Compressing Networks

>Authors: Vaggelis Dorovatas, Georgios Paraskevopoulos, Alexandros Potamianos

>2025-06-11

> http://arxiv.org/abs/2506.09714v1

Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
**pruning** techniques, enables significantly better **sparsity**-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.


## Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks

>Authors: Huan Lin, Chenguang Zhu, Lianghui Ding, Feng Yang

>2025-06-11

> http://arxiv.org/abs/2506.09703v1

Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to
address communication network split issues and restore connectivity. However,
existing graph learning-based resilient algorithms face over-aggregation and
non-convergence problems caused by uneven and **sparse** topology under massive
damage scenarios. To alleviate these problems, we propose a novel Multi-Level
Damage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery
trajectories by mining information from destroyed UAVs. We first introduce a
Multi-Branch Damage Attention (MBDA) module, which forms a sequence of
multi-hop Damage Attentive Graphs (mDAG) with different ranges of receptive
fields. Each mDAG links only remaining and damaged nodes to ensure a more even
degree distribution for mitigating over-aggregation, and utilizes multi-hop
dilation to establish more links for **sparse** topology enhancement. To resort to
the mDAG, we propose a Dilated Graph Convolution Network (DGCN), which
generates the optimal recovery trajectories with theoretically proven
convergence under massive damage cases. Simulation results show that the
proposed algorithm can guarantee the connectivity restoration under large swarm
and damage scales, while significantly expediting the recovery time by 75.94%
and improving the topology uniformity after recovery.


## SparseSSM Efficient Selective Structured State Space Models Can Be Pruned in One-Shot

>Authors: Kaiwen Tuo, Huan Wang

>2025-06-11

> http://arxiv.org/abs/2506.09613v1

State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot **pruning** methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
**pruning** framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) **pruning**, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured **sparsity**. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art **pruning** algorithm for Mamba-based LLMs.


## Unmasking real-world audio deepfakes A data-centric approach

>Authors: David Combei, Adriana Stan, Dan Oneata, Nicolas Müller, Horia Cucu

>2025-06-11

> http://arxiv.org/abs/2506.09606v1

The growing prevalence of real-world deepfakes presents a critical challenge
for existing detection systems, which are often evaluated on datasets collected
just for scientific purposes. To address this gap, we introduce a novel dataset
of real-world audio deepfakes. Our analysis reveals that these real-world
examples pose significant challenges, even for the most performant detection
models. Rather than increasing model complexity or exhaustively search for a
better alternative, in this work we focus on a data-centric paradigm, employing
strategies like dataset curation, **pruning**, and augmentation to improve model
robustness and generalization.
  Through these methods, we achieve a 55% relative reduction in EER on the
In-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on
our newly proposed real-world deepfakes dataset, AI4T. These results highlight
the transformative potential of data-centric approaches in enhancing deepfake
detection for real-world applications. Code and data available at:
https://github.com/davidcombei/AI4T.


## Attention-Based Map Encoding for Learning Generalized Legged Locomotion

>Authors: Junzhe He, Chong Zhang, Fabian Jenelten, Ruben Grandia, Moritz BÄcher, Marco Hutter

>2025-06-11

> http://arxiv.org/abs/2506.09588v1

Dynamic locomotion of legged robots is a critical yet challenging topic in
expanding the operational range of mobile robots. It requires precise planning
when possible footholds are **sparse**, robustness against uncertainties and
disturbances, and generalizability across diverse terrains. While traditional
model-based controllers excel at planning on complex terrains, they struggle
with real-world uncertainties. Learning-based controllers offer robustness to
such uncertainties but often lack precision on terrains with **sparse** steppable
areas. Hybrid methods achieve enhanced robustness on **sparse** terrains by
combining both methods but are computationally demanding and constrained by the
inherent limitations of model-based planners. To achieve generalized legged
locomotion on diverse terrains while preserving the robustness of
learning-based controllers, this paper proposes to learn an attention-based map
encoding conditioned on robot proprioception, which is trained as part of the
end-to-end controller using reinforcement learning. We show that the network
learns to focus on steppable areas for future footholds when the robot
dynamically navigates diverse and challenging terrains. We synthesize behaviors
that exhibit robustness against uncertainties while enabling precise and agile
traversal of **sparse** terrains. Additionally, our method offers a way to
interpret the topographical perception of a neural network. We have trained two
controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot
respectively and tested the resulting controllers in the real world under
various challenging indoor and outdoor scenarios, including ones unseen during
training.


## Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities

>Authors: Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, David Sobrín-Hidalgo, Ángel Manuel Guerrero-Higueras, Vicente MatellÁn-Olivera

>2025-06-11

> http://arxiv.org/abs/2506.09581v1

Large Language Models (LLMs) have experienced great advancements in the last
year resulting in an increase of these models in several fields to face natural
language tasks. The integration of these models in robotics can also help to
improve several aspects such as human-robot interaction, navigation, planning
and decision-making. Therefore, this paper introduces llama\_ros, a tool
designed to integrate **quantize**d Large Language Models (LLMs) into robotic
systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,
llama\_ros enables the efficient execution of **quantize**d LLMs as edge artificial
intelligence (AI) in robotics systems with resource-constrained environments,
addressing the challenges of computational efficiency and memory limitations.
By deploying **quantize**d LLMs, llama\_ros empowers robots to leverage the natural
language understanding and generation for enhanced decision-making and
interaction which can be paired with prompt engineering, knowledge graphs,
ontologies or other tools to improve the capabilities of autonomous robots.
Additionally, this paper provides insights into some use cases of using
llama\_ros for planning and explainability in robotics.


## Understanding the Performance and Power of LLM Inferencing on Edge Accelerators

>Authors: Mayank Arya, Yogesh Simmhan

>2025-06-11

> http://arxiv.org/abs/2506.09554v2

Large Language Models (LLMs) have demonstrated exceptional benefits to a wide
range of domains, for tasks as diverse as code generation and robot navigation.
While LLMs are usually served from cloud data centers, mission-critical and
privacy-sensitive applications may require local hosting of open LLM models.
Given the large GPU memory footprint needed for LLMs, edge accelerators such as
Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.
However, the feasibility and performance of LLM inference on edge accelerators
is under-explored. This study presents a detailed evaluation of LLM inference
on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B
parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen. We
investigate the impact of varying batch sizes, sequence lengths, and
**quantization** levels on latency, throughput, and perplexity, and also explore
various custom power modes on the Orin AGX to perform power and energy
consumption analysis. Our findings offer interesting insights on the trade-offs
between efficiency, inference speed and resource use, e.g., increasing the
sequence length causes a decrease in token throughput and **quantization** causes
smaller LLMs to be slower. These results can help optimize LLM serving on edge
accelerators for practical applications.


## HAIF-GS Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene

>Authors: Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang

>2025-06-11

> http://arxiv.org/abs/2506.09518v1

Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through **sparse** anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.


## TinySplat Feedforward Approach for Generating Compact 3D Scene Representation

>Authors: Zetian Song, Jiaye Fu, Jiaqi Zhang, Xiaohan Lu, Chuanmin Jia, Siwei Ma, Wen Gao

>2025-06-11

> http://arxiv.org/abs/2506.09479v1

The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from **sparse** input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.


## Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking

>Authors: Piyush Mishra, Philippe Roudot

>2025-06-11

> http://arxiv.org/abs/2506.09441v1

Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally **sparse** scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.


## ODG Occupancy Prediction Using Dual Gaussians

>Authors: Yunxiao Shi, Yinhao Zhu, Shizhong Han, Jisoo Jeong, Amin Ansari, Hong Cai, Fatih Porikli

>2025-06-11

> http://arxiv.org/abs/2506.09417v2

Occupancy prediction infers fine-grained 3D geometry and semantics from
camera images of the surrounding environment, making it a critical perception
task for autonomous driving. Existing methods either adopt dense grids as scene
representation, which is difficult to scale to high resolution, or learn the
entire scene using a single set of **sparse** queries, which is insufficient to
handle the various object characteristics. In this paper, we present ODG, a
hierarchical dual **sparse** Gaussian representation to effectively capture complex
scene dynamics. Building upon the observation that driving scenes can be
universally decomposed into static and dynamic counterparts, we define dual
Gaussian queries to better model the diverse scene objects. We utilize a
hierarchical Gaussian transformer to predict the occupied voxel centers and
semantic classes along with the Gaussian parameters. Leveraging the real-time
rendering capability of 3D Gaussian Splatting, we also impose rendering
supervision with available depth and semantic map annotations injecting
pixel-level alignment to boost occupancy learning. Extensive experiments on the
Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets
new state-of-the-art results while maintaining low inference cost.


## Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization

>Authors: Shengda Gu, Kai Li, Junliang Xing, Yifan Zhang, Jian Cheng

>2025-06-11

> http://arxiv.org/abs/2506.09404v1

Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.


## SLED A Speculative LLM Decoding Framework for Efficient Edge Serving

>Authors: Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Dimitrios Nikolopoulos

>2025-06-11

> http://arxiv.org/abs/2506.09397v1

Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive **quantization**, **pruning**, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding **acceleration** technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.


## A new approach for image segmentation based on diffeomorphic registration and gradient fields

>Authors: Junchao Zhou

>2025-06-11

> http://arxiv.org/abs/2506.09357v1

Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU **acceleration** using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.


## DIVE into MoE Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts

>Authors: Yuchen Feng, Bowen Shen, Naibin Gu, Jiaxuan Zhao, Peng Fu, Zheng Lin, Weiping Wang

>2025-06-11

> http://arxiv.org/abs/2506.09351v1

Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, **pruning**-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes **pruning** and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing **pruning** and MoE reconstruction methods with
the same number of activated parameters.


## Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation

>Authors: Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang

>2025-06-11

> http://arxiv.org/abs/2506.09350v1

Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the **KV** cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2


## Latent Multi-Head Attention for Small Language Models

>Authors: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat

>2025-06-11

> http://arxiv.org/abs/2506.09342v1

We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% **KV**-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.


## Quantum Algorithm Software for Condensed Matter Physics

>Authors: T. Farajollahpour

>2025-06-11

> http://arxiv.org/abs/2506.09308v1

This report offers a comprehensive analysis of the evolving landscape of
quantum algorithm software specifically tailored for condensed matter physics.
It examines fundamental quantum algorithms such as Variational Quantum
Eigensolver (VQE), Quantum Phase Estimation (QPE), Quantum Annealing (QA),
Quantum Approximate Optimization Algorithm (QAOA), and Quantum Machine Learning
(QML) as applied to key condensed matter problems including strongly correlated
systems, topological phases, and quantum magnetism. This review details leading
software development kits (SDKs) like Qiskit, Cirq, PennyLane, and Q\#, and
profiles key academic, commercial, and governmental initiatives driving
innovation in this domain. Furthermore, it assesses current challenges,
including hardware limitations, algorithmic scalability, and error mitigation,
and explores future trajectories, anticipating new algorithmic breakthroughs,
software enhancements, and the impact of next-generation quantum hardware. The
central theme emphasizes the critical role of a co-design approach, where
algorithms, software, and hardware evolve in tandem, and highlights the
necessity of standardized benchmarks to accelerate progress towards leveraging
quantum computation for transformative discoveries in condensed matter physics.


## ScalableHD Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs

>Authors: Dhruv Parikh, Viktor Prasanna

>2025-06-10

> http://arxiv.org/abs/2506.09282v1

Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.


## FastFLUX Pruning FLUX with Block-wise Replacement and Sandwich Training

>Authors: Fuhan Cai, Yong Guo, Jie Li, Wenbo Li, Xiangzhong Fang, Jian Chen

>2025-06-10

> http://arxiv.org/abs/2506.10035v1

Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing **acceleration**
methods (e.g., single-step distillation and attention **pruning**) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
**pruning** framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.


## Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs

>Authors: Greyson Brothers

>2025-06-10

> http://arxiv.org/abs/2506.09215v1

We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector **quantization** with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector **quantize**r within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.


## The Quantum Paldus Transform Efficient Circuits with Applications

>Authors: Jędrzej Burkat, Nathan Fitzpatrick

>2025-06-10

> http://arxiv.org/abs/2506.09151v1

We present the Quantum Paldus Transform: an efficient quantum algorithm for
block-diagonalising fermionic, spin-free Hamiltonians in the second
quantisation. Our algorithm implements an isometry between the occupation
number basis of a fermionic Fock space of $2d$ modes, and the Gelfand-Tsetlin
(GT) states spanning irreducible representations of the group $U(d) \times
SU(2)$. The latter forms a basis indexed by well-defined values of total
particle number $N$, global spin $S$, spin projection $M$, and $U(d)$ GT
patterns. This realises the antisymmetric unitary-unitary duality discovered by
Howe and developed into the Unitary Group Approach (UGA) for computational
chemistry by Paldus and Shavitt in the 1970s. The Paldus transform lends tools
from the UGA readily applicable to quantum computational chemistry, leading to
maximally **sparse** representations of spin-free Hamiltonians, efficient
preparation of Configuration State Functions, and a direct interpretation of
quantum chemistry reduced density matrix elements in terms of $SU(2)$ angular
momentum coupling. The transform also enables the encoding of quantum
information into novel Decoherence-Free Subsystems for use in communication and
error mitigation. Our work can be seen as a generalisation of the quantum Schur
transform for the second quantisation, made tractable by the Pauli exclusion
principle. Alongside self-contained derivations of the underlying dualities, we
provide fault-tolerant circuit compilation methods for the Paldus transform
with $\mathcal{O}(d^3)$ Toffoli complexity, paving the way for significant
advancements in quantum simulation on quantum computers enabled by the UGA
paradigm.


## Understanding Task Vectors in In-Context Learning Emergence, Functionality, and Limitations

>Authors: Yuxin Dong, Jiachen Jiang, Zhihui Zhu, Xia Ning

>2025-06-10

> http://arxiv.org/abs/2506.09048v1

Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.


## FZOO Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed

>Authors: Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang

>2025-06-10

> http://arxiv.org/abs/2506.09034v1

Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.


## CodeBrain Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model

>Authors: Jingying Ma, Feng Wu, Qika Lin, Yucheng Xing, Chenyu Liu, Ziyu Jia, Mengling Feng

>2025-06-10

> http://arxiv.org/abs/2506.09110v1

Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model **sparse** long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.


## Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs

>Authors: Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, An Chen

>2025-06-10

> http://arxiv.org/abs/2506.09104v1

As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely **low-bit** **quantization**, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive **quantization** framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training **quantization** (PTQ) with
distillation-based **quantization**-aware training (Distill-QAT) for INT2
instruction-tuned LLM **quantization**. UPQ first **quantize**s FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the **quantization**
error introduced by subsequent INT2 **quantization**. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can **quantize** open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.


## Hardware Limitations and Optimization Approach in 1-Bit RIS Design at 28 GHz

>Authors: Hossein Rezaei, Mehmet Emin Arslan, George Yammine, Niels Neumann, Norman Franchi

>2025-06-10

> http://arxiv.org/abs/2506.08930v1

Reconfigurable intelligent surfaces (RIS) have emerged as a transformative
technology for electromagnetic (EM) wave manipulation, offering unprecedented
control over wave reflections compared to traditional metallic reflectors. By
utilizing an array of tunable elements, RIS can steer and shape electromagnetic
waves to enhance signal quality in wireless communication and radar systems.
However, practical implementations face significant challenges due to hardware
limitations and phase **quantization** errors. In this work, a 1-bit RIS prototype
operating at 28 GHz is developed to experimentally evaluate the impact of
hardware constraints on RIS performance. Unlike conventional studies that model
RIS as an ideal phase-shift matrix, this study accounts for physical parameters
that influence the actual reflection pattern. In particular, the presence of
specular reflection due to hardware limitations is investigated. Additionally,
the effects of phase **quantization** errors, which stem from the discrete nature
of RIS elements, are analyzed, and a genetic algorithm (GA)-based optimization
is introduced to mitigate these errors. The proposed optimization strategy
effectively reduces gain degradation at the desired angle caused by 1-bit
**quantization**, enhancing the overall performance of RIS. The effectiveness of
the approach is validated through measurements, underscoring the importance of
advanced phase control techniques in improving the functionality of RIS.


## PlantBert An Open Source Language Model for Plant Science

>Authors: Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri

>2025-06-10

> http://arxiv.org/abs/2506.08897v1

The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.


## SeerAttention-R Sparse Attention Adaptation for Long Reasoning

>Authors: Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang

>2025-06-10

> http://arxiv.org/abs/2506.08889v1

We introduce SeerAttention-R, a **sparse** attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
**sparsity** through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large **sparse** attention
block sizes (64/128). Using TileLang, we develop a highly optimized **sparse**
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% **sparsity**. Code is available at:
https://github.com/microsoft/SeerAttention.


## HunyuanVideo-HOMA Generic Human-Object Interaction in Multimodal Driven Human Animation

>Authors: Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao, Zhiyong Xu, Hongmei Wang, Qin Lin, Yuan Zhou, Qinglin Lu, Fan Tang

>2025-06-10

> http://arxiv.org/abs/2506.08797v1

To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through **sparse**, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.


## POLARON Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration

>Authors: Mukul Lokhande, Santosh Kumar Vishvakarma

>2025-06-10

> http://arxiv.org/abs/2506.08785v1

The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates **quantization**-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI **acceleration** at edge.


## sparseGeoHOPCA A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation

>Authors: Renjie Xu, Chong Wu, Maolin Che, Zhuoheng Ran, Yimin Wei, Hong Yan

>2025-06-10

> http://arxiv.org/abs/2506.08670v1

We propose **sparse**GeoHOPCA, a novel framework for **sparse** higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
**sparse** objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that **sparse**GeoHOPCA accurately
recovers **sparse** supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.


## Diversity-Guided MLP Reduction for Efficient Large Vision Transformers

>Authors: Chengchao Shen, Hourun Zhu, Gongfan Fang, Jianxin Wang, Xinchao Wang

>2025-06-10

> http://arxiv.org/abs/2506.08591v1

Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight **pruning** strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.


## k-Planar and Fan-Crossing Drawings and Transductions of Planar Graphs

>Authors: Petr Hliněný, Jan Jedelský

>2025-06-10

> http://arxiv.org/abs/2506.08585v1

We introduce a two-way connection between FO transductions (logical
transformations) of planar graphs, and a certain variant of fan-crossing
(fan-planar) drawings of graphs which for bounded-degree graphs essentially
reduces to being k-planar for fixed k. For graph classes, this connection
allows to derive non-transducibility results from nonexistence of the said
drawings and, conversely, from nonexistence of a transduction to derive
nonexistence of the said drawings. For example, the class of 3D-grids is not
k-planar for any fixed k. We hope that this connection will help to draw a path
to a possible proof that not all toroidal graphs are transducible from planar
graphs.
  Our characterization can be extended to any fixed surface instead of the
plane. The result is based on a very recent characterization of weakly **sparse**
FO transductions of classes of bounded expansion by [Gajarsk\'y, G{\l}adkowski,
Jedelsk\'y, Pilipczuk and Toru\'nczyk, arXiv:2505.15655].


## The Geometries of Truth Are Orthogonal Across Tasks

>Authors: Waiss Azizian, Michael Kirchhof, Eugene Ndiaye, Louis Bethune, Michal Klein, Pierre Ablin, Marco Cuturi

>2025-06-10

> http://arxiv.org/abs/2506.08572v1

Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with **sparsity**-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.


## Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models

>Authors: Srinivasan Kidambi, Pravin Nair

>2025-06-10

> http://arxiv.org/abs/2506.08520v1

Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient **acceleration** in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.


## CoQMoE Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA

>Authors: Jiale Dong, Hao Wu, Zihao Wang, Wenqi Lou, Zhendong Zheng, Lei Gong, Chao Wang, Xuehai Zhou

>2025-06-10

> http://arxiv.org/abs/2506.08496v1

Vision Transformers (ViTs) exhibit superior performance in computer vision
tasks but face deployment challenges on resource-constrained devices due to
high computational/memory demands. While Mixture-of-Experts Vision Transformers
(MoE-ViTs) mitigate this through a scalable architecture with sub-linear
computational growth, their hardware implementation on FPGAs remains
constrained by resource limitations. This paper proposes a novel accelerator
for efficiently implementing **quantize**d MoE models on FPGAs through two key
innovations: (1) A dual-stage **quantization** scheme combining
precision-preserving complex **quantize**rs with hardware-friendly simplified
**quantize**rs via scale reparameterization, with only 0.28 $\%$ accuracy loss
compared to full precision; (2) A resource-aware accelerator architecture
featuring latency-optimized streaming attention kernels and reusable linear
operators, effectively balancing performance and resource consumption.
Experimental results demonstrate that our accelerator achieves nearly 155
frames per second, a 5.35$\times$ improvement in throughput, and over $80\%$
energy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators,
while maintaining $<1\%$ accuracy loss across vision benchmarks. Our
implementation is available at https://github.com/DJ000011/CoQMoE.


## Fairness is Not Silence Unmasking Vacuous Neutrality in Small Language Models

>Authors: Sumanth Manduru, Carlotta Domeniconi

>2025-06-10

> http://arxiv.org/abs/2506.08487v1

The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
**quantization** improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.


## Olica Efficient Structured Pruning of Large Language Models without Retraining

>Authors: Jiujun He, Huazhen Lin

>2025-06-10

> http://arxiv.org/abs/2506.08436v1

Most existing structured **pruning** methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a **pruning** framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by **pruning** the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.


## Draft-based Approximate Inference for LLMs

>Authors: Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee

>2025-06-10

> http://arxiv.org/abs/2506.08373v1

Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(**KV**) cache dropping, **sparse** attention, and prompt compression, typically rely
on rough predictions of token or **KV** pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and **KV** pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) Spec**KV**, which
leverages a draft output to accurately assess the importance of each **KV** pair
for more effective **KV** cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference **acceleration**, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.


## DEAL Disentangling Transformer Head Activations for LLM Steering

>Authors: Li-Ming Zhan, Bo Liu, Zexin Lu, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu

>2025-06-10

> http://arxiv.org/abs/2506.08359v1

Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-**quantize**d
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each **quantize**d with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.


## Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating

>Authors: Guandong Li, Mengxia Ye

>2025-06-10

> http://arxiv.org/abs/2506.08324v2

Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, **sparse** distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.


## An efficient Fourier spectral algorithm for the Bogoliubov-de Gennes excitation eigenvalue problem

>Authors: Yu Li, Zhixuan Li, Manting Xie, Yong Zhang

>2025-06-10

> http://arxiv.org/abs/2506.08308v1

In this paper, we propose an efficient Fourier spectral algorithm for an
eigenvalue problem, that is, the Bogoliubov-de Gennes (BdG) equation arsing
from spin-1 Bose-Einstein condensates (BEC) to describe the
elementary/collective excitations around the mean-field ground state. The BdG
equation is essentially a constrained eigenvalue/eigenfunction system. Firstly,
we investigate its analytical properties, including exact eigenpairs,
generalized nullspace, and bi-orthogonality of eigenspaces. Secondly, by
combining the standard Fourier spectral method for spatial discretization and a
stable Gram-Schmidt bi-orthogonal algorithm, we develop a subspace iterative
solver for such a large-scale dense eigenvalue problem, and it proves to be
numerically stable, efficient, and accurate. Our solver is matrix-free and the
operator-function evaluation is accelerated by discrete Fast Fourier Transform
(FFT) with almost optimal efficiency. Therefore, it is memory-friendly and
efficient for large-scale problems. Furthermore, we give a rigorous and
detailed numerical analysis on the stability and spectral convergence. Finally,
we present extensive numerical results to illustrate the spectral accuracy and
efficiency, and investigate the excitation spectrum and Bogoliubov amplitudes
around the ground state in 1-3 spatial dimensions.


## AutoSDT Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists

>Authors: Yifei Li, Hanane Nour Moussa, Ziru Chen, Shijie Chen, Botao Yu, Mingyi Xue, Benjamin Burns, Tzu-Yao Chiu, Vishal Dey, Zitong Lu, Chen Wei, Qianheng Zhang, Tianyu Zhang, Song Gao, Xuhui Huang, Xia Ning, Nesreen K. Ahmed, Ali Payani, Huan Sun

>2025-06-09

> http://arxiv.org/abs/2506.08140v1

Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.


## Vision Transformers Don't Need Trained Registers

>Authors: Nick Jiang, Amil Dravid, Alexei Efros, Yossi Gandelsman

>2025-06-09

> http://arxiv.org/abs/2506.08010v2

We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
**sparse** set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.


## Aligning Text, Images, and 3D Structure Token-by-Token

>Authors: Aadarsh Sahoo, Vansh Tibrewal, Georgia Gkioxari

>2025-06-09

> http://arxiv.org/abs/2506.08002v1

Creating machines capable of understanding the world in 3D is essential in
assisting designers that build and edit 3D environments and robots navigating
and interacting within a three-dimensional space. Inspired by advances in
language and image modeling, we investigate the potential of autoregressive
models for a new modality: structured 3D scenes. To this end, we propose a
unified LLM framework that aligns language, images, and 3D scenes and provide a
detailed ''cookbook'' outlining critical design choices for achieving optimal
training and performance addressing key questions related to data
representation, modality-specific objectives, and more. We evaluate performance
across four core 3D tasks -- rendering, recognition, instruction-following, and
question-answering -- and four 3D datasets, synthetic and real-world. We extend
our approach to reconstruct complex 3D object shapes by enriching our 3D
modality with **quantize**d shape encodings, and show our model's effectiveness on
real-world 3D object recognition tasks. Project webpage:
https://glab-caltech.github.io/kyvo/


## A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle

>Authors: Amirreza Yasami, Mohammadali Tofigh, Mahdi Shahbakhti, Charles Robert Koch

>2025-06-09

> http://arxiv.org/abs/2506.07929v1

Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, **acceleration**,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.


## Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model

>Authors: Xiaoli Wei, Chunxia Zhang, Baisong Jiang, Anxiang Di, Deng Xiong, Jiangshe Zhang, Mingming Gong

>2025-06-09

> http://arxiv.org/abs/2506.07923v1

Seismic data interpolation is a critical pre-processing step for improving
seismic imaging quality and remains a focus of academic innovation. To address
the computational inefficiencies caused by extensive iterative resampling in
current plug-and-play diffusion interpolation methods, we propose the
diffusion-enhanced **sparse** attention transformer (Diff-spaformer), a novel deep
learning framework. Our model integrates transformer architectures and
diffusion models via a Seismic Prior Extraction Network (SPEN), which serves as
a bridge module. Full-layer **sparse** multi-head attention and feed-forward
propagation capture global information distributions, while the diffusion model
provides robust prior guidance. To mitigate the computational burden of
high-dimensional representations, self-attention is computed along the channel
rather than the spatial dimension. We show that using negative squared
Euclidean distance to compute **sparse** affinity matrices better suits seismic
data modeling, enabling broader contribution from amplitude feature nodes. An
adaptive ReLU function further discards low or irrelevant self-attention
values. We conduct training within a single-stage optimization framework,
requiring only a few reverse diffusion sampling steps during inference.
Extensive experiments demonstrate improved interpolation fidelity and
computational efficiency for both random and continuous missing data, offering
a new paradigm for high-efficiency seismic data reconstruction under complex
geological conditions.


## Speedy Deformable 3D Gaussian Splatting Fast Rendering and Compression of Dynamic Scenes

>Authors: Allen Tu, Haiyang Ying, Alex Hanson, Yonghan Lee, Tom Goldstein, Matthias Zwicker

>2025-06-09

> http://arxiv.org/abs/2506.07917v1

Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity **pruning** score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth **pruning** mechanism that
improves **pruning** robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.


## MiniCPM4 Ultra-Efficient LLMs on End Devices

>Authors: MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun

>2025-06-09

> http://arxiv.org/abs/2506.07900v1

This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable **sparse** attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates **sparse** attention, model **quantization**, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.


## Evaluating Large Language Models on the Frame and Symbol Grounding Problems A Zero-shot Benchmark

>Authors: Shoko Oka

>2025-06-09

> http://arxiv.org/abs/2506.07896v1

Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, **quantization**, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.


## CrosswalkNet An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing

>Authors: Zubin Bhuyan, Yuanchang Xie, AngkeaReach Rith, Xintong Yan, Nasko Apostolov, Jimi Oke, Chengbo Ai

>2025-06-09

> http://arxiv.org/abs/2506.07885v1

With the increasing availability of aerial and satellite imagery, deep
learning presents significant potential for transportation asset management,
safety analysis, and urban planning. This study introduces CrosswalkNet, a
robust and efficient deep learning framework designed to detect various types
of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet
incorporates a novel detection approach that improves upon traditional object
detection strategies by utilizing oriented bounding boxes (OBB), enhancing
detection precision by accurately capturing crosswalks regardless of their
orientation. Several optimization techniques, including Convolutional Block
Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine
annealing, are implemented to maximize performance and efficiency. A
comprehensive dataset comprising over 23,000 annotated crosswalk instances is
utilized to train and validate the proposed framework. The best-performing
model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial
imagery from Massachusetts, demonstrating its accuracy and effectiveness.
CrosswalkNet has also been successfully applied to datasets from New Hampshire,
Virginia, and Maine without transfer learning or fine-tuning, showcasing its
robustness and strong generalization capability. Additionally, the crosswalk
detection results, processed using High-Performance Computing (HPC) platforms
and provided in polygon shapefile format, have been shown to accelerate data
processing and detection, supporting real-time analysis for safety and mobility
applications. This integration offers policymakers, transportation engineers,
and urban planners an effective instrument to enhance pedestrian safety and
improve urban mobility.


## Teaching special relativity in elementary physics or upper high school courses

>Authors: Maria Grazia Blumetti, Biagio Buonaura, Giuseppe Giuliani, Marco Litterio

>2025-06-09

> http://arxiv.org/abs/2506.07872v1

This paper aims to provide teachers with a tool to teach the essential
features of special relativity, considering the students' difficulties
highlighted by numerous studies. Our proposal presents special relativity as
the solution to the troubles of Newtonian dynamics, exemplified by the
infinities of Newtonian uniformly accelerated motion. The paper's main section
uses thought experiments with the exchange of flashes of light of null duration
between two inertial reference frames to derive the kinematics effect of
special relativity (time dilation, length contraction, Doppler effect,
relativity of simultaneity, and Lorentz transformations). Simulations
illustrate the results of the simple calculations. The discussion of
experimental corroborations of the kinematics effects of special relativity
complements the theoretical treatments. The Doppler effect, typically treated
within the wave description of light, is addressed as an application of energy
and linear momentum conservation during the emission or absorption of a photon
by an atom (or a nucleus). When opportune, the paper suggests implementing
teaching practices in the topics developed for teachers. Two of us' preliminary
tests in the classroom ask for a wider one, including standard evaluation
procedures of students' learning.


## Learning to Focus Causal Attention Distillation via Gradient-Guided Token Pruning

>Authors: Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, Yankai Lin

>2025-06-09

> http://arxiv.org/abs/2506.07851v1

Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.


## Training Superior Sparse Autoencoders for Instruct Models

>Authors: Jiaming Li, Haoran Ye, Yukun Chen, Xinyue Li, Lei Zhang, Hamid Alinejad-Rokny, Jimmy Chih-Hsien Peng, Min Yang

>2025-06-09

> http://arxiv.org/abs/2506.07691v1

As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.


## MCPWorld A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents

>Authors: Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, Mao Qin, Yinxiao Chen, Chen Peng, Shangguang Wang, Mengwei Xu

>2025-06-09

> http://arxiv.org/abs/2506.07672v1

(M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU **acceleration** support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.


## ProSplat Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views

>Authors: Xiaohan Lu, Jiaye Fu, Jiaqi Zhang, Zetian Song, Chuanmin Jia, Siwei Ma

>2025-06-09

> http://arxiv.org/abs/2506.07670v1

Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising
results for novel view synthesis (NVS) from **sparse** input views, particularly
under narrow-baseline conditions. However, its performance significantly
degrades in wide-baseline scenarios due to limited texture details and
geometric inconsistencies across views. To address these challenges, in this
paper, we propose ProSplat, a two-stage feed-forward framework designed for
high-fidelity rendering under wide-baseline conditions. The first stage
involves generating 3D Gaussian primitives via a 3DGS generator. In the second
stage, rendered views from these primitives are enhanced through an improvement
model. Specifically, this improvement model is based on a one-step diffusion
model, further optimized by our proposed Maximum Overlap Reference view
Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI
supplements missing texture and color by strategically selecting a reference
view with maximum viewpoint overlap, while DWEA enforces geometric consistency
using epipolar constraints. Additionally, we introduce a divide-and-conquer
training strategy that aligns data distributions between the two stages through
joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K
datasets under wide-baseline settings. Experimental results demonstrate that
ProSplat achieves an average improvement of 1 dB in PSNR compared to recent
SOTA methods.


## Event-Priori-Based Vision-Language Model for Efficient Visual Understanding

>Authors: Haotong Qin, Cheng Hu, Michele Magno

>2025-06-09

> http://arxiv.org/abs/2506.07627v1

Large Language Model (LLM)-based Vision-Language Models (VLMs) have
substantially extended the boundaries of visual understanding capabilities.
However, their high computational demands hinder deployment on
resource-constrained edge devices. A key source of inefficiency stems from the
VLM's need to process dense and redundant visual information. Visual inputs
contain significant regions irrelevant to text semantics, rendering the
associated computations ineffective for inference. This paper introduces a
novel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core
contribution is a novel mechanism leveraging motion priors derived from dynamic
event vision to enhance VLM efficiency. Inspired by human visual cognition,
EP-VLM first employs event data to guide the patch-wise sparsification of RGB
visual inputs, progressively concentrating VLM computation on salient regions
of the visual input. Subsequently, we construct a position-preserving
tokenization strategy for the visual encoder within the VLM architecture. This
strategy processes the event-guided, unstructured, **sparse** visual input while
accurately preserving positional understanding within the visual input.
Experimental results demonstrate that EP-VLM achieves significant efficiency
improvements while maintaining nearly lossless accuracy compared to baseline
models from the Qwen2-VL series. For instance, against the original
Qwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the
original accuracy on the RealWorldQA dataset. This work demonstrates the
potential of event-based vision priors for improving VLM inference efficiency,
paving the way for creating more efficient and deployable VLMs for sustainable
visual understanding at the edge.


## IDENT Review Recent Advances in Identification of Differential Equations from Noisy Data

>Authors: Roy Y. He, Hao Liu, Wenjing Liao, Sung Ha Kang

>2025-06-09

> http://arxiv.org/abs/2506.07604v1

Differential equations and numerical methods are extensively used to model
various real-world phenomena in science and engineering. With modern
developments, we aim to find the underlying differential equation from a single
observation of time-dependent data. If we assume that the differential equation
is a linear combination of various linear and nonlinear differential terms,
then the identification problem can be formulated as solving a linear system.
The goal then reduces to finding the optimal coefficient vector that best
represents the time derivative of the given data. We review some recent works
on the identification of differential equations. We find some common themes for
the improved accuracy: (i) The formulation of linear system with proper
denoising is important, (ii) how to utilize **sparsity** and model selection to
find the correct coefficient support needs careful attention, and (iii) there
are ways to improve the coefficient recovery. We present an overview and
analysis of these approaches about some recent developments on the topic.


## TwinBreak Jailbreaking LLM Security Alignments based on Twin Prompts

>Authors: Torsten Krauß, Hamid Dashtbani, Alexandra Dmitrienko

>2025-06-09

> http://arxiv.org/abs/2506.07596v1

Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.


## MoQAE Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts

>Authors: Wei Tao, Haocheng Lu, Xiaoyang Qu, Bin Zhang, Kai Lu, Jiguang Wan, Jianzong Wang

>2025-06-09

> http://arxiv.org/abs/2506.07533v1

One of the primary challenges in optimizing large language models (LLMs) for
long-context inference lies in the high memory consumption of the Key-Value
(**KV**) cache. Existing approaches, such as **quantization**, have demonstrated
promising results in reducing memory usage. However, current **quantization**
methods cannot take both effectiveness and efficiency into account. In this
paper, we propose MoQAE, a novel mixed-precision **quantization** method via
mixture of **quantization**-aware experts. First, we view different **quantization**
bit-width configurations as experts and use the traditional mixture of experts
(MoE) method to select the optimal configuration. To avoid the inefficiency
caused by inputting tokens one by one into the router in the traditional MoE
method, we input the tokens into the router chunk by chunk. Second, we design a
lightweight router-only fine-tuning process to train MoQAE with a comprehensive
loss to learn the trade-off between model accuracy and memory usage. Finally,
we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to
further reduce the inference overhead. Extensive experiments on multiple
benchmark datasets demonstrate that our method outperforms state-of-the-art **KV**
cache **quantization** approaches in both efficiency and effectiveness.


## STREAMINGGS Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support

>Authors: Chenqi Zhang, Yu Feng, Jieru Zhao, Guangda Liu, Wenchao Ding, Chentao Wu, Minyi Guo

>2025-06-09

> http://arxiv.org/abs/2506.09070v1

3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
**sparse** Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.


## Graph-of-Causal Evolution Challenging Chain-of-Model for Reasoning

>Authors: Libo Wang

>2025-06-09

> http://arxiv.org/abs/2506.07501v1

In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and **sparse** causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.


## LiteVLM A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments

>Authors: Jin Huang, Yuchao Jin, Le An, Josh Park

>2025-06-09

> http://arxiv.org/abs/2506.07416v1

This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training **quantization**. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.


## MapBERT Bitwise Masked Modeling for Real-Time Semantic Mapping Generation

>Authors: Yijie Deng, Shuaihang Yuan, Congcong Wen, Hao Huang, Anthony Tzes, Geeta Chandra Raju Bethala, Yi Fang

>2025-06-09

> http://arxiv.org/abs/2506.07350v1

Spatial awareness is a critical capability for embodied agents, as it enables
them to anticipate and reason about unobserved regions. The primary challenge
arises from learning the distribution of indoor semantics, complicated by
**sparse**, imbalanced object categories and diverse spatial scales. Existing
methods struggle to robustly generate unobserved areas in real time and do not
generalize well to new environments. To this end, we propose \textbf{MapBERT},
a novel framework designed to effectively model the distribution of unseen
spaces. Motivated by the observation that the one-hot encoding of semantic maps
aligns naturally with the binary structure of bit encoding, we, for the first
time, leverage a lookup-free BitVAE to encode semantic maps into compact
bitwise tokens. Building on this, a masked transformer is employed to infer
missing regions and generate complete semantic maps from limited observations.
To enhance object-centric reasoning, we propose an object-aware masking
strategy that masks entire object categories concurrently and pairs them with
learnable embeddings, capturing implicit relationships between object
embeddings and spatial tokens. By learning these relationships, the model more
effectively captures indoor semantic distributions crucial for practical
robotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves
state-of-the-art semantic map generation, balancing computational efficiency
with accurate reconstruction of unobserved regions.


## Improving LLM Reasoning through Interpretable Role-Playing Steering

>Authors: Anyi Wang, Dong Shu, Yifan Wang, Yunpu Ma, Mengnan Du

>2025-06-09

> http://arxiv.org/abs/2506.07335v1

Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.


## Graph-KV Breaking Sequence via Injecting Structural Biases into Large Language Models

>Authors: Haoyu Wang, Peihao Wang, Mufei Li, Shikun Liu, Siqi Miao, Zhangyang Wang, Pan Li

>2025-06-09

> http://arxiv.org/abs/2506.07334v1

Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-**KV** with the
potential to overcome this limitation. Graph-**KV** leverages the **KV**-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the **KV**-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-**KV** across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-**KV**
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-**KV** data are publicly
available.


## Paged Attention Meets FlexAttention Unlocking Long-Context Efficiency in Deployed Inference

>Authors: Thomas Joshi, Herman Saini, Neil Dhillon, Antoni Viros i Martin, Kaoutar El Maghraoui

>2025-06-08

> http://arxiv.org/abs/2506.07311v1

Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (**KV**) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic **KV** cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered **KV** data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global **KV** cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.


## Exploring the Impact of Temperature on Large Language ModelsHot or Cold?

>Authors: Lujun Li, Lama Sleem, Niccolo' Gentile, Geoffrey Nichil, Radu State

>2025-06-08

> http://arxiv.org/abs/2506.07295v1

The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit **quantize**d models. By evaluating
temperature effects up to 4.0 in three **quantize**d models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.


## ReStNet A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices

>Authors: Maoyu Wang, Yao Lu, Jiaqi Nie, Zeyu Wang, Yun Lin, Qi Xuan, Guan Gui

>2025-06-08

> http://arxiv.org/abs/2506.09066v1

With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as **pruning**, **quantization**, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.


## GGBall Graph Generative Model on Poincaré Ball

>Authors: Tianci Bu, Chuanrui Wang, Hao Ma, Haoren Zheng, Xin Lu, Tailin Wu

>2025-06-08

> http://arxiv.org/abs/2506.07198v1

Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector **quantization** helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.


## Learning Compact Vision Tokens for Efficient Large Multimodal Models

>Authors: Hao Tang, Chengchao Shen

>2025-06-08

> http://arxiv.org/abs/2506.07138v1

Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference **acceleration**. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.


## MAGNet A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection

>Authors: Weihan Lu, Hong Cai Chen

>2025-06-08

> http://arxiv.org/abs/2506.07126v1

Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to **sparse** violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.


## Accelerating Two-Dimensional Materials Research via a Universal Interatomic Potential and Large Language Model Agent

>Authors: Haidi Wang, Yufan Yao, Haonan Song, Xiaofeng Liu, Zhao Chen, Weiwei Chen, Weiduo Zhu, Zhongjun Li, Jinlong Yang

>2025-06-08

> http://arxiv.org/abs/2506.07043v1

Accurate interatomic potentials (IAPs) are essential for modeling the
potential energy surfaces (PES) that govern atomic interactions in materials.
However, most existing IAPs are developed for bulk materials and struggle to
accurately and efficiently capture the diverse chemical environment of
two-dimensional (2D) materials. This limitation poses a significant barrier to
the large-scale design and simulation of emerging 2D systems. To address this
challenge, we present a universal interatomic potential tailored for 2D
materials. Our model is trained on a dataset comprising 327,062
structure-energy-force-stress mappings derived from 20,114 2D materials,
spanning 89 chemical elements. The results show high predictive accuracy, with
mean absolute errors of 6 meV/atom for energies, 80 meV/\AA for atomic forces,
and 0.067 GPa for stress tensors. It demonstrates broad applicability across a
range of atomistic tasks, including structural relaxation, lattice dynamics,
molecular dynamics, material discovery, and so on. To further enhance usability
and accessibility, we introduce an intelligent agent powered by a large
language model (LLM), enabling natural language interaction for 2D materials
property simulations. Our work provides not only a precise and universal IAP
for 2D systems, but also an intelligent, user-friendly platform that enables
high-throughput screening, property prediction, and theoretical exploration,
thereby accelerating advances in 2D materials research.


## Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test

>Authors: Xiaoyuan Zhu, Yaowen Ye, Tianyi Qiu, Hanlin Zhu, Sijun Tan, Ajraf Mannan, Jonathan Michala, Raluca Ada Popa, Willie Neiswanger

>2025-06-08

> http://arxiv.org/abs/2506.06975v3

As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve **quantize**d or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including **quantization**, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.


## Towards AI-Native Fronthaul Neural Compression for NextG Cloud RAN

>Authors: Chenghong Bian, Yulin Shao, Deniz Gunduz

>2025-06-07

> http://arxiv.org/abs/2506.06925v1

The rapid growth of data traffic and the emerging AI-native wireless
architectures in NextG cellular systems place new demands on the fronthaul
links of Cloud Radio Access Networks (C-RAN). In this paper, we investigate
neural compression techniques for the Common Public Radio Interface (CPRI),
aiming to reduce the fronthaul bandwidth while preserving signal quality. We
introduce two deep learning-based compression algorithms designed to optimize
the transformation of wireless signals into bit sequences for CPRI
transmission. The first algorithm utilizes a non-linear transformation coupled
with scalar/vector **quantization** based on a learned codebook. The second
algorithm generates a latent vector transformed into a variable-length output
bit sequence via arithmetic encoding, guided by the predicted probability
distribution of each latent element. Novel techniques such as a shared weight
model for storage-limited devices and a successive refinement model for
managing multiple CPRI links with varying Quality of Service (QoS) are
proposed. Extensive simulation results demonstrate notable Error Vector
Magnitude (EVM) gains with improved rate-distortion performance for both
algorithms compared to traditional methods. The proposed solutions are robust
to variations in channel conditions, modulation formats, and noise levels,
highlighting their potential for enabling efficient and scalable fronthaul in
NextG AI-native networks as well as aligning with the current 3GPP research
directions.


## High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations

>Authors: Ziwei Li, Yuhan Duan, Tianyu Xiong, Yi-Tang Chen, Wei-Lun Chao, Han-Wei Shen

>2025-06-07

> http://arxiv.org/abs/2506.06858v1

Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.


## An Efficient Digital Watermarking Technique for Small Scale devices

>Authors: Kaushik Talathi, Aparna Santra Biswas

>2025-06-07

> http://arxiv.org/abs/2506.06691v1

In the age of IoT and mobile platforms, ensuring that content stay authentic
whilst avoiding overburdening limited hardware is a key problem. This study
introduces hybrid Fast Wavelet Transform & Additive Quantization index
Modulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures
digital pictures on low-power, memory-constrained small scale devices to
achieve a balanced trade-off among robustness, imperceptibility, and
computational efficiency. The method embeds watermark in the luminance
component of YCbCr color space using low-frequency FWT sub-bands, minimizing
perceptual distortion, using additive QIM for simplicity. Both the extraction
and embedding processes run in less than 40 ms and require minimum RAM when
tested on a Raspberry Pi 5. Quality assessments on standard and high-resolution
images yield PSNR greater than equal to 34 dB and SSIM greater than equal to
0.97, while robustness verification includes various geometric and
signal-processing attacks demonstrating near-zero bit error rates and NCC
greater than equal to 0.998. Using a mosaic-based watermark, redundancy added
enhancing robustness without reducing throughput, which peaks at 11 MP/s. These
findings show that FWT-AQIM provides an efficient, scalable solution for
real-time, secure watermarking in bandwidth- and power-constrained contexts,
opening the way for dependable content protection in developing IoT and
multimedia applications.


## Spark Transformer Reactivating Sparsity in FFN and Attention

>Authors: Chong You, Kan Wu, Zhipeng Jia, Lin Chen, Srinadh Bhojanapalli, Jiaxian Guo, Utku Evci, Jan Wassenberg, Praneeth Netrapalli, Jeremiah J. Willcock, Suvinay Subramanian, Felix Chern, Alek Andreev, Shreya Pathak, Felix Yu, Prateek Jain, David E. Culler, Henry M. Levy, Sanjiv Kumar

>2025-06-07

> http://arxiv.org/abs/2506.06644v1

The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation **sparsity** for
enhancing large model efficiency. While notable progress has been made in
translating such **sparsity** to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation **sparsity** often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of **sparse** activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation **sparsity** in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes **sparsity** via top-k masking for
explicit control over **sparsity** level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced **sparsity**, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant **sparsity**: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
**sparsity** translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.


## GELD A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales

>Authors: Yubin Xiao, Di Wang, Rui Cao, Xuan Wu, Boyang Li, You Zhou

>2025-06-07

> http://arxiv.org/abs/2506.06634v1

The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.


## Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit

>Authors: Charles Goddard, Fernando Fernandes Neto

>2025-06-07

> http://arxiv.org/abs/2506.06607v1

We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a **sparse** linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
**sparse** coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.


## Stacey Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization

>Authors: Xinyu Luo, Cedar Site Bai, Bolian Li, Petros Drineas, Ruqi Zhang, Brian Bullins

>2025-06-07

> http://arxiv.org/abs/2506.06606v1

While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .


## Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data

>Authors: Aryaman Gupta, Yusuf Umut Ciftci, Somil Bansal

>2025-06-06

> http://arxiv.org/abs/2506.06570v1

As robotic systems become increasingly integrated into real-world
environments, ranging from autonomous vehicles to household assistants, they
inevitably encounter diverse and unstructured scenarios that lead to failures.
While such failures pose safety and reliability challenges, they also provide
rich perceptual data for improving future performance. However, manually
analyzing large-scale failure datasets is impractical. In this work, we present
a method for automatically organizing large-scale robotic failure data into
semantically meaningful clusters, enabling scalable learning from failure
without human supervision. Our approach leverages the reasoning capabilities of
Multimodal Large Language Models (MLLMs), trained on internet-scale data, to
infer high-level failure causes from raw perceptual trajectories and discover
interpretable structure within uncurated failure logs. These semantic clusters
reveal latent patterns and hypothesized causes of failure, enabling scalable
learning from experience. We demonstrate that the discovered failure modes can
guide targeted data collection for policy refinement, accelerating iterative
improvement in agent policies and overall safety. Additionally, we show that
these semantic clusters can be employed for online failure detection, offering
a lightweight yet powerful safeguard for real-time adaptation. We demonstrate
that this framework enhances robot learning and robustness by transforming
real-world failures into actionable and interpretable signals for adaptation.


## Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks

>Authors: Zijiang Yan, Hao Zhou, Jianhua Pei, Hina Tabassum

>2025-06-06

> http://arxiv.org/abs/2506.06532v1

Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.


## Lecture Notes in Loop Quantum Gravity. LN3 Boundary equations for Ashtekar-Barbero-Immirzi model

>Authors: L. Fatibene, A. Orizzonte

>2025-06-06

> http://arxiv.org/abs/2506.06510v1

We shall here perform the canonical analysis of field equations of ABI model
in order to determine constraint equations. We shall show that one can use
algebraic constraints in the covariant framework to fix $k^i$ as a function of
the frame and obtain a model where $(A^i_a, E_i^a)$ is a pair of independent
fields which are also a pair of conjugated fields.
  We shall not impose any relation on Immirzi parameter $\beta$ and Holst
parameter $\gamma$, still constraint equations will depend on $\beta$ only and
they agree with standard result of LQG which are obtained by a suitable
canonical transformation on a leaf of the ADM foliation used to define a
Hamiltonian framework. We eventually state the scheme for **quantization** that
will be discussed in the following lecture notes.


## Towards Infant Sleep-Optimized Driving Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control

>Authors: Ruitao Chen, Mozhang Guo, Jinge Li

>2025-06-06

> http://arxiv.org/abs/2506.06459v1

Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden **acceleration**, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of **acceleration**, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.


## Saffron-1 Towards an Inference Scaling Paradigm for LLM Safety Assurance

>Authors: Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong

>2025-06-06

> http://arxiv.org/abs/2506.06444v1

Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .


## Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias

>Authors: Yuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing Yang

>2025-06-06

> http://arxiv.org/abs/2506.06280v1

Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight
matrices has been an active area of research in recent years. At a high level,
eigenspectrum analysis of DNNs involves measuring the heavytailness of the
empirical spectral densities (ESD) of weight matrices. It provides insight into
how well a model is trained and can guide decisions on assigning better
layer-wise training hyperparameters. In this paper, we address a challenge
associated with such eigenspectrum methods: the impact of the aspect ratio of
weight matrices on estimated heavytailness metrics. We demonstrate that
matrices of varying sizes (and aspect ratios) introduce a non-negligible bias
in estimating heavytailness metrics, leading to inaccurate model diagnosis and
layer-wise hyperparameter assignment. To overcome this challenge, we propose
FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the
weight matrices by subsampling submatrices with a fixed aspect ratio. Instead
of measuring the heavytailness of the original ESD, we measure the average ESD
of these subsampled submatrices. We show that measuring the heavytailness of
these submatrices with the fixed aspect ratio can effectively mitigate the
aspect ratio bias. We validate our approach across various optimization
techniques and application domains that involve eigenspectrum analysis of
weights, including image classification in computer vision (CV) models,
scientific machine learning (SciML) model training, and large language model
(LLM) **pruning**. Our results show that despite its simplicity, FARMS uniformly
improves the accuracy of eigenspectrum analysis while enabling more effective
layer-wise hyperparameter assignment in these application domains. In one of
the LLM **pruning** experiments, FARMS reduces the perplexity of the LLaMA-7B model
by 17.3% when compared with the state-of-the-art method.


## RecGPT A Foundation Model for Sequential Recommendation

>Authors: Yangqin Jiang, Xubin Ren, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang

>2025-06-06

> http://arxiv.org/abs/2506.06270v1

This work addresses a fundamental barrier in recommender systems: the
inability to generalize across domains without extensive retraining.
Traditional ID-based approaches fail entirely in cold-start and cross-domain
scenarios where new users or items lack sufficient interaction history.
Inspired by foundation models' cross-domain success, we develop a foundation
model for sequential recommendation that achieves genuine zero-shot
generalization capabilities. Our approach fundamentally departs from existing
ID-based methods by deriving item representations exclusively from textual
features. This enables immediate embedding of any new item without model
retraining. We introduce unified item tokenization with Finite Scalar
Quantization that transforms heterogeneous textual descriptions into
standardized discrete tokens. This eliminates domain barriers that plague
existing systems. Additionally, the framework features hybrid
bidirectional-causal attention that captures both intra-item token coherence
and inter-item sequential dependencies. An efficient catalog-aware beam search
decoder enables real-time token-to-item mapping. Unlike conventional approaches
confined to their training domains, RecGPT naturally bridges diverse
recommendation contexts through its domain-invariant tokenization mechanism.
Comprehensive evaluations across six datasets and industrial scenarios
demonstrate consistent performance advantages.


## Can Theoretical Physics Research Benefit from Language Agents?

>Authors: Sirui Lu, Zhijing Jin, Terry Jingchen Zhang, Pavel Kos, J. Ignacio Cirac, Bernhard Schölkopf

>2025-06-06

> http://arxiv.org/abs/2506.06214v1

Large Language Models (LLMs) are rapidly advancing across diverse domains,
yet their application in theoretical physics research is not yet mature. This
position paper argues that LLM agents can potentially help accelerate
theoretical, computational, and applied physics when properly integrated with
domain knowledge and toolbox. We analyze current LLM capabilities for physics
-- from mathematical reasoning to code generation -- identifying critical gaps
in physical intuition, constraint satisfaction, and reliable reasoning. We
envision future physics-specialized LLMs that could handle multimodal data,
propose testable hypotheses, and design experiments. Realizing this vision
requires addressing fundamental challenges: ensuring physical consistency, and
developing robust verification methods. We call for collaborative efforts
between physics and AI communities to help advance scientific discovery in
physics.


## Reusing Trajectories in Policy Gradients Enables Fast Convergence

>Authors: Alessandro Montenegro, Federico Mansutti, Marco Mussi, Matteo Papini, Alberto Maria Metelli

>2025-06-06

> http://arxiv.org/abs/2506.06178v1

Policy gradient (PG) methods are a class of effective reinforcement learning
algorithms, particularly when dealing with continuous control problems. These
methods learn the parameters of parametric policies via stochastic gradient
ascent, typically using on-policy trajectory data to estimate the policy
gradient. However, such reliance on fresh data makes them sample-inefficient.
Indeed, vanilla PG methods require $O(\epsilon^{-2})$ trajectories to reach an
$\epsilon$-approximate stationary point. A common strategy to improve
efficiency is to reuse off-policy information from past iterations, such as
previous gradients or trajectories. While gradient reuse has received
substantial theoretical attention, leading to improved rates of
$O(\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored
from a theoretical perspective. In this work, we provide the first rigorous
theoretical evidence that extensive reuse of past off-policy trajectories can
significantly accelerate convergence in PG methods. We introduce a power mean
correction to the multiple importance weighting estimator and propose RPG
(Retrospective Policy Gradient), a PG algorithm that combines old and new
trajectories for policy updates. Through a novel analysis, we show that, under
established assumptions, RPG achieves a sample complexity of
$\widetilde{O}(\epsilon^{-1})$, the best known rate in the literature. We
further validate empirically our approach against PG methods with
state-of-the-art rates.


## Venus Cloud Research Progress and Perspectives

>Authors: Longkang Dai, Dmitrij V. Titov, Wencheng D. Shao, Xi Zhang, Jun Cui, Siteng Fan

>2025-06-06

> http://arxiv.org/abs/2506.06164v1

Venus has regained attention on the international stage with the approval of
three new missions by ESA and NASA. As the twin sister of Earth, Venus exhibits
a distinct atmosphere, which casts a veil of mystery over the planetary
evolution and is of great scientific significance. One of the most important
components of Venus-the cloud-is believed to have significantly regulated its
climate evolution and affect the environmental habitability. However, due to
**sparse** in-situ measurements and the limitation of remote sensing, properties of
these clouds remain largely unknown. Based on research conducted in past
decades, this article reviews the observational structure of cloud properties,
the progress of microphysical and simplified cloud model developments, and
perspectives of future directions of this research field. Several possible
solutions to the challenges associated with the coupling effect, ultraviolet
absorption, and habitability are proposed and discussed in details, providing
insights for future Venus' explorations.


## Bidirectional Image-Event Guided Low-Light Image Enhancement

>Authors: Zhanwen Liu, Huanna Song, Yang Wang, Nan Yang, Shangyu Xie, Yisheng An, Xiangmo Zhao

>2025-06-06

> http://arxiv.org/abs/2506.06120v1

Under extreme low-light conditions, traditional frame-based cameras, due to
their limited dynamic range and temporal resolution, face detail loss and
motion blur in captured images. To overcome this bottleneck, researchers have
introduced event cameras and proposed event-guided low-light image enhancement
algorithms. However, these methods neglect the influence of global
low-frequency noise caused by dynamic lighting conditions and local structural
discontinuities in **sparse** event data. To address these issues, we propose an
innovative Bidirectional guided Low-light Image Enhancement framework (BiLIE).
Specifically, to mitigate the significant low-frequency noise introduced by
global illumination step changes, we introduce the frequency high-pass
filtering-based Event Feature Enhancement (EFE) module at the event
representation level to suppress the interference of low-frequency information,
and preserve and highlight the high-frequency edges.Furthermore, we design a
Bidirectional Cross Attention Fusion (BCAF) mechanism to acquire high-frequency
structures and edges while suppressing structural discontinuities and local
noise introduced by **sparse** event guidance, thereby generating smoother fused
representations.Additionally, considering the poor visual quality and color
bias in existing datasets, we provide a new dataset (RELIE), with high-quality
ground truth through a reliable enhancement scheme. Extensive experimental
results demonstrate that our proposed BiLIE outperforms state-of-the-art
methods by 0.96dB in PSNR and 0.03 in LPIPS.


## Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU

>Authors: Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Weifeng Liu, Qingxiao Sun

>2025-06-06

> http://arxiv.org/abs/2506.06095v1

Large language models are popular around the world due to their powerful
understanding capabilities. As the core component of LLMs, accelerating
Transformer through parallelization has gradually become a hot research topic.
Mask layers introduce **sparsity** into Transformer to reduce calculations.
However, previous works rarely focus on the performance optimization of **sparse**
Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of
mixed-type operators and fail to adapt to various sequence lengths. To address
the above problems, we propose STOF, a framework that incorporates
optimizations for Sparse Transformer via flexible masking and operator fusion
on GPU. We firstly unify the storage format and kernel implementation for the
multi-head attention. Then, we map fusion schemes to compilation templates and
determine the optimal parameter setting through a two-stage search engine. The
experimental results show that compared to the state-of-the-art work, STOF
achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end
inference.


## BEAST Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning

>Authors: Hongyi Zhou, Weiran Liao, Xi Huang, Yucheng Tang, Fabian Otto, Xiaogang Jia, Xinkai Jiang, Simon Hilber, Ge Li, Qian Wang, Ömer Erdinç Yağmurlu, Nils Blank, Moritz Reuss, Rudolf Lioutikov

>2025-06-06

> http://arxiv.org/abs/2506.06072v2

We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel
action tokenizer that encodes action sequences into compact discrete or
continuous tokens using B-splines. In contrast to existing action tokenizers
based on vector **quantization** or byte pair encoding, BEAST requires no separate
tokenizer training and consistently produces tokens of uniform length, enabling
fast action sequence generation via parallel decoding. Leveraging our B-spline
formulation, BEAST inherently ensures generating smooth trajectories without
discontinuities between adjacent segments. We extensively evaluate BEAST by
integrating it with three distinct model architectures: a Variational
Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with
discrete tokens, and Florence-2, a pretrained Vision-Language Model with an
encoder-decoder architecture, demonstrating BEAST's compatibility and
scalability with large pretrained models. We evaluate BEAST across three
established benchmarks consisting of 166 simulated tasks and on three distinct
robot settings with a total of 8 real-world tasks. Experimental results
demonstrate that BEAST (i) significantly reduces both training and inference
computational costs, and (ii) consistently generates smooth, high-frequency
control signals suitable for continuous control tasks while (iii) reliably
achieves competitive task success rates compared to state-of-the-art methods.


## Efficient Memory Tiering in a Virtual Machine

>Authors: Chandra Prakash, Aravinda Prasad, Sandeep Kumar, Sreenivas Subramoney

>2025-06-06

> http://arxiv.org/abs/2506.06067v1

Memory tiering is the norm to effectively tackle the increasing server memory
total cost of ownership (TCO) and the growing data demands of modern data
center workloads. However, the host-based state-of-the-art memory tiering
solutions can be inefficient for a virtualized environment when (i) the
frequently accessed data are scattered across the guest physical address space
or (ii) the accesses to a huge page inside the guest are skewed due to a small
number of subpages being hot. Scattered or skewed accesses make the whole huge
page look hot in the host address space. This results in host selecting and
placing **sparse**ly accessed huge pages in near memory, wasting costly near memory
resources.
  We propose a host-agnostic technique employed inside the guest that exploits
the two-level address translation in a virtualized environment to consolidate
the scattered and skewed accesses to a set of guest physical address ranges.
Consolidation transforms **sparse**ly hot huge pages to densely hot huge pages in
the host address space context. As a consequence, host-based tiering solutions
can place densely hot huge pages in near memory, improving near memory
utilization. Our evaluation of our technique on standalone real-world
benchmarks with state-of-the-art host-based tiering show 50-70% reduction in
near memory consumption at similar performance levels, while evaluation at
scale improves performance by 10-13% with similar memory TCO.


## Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning

>Authors: Yujia Huo, Jianchun Liu, Hongli Xu, Zhenguo Ma, Shilong Wang, Liusheng Huang

>2025-06-06

> http://arxiv.org/abs/2506.05977v1

Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence **acceleration** ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.


## AQUATIC-Diff Additive Quantization for Truly Tiny Compressed Diffusion Models

>Authors: Adil Hasan, Thomas Peyrin

>2025-06-06

> http://arxiv.org/abs/2506.05960v2

Significant investments have been made towards the commodification of
diffusion models for generation of diverse media. Their mass-market adoption is
however still hobbled by the intense hardware resource requirements of
diffusion model inference. Model **quantization** strategies tailored specifically
towards diffusion models have been useful in easing this burden, yet have
generally explored the Uniform Scalar Quantization (USQ) family of **quantization**
methods. In contrast, Vector Quantization (VQ) methods, which operate on groups
of multiple related weights as the basic unit of compression, have seen
substantial success in Large Language Model (LLM) **quantization**. In this work,
we apply codebook-based additive vector **quantization** to the problem of
diffusion model compression. Our resulting approach achieves a new Pareto
frontier for the extremely **low-bit** weight **quantization** on the standard
class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.
Notably, we report sFID 1.92 points lower than the full-precision model at W4A8
and the best-reported results for FID, sFID and ISC at W2A8. We are also able
to demonstrate FLOPs savings on arbitrary hardware via an efficient inference
kernel, as opposed to savings resulting from small integer operations which may
lack broad hardware support.


## MOGO Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation

>Authors: Dongjie Fu, Tengjiao Sun, Pengcheng Fang, Xiaohao Cai, Hansung Kim

>2025-06-06

> http://arxiv.org/abs/2506.05952v1

Recent advances in transformer-based text-to-motion generation have led to
impressive progress in synthesizing high-quality human motion. Nevertheless,
jointly achieving high fidelity, streaming capability, real-time
responsiveness, and scalability remains a fundamental challenge. In this paper,
we propose MOGO (Motion Generation with One-pass), a novel autoregressive
framework tailored for efficient and real-time 3D motion generation. MOGO
comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual
vector **quantization** module that hierarchically discretizes motion sequences
with learnable scaling to produce compact yet expressive representations; and
(2) RQHC-Transformer, a residual **quantize**d hierarchical causal transformer that
generates multi-layer motion tokens in a single forward pass, significantly
reducing inference latency. To enhance semantic fidelity, we further introduce
a text condition alignment mechanism that improves motion decoding under
textual control. Extensive experiments on benchmark datasets including
HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or
superior generation quality compared to state-of-the-art transformer-based
methods, while offering substantial improvements in real-time performance,
streaming generation, and generalization under zero-shot settings.


## MoA Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models

>Authors: Jie Cao, Tianwei Lin, Hongyang He, Rolan Yan, Wenqiao Zhang, Juncheng Li, Dongping Zhang, Siliang Tang, Yueting Zhuang

>2025-06-06

> http://arxiv.org/abs/2506.05928v1

Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts
(MoE) to further enhance the performance of parameter-efficient fine-tuning
(PEFT) methods in Large Language Model (LLM) applications. Existing methods
employ \emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with
either similar or identical structures and capacities. However, these
approaches often suffer from representation collapse and expert load imbalance,
which negatively impact the potential of LLMs. To address these challenges, we
propose a \emph{heterogeneous} \textbf{Mixture-of-Adapters (MoA)} approach.
This method dynamically integrates PEFT adapter experts with diverse
structures, leveraging their complementary representational capabilities to
foster expert specialization, thereby enhancing the effective transfer of
pre-trained knowledge to downstream tasks. MoA supports two variants:
\textbf{(i)} \textit{Soft MoA} achieves fine-grained integration by performing
a weighted fusion of all expert outputs; \textbf{(ii)} \textit{Sparse MoA}
activates adapter experts **sparse**ly based on their contribution, achieving this
with negligible performance degradation. Experimental results demonstrate that
heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance
and parameter efficiency. Our project is available at
https://github.com/DCDmllm/MoA.


## Token Transforming A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration

>Authors: Fanhu Zeng, Deli Yu, Zhenglun Kong, Hao Tang

>2025-06-06

> http://arxiv.org/abs/2506.05709v1

Vision transformers have been widely explored in various vision tasks. Due to
heavy computational cost, much interest has aroused for compressing vision
transformer dynamically in the aspect of tokens. Current methods mainly pay
attention to token **pruning** or merging to reduce token numbers, in which tokens
are compressed exclusively, causing great information loss and therefore
post-training is inevitably required to recover the performance. In this paper,
we rethink token reduction and unify the process as an explicit form of token
matrix transformation, in which all existing methods are constructing special
forms of matrices within the framework. Furthermore, we propose a many-to-many
Token Transforming framework that serves as a generalization of all existing
methods and reserves the most information, even enabling training-free
**acceleration**. We conduct extensive experiments to validate our framework.
Specifically, we reduce 40% FLOPs and accelerate DeiT-S by $\times$1.5 with
marginal 0.1% accuracy drop. Furthermore, we extend the method to dense
prediction tasks including segmentation, object detection, depth estimation,
and language model generation. Results demonstrate that the proposed method
consistently achieves substantial improvements, offering a better
computation-performance trade-off, impressive budget reduction and inference
**acceleration**.


## Bridging the Modality Gap Softly Discretizing Audio Representation for LLM-based Automatic Speech Recognition

>Authors: Mu Yang, Szu-Jui Chen, Jiamin Xie, John Hansen

>2025-06-06

> http://arxiv.org/abs/2506.05706v1

One challenge of integrating speech input with large language models (LLMs)
stems from the discrepancy between the continuous nature of audio data and the
discrete token-based paradigm of LLMs. To mitigate this gap, we propose a
method for integrating vector **quantization** (VQ) into LLM-based automatic speech
recognition (ASR). Using the LLM embedding table as the VQ codebook, the VQ
module aligns the continuous representations from the audio encoder with the
discrete LLM inputs, enabling the LLM to operate on a discretized audio
representation that better reflects the linguistic structure. We further create
a soft "discretization" of the audio representation by updating the codebook
and performing a weighted sum over the codebook embeddings. Empirical results
demonstrate that our proposed method significantly improves upon the LLM-based
ASR baseline, particularly in out-of-domain conditions. This work highlights
the potential of soft discretization as a modality bridge in LLM-based ASR.


## Simulation Everywhere An Evolutionary Expansion of Discrete-Event Modeling and Simulation research and practice

>Authors: Ikpe Justice Akpan, Godwin E. Etti

>2025-06-06

> http://arxiv.org/abs/2506.05698v1

Simulation was launched in the 1950s, nicknamed a tool of "last resort." Over
the years, this Operations Research (OR) method has made significant progress,
and utilizing the accelerated advances in computer science (hardware and
software, processing speed, and advanced information visualization
capabilities) to improve simulation usability in research and practice. After
overcoming the initial obstacles and the scare of outliving its usefulness in
the 2000s, computer simulation has remained a popular OR tool applied in
diverse industries and sectors, earning its popularity leading to the term
"simulation everywhere." This study uses bibliographic data from research and
practice literature to evaluate the evolutionary expansion in simulation,
focusing on discrete-event simulation (DES). The results show asymmetrical but
positive yearly literature out-put, broadened DES adoption in diverse fields,
and sustained relevance as a scientific method for tackling old, new, and
emerging issues. Also, DES is an essential tool in Industry 4.0 and plays a
central role in digital transformation that has swept the industrial space,
from manufacturing to healthcare and other sectors. With the emergence, ongoing
adoption, and deployment of generative artificial intelligence (GenAI), future
studies seek ways to integrate GenAI in DES to remain relevant and improve the
modeling and simulation processes.


## EdgeProfiler A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model

>Authors: Alyssa Pinnock, Shakya Jayakody, Kawsher A Roxy, Md Rubel Ahmed

>2025-06-06

> http://arxiv.org/abs/2506.09061v1

This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive **quantization**
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit **quantization** reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.


## BAQ Efficient Bit Allocation Quantization for Large Language Models

>Authors: Chao Zhang, Li Wang, Samson Lasaulce, Merouane Debbah

>2025-06-06

> http://arxiv.org/abs/2506.05664v1

Post-training model **quantization** is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to **quantization**
noise. In this paper, we propose a novel framework for allocating **quantization**
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
**quantization** loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard **quantization** pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.


## FedShield-LLM A Secure and Scalable Federated Fine-Tuned Large Language Model

>Authors: Md Jueal Mia, M. Hadi Amini

>2025-06-06

> http://arxiv.org/abs/2506.05640v1

Federated Learning (FL) offers a decentralized framework for training and
fine-tuning Large Language Models (LLMs) by leveraging computational resources
across organizations while keeping sensitive data on local devices. It
addresses privacy and security concerns while navigating challenges associated
with the substantial computational demands of LLMs, which can be prohibitive
for small and medium-sized organizations. FL supports the development of
task-specific LLMs for cross-silo applications through fine-tuning but remains
vulnerable to inference attacks, such as membership inference and gradient
inversion, which threaten data privacy. Prior studies have utilized
Differential Privacy (DP) in LLM fine-tuning, which, despite being effective at
preserving privacy, can degrade model performance. To overcome these
challenges, we propose a novel method, FedShield-LLM, that uses **pruning** with
Fully Homomorphic Encryption (FHE) for Low-Rank Adaptation (LoRA) parameters,
enabling secure computations on encrypted model updates while mitigating the
attack surface by deactivating less important LoRA parameters. Furthermore,
optimized federated algorithms for cross-silo environments enhance scalability
and efficiency. Parameter-efficient fine-tuning techniques like LoRA
substantially reduce computational and communication overhead, making FL
feasible for resource-constrained clients. Experimental results show that the
proposed method outperforms existing methods while maintaining robust privacy
protection, enabling organizations to collaboratively train secure and
efficient LLMs.
  The code and data are available at,
https://github.com/solidlabnetwork/fedshield-llm

