# 2025-04-14

# Table of Contents
* [Steering CLIP's vision transformer with sparse autoencoders](#Steering-CLIP's-vision-transformer-with-sparse-autoencoders)
* [ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance](#ModernBERT-or-DeBERTaV3?-Examining-Architecture-and-Data-Influence-on-Transformer-Encoder-Models-Performance)
* [Transformer Learns Optimal Variable Selection in Group-Sparse Classification](#Transformer-Learns-Optimal-Variable-Selection-in-Group-Sparse-Classification)
* [Efficient Mixture of Geographical Species for On Device Wildlife Monitoring](#Efficient-Mixture-of-Geographical-Species-for-On-Device-Wildlife-Monitoring)
* [Preserving Privacy Without Compromising Accuracy Machine Unlearning for Handwritten Text Recognition](#Preserving-Privacy-Without-Compromising-Accuracy-Machine-Unlearning-for-Handwritten-Text-Recognition)
* [A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English](#A-Survey-of-Machine-Learning-Models-and-Datasets-for-the-Multi-label-Classification-of-Textual-Hate-Speech-in-English)
* [Hardware, Algorithms, and Applications of the Neuromorphic Vision Sensor a Review](#Hardware,-Algorithms,-and-Applications-of-the-Neuromorphic-Vision-Sensor-a-Review)
* [PlugSelect Pruning Channels with Plug-and-Play Flexibility for Electroencephalography-based Brain Computer Interface](#PlugSelect-Pruning-Channels-with-Plug-and-Play-Flexibility-for-Electroencephalography-based-Brain-Computer-Interface)
* [A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification](#A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Medical-Image-Classification)
* [Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion](#Muon-Accelerated-Attention-Distillation-for-Real-Time-Edge-Synthesis-via-Optimized-Latent-Diffusion)
* [Ego4o Egocentric Human Motion Capture and Understanding from Multi-Modal Input](#Ego4o-Egocentric-Human-Motion-Capture-and-Understanding-from-Multi-Modal-Input)
* [MixDiT Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization](#MixDiT-Accelerating-Image-Diffusion-Transformer-Inference-with-Mixed-Precision-MX-Quantization)
* [Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash](#Scaling-Up-On-Device-LLMs-via-Active-Weight-Swapping-Between-DRAM-and-Flash)
* [Jupiter Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices](#Jupiter-Fast-and-Resource-Efficient-Collaborative-Inference-of-Generative-LLMs-on-Edge-Devices)
* [DrivAer Transformer A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset](#DrivAer-Transformer-A-high-precision-and-fast-prediction-method-for-vehicle-aerodynamic-drag-coefficient-based-on-the-DrivAerNet++-dataset)
* [SAEs $\textit{Can}$ Improve Unlearning Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](#SAEs-$\textit{Can}$-Improve-Unlearning-Dynamic-Sparse-Autoencoder-Guardrails-for-Precision-Unlearning-in-LLMs)
* [Multi-person Physics-based Pose Estimation for Combat Sports](#Multi-person-Physics-based-Pose-Estimation-for-Combat-Sports)
* [On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction](#On-the-Practice-of-Deep-Hierarchical-Ensemble-Network-for-Ad-Conversion-Rate-Prediction)
* [Empowering Vector Architectures for ML The CAMP Architecture for Matrix Multiplication](#Empowering-Vector-Architectures-for-ML-The-CAMP-Architecture-for-Matrix-Multiplication)
* [The AI Scientist-v2 Workshop-Level Automated Scientific Discovery via Agentic Tree Search](#The-AI-Scientist-v2-Workshop-Level-Automated-Scientific-Discovery-via-Agentic-Tree-Search)
* [Vector Quantized-Elites Unsupervised and Problem-Agnostic Quality-Diversity Optimization](#Vector-Quantized-Elites-Unsupervised-and-Problem-Agnostic-Quality-Diversity-Optimization)
* [Robust Hallucination Detection in LLMs via Adaptive Token Selection](#Robust-Hallucination-Detection-in-LLMs-via-Adaptive-Token-Selection)
* [Deceptive Automated Interpretability Language Models Coordinating to Fool Oversight Systems](#Deceptive-Automated-Interpretability-Language-Models-Coordinating-to-Fool-Oversight-Systems)
* [Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models](#Cluster-Driven-Expert-Pruning-for-Mixture-of-Experts-Large-Language-Models)
* [Apt-Serve Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving](#Apt-Serve-Adaptive-Request-Scheduling-on-Hybrid-Cache-for-Scalable-LLM-Inference-Serving)
* [UniCAIM A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference](#UniCAIM-A-Unified-CAM/CIM-Architecture-with-Static-Dynamic-KV-Cache-Pruning-for-Efficient-Long-Context-LLM-Inference)
* [LoRI Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation](#LoRI-Reducing-Cross-Task-Interference-in-Multi-Task-Low-Rank-Adaptation)
* [ThermoStereoRT Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement](#ThermoStereoRT-Thermal-Stereo-Matching-in-Real-Time-via-Knowledge-Distillation-and-Attention-based-Refinement)
* [Task-Circuit Quantization Leveraging Knowledge Localization and Interpretability for Compression](#Task-Circuit-Quantization-Leveraging-Knowledge-Localization-and-Interpretability-for-Compression)
* [Representation Meets Optimization Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology](#Representation-Meets-Optimization-Training-PINNs-and-PIKANs-for-Gray-Box-Discovery-in-Systems-Pharmacology)
* [DLTPose 6DoF Pose Estimation From Accurate Dense Surface Point Estimates](#DLTPose-6DoF-Pose-Estimation-From-Accurate-Dense-Surface-Point-Estimates)
* [Adaptive Vision-Guided Robotic Arm Control for Precision Pruning in Dynamic Orchard Environments](#Adaptive-Vision-Guided-Robotic-Arm-Control-for-Precision-Pruning-in-Dynamic-Orchard-Environments)
* [Few-Shot Adaptation of Grounding DINO for Agricultural Domain](#Few-Shot-Adaptation-of-Grounding-DINO-for-Agricultural-Domain)
* [Adaptive Computation Pruning for the Forgetting Transformer](#Adaptive-Computation-Pruning-for-the-Forgetting-Transformer)
* [LVC A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding](#LVC-A-Lightweight-Compression-Framework-for-Enhancing-VLMs-in-Long-Video-Understanding)
* [DyDiT++ Dynamic Diffusion Transformers for Efficient Visual Generation](#DyDiT++-Dynamic-Diffusion-Transformers-for-Efficient-Visual-Generation)
* [A Survey of New Mid-Band/FR3 for 6G Channel Measurement, Characterization and Modeling in Outdoor Environment](#A-Survey-of-New-Mid-Band/FR3-for-6G-Channel-Measurement,-Characterization-and-Modeling-in-Outdoor-Environment)
* [BBQRec Behavior-Bind Quantization for Multi-Modal Sequential Recommendation](#BBQRec-Behavior-Bind-Quantization-for-Multi-Modal-Sequential-Recommendation)
* [FuseRL Dense Preference Optimization for Heterogeneous Model Fusion](#FuseRL-Dense-Preference-Optimization-for-Heterogeneous-Model-Fusion)
* [A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication](#A-Streamable-Neural-Audio-Codec-with-Residual-Scalar-Vector-Quantization-for-Real-Time-Communication)
* [GIGA Generalizable Sparse Image-driven Gaussian Avatars](#GIGA-Generalizable-Sparse-Image-driven-Gaussian-Avatars)
* [SPIRe Boosting LLM Inference Throughput with Speculative Decoding](#SPIRe-Boosting-LLM-Inference-Throughput-with-Speculative-Decoding)
* [Unifying Autoregressive and Diffusion-Based Sequence Generation](#Unifying-Autoregressive-and-Diffusion-Based-Sequence-Generation)
* [HiFlow Training-free High-Resolution Image Generation with Flow-Aligned Guidance](#HiFlow-Training-free-High-Resolution-Image-Generation-with-Flow-Aligned-Guidance)
* [Optuna vs Code Llama Are LLMs a New Paradigm for Hyperparameter Tuning?](#Optuna-vs-Code-Llama-Are-LLMs-a-New-Paradigm-for-Hyperparameter-Tuning?)
* [A Corrector-aided Look-ahead Distance-based Guidance for Reference Path Following with an Efficient Midcourse Guidance Strategy](#A-Corrector-aided-Look-ahead-Distance-based-Guidance-for-Reference-Path-Following-with-an-Efficient-Midcourse-Guidance-Strategy)
* [Generalized Parameter Lifting Finer Abstractions for Parametric Markov Chains](#Generalized-Parameter-Lifting-Finer-Abstractions-for-Parametric-Markov-Chains)
* [Mosaic Composite Projection Pruning for Resource-efficient LLMs](#Mosaic-Composite-Projection-Pruning-for-Resource-efficient-LLMs)
* [Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching](#Accelerating-LLM-Inference-Throughput-via-Asynchronous-KV-Cache-Prefetching)
* [End-to-End Dialog Neural Coreference Resolution Balancing Efficiency and Accuracy in Large-Scale Systems](#End-to-End-Dialog-Neural-Coreference-Resolution-Balancing-Efficiency-and-Accuracy-in-Large-Scale-Systems)
* [Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation](#Robust-and-Efficient-Average-Consensus-with-Non-Coherent-Over-the-Air-Aggregation)
* [Point-based Instance Completion with Scene Constraints](#Point-based-Instance-Completion-with-Scene-Constraints)
* [TAGC Optimizing Gradient Communication in Distributed Transformer Training](#TAGC-Optimizing-Gradient-Communication-in-Distributed-Transformer-Training)
* [DEL Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding](#DEL-Context-Aware-Dynamic-Exit-Layer-for-Efficient-Self-Speculative-Decoding)
* [SciSciGPT Advancing Human-AI Collaboration in the Science of Science](#SciSciGPT-Advancing-Human-AI-Collaboration-in-the-Science-of-Science)
* [Beam-driven plasma-wakefield acceleration](#Beam-driven-plasma-wakefield-acceleration)
* [Optimizing Large Language Models Metrics, Energy Efficiency, and Case Study Insights](#Optimizing-Large-Language-Models-Metrics,-Energy-Efficiency,-and-Case-Study-Insights)
* [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](#Efficient-Reinforcement-Finetuning-via-Adaptive-Curriculum-Learning)
* [Exponential Quantum Speedup for Simulating Classical Lattice Dynamics](#Exponential-Quantum-Speedup-for-Simulating-Classical-Lattice-Dynamics)
* [EffOWT Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively](#EffOWT-Transfer-Visual-Language-Models-to-Open-World-Tracking-Efficiently-and-Effectively)
* [Algorithm Discovery With LLMs Evolutionary Search Meets Reinforcement Learning](#Algorithm-Discovery-With-LLMs-Evolutionary-Search-Meets-Reinforcement-Learning)
* [State Tuning State-based Test-Time Scaling on RWKV-7](#State-Tuning-State-based-Test-Time-Scaling-on-RWKV-7)
* [MIAT Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction](#MIAT-Maneuver-Intention-Aware-Transformer-for-Spatio-Temporal-Trajectory-Prediction)
* [Secure Communications for All Users in Low-Resolution IRS-aided Systems Under Imperfect and Unknown CSI](#Secure-Communications-for-All-Users-in-Low-Resolution-IRS-aided-Systems-Under-Imperfect-and-Unknown-CSI)
* [One Quantizer is Enough Toward a Lightweight Audio Codec](#One-Quantizer-is-Enough-Toward-a-Lightweight-Audio-Codec)
* [Inter-event Interval Microscopy for Event Cameras](#Inter-event-Interval-Microscopy-for-Event-Cameras)
* [Attentional Graph Meta-Learning for Indoor Localization Using Extremely Sparse Fingerprints](#Attentional-Graph-Meta-Learning-for-Indoor-Localization-Using-Extremely-Sparse-Fingerprints)
* [Dynamic Vision Mamba](#Dynamic-Vision-Mamba)
* [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations?-A-Case-Study-with-AMRs)
* [Achieving binary weight and activation for LLMs using Post-Training Quantization](#Achieving-binary-weight-and-activation-for-LLMs-using-Post-Training-Quantization)
* [Can LLM-Driven Hard Negative Sampling Empower Collaborative Filtering? Findings and Potentials](#Can-LLM-Driven-Hard-Negative-Sampling-Empower-Collaborative-Filtering?-Findings-and-Potentials)
* [TactileNet Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment](#TactileNet-Bridging-the-Accessibility-Gap-with-AI-Generated-Tactile-Graphics-for-Individuals-with-Vision-Impairment)
* [Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](#Are-You-Getting-What-You-Pay-For?-Auditing-Model-Substitution-in-LLM-APIs)
* [LagKV Lag-Relative Information of the KV Cache Tells Which Tokens Are Important](#LagKV-Lag-Relative-Information-of-the-KV-Cache-Tells-Which-Tokens-Are-Important)


## Steering CLIP's vision transformer with sparse autoencoders

>Authors: Sonia Joseph, Praneet Suresh, Ethan Goldfarb, Lorenz Hufe, Yossi Gandelsman, Robert Graham, Danilo Bzdok, Wojciech Samek, Blake Aaron Richards

>2025-04-11

> http://arxiv.org/abs/2504.08729v1

While vision models are highly capable, their internal mechanisms remain
poorly understood -- a challenge which **sparse** autoencoders (SAEs) have helped
address in language, but which remains underexplored in vision. We address this
gap by training SAEs on CLIP's vision transformer and uncover key differences
between vision and language processing, including distinct **sparsity** patterns
for SAEs trained across layers and token types. We then provide the first
systematic analysis on the steerability of CLIP's vision transformer by
introducing metrics to quantify how precisely SAE features can be steered to
affect the model's output. We find that 10-15\% of neurons and features are
steerable, with SAEs providing thousands more steerable features than the base
model. Through targeted suppression of SAE features, we then demonstrate
improved performance on three vision disentanglement tasks (CelebA, Waterbirds,
and typographic attacks), finding optimal disentanglement in middle model
layers, and achieving state-of-the-art performance on defense against
typographic attacks.


## ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance

>Authors: Wissam Antoun, Benoît Sagot, Djamé Seddah

>2025-04-11

> http://arxiv.org/abs/2504.08716v1

Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce
architectural advancements aimed at improving efficiency and performance.
Although the authors of ModernBERT report improved performance over DeBERTaV3
on several benchmarks, the lack of disclosed training data and the absence of
comparisons using a shared dataset make it difficult to determine whether these
gains are due to architectural improvements or differences in training data. In
this work, we conduct a controlled study by pretraining ModernBERT on the same
dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of
model design. Our results show that the previous model generation remains
superior in sample efficiency and overall benchmark performance, with
ModernBERT's primary advantage being faster training and inference speed.
However, the new proposed model still provides meaningful architectural
improvements compared to earlier models such as BERT and RoBERTa. Additionally,
we observe that high-quality pre-training data accelerates convergence but does
not significantly improve final performance, suggesting potential benchmark
saturation. These findings show the importance of disentangling pretraining
data from architectural innovations when evaluating transformer models.


## Transformer Learns Optimal Variable Selection in Group-Sparse Classification

>Authors: Chenyang Zhang, Xuran Meng, Yuan Cao

>2025-04-11

> http://arxiv.org/abs/2504.08638v1

Transformers have demonstrated remarkable success across various
applications. However, the success of transformers have not been understood in
theory. In this work, we give a case study of how transformers can be trained
to learn a classic statistical model with "group **sparsity**", where the input
variables form multiple groups, and the label only depends on the variables
from one of the groups. We theoretically demonstrate that, a one-layer
transformer trained by gradient descent can correctly leverage the attention
mechanism to select variables, disregarding irrelevant ones and focusing on
those beneficial for classification. We also demonstrate that a well-pretrained
one-layer transformer can be adapted to new downstream tasks to achieve good
prediction accuracy with a limited number of samples. Our study sheds light on
how transformers effectively learn structured data.


## Efficient Mixture of Geographical Species for On Device Wildlife Monitoring

>Authors: Emmanuel Azuh Mensah, Joban Mand, Yueheng Ou, Min Jang, Kurtis Heimerl

>2025-04-11

> http://arxiv.org/abs/2504.08620v1

Efficient on-device models have become attractive for near-sensor insight
generation, of particular interest to the ecological conservation community.
For this reason, deep learning researchers are proposing more approaches to
develop lower compute models. However, since vision transformers are very new
to the edge use case, there are still unexplored approaches, most notably
conditional execution of subnetworks based on input data. In this work, we
explore the training of a single species detector which uses conditional
computation to bias structured sub networks in a geographically-aware manner.
We propose a method for **pruning** the expert model per location and demonstrate
conditional computation performance on two geographically distributed datasets:
iNaturalist and iWildcam.


## Preserving Privacy Without Compromising Accuracy Machine Unlearning for Handwritten Text Recognition

>Authors: Lei Kang, Xuanshuo Fu, Lluis Gomez, Alicia Fornés, Ernest Valveny, Dimosthenis Karatzas

>2025-04-11

> http://arxiv.org/abs/2504.08616v1

Handwritten Text Recognition (HTR) is essential for document analysis and
digitization. However, handwritten data often contains user-identifiable
information, such as unique handwriting styles and personal lexicon choices,
which can compromise privacy and erode trust in AI services. Legislation like
the ``right to be forgotten'' underscores the necessity for methods that can
expunge sensitive information from trained models. Machine unlearning addresses
this by selectively removing specific data from models without necessitating
complete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,
where safeguarding privacy leads to diminished model performance. In this
paper, we introduce a novel two-stage unlearning strategy for a multi-head
transformer-based HTR model, integrating **pruning** and random labeling. Our
proposed method utilizes a writer classification head both as an indicator and
a trigger for unlearning, while maintaining the efficacy of the recognition
head. To our knowledge, this represents the first comprehensive exploration of
machine unlearning within HTR tasks. We further employ Membership Inference
Attacks (MIA) to evaluate the effectiveness of unlearning user-identifiable
information. Extensive experiments demonstrate that our approach effectively
preserves privacy while maintaining model accuracy, paving the way for new
research directions in the document analysis community. Our code will be
publicly available upon acceptance.


## A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English

>Authors: Julian Bäumler, Louis Blöcher, Lars-Joel Frey, Xian Chen, Markus Bayer, Christian Reuter

>2025-04-11

> http://arxiv.org/abs/2504.08609v1

The dissemination of online hate speech can have serious negative
consequences for individuals, online communities, and entire societies. This
and the large volume of hateful online content prompted both practitioners',
i.e., in content moderation or law enforcement, and researchers' interest in
machine learning models to automatically classify instances of hate speech.
Whereas most scientific works address hate speech classification as a binary
task, practice often requires a differentiation into sub-types, e.g., according
to target, severity, or legality, which may overlap for individual content.
Hence, researchers created datasets and machine learning models that approach
hate speech classification in textual data as a multi-label problem. This work
presents the first systematic and comprehensive survey of scientific literature
on this emerging research landscape in English (N=46). We contribute with a
concise overview of 28 datasets suited for training multi-label classification
models that reveals significant heterogeneity regarding label-set, size,
meta-concept, annotation process, and inter-annotator agreement. Our analysis
of 24 publications proposing suitable classification models further establishes
inconsistency in evaluation and a preference for architectures based on
Bidirectional Encoder Representation from Transformers (BERT) and Recurrent
Neural Networks (RNNs). We identify imbalanced training data, reliance on
crowdsourcing platforms, small and **sparse** datasets, and missing methodological
alignment as critical open issues and formulate ten recommendations for
research.


## Hardware, Algorithms, and Applications of the Neuromorphic Vision Sensor a Review

>Authors: Claudio Cimarelli, Jose Andres Millan-Romera, Holger Voos, Jose Luis Sanchez-Lopez

>2025-04-11

> http://arxiv.org/abs/2504.08588v1

Neuromorphic, or event, cameras represent a transformation in the classical
approach to visual sensing encodes detected instantaneous per-pixel
illumination changes into an asynchronous stream of event packets. Their
novelty compared to standard cameras lies in the transition from capturing full
picture frames at fixed time intervals to a **sparse** data format which, with its
distinctive qualities, offers potential improvements in various applications.
However, these advantages come at the cost of reinventing algorithmic
procedures or adapting them to effectively process the new data format.
  In this survey, we systematically examine neuromorphic vision along three
main dimensions. First, we highlight the technological evolution and
distinctive hardware features of neuromorphic cameras from their inception to
recent models. Second, we review image processing algorithms developed
explicitly for event-based data, covering key works on feature detection,
tracking, and optical flow -which form the basis for analyzing image elements
and transformations -as well as depth and pose estimation or object
recognition, which interpret more complex scene structures and components.
These techniques, drawn from classical computer vision and modern data-driven
approaches, are examined to illustrate the breadth of applications for
event-based cameras. Third, we present practical application case studies
demonstrating how event cameras have been successfully used across various
industries and scenarios. Finally, we analyze the challenges limiting
widespread adoption, identify significant research gaps compared to standard
imaging techniques, and outline promising future directions and opportunities
that neuromorphic vision offers.


## PlugSelect Pruning Channels with Plug-and-Play Flexibility for Electroencephalography-based Brain Computer Interface

>Authors: Xue Yuan, Keren Shi, Ning Jiang, Jiayuan He

>2025-04-11

> http://arxiv.org/abs/2504.08486v1

Automatic minimization and optimization of the number of the electrodes is
essential for the practical application of electroencephalography (EEG)-based
brain computer interface (BCI). Previous methods typically require additional
training costs or rely on prior knowledge assumptions. This study proposed a
novel channel **pruning** model, plug-and-select (PlugSelect), applicable across a
broad range of BCI paradigms with no additional training cost and plug-and-play
functionality. It integrates gradients along the input path to globally infer
the causal relationships between input channels and outputs, and ranks the
contribution sequences to identify the most highly attributed channels. The
results showed that for three BCI paradigms, i.e., auditory attention decoding
(AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce
the number of channels by at least half while effectively maintaining decoding
performance and improving efficiency. The outcome benefits the design of
wearable EEG-based devices, facilitating the practical application of BCI
technology.


## A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification

>Authors: Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens

>2025-04-11

> http://arxiv.org/abs/2504.08481v1

In many medical imaging tasks, convolutional neural networks (CNNs)
efficiently extract local features hierarchically. More recently, vision
transformers (ViTs) have gained popularity, using self-attention mechanisms to
capture global dependencies, but lacking the inherent spatial localization of
convolutions. Therefore, hybrid models combining CNNs and ViTs have been
developed to combine the strengths of both architectures. However, such hybrid
CNN-ViT models are difficult to interpret, which hinders their application in
medical imaging. In this work, we introduce an interpretable-by-design hybrid
fully convolutional CNN-Transformer architecture for medical image
classification. Unlike widely used post-hoc saliency methods for ViTs, our
approach generates faithful and localized evidence maps that directly reflect
the model's decision process. We evaluated our method on two medical image
classification tasks using color fundus images. Our model not only achieves
state-of-the-art predictive performance compared to both black-box and
interpretable models but also provides class-specific **sparse** evidence maps in a
single forward pass. The code is available at:
https://anonymous.4open.science/r/Expl-CNN-Transformer/.


## Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion

>Authors: Weiye Chen, Qingen Zhu, Qian Long

>2025-04-11

> http://arxiv.org/abs/2504.08451v1

Recent advances in visual synthesis have leveraged diffusion models and
attention mechanisms to achieve high-fidelity artistic style transfer and
photorealistic text-to-image generation. However, real-time deployment on edge
devices remains challenging due to computational and memory constraints. We
propose Muon-AD, a co-designed framework that integrates the Muon optimizer
with attention distillation for real-time edge synthesis. By eliminating
gradient conflicts through orthogonal parameter updates and dynamic **pruning**,
Muon-AD achieves 3.2 times faster convergence compared to Stable
Diffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4%
higher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and
enables 24FPS real-time generation through mixed-precision **quantization** and
curriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture
demonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we
show a 65% reduction in communication overhead during distributed training and
real-time 10s/image generation on edge GPUs. These advancements pave the way
for democratizing high-quality visual synthesis in resource-constrained
environments.


## Ego4o Egocentric Human Motion Capture and Understanding from Multi-Modal Input

>Authors: Jian Wang, Rishabh Dabral, Diogo Luvizon, Zhe Cao, Lingjie Liu, Thabo Beeler, Christian Theobalt

>2025-04-11

> http://arxiv.org/abs/2504.08449v1

This work focuses on tracking and understanding human motion using consumer
wearable devices, such as VR/AR headsets, smart glasses, cellphones, and
smartwatches. These devices provide diverse, multi-modal sensor inputs,
including egocentric images, and 1-3 **sparse** IMU sensors in varied combinations.
Motion descriptions can also accompany these signals. The diverse input
modalities and their intermittent availability pose challenges for consistent
motion capture and understanding. In this work, we present Ego4o (o for omni),
a new framework for simultaneous human motion capture and understanding from
multi-modal egocentric inputs. This method maintains performance with partial
inputs while achieving better results when multiple modalities are combined.
First, the IMU sensor inputs, the optional egocentric image, and text
description of human motion are encoded into the latent space of a motion
VQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized
to track human motion. When motion descriptions are unavailable, the latent
vectors can be input into a multi-modal LLM to generate human motion
descriptions, which can further enhance motion capture accuracy. Quantitative
and qualitative evaluations demonstrate the effectiveness of our method in
predicting accurate human motion and high-quality motion descriptions.


## MixDiT Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization

>Authors: Daeun Kim, Jinwoo Hwang, Changhun Oh, Jongse Park

>2025-04-11

> http://arxiv.org/abs/2504.08398v1

Diffusion Transformer (DiT) has driven significant progress in image
generation tasks. However, DiT inferencing is notoriously compute-intensive and
incurs long latency even on datacenter-scale GPUs, primarily due to its
iterative nature and heavy reliance on GEMM operations inherent to its
encoder-based structure. To address the challenge, prior work has explored
**quantization**, but achieving low-precision **quantization** for DiT inferencing with
both high accuracy and substantial speedup remains an open problem. To this
end, this paper proposes MixDiT, an algorithm-hardware co-designed **acceleration**
solution that exploits mixed Microscaling (MX) formats to **quantize** DiT
activation values. MixDiT **quantize**s the DiT activation tensors by selectively
applying higher precision to magnitude-based outliers, which produce
mixed-precision GEMM operations. To achieve tangible speedup from the
mixed-precision arithmetic, we design a MixDiT accelerator that enables
precision-flexible multiplications and efficient MX precision conversions. Our
experimental results show that MixDiT delivers a speedup of 2.10-5.32 times
over RTX 3090, with no loss in FID.


## Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash

>Authors: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao

>2025-04-11

> http://arxiv.org/abs/2504.08378v1

Large language models (LLMs) are increasingly being deployed on mobile
devices, but the limited DRAM capacity constrains the deployable model size.
This paper introduces ActiveFlow, the first LLM inference framework that can
achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the
scaling up of deployable model sizes. The framework is based on the novel
concept of active weight DRAM-flash swapping and incorporates three novel
techniques: (1) Cross-layer active weights preloading. It uses the activations
from the current layer to predict the active weights of several subsequent
layers, enabling computation and data loading to overlap, as well as
facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It
adjusts the active weights to align with the dense-model output distribution,
compensating for approximations introduced by contextual **sparsity**. (3) Active
weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation
among the hot weight cache, preloaded active weights, and computation-involved
weights based on available memory. Results show ActiveFlow achieves the
performance-cost Pareto frontier compared to existing efficiency optimization
methods.


## Jupiter Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices

>Authors: Shengyuan Ye, Bei Ouyang, Liekang Zeng, Tianyi Qian, Xiaowen Chu, Jian Tang, Xu Chen

>2025-04-11

> http://arxiv.org/abs/2504.08242v1

Generative large language models (LLMs) have garnered significant attention
due to their exceptional capabilities in various AI tasks. Traditionally
deployed in cloud datacenters, LLMs are now increasingly moving towards more
accessible edge platforms to protect sensitive user data and ensure privacy
preservation. The limited computational resources of individual edge devices,
however, can result in excessively prolonged inference latency and overwhelmed
memory usage. While existing research has explored collaborative edge computing
to break the resource wall of individual devices, these solutions yet suffer
from massive communication overhead and under-utilization of edge resources.
Furthermore, they focus exclusively on optimizing the prefill phase, neglecting
the crucial autoregressive decoding phase for generative LLMs. To address that,
we propose Jupiter, a fast, scalable, and resource-efficient collaborative edge
AI system for generative LLM inference. Jupiter introduces a flexible pipelined
architecture as a principle and differentiates its system design according to
the differentiated characteristics of the prefill and decoding phases. For
prefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and
develops a meticulous parallelism planning strategy to maximize resource
efficiency; For decoding, Jupiter devises an effective outline-based pipeline
parallel decoding mechanism combined with speculative decoding, which further
magnifies inference **acceleration**. Extensive evaluation based on realistic
implementation demonstrates that Jupiter remarkably outperforms
state-of-the-art approaches under various edge environment setups, achieving up
to 26.1x end-to-end latency reduction while rendering on-par generation
quality.


## DrivAer Transformer A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset

>Authors: Jiaqi He, Xiangwen Luo, Yiping Wang

>2025-04-11

> http://arxiv.org/abs/2504.08217v1

At the current stage, deep learning-based methods have demonstrated excellent
capabilities in evaluating aerodynamic performance, significantly reducing the
time and cost required for traditional computational fluid dynamics (CFD)
simulations. However, when faced with the task of processing extremely complex
three-dimensional (3D) vehicle models, the lack of large-scale datasets and
training resources, coupled with the inherent diversity and complexity of the
geometry of different vehicle models, means that the prediction accuracy and
versatility of these networks are still not up to the level required for
current production. In view of the remarkable success of Transformer models in
the field of natural language processing and their strong potential in the
field of image processing, this study innovatively proposes a point cloud
learning framework called DrivAer Transformer (DAT). The DAT structure uses the
DrivAerNet++ dataset, which contains high-fidelity CFD data of
industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag
directly from 3D meshes, thus avoiding the limitations of traditional methods
such as 2D image rendering or signed distance fields (SDF). DAT enables fast
and accurate drag prediction, driving the evolution of the aerodynamic
evaluation process and laying the critical foundation for introducing a
data-driven approach to automotive design. The framework is expected to
accelerate the vehicle design process and improve development efficiency.


## SAEs $\textit{Can}$ Improve Unlearning Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs

>Authors: Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith

>2025-04-11

> http://arxiv.org/abs/2504.08192v1

Machine unlearning is a promising approach to improve LLM safety by removing
unwanted knowledge from the model. However, prevailing gradient-based
unlearning methods suffer from issues such as high computational costs,
hyperparameter instability, poor sequential unlearning capability,
vulnerability to relearning attacks, low data efficiency, and lack of
interpretability. While Sparse Autoencoders are well-suited to improve these
aspects by enabling targeted activation-based unlearning, prior approaches
underperform gradient-based methods. This work demonstrates that, contrary to
these earlier findings, SAEs can significantly improve unlearning when employed
dynamically. We introduce $\textbf{Dynamic DAE Guardrails}$ (DSG), a novel
method for precision unlearning that leverages principled feature selection and
a dynamic classifier. Our experiments show DSG substantially outperforms
leading unlearning methods, achieving superior forget-utility trade-offs. DSG
addresses key drawbacks of gradient-based approaches for unlearning -- offering
enhanced computational efficiency and stability, robust performance in
sequential unlearning, stronger resistance to relearning attacks, better data
efficiency including zero-shot settings, and more interpretable unlearning.


## Multi-person Physics-based Pose Estimation for Combat Sports

>Authors: Hossein Feiz, David Labbé, Thomas Romeas, Jocelyn Faubert, Sheldon Andrews

>2025-04-11

> http://arxiv.org/abs/2504.08175v1

We propose a novel framework for accurate 3D human pose estimation in combat
sports using **sparse** multi-camera setups. Our method integrates robust
multi-view 2D pose tracking via a transformer-based top-down approach,
employing epipolar geometry constraints and long-term video object segmentation
for consistent identity tracking across views. Initial 3D poses are obtained
through weighted triangulation and spline smoothing, followed by kinematic
optimization to refine pose accuracy. We further enhance pose realism and
robustness by introducing a multi-person physics-based trajectory optimization
step, effectively addressing challenges such as rapid motions, occlusions, and
close interactions. Experimental results on diverse datasets, including a new
benchmark of elite boxing footage, demonstrate state-of-the-art performance.
Additionally, we release comprehensive annotated video datasets to advance
future research in multi-person pose estimation for combat sports.


## On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction

>Authors: Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal

>2025-04-10

> http://arxiv.org/abs/2504.08169v1

The predictions of click through rate (CTR) and conversion rate (CVR) play a
crucial role in the success of ad-recommendation systems. A Deep Hierarchical
Ensemble Network (DHEN) has been proposed to integrate multiple feature
crossing modules and has achieved great success in CTR prediction. However, its
performance for CVR prediction is unclear in the conversion ads setting, where
an ad bids for the probability of a user's off-site actions on a third party
website or app, including purchase, add to cart, sign up, etc. A few challenges
in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a
few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve
the best trade-off between efficiency and efficacy? 3) What hyper-parameters to
choose in each feature-crossing module? Orthogonal to the model architecture,
the input personalization features also significantly impact model performance
with a high degree of freedom. In this paper, we attack this problem and
present our contributions biased to the applied data science side, including:
  First, we propose a multitask learning framework with DHEN as the single
backbone model architecture to predict all CVR tasks, with a detailed study on
how to make DHEN work effectively in practice; Second, we build both on-site
real-time user behavior sequences and off-site conversion event sequences for
CVR prediction purposes, and conduct ablation study on its importance; Last but
not least, we propose a self-supervised auxiliary loss to predict future
actions in the input sequence, to help resolve the label **sparse**ness issue in
CVR prediction.
  Our method achieves state-of-the-art performance compared to previous single
feature crossing modules with pre-trained user personalization features.


## Empowering Vector Architectures for ML The CAMP Architecture for Matrix Multiplication

>Authors: Mohammadreza Esmali Nojehdeh, Hossein Mokhtarnia, Julian Pavon Rivera, Narcis Rodas Quiroga, Roger Figueras Bagué, Enrico Reggiani, Miquel Moreto, Osman Unsal, Adrian Cristal, Eduard Ayguade

>2025-04-10

> http://arxiv.org/abs/2504.08137v1

This study presents the Cartesian Accumulative Matrix Pipeline (CAMP)
architecture, a novel approach designed to enhance matrix multiplication in
Vector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units.
CAMP improves the processing efficiency of Quantized Neural Networks (QNNs).
Matrix multiplication is a cornerstone of machine learning applications, and
its **quantize**d versions are increasingly popular for more efficient operations.
Unfortunately, existing VAs and SIMD-support units struggle to efficiently
handle these **quantize**d formats. In this work, we propose CAMP, a simple yet
effective architecture that leverages a hybrid multiplier. The CAMP
architecture significantly advances the performance of vector architectures in
handling **quantize**d data, enabling more efficient execution of matrix
multiplication across various platforms, specifically targeting the ARMv8
Scalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In
addition to increasing throughput, CAMP's architectural design also contributes
to energy efficiency, making it an effective solution for low-power
applications. Evaluations on a range of Large Language Models (LLMs) and
Convolutional Neural Networks (CNNs) demonstrate that matrix multiplication
operations using the proposed micro-architecture achieve up to 17$\times$ and
23$\times$ performance improvements compared to their respective baselines, the
ARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore,
synthesis and place-and-route (PnR) of the CAMP micro-architecture using
Synopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for
the RISC-V SoC -- add only 1\% and 4\% area overhead, respectively, compared to
the baseline designs.


## The AI Scientist-v2 Workshop-Level Automated Scientific Discovery via Agentic Tree Search

>Authors: Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha

>2025-04-10

> http://arxiv.org/abs/2504.08066v1

AI is increasingly playing a pivotal role in transforming how scientific
discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic
system capable of producing the first entirely AI generated
peer-review-accepted workshop paper. This system iteratively formulates
scientific hypotheses, designs and executes experiments, analyzes and
visualizes data, and autonomously authors scientific manuscripts. Compared to
its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2
eliminates the reliance on human-authored code templates, generalizes
effectively across diverse machine learning domains, and leverages a novel
progressive agentic tree-search methodology managed by a dedicated experiment
manager agent. Additionally, we enhance the AI reviewer component by
integrating a Vision-Language Model (VLM) feedback loop for iterative
refinement of content and aesthetics of the figures. We evaluated The AI
Scientist-v2 by submitting three fully autonomous manuscripts to a
peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough
scores to exceed the average human acceptance threshold, marking the first
instance of a fully AI-generated paper successfully navigating a peer review.
This accomplishment highlights the growing capability of AI in conducting all
aspects of scientific research. We anticipate that further advancements in
autonomous scientific discovery technologies will profoundly impact human
knowledge generation, enabling unprecedented scalability in research
productivity and significantly accelerating scientific breakthroughs, greatly
benefiting society at large. We have open-sourced the code at
https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of
this transformative technology. We also discuss the role of AI in science,
including AI safety.


## Vector Quantized-Elites Unsupervised and Problem-Agnostic Quality-Diversity Optimization

>Authors: Constantinos Tsakonas, Konstantinos Chatzilygeroudis

>2025-04-10

> http://arxiv.org/abs/2504.08057v1

Quality-Diversity algorithms have transformed optimization by prioritizing
the discovery of diverse, high-performing solutions over a single optimal
result. However, traditional Quality-Diversity methods, such as MAP-Elites,
rely heavily on predefined behavioral descriptors and complete prior knowledge
of the task to define the behavioral space grid, limiting their flexibility and
applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites),
a novel Quality-Diversity algorithm that autonomously constructs a structured
behavioral space grid using unsupervised learning, eliminating the need for
prior task-specific knowledge. At the core of VQ-Elites is the integration of
Vector Quantized Variational Autoencoders, which enables the dynamic learning
of behavioral descriptors and the generation of a structured, rather than
unstructured, behavioral space grid - a significant advancement over existing
unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as
a flexible, robust, and task-agnostic optimization framework. To further
enhance the performance of unsupervised Quality-Diversity algorithms, we
introduce two key components: behavioral space bounding and cooperation
mechanisms, which significantly improve convergence and performance. We
validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering
tasks. The results demonstrate its ability to efficiently generate diverse,
high-quality solutions, emphasizing its adaptability, scalability, robustness
to hyperparameters, and potential to extend Quality-Diversity optimization to
complex, previously inaccessible domains.


## Robust Hallucination Detection in LLMs via Adaptive Token Selection

>Authors: Mengjia Niu, Hamed Haddadi, Guansong Pang

>2025-04-10

> http://arxiv.org/abs/2504.07863v1

Hallucinations in large language models (LLMs) pose significant safety
concerns that impede their broader deployment. Recent research in hallucination
detection has demonstrated that LLMs' internal representations contain
truthfulness hints, which can be harnessed for detector training. However, the
performance of these detectors is heavily dependent on the internal
representations of predetermined tokens, fluctuating considerably when working
on free-form generations with varying lengths and **sparse** distributions of
hallucinated entities. To address this, we propose HaMI, a novel approach that
enables robust detection of hallucinations through adaptive selection and
learning of critical tokens that are most indicative of hallucinations. We
achieve this robustness by an innovative formulation of the Hallucination
detection task as Multiple Instance (HaMI) learning over token-level
representations within a sequence, thereby facilitating a joint optimisation of
token selection and hallucination detection on generation sequences of diverse
forms. Comprehensive experimental results on four hallucination benchmarks show
that HaMI significantly outperforms existing state-of-the-art approaches.


## Deceptive Automated Interpretability Language Models Coordinating to Fool Oversight Systems

>Authors: Simon Lermen, Mateusz Dziemian, Natalia Pérez-Campanero Antolín

>2025-04-10

> http://arxiv.org/abs/2504.07831v1

We demonstrate how AI agents can coordinate to deceive oversight systems
using automated interpretability of neural networks. Using **sparse** autoencoders
(SAEs) as our experimental framework, we show that language models (Llama,
DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that
evade detection. Our agents employ steganographic methods to hide information
in seemingly innocent explanations, successfully fooling oversight models while
achieving explanation quality comparable to reference labels. We further find
that models can scheme to develop deceptive strategies when they believe the
detection of harmful features might lead to negative consequences for
themselves. All tested LLM agents were capable of deceiving the overseer while
achieving high interpretability scores comparable to those of reference labels.
We conclude by proposing mitigation strategies, emphasizing the critical need
for robust understanding and defenses against deception.


## Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models

>Authors: Hongcheng Guo, Juntao Yao, Boyang Wang, Junjia Du, Shaosheng Cao, Donglin Di, Shun Zhang, Zhoujun Li

>2025-04-10

> http://arxiv.org/abs/2504.07807v1

Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm
for scaling large language models (LLMs) with **sparse** activation of
task-specific experts. Despite their computational efficiency during inference,
the massive overall parameter footprint of MoE models (e.g., GPT-4) introduces
critical challenges for practical deployment. Current **pruning** approaches often
fail to address two inherent characteristics of MoE systems: 1).intra-layer
expert homogeneity where experts within the same MoE layer exhibit functional
redundancy, and 2). inter-layer similarity patterns where deeper layers tend to
contain progressively more homogeneous experts. To tackle these issues, we
propose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework
for adaptive task-specific compression of MoE LLMs. C-Prune operates through
layer-wise expert clustering, which groups functionally similar experts within
each MoE layer using parameter similarity metrics, followed by global cluster
**pruning**, which eliminates redundant clusters across all layers through a
unified importance scoring mechanism that accounts for cross-layer homogeneity.
We validate C-Prune through extensive experiments on multiple MoE models and
benchmarks. The results demonstrate that C-Prune effectively reduces model size
while outperforming existing MoE **pruning** methods.


## Apt-Serve Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving

>Authors: Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen

>2025-04-10

> http://arxiv.org/abs/2504.07494v1

Large language model (LLM) inference serving systems are essential to various
LLM-based applications. As demand for LLM services continues to grow, scaling
these systems to handle high request rates while meeting latency Service-Level
Objectives (SLOs), referred to as effective throughput, becomes critical.
However, existing systems often struggle to improve effective throughput,
primarily due to a significant decline in Time To First Token (TTFT) SLO
attainment. We identify two major causes of this bottleneck: (1)
memory-intensive **KV** cache that limits batch size expansion under GPU memory
constraints, and (2) rigid batch composition enforced by the default
First-Come-First-Serve scheduling policy. In this paper, we introduce
Apt-Serve, a scalable framework designed to enhance effective throughput in LLM
inference serving. Apt-Serve features a new hybrid cache scheme that combines
**KV** cache with a memory-efficient hidden cache for reusable input hidden state
vectors, allowing large batch sizes and improving request concurrency. Based on
the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism
that dynamically optimizes batch composition. We formally define the adaptive
scheduling optimization problem and propose an efficient algorithm with
theoretical guarantees. Extensive evaluations on three real-world datasets and
LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up
to 8.8x improvement in effective throughput compared to the state-of-the-art
inference serving systems.


## UniCAIM A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference

>Authors: Weikai Xu, Wenxuan Zeng, Qianqian Huang, Meng Li, Ru Huang

>2025-04-10

> http://arxiv.org/abs/2504.07479v1

Transformer-based large language models (LLMs) have achieved impressive
performance in various natural language processing (NLP) applications. However,
the high memory and computation cost induced by the **KV** cache limits the
inference efficiency, especially for long input sequences. Compute-in-memory
(CIM)-based accelerators have been proposed for LLM **acceleration** with **KV** cache
**pruning**. However, as existing accelerators only support static **pruning** with a
fixed pattern or dynamic **pruning** with primitive implementations, they suffer
from either high accuracy degradation or low efficiency. In this paper, we
propose a ferroelectric FET (FeFET)-based unified content addressable memory
(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous
support for static and dynamic **pruning** with 3 computation modes: 1) in the CAM
mode, UniCAIM enables approximate similarity measurement in O(1) time for
dynamic **KV** cache **pruning** with high energy efficiency; 2) in the charge-domain
CIM mode, static **pruning** can be supported based on accumulative similarity
score, which is much more flexible compared to fixed patterns; 3) in the
current-domain mode, exact attention computation can be conducted with a subset
of selected **KV** cache. We further propose a novel CAM/CIM cell design that
leverages the multi-level characteristics of FeFETs for signed multibit storage
of the **KV** cache and in-place attention computation. With extensive experimental
results, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)
by 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit
level, along with high accuracy comparable with dense attention at the
application level, showing its great potential for efficient long-context LLM
inference.


## LoRI Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation

>Authors: Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein

>2025-04-10

> http://arxiv.org/abs/2504.07448v1

Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient
fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs
notable overhead and suffers from parameter interference in multi-task
scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet
effective approach that freezes the projection matrices $A$ as random
projections and sparsifies the matrices $B$ using task-specific masks. This
design substantially reduces the number of trainable parameters while
maintaining strong task performance. Moreover, LoRI minimizes cross-task
interference in adapter merging by leveraging the orthogonality between adapter
subspaces, and supports continual learning by using **sparsity** to mitigate
catastrophic forgetting. Extensive experiments across natural language
understanding, mathematical reasoning, code generation, and safety alignment
tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT
methods, while using up to 95% fewer trainable parameters than LoRA. In
multi-task experiments, LoRI enables effective adapter merging and continual
learning with reduced cross-task interference. Code is available at:
https://github.com/juzhengz/LoRI


## ThermoStereoRT Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement

>Authors: Anning Hu, Ang Li, Xirui Jin, Danping Zou

>2025-04-10

> http://arxiv.org/abs/2504.07418v1

We introduce ThermoStereoRT, a real-time thermal stereo matching method
designed for all-weather conditions that recovers disparity from two rectified
thermal stereo images, envisioning applications such as night-time drone
surveillance or under-bed cleaning robots. Leveraging a lightweight yet
powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal
images and employs multi-scale attention mechanisms to produce an initial
disparity map. To refine this map, we design a novel channel and spatial
attention module. Addressing the challenge of **sparse** ground truth data in
thermal imagery, we utilize knowledge distillation to boost performance without
increasing computational demands. Comprehensive evaluations on multiple
datasets demonstrate that ThermoStereoRT delivers both real-time capacity and
robust accuracy, making it a promising solution for real-world deployment in
various challenging environments. Our code will be released on
https://github.com/SJTU-ViSYS-team/ThermoStereoRT


## Task-Circuit Quantization Leveraging Knowledge Localization and Interpretability for Compression

>Authors: Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, Mohit Bansal

>2025-04-10

> http://arxiv.org/abs/2504.07389v1

Post-training **quantization** (PTQ) reduces a model's memory footprint by
mapping full precision weights into low bit weights without costly retraining,
but can degrade its downstream performance especially in low 2- to 3-bit
settings. We develop a new mixed-precision PTQ approach, Task-Circuit
Quantization (TaCQ), that draws parallels to automated circuit discovery,
directly conditioning the **quantization** process on specific weight circuits --
which we define as sets of weights associated with downstream task performance.
These weights are kept as 16-bit weights, while others are **quantize**d,
maintaining performance while only adding a marginal memory cost. Specifically,
TaCQ contrasts un**quantize**d model weights with a uniformly-**quantize**d model to
estimate the expected change in weights due to **quantization** and uses gradient
information to predict the resulting impact on task performance, allowing us to
preserve task-specific weights. We compare TaCQ-based **quantization** to existing
mixed-precision **quantization** methods when conditioning both on general-purpose
and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for
both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the
same calibration data and a lower weight budget, achieving major improvements
in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of
Llama-3-8B-Instruct's un**quantize**d 16-bit MMLU performance, obtaining a 5.25%
absolute improvement over SPQR. We also observe consistently large gains over
existing methods in the 2-bit regime, with an average gain of 14.74% over the
strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without
conditioning on specific tasks, showing TaCQ's ability to identify important
weights is not limited to task-conditioned settings.


## Representation Meets Optimization Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology

>Authors: Nazanin Ahmadi Daryakenari, Khemraj Shukla, George Em Karniadakis

>2025-04-10

> http://arxiv.org/abs/2504.07379v1

Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as
an effective counterpart to the original multilayer perceptron-based
Physics-Informed Neural Networks (PINNs). Both representation models can
address inverse problems and facilitate gray-box system identification.
However, a comprehensive understanding of their performance in terms of
accuracy and speed remains underexplored. In particular, we introduce a
modified PIKAN architecture, tanh-cPIKAN, which is based on Chebyshev
polynomials for parametrization of the univariate functions with an extra
nonlinearity for enhanced performance. We then present a systematic
investigation of how choices of the optimizer, representation, and training
configuration influence the performance of PINNs and PIKANs in the context of
systems pharmacology modeling. We benchmark a wide range of first-order,
second-order, and hybrid optimizers, including various learning rate
schedulers. We use the new Optax library to identify the most effective
combinations for learning gray-boxes under ill-posed, non-unique, and
data-**sparse** conditions. We examine the influence of model architecture (MLP vs.
KAN), numerical precision (single vs. double), the need for warm-up phases for
second-order methods, and sensitivity to the initial learning rate. We also
assess the optimizer scalability for larger models and analyze the trade-offs
introduced by JAX in terms of computational efficiency and numerical accuracy.
Using two representative systems pharmacology case studies - a pharmacokinetics
model and a chemotherapy drug-response model - we offer practical guidance on
selecting optimizers and representation models/architectures for robust and
efficient gray-box discovery. Our findings provide actionable insights for
improving the training of physics-informed networks in biomedical applications
and beyond.


## DLTPose 6DoF Pose Estimation From Accurate Dense Surface Point Estimates

>Authors: Akash Jadhav, Michael Greenspan

>2025-04-09

> http://arxiv.org/abs/2504.07335v1

We propose DLTPose, a novel method for 6DoF object pose estimation from RGB-D
images that combines the accuracy of **sparse** keypoint methods with the
robustness of dense pixel-wise predictions. DLTPose predicts per-pixel radial
distances to a set of minimally four keypoints, which are then fed into our
novel Direct Linear Transform (DLT) formulation to produce accurate 3D object
frame surface estimates, leading to better 6DoF pose estimation. Additionally,
we introduce a novel symmetry-aware keypoint ordering approach, designed to
handle object symmetries that otherwise cause inconsistencies in keypoint
assignments. Previous keypoint-based methods relied on fixed keypoint
orderings, which failed to account for the multiple valid configurations
exhibited by symmetric objects, which our ordering approach exploits to enhance
the model's ability to learn stable keypoint representations. Extensive
experiments on the benchmark LINEMOD, Occlusion LINEMOD and YCB-Video datasets
show that DLTPose outperforms existing methods, especially for symmetric and
occluded objects, demonstrating superior Mean Average Recall values of 86.5%
(LM), 79.7% (LM-O) and 89.5% (YCB-V). The code is available at
https://anonymous.4open.science/r/DLTPose_/ .


## Adaptive Vision-Guided Robotic Arm Control for Precision Pruning in Dynamic Orchard Environments

>Authors: Dawood Ahmed, Basit Muhammad Imran, Martin Churuvija, Manoj Karkee

>2025-04-09

> http://arxiv.org/abs/2504.07309v1

This study presents a vision-guided robotic control system for automated
fruit tree **pruning** applications. Traditional agricultural practices rely on
labor-intensive tasks and processes that lack scalability and efficiency,
creating a pressing need for automation research to address growing demands for
higher crop yields, scalable operations, and reduced manual labor. To this end,
this paper proposes a novel algorithm for robust and automated fruit **pruning** in
dense orchards. The proposed algorithm utilizes CoTracker, that is designed to
track 2D feature points in video sequences with significant robustness and
accuracy, while leveraging joint attention mechanisms to account for
inter-point dependencies, enabling robust and precise tracking under
challenging and sophisticated conditions. To validate the efficacy of
CoTracker, a Universal Robots manipulator UR5e is employed in a Gazebo
simulation environment mounted on ClearPath Robotics Warthog robot featuring an
Intel RealSense D435 camera. The system achieved a 93% success rate in **pruning**
trials and with an average end trajectory error of 0.23 mm. The vision
controller demonstrated robust performance in handling occlusions and
maintaining stable trajectories as the arm move towards the target point. The
results validate the effectiveness of integrating vision-based tracking with
kinematic control for precision agricultural tasks. Future work will focus on
real-world implementation and the integration of 3D reconstruction techniques
for enhanced adaptability in dynamic environments.


## Few-Shot Adaptation of Grounding DINO for Agricultural Domain

>Authors: Rajhans Singh, Rafael Bidese Puhl, Kshitiz Dhakal, Sudhir Sornapudi

>2025-04-09

> http://arxiv.org/abs/2504.07252v1

Deep learning models are transforming agricultural applications by enabling
automated phenotyping, monitoring, and yield estimation. However, their
effectiveness heavily depends on large amounts of annotated training data,
which can be labor and time intensive. Recent advances in open-set object
detection, particularly with models like Grounding-DINO, offer a potential
solution to detect regions of interests based on text prompt input. Initial
zero-shot experiments revealed challenges in crafting effective text prompts,
especially for complex objects like individual leaves and visually similar
classes. To address these limitations, we propose an efficient few-shot
adaptation method that simplifies the Grounding-DINO architecture by removing
the text encoder module (BERT) and introducing a randomly initialized trainable
text embedding. This method achieves superior performance across multiple
agricultural datasets, including plant-weed detection, plant counting, insect
identification, fruit counting, and remote sensing tasks. Specifically, it
demonstrates up to a $\sim24\%$ higher mAP than fully fine-tuned YOLO models on
agricultural datasets and outperforms previous state-of-the-art methods by
$\sim10\%$ in remote sensing, under few-shot learning conditions. Our method
offers a promising solution for automating annotation and accelerating the
development of specialized agricultural AI solutions.


## Adaptive Computation Pruning for the Forgetting Transformer

>Authors: Zhixuan Lin, Johan Obando-Ceron, Xu Owen He, Aaron Courville

>2025-04-09

> http://arxiv.org/abs/2504.06949v1

The recently proposed Forgetting Transformer (FoX) incorporates a forget gate
into softmax attention and has shown consistently better or on-par performance
compared to the standard RoPE-based Transformer. Notably, many attention heads
in FoX tend to forget quickly, causing their output at each timestep to rely
primarily on the local context. Based on this observation, we propose Adaptive
Computation Pruning (ACP) for FoX, a method that dynamically prunes
computations involving input-output dependencies that are strongly decayed by
the forget gate. This is achieved using a dynamically set **pruning** threshold
that ensures that the pruned attention weights remain negligible. We apply ACP
to language model pretraining with FoX and show it consistently reduces the
number of FLOPs in softmax attention by around 70% across different model sizes
and context lengths, resulting in a roughly 10% to 35% improvement in training
throughput. Furthermore, longer context lengths yield greater computational
savings. All these speed improvements are achieved without any performance
degradation. We also perform several analyses to provide deeper insights into
our method, such as examining the **pruning** patterns and analyzing the
distribution of FLOP savings across different attention heads. Our code is
available at https://github.com/zhixuan-lin/arctic-fox.


## LVC A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding

>Authors: Ziyi Wang, Haoran Wu, Yiming Rong, Deyang Jiang, Yixin Zhang, Yunlong Zhao, Shuang Xu, Bo XU

>2025-04-09

> http://arxiv.org/abs/2504.06835v1

Long video understanding is a complex task that requires both spatial detail
and temporal awareness. While Vision-Language Models (VLMs) obtain frame-level
understanding capabilities through multi-frame input, they suffer from
information loss due to the **sparse** sampling strategy. In contrast, Video Large
Language Models (Video-LLMs) capture temporal relationships within visual
features but are limited by the scarcity of high-quality video-text datasets.
To transfer long video understanding capabilities to VLMs with minimal data and
computational cost, we propose Lightweight Video Compression (LVC), a novel
method featuring the Query-Attention Video Compression mechanism, which
effectively tackles the **sparse** sampling problem in VLMs. By training only the
alignment layer with 10k short video-text pairs, LVC significantly enhances the
temporal reasoning abilities of VLMs. Extensive experiments show that LVC
provides consistent performance improvements across various models, including
the InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC
achieves scores of 68.2 and 65.9 on the long video understanding benchmarks
MLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.
The enhanced models and code will be publicly available soon.


## DyDiT++ Dynamic Diffusion Transformers for Efficient Visual Generation

>Authors: Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You

>2025-04-09

> http://arxiv.org/abs/2504.06803v1

Diffusion Transformer (DiT), an emerging diffusion model for visual
generation, has demonstrated superior performance but suffers from substantial
computational costs. Our investigations reveal that these costs primarily stem
from the \emph{static} inference paradigm, which inevitably introduces
redundant computation in certain \emph{diffusion timesteps} and \emph{spatial
regions}. To overcome this inefficiency, we propose \textbf{Dy}namic
\textbf{Di}ffusion \textbf{T}ransformer (DyDiT), an architecture that
\emph{dynamically} adjusts its computation along both \emph{timestep} and
\emph{spatial} dimensions. Specifically, we introduce a \emph{Timestep-wise
Dynamic Width} (TDW) approach that adapts model width conditioned on the
generation timesteps. In addition, we design a \emph{Spatial-wise Dynamic
Token} (SDT) strategy to avoid redundant computation at unnecessary spatial
locations. TDW and SDT can be seamlessly integrated into DiT and significantly
accelerates the generation process. Building on these designs, we further
enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with
flow matching-based generation, enhancing its versatility. Furthermore, we
enhance DyDiT to tackle more complex visual generation tasks, including video
generation and text-to-image generation, thereby broadening its real-world
applications. Finally, to address the high cost of full fine-tuning and
democratize technology access, we investigate the feasibility of training DyDiT
in a parameter-efficient manner and introduce timestep-based dynamic LoRA
(TD-LoRA). Extensive experiments on diverse visual generation models, including
DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.


## A Survey of New Mid-Band/FR3 for 6G Channel Measurement, Characterization and Modeling in Outdoor Environment

>Authors: Haiyang Miao, Jianhua Zhang, Pan Tang, Jie Meng, Qi Zhen, Ximan Liu, Enrui Liu, Peijie Liu, Lei Tian, Guangyi Liu

>2025-04-09

> http://arxiv.org/abs/2504.06727v1

The new mid-band (6-24 GHz) has attracted significant attention from both
academia and industry, which is the spectrum with continuous bandwidth that
combines the coverage benefits of low frequency with the capacity advantages of
high frequency. Since outdoor environments represent the primary application
scenario for mobile communications, this paper presents the first comprehensive
review and summary of multi-scenario and multi-frequency channel
characteristics based on extensive outdoor new mid-band channel measurement
data, including UMa, UMi, and O2I. Specifically, a survey of the progress of
the channel characteristics is presented, such as path loss, delay spread,
angular spread, channel **sparsity**, capacity and near-field spatial
non-stationary characteristics. Then, considering that satellite communication
will be an important component of future communication systems, we examine the
impact of clutter loss in air-ground communications. Our analysis of the
frequency dependence of mid-band clutter loss suggests that its impact is not
significant. Additionally, given that penetration loss is frequency-dependent,
we summarize its variation within the FR3 band. Based on experimental results,
comparisons with the standard model reveal that while the 3GPP TR 38.901 model
remains a useful reference for penetration loss in wood and glass, it shows
significant deviations for concrete and glass, indicating the need for further
refinement. In summary, the findings of this survey provide both empirical data
and theoretical support for the deployment of mid-band in future communication
systems, as well as guidance for optimizing mid-band base station deployment in
the outdoor environment. This survey offers the reference for improving
standard models and advancing channel modeling.


## BBQRec Behavior-Bind Quantization for Multi-Modal Sequential Recommendation

>Authors: Kaiyuan Li, Rui Xiang, Yong Bai, Yongxiang Tang, Yanhua Cheng, Xialong Liu, Peng Jiang, Kun Gai

>2025-04-09

> http://arxiv.org/abs/2504.06636v1

Multi-modal sequential recommendation systems leverage auxiliary signals
(e.g., text, images) to alleviate data **sparsity** in user-item interactions.
While recent methods exploit large language models to encode modalities into
discrete semantic IDs for autoregressive prediction, we identify two critical
limitations: (1) Existing approaches adopt fragmented **quantization**, where
modalities are independently mapped to semantic spaces misaligned with
behavioral objectives, and (2) Over-reliance on semantic IDs disrupts
inter-modal semantic coherence, thereby weakening the expressive power of
multi-modal representations for modeling diverse user preferences.
  To address these challenges, we propose a Behavior-Bind multi-modal
Quantization for Sequential Recommendation (BBQRec for short) featuring
dual-aligned **quantization** and semantics-aware sequence modeling. First, our
behavior-semantic alignment module disentangles modality-agnostic behavioral
patterns from noisy modality-specific features through contrastive codebook
learning, ensuring semantic IDs are inherently tied to recommendation tasks.
Second, we design a discretized similarity reweighting mechanism that
dynamically adjusts self-attention scores using **quantize**d semantic
relationships, preserving multi-modal synergies while avoiding invasive
modifications to the sequence modeling architecture. Extensive evaluations
across four real-world benchmarks demonstrate BBQRec's superiority over the
state-of-the-art baselines.


## FuseRL Dense Preference Optimization for Heterogeneous Model Fusion

>Authors: Longguang Zhong, Fanqi Wan, Ziyi Yang, Guosheng Liang, Tianyuan Shi, Xiaojun Quan

>2025-04-09

> http://arxiv.org/abs/2504.06562v1

Heterogeneous model fusion enhances the performance of LLMs by integrating
the knowledge and capabilities of multiple structurally diverse models.
However, existing approaches often rely solely on selecting the best output for
each prompt from source models, which underutilizes their full potential due to
limited source knowledge and results in **sparse** optimization signals. To address
this limitation, we propose FuseRL, a novel two-stage framework comprising
FuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT
establishes a robust initialization by integrating the strengths of
heterogeneous source models through weighted supervised fine-tuning (SFT) on
diverse outputs for each prompt. FusePO optimizes weighted preferences based on
the outputs of multiple source models to enable superior alignment performance.
Extensive experiments demonstrate the effectiveness of our framework across
various preference alignment methods, including RLOO, DPO, and SimPO. Using
Llama-3.1-8B-Instruct as the target model, our approach achieves
state-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard
benchmarks. Further analysis suggests that FuseSFT regularizes the training
process to reduce overfitting, while FusePO introduces dense and diverse
signals for preference optimization.


## A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication

>Authors: Xiao-Hang Jiang, Yang Ai, Rui-Chen Zheng, Zhen-Hua Ling

>2025-04-09

> http://arxiv.org/abs/2504.06561v1

This paper proposes StreamCodec, a streamable neural audio codec designed for
real-time communication. StreamCodec adopts a fully causal, symmetric
encoder-decoder structure and operates in the modified discrete cosine
transform (MDCT) domain, aiming for low-latency inference and real-time
efficient generation. To improve codebook utilization efficiency and compensate
for the audio quality loss caused by structural causality, StreamCodec
introduces a novel residual scalar-vector **quantize**r (RSVQ). The RSVQ
sequentially connects scalar **quantize**rs and improved vector **quantize**rs in a
residual manner, constructing coarse audio contours and refining acoustic
details, respectively. Experimental results confirm that the proposed
StreamCodec achieves decoded audio quality comparable to advanced
non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS
dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed
latency of only 20 ms and achieves a generation speed nearly 20 times real-time
on a CPU, with a lightweight model size of just 7M parameters, making it highly
suitable for real-time communication applications.


## GIGA Generalizable Sparse Image-driven Gaussian Avatars

>Authors: Anton Zubekhin, Heming Zhu, Paulo Gotardo, Thabo Beeler, Marc Habermann, Christian Theobalt

>2025-04-08

> http://arxiv.org/abs/2504.07144v1

Driving a high-quality and photorealistic full-body human avatar, from only a
few RGB cameras, is a challenging problem that has become increasingly relevant
with emerging virtual reality technologies. To democratize such technology, a
promising solution may be a generalizable method that takes **sparse** multi-view
images of an unseen person and then generates photoreal free-view renderings of
such identity. However, the current state of the art is not scalable to very
large datasets and, thus, lacks in diversity and photorealism. To address this
problem, we propose a novel, generalizable full-body model for rendering
photoreal humans in free viewpoint, as driven by **sparse** multi-view video. For
the first time in literature, our model can scale up training to thousands of
subjects while maintaining high photorealism. At the core, we introduce a
MultiHeadUNet architecture, which takes **sparse** multi-view images in texture
space as input and predicts Gaussian primitives represented as 2D texels on top
of a human body mesh. Importantly, we represent **sparse**-view image information,
body shape, and the Gaussian parameters in 2D so that we can design a deep and
scalable architecture entirely based on 2D convolutions and attention
mechanisms. At test time, our method synthesizes an articulated 3D
Gaussian-based avatar from as few as four input views and a tracked body
template for unseen identities. Our method excels over prior works by a
significant margin in terms of cross-subject generalization capability as well
as photorealism.


## SPIRe Boosting LLM Inference Throughput with Speculative Decoding

>Authors: Sanjit Neelam, Daniel Heinlein, Vaclav Cvicek, Akshay Mishra, Reiner Pope

>2025-04-08

> http://arxiv.org/abs/2504.06419v1

Speculative decoding (SD) has been shown to reduce the latency of
autoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing
throughput and therefore reducing the cost per token requires decoding with
large batch sizes. Recent work shows that SD can accelerate decoding with large
batch sizes too if the context is sufficiently long and the draft model's **KV**
cache is **sparse**. We introduce SPIRe, a draft model that combines static **sparse**
attention, pruned initialization, and feedback memory to increase the modeled
throughput of speculative decoding by over 100% compared to speculation with a
much smaller draft model and by over 35% compared to the strong baseline of
**sparse** self-speculation. Our approach is particularly effective when context
lengths vary significantly across requests.


## Unifying Autoregressive and Diffusion-Based Sequence Generation

>Authors: Nima Fathi, Torsten Scholak, Pierre-André Noël

>2025-04-08

> http://arxiv.org/abs/2504.06416v1

We present significant extensions to diffusion-based sequence generation
models, blurring the line with autoregressive language models. We introduce
hyperschedules, which assign distinct noise schedules to individual token
positions, generalizing both autoregressive models (e.g., GPT) and conventional
diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two
hybrid token-wise noising processes that interpolate between absorbing and
uniform processes, enabling the model to fix past mistakes, and we introduce a
novel inference algorithm that leverages this new feature in a simplified
context inspired from MDLM. To support efficient training and inference, we
design attention masks compatible with **KV**-caching. Our methods achieve
state-of-the-art perplexity and generate diverse, high-quality sequences across
standard benchmarks, suggesting a promising path for autoregressive
diffusion-based sequence generation.


## HiFlow Training-free High-Resolution Image Generation with Flow-Aligned Guidance

>Authors: Jiazi Bu, Pengyang Ling, Yujie Zhou, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang

>2025-04-08

> http://arxiv.org/abs/2504.06232v1

Text-to-image (T2I) diffusion/flow models have drawn considerable attention
recently due to their remarkable ability to deliver flexible visual creations.
Still, high-resolution image synthesis presents formidable challenges due to
the scarcity and complexity of high-resolution content. To this end, we present
HiFlow, a training-free and model-agnostic framework to unlock the resolution
potential of pre-trained flow models. Specifically, HiFlow establishes a
virtual reference flow within the high-resolution space that effectively
captures the characteristics of low-resolution flow information, offering
guidance for high-resolution generation through three key aspects:
initialization alignment for low-frequency consistency, direction alignment for
structure preservation, and **acceleration** alignment for detail fidelity. By
leveraging this flow-aligned guidance, HiFlow substantially elevates the
quality of high-resolution image synthesis of T2I models and demonstrates
versatility across their personalized variants. Extensive experiments validate
HiFlow's superiority in achieving superior high-resolution image quality over
current state-of-the-art methods.


## Optuna vs Code Llama Are LLMs a New Paradigm for Hyperparameter Tuning?

>Authors: Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte

>2025-04-08

> http://arxiv.org/abs/2504.06006v1

Optimal hyperparameter selection is critical for maximizing neural network
performance, especially as models grow in complexity. This work investigates
the viability of using large language models (LLMs) for hyperparameter
optimization by employing a fine-tuned version of Code Llama. Through
parameter-efficient fine-tuning using LoRA, we adapt the LLM to generate
accurate and efficient hyperparameter recommendations tailored to diverse
neural network architectures. Unlike traditional methods such as Optuna, which
rely on exhaustive trials, the proposed approach achieves competitive or
superior results in terms of Root Mean Square Error (RMSE) while significantly
reducing computational overhead. Our approach highlights that LLM-based
optimization not only matches state-of-the-art methods like Tree-structured
Parzen Estimators but also accelerates the tuning process. This positions LLMs
as a promising alternative to conventional optimization techniques,
particularly for rapid experimentation. Furthermore, the ability to generate
hyperparameters in a single inference step makes this method particularly
well-suited for resource-constrained environments such as edge devices and
mobile applications, where computational efficiency is paramount. The results
confirm that LLMs, beyond their efficiency, offer substantial time savings and
comparable stability, underscoring their value in advancing machine learning
workflows. All generated hyperparameters are included in the LEMUR Neural
Network (NN) Dataset, which is publicly available and serves as an open-source
benchmark for hyperparameter optimization research.


## A Corrector-aided Look-ahead Distance-based Guidance for Reference Path Following with an Efficient Midcourse Guidance Strategy

>Authors: Reva Dhillon, Agni Ravi Deepa, Hrishav Das, Subham Basak, Satadal Ghosh

>2025-04-08

> http://arxiv.org/abs/2504.05975v1

Efficient path-following is crucial in most of the applications of autonomous
vehicles (UxV). Among various guidance strategies presented in literature,
look-ahead distance ($L_1$)-based guidance method has received significant
attention due to its ease in implementation and ability to maintain a low
cross-track error while following simpler reference paths and generate bounded
lateral **acceleration** commands. However, the constant value of $L_1$ becomes
problematic when the UxV is far away from the reference path and also produce
higher cross-track error while following complex reference paths having high
variation in radius of curvature. To address these challenges, the notion of
look-ahead distance is leveraged in a novel way to develop a two-phase guidance
strategy. Initially, when the UxV is far from the reference path, an optimized
$L_1$ selection strategy is developed to guide the UxV toward the reference
path in order to maintain minimal lateral **acceleration** command. Once the
vehicle reaches a close vicinity of the reference path, a novel notion of
corrector point is incorporated in the constant $L_1$-based guidance scheme to
generate the lateral **acceleration** command that effectively reduces the root
mean square of the cross-track error thereafter. Simulation results demonstrate
that this proposed corrector point and look-ahead point pair-based guidance
strategy along with the developed midcourse guidance scheme outperforms the
conventional constant $L_1$ guidance scheme both in terms of feasibility and
measures of effectiveness like cross-track error and lateral **acceleration**
requirements.


## Generalized Parameter Lifting Finer Abstractions for Parametric Markov Chains

>Authors: Linus Heck, Tim Quatmann, Jip Spel, Joost-Pieter Katoen, Sebastian Junges

>2025-04-08

> http://arxiv.org/abs/2504.05965v1

Parametric Markov chains (pMCs) are Markov chains (MCs) with symbolic
probabilities. A pMC encodes a family of MCs, where each member is obtained by
replacing parameters with constants. The parameters allow encoding dependencies
between transitions, which sets pMCs apart from interval MCs. The verification
problem for pMCs asks whether each MC in the corresponding family satisfies a
given temporal specification. The state-of-the-art approach for this problem is
parameter lifting (PL) -- an abstraction-refinement loop that abstracts the pMC
to a non-parametric model analyzed with standard probabilistic model checking
techniques. This paper presents two key improvements to tackle the main
limitations of PL. First, we introduce generalized parameter lifting (GPL) to
lift various restrictive assumptions made by PL. Second, we present a big-step
transformation algorithm that reduces parameter dependencies in pMCs and,
therefore, results in tighter approximations. Experiments show that GPL is
widely applicable and that the big-step transformation accelerates pMC
verification by up to orders of magnitude.


## Mosaic Composite Projection Pruning for Resource-efficient LLMs

>Authors: Bailey J. Eccles, Leon Wong, Blesson Varghese

>2025-04-08

> http://arxiv.org/abs/2504.06323v1

Extensive compute and memory requirements limit the deployment of large
language models (LLMs) on any hardware. Compression methods, such as **pruning**,
can reduce model size, which in turn reduces resource requirements.
State-of-the-art **pruning** is based on coarse-grained methods. They are
time-consuming and inherently remove critical model parameters, adversely
impacting the quality of the pruned model. This paper introduces projection
**pruning**, a novel fine-grained method for **pruning** LLMs. In addition, LLM
projection **pruning** is enhanced by a new approach we refer to as composite
projection **pruning** - the synergistic combination of unstructured **pruning** that
retains accuracy and structured **pruning** that reduces model size. We develop
Mosaic, a novel system to create and deploy pruned LLMs using composite
projection **pruning**. Mosaic is evaluated using a range of performance and
quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is
7.19x faster in producing models than existing approaches. Mosaic models
achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models
obtained from coarse-grained **pruning**. Up to 67% faster inference and 68% lower
GPU memory use is noted for Mosaic models.


## Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching

>Authors: Yanhao Dong, Yubo Miao, Weinan Li, Xiao Zheng, Chao Wang, Feng Lyu

>2025-04-08

> http://arxiv.org/abs/2504.06319v1

Large Language Models (LLMs) exhibit pronounced memory-bound characteristics
during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In
this paper, we propose an L2 Cache-oriented asynchronous **KV** Cache prefetching
method to break through the memory bandwidth bottleneck in LLM inference
through computation-load overlap. By strategically scheduling idle memory
bandwidth during active computation windows, our method proactively prefetches
required **KV** Cache into GPU L2 cache, enabling high-speed L2 cache hits for
subsequent accesses and effectively hiding HBM access latency within
computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that
the proposed method achieves 2.15x improvement in attention kernel efficiency
and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art
baseline FlashAttention-3. Notably, our solution maintains orthogonality to
existing optimization techniques and can be integrated with current inference
frameworks, providing a scalable latency-hiding solution for next-generation
LLM inference engines.


## End-to-End Dialog Neural Coreference Resolution Balancing Efficiency and Accuracy in Large-Scale Systems

>Authors: Zhang Dong, Songhang deng, Mingbang Wang, Le Dai, Jiyuan Li, Xingzu Liu, Ruilin Nong

>2025-04-08

> http://arxiv.org/abs/2504.05824v1

Large-scale coreference resolution presents a significant challenge in
natural language processing, necessitating a balance between efficiency and
accuracy. In response to this challenge, we introduce an End-to-End Neural
Coreference Resolution system tailored for large-scale applications. Our system
efficiently identifies and resolves coreference links in text, ensuring minimal
computational overhead without compromising on performance. By utilizing
advanced neural network architectures, we incorporate various contextual
embeddings and attention mechanisms, which enhance the quality of predictions
for coreference pairs. Furthermore, we apply optimization strategies to
accelerate processing speeds, making the system suitable for real-world
deployment. Extensive evaluations conducted on benchmark datasets demonstrate
that our model achieves improved accuracy compared to existing approaches,
while effectively maintaining rapid inference times. Rigorous testing confirms
the ability of our system to deliver precise coreference resolutions
efficiently, thereby establishing a benchmark for future advancements in this
field.


## Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation

>Authors: Yuhang Deng, Zheng Chen, Erik G. Larsson

>2025-04-08

> http://arxiv.org/abs/2504.05729v1

Non-coherent over-the-air (OTA) computation has garnered increasing attention
for its advantages in facilitating information aggregation among distributed
agents in resource-constrained networks without requiring precise channel
estimation. A promising application scenario of this method is distributed
average consensus in wireless multi-agent systems. However, in such scenario,
non-coherent interference from concurrent OTA transmissions can introduce bias
in the consensus value. To address this issue, we develop a robust distributed
average consensus algorithm by formulating the consensus problem as a
distributed optimization problem. Using decentralized projected gradient
descent (D-PGD), our proposed algorithm can achieve unbiased mean square
average consensus even in the presence of non-coherent interference and noise.
Additionally, we implement transmit power control and receive scaling
mechanisms to further accelerate convergence. Simulation results demonstrate
that our method can significantly enhance the convergence speed of the D-PGD
algorithm for OTA average consensus without compromising accuracy.


## Point-based Instance Completion with Scene Constraints

>Authors: Wesley Khademi, Li Fuxin

>2025-04-08

> http://arxiv.org/abs/2504.05698v1

Recent point-based object completion methods have demonstrated the ability to
accurately recover the missing geometry of partially observed objects. However,
these approaches are not well-suited for completing objects within a scene, as
they do not consider known scene constraints (e.g., other observed surfaces) in
their completions and further expect the partial input to be in a canonical
coordinate system, which does not hold for objects within scenes. While
instance scene completion methods have been proposed for completing objects
within a scene, they lag behind point-based object completion methods in terms
of object completion quality and still do not consider known scene constraints
during completion. To overcome these limitations, we propose a point
cloud-based instance completion model that can robustly complete objects at
arbitrary scales and pose in the scene. To enable reasoning at the scene level,
we introduce a **sparse** set of scene constraints represented as point clouds and
integrate them into our completion model via a cross-attention mechanism. To
evaluate the instance scene completion task on indoor scenes, we further build
a new dataset called ScanWCF, which contains labeled partial scans as well as
aligned ground truth scene completions that are watertight and collision-free.
Through several experiments, we demonstrate that our method achieves improved
fidelity to partial scans, higher completion quality, and greater plausibility
over existing state-of-the-art methods.


## TAGC Optimizing Gradient Communication in Distributed Transformer Training

>Authors: Igor Polyakov, Alexey Dukhanov, Egor Spirin

>2025-04-08

> http://arxiv.org/abs/2504.05638v1

The increasing complexity of large language models (LLMs) necessitates
efficient training strategies to mitigate the high computational costs
associated with distributed training. A significant bottleneck in this process
is gradient synchronization across multiple GPUs, particularly in the
zero-redundancy parallelism mode. In this paper, we introduce Transformer-Aware
Gradient Compression (TAGC), an optimized gradient compression algorithm
designed specifically for transformer-based models. TAGC extends the lossless
homomorphic compression method by adapting it for sharded models and
incorporating transformer-specific optimizations, such as layer-selective
compression and dynamic sparsification. Our experimental results demonstrate
that TAGC accelerates training by up to 15% compared to the standard Fully
Sharded Data Parallel (FSDP) approach, with minimal impact on model quality. We
integrate TAGC into the PyTorch FSDP framework, the implementation is publicly
available at https://github.com/ipolyakov/TAGC.


## DEL Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding

>Authors: Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang, Murali Annavaram

>2025-04-08

> http://arxiv.org/abs/2504.05598v1

Speculative Decoding (SD) is a widely used approach to accelerate the
inference of large language models (LLMs) without reducing generation quality.
It operates by first using a compact model to draft multiple tokens
efficiently, followed by parallel verification using the target LLM. This
approach leads to faster inference compared to auto-regressive decoding. While
there are multiple approaches to create a draft model, one promising approach
is to use early-exit methods. These methods draft candidate tokens by using a
subset of layers of the primary model and applying the remaining layers for
verification, allowing a single model to handle both drafting and verification.
While this technique reduces memory usage and computational cost, its
performance relies on the choice of the exit layer for drafting and the number
of tokens drafted (speculation length) in each SD round. Prior works use
hyperparameter exploration to statically select these values. However, our
evaluations show that these hyperparameter values are task-specific, and even
within a task they are dependent on the current sequence context. We introduce
DEL, a plug-and-play method that adaptively selects the exit layer and
speculation length during inference. DEL dynamically tracks the token
acceptance rate if the tokens are drafted at each layer of an LLM and uses that
knowledge to heuristically select the optimal exit layer and speculation
length. Our experiments across a broad range of models and downstream tasks
show that DEL achieves overall speedups of $2.16\times$$\sim$$2.50\times$ over
vanilla auto-regressive decoding and improves upon the state-of-the-art SD
methods by up to $0.27\times$.


## SciSciGPT Advancing Human-AI Collaboration in the Science of Science

>Authors: Erzhuo Shao, Yifang Wang, Yifan Qian, Zhenyu Pan, Han Liu, Dashun Wang

>2025-04-07

> http://arxiv.org/abs/2504.05559v1

The increasing availability of large-scale datasets has fueled rapid progress
across many scientific fields, creating unprecedented opportunities for
research and discovery while posing significant analytical challenges. Recent
advances in large language models (LLMs) and AI agents have opened new
possibilities for human-AI collaboration, offering powerful tools to navigate
this complex research landscape. In this paper, we introduce SciSciGPT, an
open-source, prototype AI collaborator that uses the science of science as a
testbed to explore the potential of LLM-powered research tools. SciSciGPT
automates complex workflows, supports diverse analytical approaches,
accelerates research prototyping and iteration, and facilitates
reproducibility. Through case studies, we demonstrate its ability to streamline
a wide range of empirical and analytical research tasks while highlighting its
broader potential to advance research. We further propose an LLM Agent
capability maturity model for human-AI collaboration, envisioning a roadmap to
further improve and expand upon frameworks like SciSciGPT. As AI capabilities
continue to evolve, frameworks like SciSciGPT may play increasingly pivotal
roles in scientific research and discovery, unlocking further opportunities. At
the same time, these new advances also raise critical challenges, from ensuring
transparency and ethical use to balancing human and AI contributions.
Addressing these issues may shape the future of scientific inquiry and inform
how we train the next generation of scientists to thrive in an increasingly
AI-integrated research ecosystem.


## Beam-driven plasma-wakefield acceleration

>Authors: C. A. Lindstrøm, S. Corde, R. D'Arcy, S. Gessner, M. Gilljohann, M. J. Hogan, J. Osterhoff

>2025-04-07

> http://arxiv.org/abs/2504.05558v1

Beam-driven plasma-wakefield **acceleration** (PWFA) has emerged as a
transformative technology with the potential to revolutionize the field of
particle **acceleration**, especially toward compact accelerators for high-energy
and high-power applications. Charged particle beams are used to excite density
waves in plasma with accelerating fields reaching up to 100 GV/m, thousands of
times stronger than the fields provided by radio-frequency cavities.
Plasma-wakefield-accelerator research has matured over the span of four decades
from basic concepts and proof-of-principle experiments to a rich and rapidly
progressing sub-field with dedicated experimental facilities and
state-of-the-art simulation codes. We review the physics, including theory of
linear and nonlinear plasma wakefields as well as beam dynamics of both the
wakefield driver and trailing bunches accelerating in the plasma wake, and
address challenges associated with energy efficiency and preservation of beam
quality. Advanced topics such as positron **acceleration**, self-modulation,
internal injection, long-term plasma evolution and multistage **acceleration** are
discussed. Simulation codes and major experiments are surveyed, spanning the
use of electron, positron and proton bunches as wakefield drivers. Finally, we
look ahead to future particle colliders and light sources based on plasma
technology.


## Optimizing Large Language Models Metrics, Energy Efficiency, and Case Study Insights

>Authors: Tahniat Khan, Soroor Motie, Sedef Akinli Kocak, Shaina Raza

>2025-04-07

> http://arxiv.org/abs/2504.06307v1

The rapid adoption of large language models (LLMs) has led to significant
energy consumption and carbon emissions, posing a critical challenge to the
sustainability of generative AI technologies. This paper explores the
integration of energy-efficient optimization techniques in the deployment of
LLMs to address these environmental concerns. We present a case study and
framework that demonstrate how strategic **quantization** and local inference
techniques can substantially lower the carbon footprints of LLMs without
compromising their operational effectiveness. Experimental results reveal that
these methods can reduce energy consumption and carbon emissions by up to 45\%
post **quantization**, making them particularly suitable for resource-constrained
environments. The findings provide actionable insights for achieving
sustainability in AI while maintaining high levels of accuracy and
responsiveness.


## Efficient Reinforcement Finetuning via Adaptive Curriculum Learning

>Authors: Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao

>2025-04-07

> http://arxiv.org/abs/2504.05520v1

Reinforcement finetuning (RFT) has shown great potential for enhancing the
mathematical reasoning capabilities of large language models (LLMs), but it is
often sample- and compute-inefficient, requiring extensive training. In this
work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a
method that significantly improves both the efficiency and final accuracy of
RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the
difficulty of training problems based on the model's recent reward signals,
ensuring that the model consistently trains on tasks that are challenging but
solvable. This adaptive sampling strategy accelerates learning by maintaining
an optimal difficulty range, avoiding wasted computation on problems that are
too easy or too hard. AdaRFT requires only a lightweight extension to standard
RFT algorithms like Proximal Policy Optimization (PPO), without modifying the
reward function or model architecture. Experiments on competition-level math
datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT
significantly improves both training efficiency and reasoning performance. We
evaluate AdaRFT across multiple data distributions and model sizes, showing
that it reduces the number of training steps by up to 2x and improves accuracy
by a considerable margin, offering a more scalable and effective RFT framework.


## Exponential Quantum Speedup for Simulating Classical Lattice Dynamics

>Authors: Xiantao Li

>2025-04-07

> http://arxiv.org/abs/2504.05453v1

Simulating large scale lattice dynamics directly is computationally demanding
due to the high complexity involved, yet such simulations are crucial for
understanding the mechanical and thermal properties of many physical systems.
In this work, we introduce a rigorous quantum framework for simulating general
harmonic lattice dynamics by reformulating the classical equations as a time
dependent Schr\"odinger equation governed by a **sparse** Hamiltonian. This
transformation allows us to exploit well established quantum Hamiltonian
simulation techniques, offering an exponential speedup with respect to the
number of atoms $N$. The overall complexity has a logarithmic dependence on
$N$, and linear dependence on both the simulation time $T$ and the Debye
frequency $\omega_D$. Key to our approach is the application of the matrix
valued Fej\'er Riesz theorem to the phonon dynamical matrix, which facilitates
the efficient construction of the underlying Hamiltonian with translational
invariance. We demonstrate the applicability of the method across a broad class
of lattice models.


## EffOWT Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively

>Authors: Bingyang Wang, Kaer Huang, Bin Li, Yiqiang Yan, Lihe Zhang, Huchuan Lu, You He

>2025-04-07

> http://arxiv.org/abs/2504.05141v2

Open-World Tracking (OWT) aims to track every object of any category, which
requires the model to have strong generalization capabilities. Trackers can
improve their generalization ability by leveraging Visual Language Models
(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are
transferred to OWT: full fine-tuning results in excessive parameter and memory
costs, while the zero-shot strategy leads to sub-optimal performance. To solve
the problem, EffOWT is proposed for efficiently transferring VLMs to OWT.
Specifically, we build a small and independent learnable side network outside
the VLM backbone. By freezing the backbone and only executing backpropagation
on the side network, the model's efficiency requirements can be met. In
addition, EffOWT enhances the side network by proposing a hybrid structure of
Transformer and CNN to improve the model's performance in the OWT field.
Finally, we implement **sparse** interactions on the MLP, thus reducing parameter
updates and memory costs significantly. Thanks to the proposed methods, EffOWT
achieves an absolute gain of 5.5% on the tracking metric OWTA for unknown
categories, while only updating 1.3% of the parameters compared to full
fine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious
improvement.


## Algorithm Discovery With LLMs Evolutionary Search Meets Reinforcement Learning

>Authors: Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre

>2025-04-07

> http://arxiv.org/abs/2504.05108v2

Discovering efficient algorithms for solving complex problems has been an
outstanding challenge in mathematics and computer science, requiring
substantial human expertise over the years. Recent advancements in evolutionary
search with large language models (LLMs) have shown promise in accelerating the
discovery of algorithms across various domains, particularly in mathematics and
optimization. However, existing approaches treat the LLM as a static generator,
missing the opportunity to update the model with the signal obtained from
evolutionary exploration. In this work, we propose to augment LLM-based
evolutionary search by continuously refining the search operator - the LLM -
through reinforcement learning (RL) fine-tuning. Our method leverages
evolutionary search as an exploration strategy to discover improved algorithms,
while RL optimizes the LLM policy based on these discoveries. Our experiments
on three combinatorial optimization tasks - bin packing, traveling salesman,
and the flatpack problem - show that combining RL and evolutionary search
improves discovery efficiency of improved algorithms, showcasing the potential
of RL-enhanced evolutionary strategies to assist computer scientists and
mathematicians for more efficient algorithm design.


## State Tuning State-based Test-Time Scaling on RWKV-7

>Authors: Liu Xiao, Li Zhiyuan, Lin Yueyu

>2025-04-07

> http://arxiv.org/abs/2504.05097v1

Test-time scaling has emerged as a prominent research direction in machine
learning, enabling models to enhance their expressive capabilities during
inference.Transformers, renowned for striking a delicate balance between
efficiency and expressiveness, have benefited from test-time scaling techniques
that leverage an expanding key-value (**KV**) cache to significantly improve
performance.In this paper, we introduce a novel state-based approach to
test-time scaling, which we term state tuning, tailored to the RNN-based RW**KV**-7
model.By exploiting the unique strengths of RW**KV**-7, our method achieves
state-of-the-art performance on the target task without altering the model's
pre-trained weights. Our approach centers on three key innovations. First, we
develop an observer framework that allows a smaller model to replicate and
learn the state dynamics of the RW**KV**-7 model. Second, we employ a kernel method
to dynamically upscale the state size, enhancing the model's capacity to
capture intricate patterns. Third, we integrate Decorrelated Backpropagation
(DBP) to optimize the upscaled state matrix, thereby improving convergence and
expressivity. By tuning only the state matrix, we demonstrate that a smaller
model can outperform larger models on the given task. This method preserves the
efficiency of the original RW**KV**-7 architecture while harnessing the power of
test-time scaling to deliver superior results. Our findings underscore the
potential of state tuning as an effective strategy for advancing model
performance in resource-constrained settings. Our code is
https://github.com/TorchRW**KV**/flash-linear-attention.


## MIAT Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction

>Authors: Chandra Raskoti, Iftekharul Islam, Xuan Wang, Weizi Li

>2025-04-07

> http://arxiv.org/abs/2504.05059v1

Accurate vehicle trajectory prediction is critical for safe and efficient
autonomous driving, especially in mixed traffic environments with both
human-driven and autonomous vehicles. However, uncertainties introduced by
inherent driving behaviors -- such as **acceleration**, deceleration, and left and
right maneuvers -- pose significant challenges for reliable trajectory
prediction. We introduce a Maneuver-Intention-Aware Transformer (MIAT)
architecture, which integrates a maneuver intention awareness mechanism with
spatiotemporal interaction modeling to enhance long-horizon trajectory
predictions. We systematically investigate the impact of varying awareness of
maneuver intention on both short- and long-horizon trajectory predictions.
Evaluated on the real-world NGSIM dataset and benchmarked against various
transformer- and LSTM-based methods, our approach achieves an improvement of up
to 4.7% in short-horizon predictions and a 1.6% in long-horizon predictions
compared to other intention-aware benchmark methods. Moreover, by leveraging an
intention awareness control mechanism, MIAT realizes an 11.1% performance boost
in long-horizon predictions, with a modest drop in short-horizon performance.


## Secure Communications for All Users in Low-Resolution IRS-aided Systems Under Imperfect and Unknown CSI

>Authors: Monir Abughalwa, Diep N. Nguyen, Dinh Thai Hoang, Thang X. Vu, Eryk Dutkiewicz, Symeon Chatzinotas

>2025-04-07

> http://arxiv.org/abs/2504.05048v2

Provisioning secrecy for all users, given the heterogeneity and uncertainty
of their channel conditions, locations, and the unknown location of the
attacker/eavesdropper, is challenging and not always feasible. This work takes
the first step to guarantee secrecy for all users where a low resolution
intelligent reflecting surfaces (IRS) is used to enhance legitimate users'
reception and thwart the potential eavesdropper (Eve) from intercepting. In
real-life scenarios, due to hardware limitations of the IRS' passive reflective
elements (PREs), the use of a full-resolution (continuous) phase shift (CPS) is
impractical. In this paper, we thus consider a more practical case where the
phase shift (PS) is modeled by a low-resolution (**quantize**d) phase shift (QPS)
while addressing the phase shift error (PSE) induced by the imperfect channel
state information (CSI). To that end, we aim to maximize the minimum secrecy
rate (SR) among all users by jointly optimizing the transmitter's beamforming
vector and the IRS's passive reflective elements (PREs) under
perfect/imperfect/unknown CSI. The resulting optimization problem is non-convex
and even more complicated under imperfect/unknown CSI. The resulting
optimization problem is non-convex and even more complicated under
imperfect/unknown CSI. To tackle it, we linearize the objective function and
decompose the problem into sequential subproblems. When the perfect CSI is not
available, we use the successive convex approximation (SCA) approach to
transform imperfect CSI related semi-infinite constraints into finite linear
matrix inequalities (LMI). We prove that our proposed algorithm converges to a
locally optimal solution with low computational complexity. Extensive
simulations with practical settings show that our approach can ensure secure
communication for all users while the IRS's PREs are **quantize**d and are affected
by the PSE.


## One Quantizer is Enough Toward a Lightweight Audio Codec

>Authors: Linwei Zhai, Han Ding, Cui Zhao, fei wang, Ge Wang, Wang Zhi, Wei Xi

>2025-04-07

> http://arxiv.org/abs/2504.04949v1

Neural audio codecs have recently gained traction for their ability to
compress high-fidelity audio and generate discrete tokens that can be utilized
in downstream generative modeling tasks. However, leading approaches often rely
on resource-intensive models and multi-**quantize**r architectures, resulting in
considerable computational overhead and constrained real-world applicability.
In this paper, we present SQCodec, a lightweight neural audio codec that
leverages a single **quantize**r to address these limitations. SQCodec explores
streamlined convolutional networks and local Transformer modules, alongside
TConv, a novel mechanism designed to capture acoustic variations across
multiple temporal scales, thereby enhancing reconstruction fidelity while
reducing model complexity. Extensive experiments across diverse datasets show
that SQCodec achieves audio quality comparable to multi-**quantize**r baselines,
while its single-**quantize**r design offers enhanced adaptability and its
lightweight architecture reduces resource consumption by an order of magnitude.
The source code is publicly available at https://github.com/zhai-lw/SQCodec.


## Inter-event Interval Microscopy for Event Cameras

>Authors: Changqing Su, Yanqin Chen, Zihan Lin, Zhen Cheng, You Zhou, Bo Xiong, Zhaofei Yu, Tiejun Huang

>2025-04-07

> http://arxiv.org/abs/2504.04924v2

Event cameras, an innovative bio-inspired sensor, differ from traditional
cameras by sensing changes in intensity rather than directly perceiving
intensity and recording these variations as a continuous stream of "events".
The intensity reconstruction from these **sparse** events has long been a
challenging problem. Previous approaches mainly focused on transforming
motion-induced events into videos or achieving intensity imaging for static
scenes by integrating modulation devices at the event camera acquisition end.
In this paper, for the first time, we achieve event-to-intensity conversion
using a static event camera for both static and dynamic scenes in fluorescence
microscopy. Unlike conventional methods that primarily rely on event
integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the
time interval between consecutive events at each pixel. With a fixed threshold
in the event camera, the time interval can precisely represent the intensity.
At the hardware level, the proposed IEIM integrates a pulse light modulation
device within a microscope equipped with an event camera, termed Pulse
Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have
collected IEIMat dataset under various scenes including high dynamic range and
high-speed scenarios. Experimental results on the IEIMat dataset demonstrate
that the proposed IEIM achieves superior spatial and temporal resolution, as
well as a higher dynamic range, with lower bandwidth compared to other methods.
The code and the IEIMat dataset will be made publicly available.


## Attentional Graph Meta-Learning for Indoor Localization Using Extremely Sparse Fingerprints

>Authors: Wenzhong Yan, Feng Yin, Jun Gao, Ao Wang, Yang Tian, Ruizhi Chen

>2025-04-07

> http://arxiv.org/abs/2504.04829v1

Fingerprint-based indoor localization is often labor-intensive due to the
need for dense grids and repeated measurements across time and space.
Maintaining high localization accuracy with extremely **sparse** fingerprints
remains a persistent challenge. Existing benchmark methods primarily rely on
the measured fingerprints, while neglecting valuable spatial and environmental
characteristics. In this paper, we propose a systematic integration of an
Attentional Graph Neural Network (AGNN) model, capable of learning spatial
adjacency relationships and aggregating information from neighboring
fingerprints, and a meta-learning framework that utilizes datasets with similar
environmental characteristics to enhance model training. To minimize the labor
required for fingerprint collection, we introduce two novel data augmentation
strategies: 1) unlabeled fingerprint augmentation using moving platforms, which
enables the semi-supervised AGNN model to incorporate information from
unlabeled fingerprints, and 2) synthetic labeled fingerprint augmentation
through environmental digital twins, which enhances the meta-learning framework
through a practical distribution alignment, which can minimize the feature
discrepancy between synthetic and real-world fingerprints effectively. By
integrating these novel modules, we propose the Attentional Graph Meta-Learning
(AGML) model. This novel model combines the strengths of the AGNN model and the
meta-learning framework to address the challenges posed by extremely **sparse**
fingerprints. To validate our approach, we collected multiple datasets from
both consumer-grade WiFi devices and professional equipment across diverse
environments. Extensive experiments conducted on both synthetic and real-world
datasets demonstrate that the AGML model-based localization method consistently
outperforms all baseline methods using **sparse** fingerprints across all evaluated
metrics.


## Dynamic Vision Mamba

>Authors: Mengxuan Wu, Zekai Li, Zhiyuan Liang, Moyang Li, Xuanlei Zhao, Samir Khaki, Zheng Zhu, Xiaojiang Peng, Konstantinos N. Plataniotis, Kai Wang, Wangbo Zhao, Yang You

>2025-04-07

> http://arxiv.org/abs/2504.04787v1

Mamba-based vision models have gained extensive attention as a result of
being computationally more efficient than attention-based models. However,
spatial redundancy still exists in these models, represented by token and block
redundancy. For token redundancy, we analytically find that early token **pruning**
methods will result in inconsistency between training and inference or
introduce extra computation for inference. Therefore, we customize token
**pruning** to fit the Mamba structure by rearranging the pruned sequence before
feeding it into the next Mamba block. For block redundancy, we allow each image
to select SSM blocks dynamically based on an empirical observation that the
inference speed of Mamba-based vision models is largely affected by the number
of SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively
reduces FLOPs with minor performance drops. We achieve a reduction of 35.2\%
FLOPs with only a loss of accuracy of 1.7\% on Vim-S. It also generalizes well
across different Mamba vision model architectures and different vision tasks.
Our code will be made public.


## Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs

>Authors: Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco

>2025-04-07

> http://arxiv.org/abs/2504.04745v1

This paper evaluates the ability of Large Language Models (LLMs) to leverage
contextual information in the form of structured linguistic representations.
Specifically, we examine the impact of encoding both short and long contexts
using Abstract Meaning Representation (AMR) structures across a diverse set of
language tasks. We perform our analysis using 8-bit **quantize**d and
instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our
results indicate that, for tasks involving short contexts, augmenting the
prompt with the AMR of the original language context often degrades the
performance of the underlying LLM. However, for tasks that involve long
contexts, such as dialogue summarization in the SAMSum dataset, this
enhancement improves LLM performance, for example, by increasing the zero-shot
cosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is
more evident in the newer and larger LLMs, but does not extend to the older or
smaller ones. In addition, we observe that LLMs can effectively reconstruct the
original text from a linearized AMR, achieving a cosine similarity of 81.3% in
the best-case scenario.


## Achieving binary weight and activation for LLMs using Post-Training Quantization

>Authors: Siqing Song, Chuang Wang, Ruiqi Wang, Yi Yang, Xuyao Zhang

>2025-04-07

> http://arxiv.org/abs/2504.05352v1

Quantizing large language models (LLMs) to 1-bit precision significantly
reduces computational costs, but existing **quantization** techniques suffer from
noticeable performance degradation when using weight and activation precisions
below 4 bits (W4A4). In this paper, we propose a post-training **quantization**
framework with W(1+1)A(1*4) configuration, where weights are **quantize**d to 1 bit
with an additional 1 bit for fine-grain grouping and activations are **quantize**d
to 1 bit with a 4-fold increase in the number of channels. For weight
**quantization**, we propose utilizing Hessian-aware fine-grained grouping along
with an EM-based **quantization** scheme. For activation **quantization**, we decompose
INT4-**quantize**d activations into a 4 * INT1 format equivalently and
simultaneously smooth the scaling factors based on **quantization** errors, which
further reduces the **quantization** errors in activations. Our method surpasses
state-of-the-art (SOTA) LLM **quantization** baselines on W2A4 across multiple
tasks, pushing the boundaries of existing LLM **quantization** methods toward fully
binarized models.


## Can LLM-Driven Hard Negative Sampling Empower Collaborative Filtering? Findings and Potentials

>Authors: Chu Zhao, Enneng Yang, Yuting Liu, Jianzhe Zhao, Guibing Guo, Xingwei Wang

>2025-04-07

> http://arxiv.org/abs/2504.04726v1

Hard negative samples can accelerate model convergence and optimize decision
boundaries, which is key to improving the performance of recommender systems.
Although large language models (LLMs) possess strong semantic understanding and
generation capabilities, systematic research has not yet been conducted on how
to generate hard negative samples effectively. To fill this gap, this paper
introduces the concept of Semantic Negative Sampling and exploreshow to
optimize LLMs for high-quality, hard negative sampling. Specifically, we design
an experimental pipeline that includes three main modules, profile generation,
semantic negative sampling, and semantic alignment, to verify the potential of
LLM-driven hard negative sampling in enhancing the accuracy of collaborative
filtering (CF). Experimental results indicate that hard negative samples
generated based on LLMs, when semantically aligned and integrated into CF, can
significantly improve CF performance, although there is still a certain gap
compared to traditional negative sampling methods. Further analysis reveals
that this gap primarily arises from two major challenges: noisy samples and
lack of behavioral constraints. To address these challenges, we propose a
framework called HNLMRec, based on fine-tuning LLMs supervised by collaborative
signals. Experimental results show that this framework outperforms traditional
negative sampling and other LLM-driven recommendation methods across multiple
datasets, providing new solutions for empowering traditional RS with LLMs.
Additionally, we validate the excellent generalization ability of the LLM-based
semantic negative sampling method on new datasets, demonstrating its potential
in alleviating issues such as data **sparsity**, popularity bias, and the problem
of false hard negative samples. Our implementation code is available at
https://github.com/user683/HNLMRec.


## TactileNet Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment

>Authors: Adnan Khan, Alireza Choubineh, Mai A. Shaaban, Abbas Akkasi, Majid Komeili

>2025-04-07

> http://arxiv.org/abs/2504.04722v1

Tactile graphics are essential for providing access to visual information for
the 43 million people globally living with vision loss, as estimated by global
prevalence data. However, traditional methods for creating these tactile
graphics are labor-intensive and struggle to meet demand. We introduce
TactileNet, the first comprehensive dataset and AI-driven framework for
generating tactile graphics using text-to-image Stable Diffusion (SD) models.
By integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes
SD models to produce high-fidelity, guideline-compliant tactile graphics while
reducing computational costs. Evaluations involving tactile experts show that
generated graphics achieve 92.86% adherence to tactile standards and 100%
alignment with natural images in posture and features. Our framework also
demonstrates scalability, generating 32,000 images (7,050 filtered for quality)
across 66 classes, with prompt editing enabling customizable outputs (e.g.,
adding/removing details). Our work empowers designers to focus on refinement,
significantly accelerating accessibility efforts. It underscores the
transformative potential of AI for social good, offering a scalable solution to
bridge the accessibility gap in education and beyond.


## Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs

>Authors: Will Cai, Tianneng Shi, Xuandong Zhao, Dawn Song

>2025-04-07

> http://arxiv.org/abs/2504.04715v1

The proliferation of Large Language Models (LLMs) accessed via black-box APIs
introduces a significant trust challenge: users pay for services based on
advertised model capabilities (e.g., size, performance), but providers may
covertly substitute the specified model with a cheaper, lower-quality
alternative to reduce operational costs. This lack of transparency undermines
fairness, erodes trust, and complicates reliable benchmarking. Detecting such
substitutions is difficult due to the black-box nature, typically limiting
interaction to input-output queries. This paper formalizes the problem of model
substitution detection in LLM APIs. We systematically evaluate existing
verification techniques, including output-based statistical tests, benchmark
evaluations, and log probability analysis, under various realistic attack
scenarios like model **quantization**, randomized substitution, and benchmark
evasion. Our findings reveal the limitations of methods relying solely on text
outputs, especially against subtle or adaptive attacks. While log probability
analysis offers stronger guarantees when available, its accessibility is often
limited. We conclude by discussing the potential of hardware-based solutions
like Trusted Execution Environments (TEEs) as a pathway towards provable model
integrity, highlighting the trade-offs between security, performance, and
provider adoption. Code is available at
https://github.com/sunblaze-ucb/llm-api-audit


## LagKV Lag-Relative Information of the KV Cache Tells Which Tokens Are Important

>Authors: Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li

>2025-04-07

> http://arxiv.org/abs/2504.04704v1

The increasing size of the Key-Value (**KV**) cache during the Large Language
Models long-context inference is the main obstacle for its balance between the
deployment cost and task accuracy. To reduce the **KV** cache size in such
scenarios, most previous efforts leveraged on the attention weight to evict
non-critical cache tokens. But there is a trade-off in those methods, they
usually require major modifiation of the inference infrastructure and
significant computation overhead. Base on the fact that the Large Lanuage
models are autoregresssive models, we propose {\it Lag**KV**}, a **KV** allocation
strategy only relying on straight forward comparison among **KV** themself. It is a
totally attention free method which offers easy integration to the main stream
inference platform and comparable performance comparing to other complicated **KV**
compression methods. Results on LongBench and PasskeyRetrieval show that, our
approach achieves nearly zero loss when the ratio is $2\times$ and $\approx
90\%$ of the original model performance for $8\times$. Especially in the
64-digit passkey retrieval task, our mehod outperforms the attention weight
based method $H_2O$ over $60\%$ with same compression ratios. Our code is
available at \url{https://github.com/AI-Lab-China-Merchants-Bank/Lag**KV**}.

