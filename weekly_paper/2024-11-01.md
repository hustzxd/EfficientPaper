# 2024-11-01

# Table of Contents
* [Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning](#Real-Time-Personalization-for-LLM-based-Recommendation-with-Customized-In-Context-Learning)
* [BUZZ Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference](#BUZZ-Beehive-structured-Sparse-KV-Cache-with-Segmented-Heavy-Hitters-for-Efficient-LLM-Inference)
* [Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback](#Online-Intrinsic-Rewards-for-Decision-Making-Agents-from-Large-Language-Model-Feedback)
* [DisenTS Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting](#DisenTS-Disentangled-Channel-Evolving-Pattern-Modeling-for-Multivariate-Time-Series-Forecasting)
* [Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis](#Epipolar-Free-3D-Gaussian-Splatting-for-Generalizable-Novel-View-Synthesis)
* [Wavelet Burst Accumulation for turbulence mitigation](#Wavelet-Burst-Accumulation-for-turbulence-mitigation)
* [MALoRA Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning](#MALoRA-Mixture-of-Asymmetric-Low-Rank-Adaptation-for-Enhanced-Multi-Task-Learning)
* [ETOEfficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses](#ETOEfficient-Transformer-based-Local-Feature-Matching-by-Organizing-Multiple-Homography-Hypotheses)
* [Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem](#Automatic-programming-via-large-language-models-with-population-self-evolution-for-dynamic-job-shop-scheduling-problem)
* [Fast and reliable atom transport by optical tweezers](#Fast-and-reliable-atom-transport-by-optical-tweezers)


## Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning

>Authors: Keqin Bao, Ming Yan, Yang Zhang, Jizhi Zhang, Wenjie Wang, Fuli Feng, Xiangnan He

>2024-10-30

> http://arxiv.org/abs/2410.23136v1

Frequently updating Large Language Model (LLM)-based recommender systems to
adapt to new user interests -- as done for traditional ones -- is impractical
due to high training costs, even with **acceleration** methods. This work explores
adapting to dynamic user interests without any model updates by leveraging
In-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot
examples provided in the input. Using new-interest examples as the ICL few-shot
examples, LLMs may learn real-time interest directly, avoiding the need for
model updates. However, existing LLM-based recommenders often lose the
in-context learning ability during recommendation tuning, while the original
LLM's in-context learning lacks recommendation-specific focus. To address this,
we propose RecICL, which customizes recommendation-specific in-context learning
for real-time recommendations. RecICL organizes training examples in an
in-context learning format, ensuring that in-context learning ability is
preserved and aligned with the recommendation task during tuning.
  Extensive experiments demonstrate RecICL's effectiveness in delivering
real-time recommendations without requiring model updates. Our code is
available at https://github.com/ym689/rec_icl.


## BUZZ Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference

>Authors: Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He

>2024-10-30

> http://arxiv.org/abs/2410.23079v1

Large language models (LLMs) are essential in natural language processing but
often struggle with inference speed and computational efficiency, limiting
real-time deployment. The key-value (KV) cache mechanism reduces computational
overhead in transformer models, but challenges in maintaining contextual
understanding remain. In this paper, we propose BUZZ, a novel KV caching
algorithm that leverages structured contextual information to minimize cache
memory usage while enhancing inference speed. BUZZ employs a beehive-structured
**sparse** cache, incorporating a sliding window to capture recent information and
dynamically segmenting historical tokens into chunks to prioritize important
tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:
CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ
(1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while
maintaining over 99% accuracy in long-text summarization, and (2) surpasses
state-of-the-art performance in multi-document question answering by
$\textbf{7.69%}$ under the same memory limit, where full cache methods
encounter out-of-memory issues. Additionally, BUZZ achieves significant
inference speedup with a $\log{n}$ time complexity. The code is available at
https://github.com/JunqiZhao888/buzz-llm.


## Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback

>Authors: Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos

>2024-10-30

> http://arxiv.org/abs/2410.23022v1

Automatically synthesizing dense rewards from natural language descriptions
is a promising paradigm in reinforcement learning (RL), with applications to
**sparse** reward problems, open-ended exploration, and hierarchical skill design.
Recent works have made promising steps by exploiting the prior knowledge of
large language models (LLMs). However, these approaches suffer from important
limitations: they are either not scalable to problems requiring billions of
environment samples; or are limited to reward functions expressible by compact
code, which may require source code and have difficulty capturing nuanced
semantics; or require a diverse offline dataset, which may not exist or be
impossible to collect. In this work, we address these limitations through a
combination of algorithmic and systems-level contributions. We propose ONI, a
distributed architecture that simultaneously learns an RL policy and an
intrinsic reward function using LLM feedback. Our approach annotates the
agent's collected experience via an asynchronous LLM server, which is then
distilled into an intrinsic reward model. We explore a range of algorithmic
choices for reward modeling with varying complexity, including hashing,
classification, and ranking models. By studying their relative tradeoffs, we
shed light on questions regarding intrinsic reward design for **sparse** reward
problems. Our approach achieves state-of-the-art performance across a range of
challenging, **sparse** reward tasks from the NetHack Learning Environment in a
simple unified process, solely using the agent's gathered experience, without
requiring external datasets nor source code. We make our code available at
\url{URL} (coming soon).


## DisenTS Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting

>Authors: Zhiding Liu, Jiqian Yang, Qingyang Mao, Yuze Zhao, Mingyue Cheng, Zhi Li, Qi Liu, Enhong Chen

>2024-10-30

> http://arxiv.org/abs/2410.22981v1

Multivariate time series forecasting plays a crucial role in various
real-world applications. Significant efforts have been made to integrate
advanced network architectures and training strategies that enhance the capture
of temporal dependencies, thereby improving forecasting accuracy. On the other
hand, mainstream approaches typically utilize a single unified model with
simplistic channel-mixing embedding or cross-channel attention operations to
account for the critical intricate inter-channel dependencies. Moreover, some
methods even trade capacity for robust prediction based on the
channel-independent assumption. Nonetheless, as time series data may display
distinct evolving patterns due to the unique characteristics of each channel
(including multiple strong seasonalities and trend changes), the unified
modeling methods could yield suboptimal results. To this end, we propose
DisenTS, a tailored framework for modeling disentangled channel evolving
patterns in general multivariate time series forecasting. The central idea of
DisenTS is to model the potential diverse patterns within the multivariate time
series data in a decoupled manner. Technically, the framework employs multiple
distinct forecasting models, each tasked with uncovering a unique evolving
pattern. To guide the learning process without supervision of pattern
partition, we introduce a novel Forecaster Aware Gate (FAG) module that
generates the routing signals adaptively according to both the forecasters'
states and input series' characteristics. The forecasters' states are derived
from the Linear Weight Approximation (LWA) strategy, which **quantize**s the
complex deep neural networks into compact matrices. Additionally, the
Similarity Constraint (SC) is further proposed to guide each model to
specialize in an underlying pattern by minimizing the mutual information
between the representations.


## Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis

>Authors: Zhiyuan Min, Yawei Luo, Jianwen Sun, Yi Yang

>2024-10-30

> http://arxiv.org/abs/2410.22817v2

Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from
**sparse**-view observations in a feed-forward inference manner, eliminating the
need for scene-specific retraining required in conventional 3DGS. However,
existing methods rely heavily on epipolar priors, which can be unreliable in
complex realworld scenes, particularly in non-overlapping and occluded regions.
In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based
model for generalizable novel view synthesis that operates independently of
epipolar line constraints. To enhance multiview feature extraction with 3D
perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view
completion pre-training on large-scale datasets. Additionally, we introduce an
Iterative Cross-view Gaussians Alignment method to ensure consistent depth
scales across different views. Our eFreeSplat represents an innovative approach
for generalizable novel view synthesis. Different from the existing pure
geometry-free methods, eFreeSplat focuses more on achieving epipolar-free
feature matching and encoding by providing 3D priors through cross-view
pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks
using the RealEstate10K and ACID datasets. Extensive experiments demonstrate
that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar
priors, achieving superior geometry reconstruction and novel view synthesis
quality. Project page: https://tatakai1.github.io/efreesplat/.


## Wavelet Burst Accumulation for turbulence mitigation

>Authors: Jerome Gilles, Stanley Osher

>2024-10-30

> http://arxiv.org/abs/2410.22802v1

In this paper, we investigate the extension of the recently proposed weighted
Fourier burst accumulation (FBA) method into the wavelet domain. The purpose of
FBA is to reconstruct a clean and sharp image from a sequence of blurred
frames. This concept lies in the construction of weights to amplify dominant
frequencies in the Fourier spectrum of each frame. The reconstructed image is
then obtained by taking the inverse Fourier transform of the average of all
processed spectra. In this paper, we first suggest to replace the rigid
registration step used in the original algorithm by a non-rigid registration in
order to be able to process sequences acquired through atmospheric turbulence.
Second, we propose to work in a wavelet domain instead of the Fourier one. This
leads us to the construction of two types of algorithms. Finally, we propose an
alternative approach to replace the weighting idea by an approach promoting the
**sparsity** in the used space. Several experiments are provided to illustrate the
efficiency of the proposed methods.


## MALoRA Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning

>Authors: Xujia Wang, Haiyan Zhao, Shuo Wang, Hanqing Wang, Zhiyuan Liu

>2024-10-30

> http://arxiv.org/abs/2410.22782v1

Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly
improved the adaptation of LLMs to downstream tasks in a resource-efficient
manner. However, in multi-task scenarios, challenges such as training imbalance
and the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which
combines LoRA with **sparse** Mixture-of-Experts, mitigates some of these issues by
promoting task-specific learning across experts. Despite this, MoLoRA remains
inefficient in terms of training speed, parameter utilization, and overall
multi-task performance. In this paper, we propose Mixture of Asymmetric
Low-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages
asymmetric optimization across LoRA experts. MALoRA reduces the number of
trainable parameters by 30% to 48%, increases training speed by 1.2x, and
matches the computational efficiency of single-task LoRA models. Additionally,
MALoRA addresses overfitting issues commonly seen in high-rank configurations,
enhancing performance stability. Extensive experiments across diverse
multi-task learning scenarios demonstrate that MALoRA consistently outperforms
all baseline methods in both inter-domain and intra-domain tasks.


## ETOEfficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses

>Authors: Junjie Ni, Guofeng Zhang, Guanglin Li, Yijin Li, Xinyang Liu, Zhaoyang Huang, Hujun Bao

>2024-10-30

> http://arxiv.org/abs/2410.22733v2

We tackle the efficiency problem of learning local feature matching. Recent
advancements have given rise to purely CNN-based and transformer-based
approaches, each augmented with deep learning techniques. While CNN-based
methods often excel in matching speed, transformer-based methods tend to
provide more accurate matches. We propose an efficient transformer-based
network architecture for local feature matching. This technique is built on
constructing multiple homography hypotheses to approximate the continuous
correspondence in the real world and uni-directional cross-attention to
accelerate the refinement. On the YFCC100M dataset, our matching accuracy is
competitive with LoFTR, a state-of-the-art transformer-based architecture,
while the inference speed is boosted to 4 times, even outperforming the
CNN-based methods. Comprehensive evaluations on other open datasets such as
Megadepth, ScanNet, and HPatches demonstrate our method's efficacy,
highlighting its potential to significantly enhance a wide array of downstream
applications.


## Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem

>Authors: Jin Huang, Xinyu Li, Liang Gao, Qihao Liu, Yue Teng

>2024-10-30

> http://arxiv.org/abs/2410.22657v1

Heuristic dispatching rules (HDRs) are widely regarded as effective methods
for solving dynamic job shop scheduling problems (DJSSP) in real-world
production environments. However, their performance is highly
scenario-dependent, often requiring expert customization. To address this,
genetic programming (GP) and gene expression programming (GEP) have been
extensively used for automatic algorithm design. Nevertheless, these approaches
often face challenges due to high randomness in the search process and limited
generalization ability, hindering the application of trained dispatching rules
to new scenarios or dynamic environments. Recently, the integration of large
language models (LLMs) with evolutionary algorithms has opened new avenues for
prompt engineering and automatic algorithm design. To enhance the capabilities
of LLMs in automatic HDRs design, this paper proposes a novel population
self-evolutionary (SeEvo) method, a general search framework inspired by the
self-reflective design strategies of human experts. The SeEvo method
accelerates the search process and enhances exploration capabilities.
Experimental results show that the proposed SeEvo method outperforms GP, GEP,
end-to-end deep reinforcement learning methods, and more than 10 common HDRs
from the literature, particularly in unseen and dynamic scenarios.


## Fast and reliable atom transport by optical tweezers

>Authors: Sunhwa Hwang, Hansub Hwang, Kangjin Kim, Andrew Byun, Seokho Jeong, Maynardo Pratama Soegianto, Jaewook Ahn

>2024-10-30

> http://arxiv.org/abs/2410.22627v1

Movable single atoms have drawn significant attention for their potentials as
flying quantum memory in non-local, dynamic quantum computing architectures.
However, when dynamic optical tweezers are employed to control atoms
opto-mechanically, conventional methods such as adiabatic controls and constant
jerk controls are either inherently slow or induce mechanical heating, leading
to atom loss over long distances or at high speeds. To address these
challenges, we explore the method known as shortcuts to adiabaticity (STA) as
an efficient alternative for fast and reliable atom transport control. We
present a series of proof-of-concept experiments demonstrating that STA-based
optical tweezer trajectories can achieve both rapid and reliable single-atom
transport. These experiments include moving atoms between two locations,
adjusting speeds en route, and navigating curved trajectories. Our results
indicate that atoms can be transported with a constant **acceleration** on average
over distances that is only limited by trap lifetime, while effectively
suppressing vibrational heating. This makes STA methods particularly
well-suited for long-distance atom transport, potentially spanning distances
over centimeter scales, such as between quantum information devices.

