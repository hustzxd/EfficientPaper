# 2024-09-24

# Table of Contents
* [GRIN: GRadient-INformed MoE](#GRIN:-GRadient-INformed-MoE)
* [WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification](#WMCodec:-End-to-End-Neural-Speech-Codec-with-Deep-Watermarking-for-Authenticity-Verification)
* [Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference](#Low-Frame-rate-Speech-Codec:-a-Codec-Designed-for-Fast-High-quality-Speech-LLM-Training-and-Inference)
* [Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics](#Multi-Grid-Graph-Neural-Networks-with-Self-Attention-for-Computational-Mechanics)
* [Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview](#Art-and-Science-of-Quantizing-Large-Scale-Models:-A-Comprehensive-Overview)


## GRIN: GRadient-INformed MoE

>Authors: Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen

>2024-09-18

> http://arxiv.org/abs/2409.12136v1

Mixture-of-Experts (MoE) models scale more effectively than dense models due
to sparse computation through expert routing, selectively activating only a
small subset of expert modules. However, sparse computation challenges
traditional training practices, as discrete expert routing hinders standard
backpropagation and thus gradient-based optimization, which are the cornerstone
of deep learning. To better pursue the scaling power of MoE, we introduce GRIN
(GRadient-INformed MoE training), which incorporates sparse gradient estimation
for expert routing and configures model parallelism to avoid token dropping.
Applying GRIN to autoregressive language modeling, we develop a top-2
16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,
outperforms a 7B dense model and matches the performance of a 14B dense model
trained on the same data. Extensive evaluations across diverse tasks
demonstrate the potential of GRIN to significantly enhance MoE efficacy,
achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.


## WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification

>Authors: Junzuo Zhou, Jiangyan Yi, Yong Ren, Jianhua Tao, Tao Wang, Chu Yuan Zhang

>2024-09-18

> http://arxiv.org/abs/2409.12121v2

Recent advances in speech spoofing necessitate stronger verification
mechanisms in neural speech codecs to ensure authenticity. Current methods
embed numerical watermarks before compression and extract them from
reconstructed speech for verification, but face limitations such as separate
training processes for the watermark and codec, and insufficient cross-modal
information integration, leading to reduced watermark imperceptibility,
extraction accuracy, and capacity. To address these issues, we propose WMCodec,
the first neural speech codec to jointly train compression-reconstruction and
watermark embedding-extraction in an end-to-end manner, optimizing both
imperceptibility and extractability of the watermark. Furthermore, We design an
iterative Attention Imprint Unit (AIU) for deeper feature integration of
watermark and speech, reducing the impact of quantization noise on the
watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec
in most quality metrics for watermark imperceptibility and consistently exceeds
both AudioSeal with Encodec and reinforced TraceableSpeech in extraction
accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16
bps, WMCodec maintains over 99% extraction accuracy under common attacks,
demonstrating strong robustness.


## Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference

>Authors: Edresson Casanova, Ryan Langman, Paarth Neekhara, Shehzeen Hussain, Jason Li, Subhankar Ghosh, Ante JukiÄ‡, Sang-gil Lee

>2024-09-18

> http://arxiv.org/abs/2409.12117v1

Large language models (LLMs) have significantly advanced audio processing
through audio codecs that convert audio into discrete tokens, enabling the
application of language modeling techniques to audio data. However, audio
codecs often operate at high frame rates, resulting in slow training and
inference, especially for autoregressive models. To address this challenge, we
present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that
leverages finite scalar quantization and adversarial training with large speech
language models to achieve high-quality audio compression with a 1.89 kbps
bitrate and 21.5 frames per second. We demonstrate that our novel codec can
make the inference of LLM-based text-to-speech models around three times faster
while improving intelligibility and producing quality comparable to previous
models.


## Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics

>Authors: Paul Garnier, Jonathan Viquerat, Elie Hachem

>2024-09-18

> http://arxiv.org/abs/2409.11899v1

Advancement in finite element methods have become essential in various
disciplines, and in particular for Computational Fluid Dynamics (CFD), driving
research efforts for improved precision and efficiency. While Convolutional
Neural Networks (CNNs) have found success in CFD by mapping meshes into images,
recent attention has turned to leveraging Graph Neural Networks (GNNs) for
direct mesh processing. This paper introduces a novel model merging
Self-Attention with Message Passing in GNNs, achieving a 15\% reduction in RMSE
on the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh
pruning technique based on Self-Attention is proposed, that leads to a robust
GNN-based multigrid approach, also reducing RMSE by 15\%. Additionally, a new
self-supervised training method based on BERT is presented, resulting in a 25\%
RMSE reduction. The paper includes an ablation study and outperforms
state-of-the-art models on several challenging datasets, promising advancements
similar to those recently achieved in natural language and image processing.
Finally, the paper introduces a dataset with meshes larger than existing ones
by at least an order of magnitude. Code and Datasets will be released at
https://github.com/DonsetPG/multigrid-gnn.


## Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview

>Authors: Yanshu Wang, Tong Yang, Xiyan Liang, Guoan Wang, Hanning Lu, Xu Zhe, Yaoming Li, Li Weitao

>2024-09-18

> http://arxiv.org/abs/2409.11650v1

This paper provides a comprehensive overview of the principles, challenges,
and methodologies associated with quantizing large-scale neural network models.
As neural networks have evolved towards larger and more complex architectures
to address increasingly sophisticated tasks, the computational and energy costs
have escalated significantly. We explore the necessity and impact of model size
growth, highlighting the performance benefits as well as the computational
challenges and environmental considerations. The core focus is on model
quantization as a fundamental approach to mitigate these challenges by reducing
model size and improving efficiency without substantially compromising
accuracy. We delve into various quantization techniques, including both
post-training quantization (PTQ) and quantization-aware training (QAT), and
analyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),
ZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine
how these methods address issues like outliers, importance weighting, and
activation quantization, ultimately contributing to more sustainable and
accessible deployment of large-scale models.

