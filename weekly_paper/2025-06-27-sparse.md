# 2025-06-27 Sparse LLM Papers

## TopK Language Models

>Authors: Ryosuke Takahashi, Tatsuro Inaba, Kentaro Inui, Benjamin Heinzerling

>2025-06-26

> http://arxiv.org/abs/2506.21468v1

Sparse autoencoders (SAEs) have become an important tool for analyzing and interpreting the activation space of transformer-based language models (LMs). This paper introduces a modification to the transformer architecture that incorporates a TopK activation function at chosen layers, making the model's hidden states equivalent to the latent features of a TopK SAE. The resulting TopK LMs maintain their original capabilities while providing robust interpretability benefits and enabling successful steering through targeted neuron interventions.

## DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs

>Authors: Ruokai Yin, Yuhang Li, Donghyun Lee, Priyadarshini Panda

>2025-06-25

> http://arxiv.org/abs/2506.20194v1

DuoGPT proposes a unified framework that constructs dual-sparse (spMspV) workloads by combining unstructured weight pruning with activation sparsity. The framework extends the Optimal Brain Compression (OBC) with activation-aware calibration and introduces output residuals from the dense model as correction terms. Evaluations on LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39Ã— compared to the baseline dense model.

## Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV Management on a Single Commodity GPU

>Authors: He Sun, Li Li, Mingjun Xiao, Chengzhong Xu

>2025-06-25

> http://arxiv.org/abs/2506.20187v1

This paper presents LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. The system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads.

## GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching

>Authors: Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping

>2025-06-25

> http://arxiv.org/abs/2506.20480v1

GPTailor develops a novel strategy to compress models by strategically combining or merging layers from finetuned model variants. The approach leads to competitive model pruning - for the Llama2-13B model families, their compressed models maintain approximately 97.3% of the original performance while removing ~25% of parameters, significantly outperforming previous state-of-the-art methods.

## Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models

>Authors: Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song

>2025-06-25

> http://arxiv.org/abs/2506.20251v1

This paper presents comprehensive safety evaluations across various mainstream quantization techniques and diverse calibration datasets. The authors propose a quantization-aware safety patching framework, Q-resafe, to efficiently restore the safety capabilities of quantized LLMs while minimizing any adverse impact on utility.

## DipSVD: Dual-importance Protected SVD for Efficient LLM Compression

>Authors: Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu

>2025-06-25

> http://arxiv.org/abs/2506.20353v1

DipSVD proposes a dual-level importance protection mechanism to enhance SVD-based compression methods for LLMs: (1) local importance protection: preserving critical singular vectors through channel-weighted data whitening; and (2) global importance protection: enabling less important layers to bear greater compression burden. The method outperforms existing SVD-based compression approaches across multiple benchmarks.

## Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models

>Authors: Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang

>2025-06-24

> http://arxiv.org/abs/2506.19697v1

This paper introduces Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation in LLM quantization rather than relying on post-hoc mitigation. OSP combines three key innovations including the Muon optimizer that eliminates privileged bases while maintaining model performance. 