# 2025-06-06

# Table of Contents
* [EPiC Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](#EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation)
* [SkipGPT Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](#SkipGPT-Dynamic-Layer-Pruning-Reinvented-with-Token-Awareness-and-Module-Decoupling)
* [FlexGS Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting](#FlexGS-Train-Once,-Deploy-Everywhere-with-Many-in-One-Flexible-3D-Gaussian-Splatting)
* [Rectified Sparse Attention](#Rectified-Sparse-Attention)
* [Structured Pruning for Diverse Best-of-N Reasoning Optimization](#Structured-Pruning-for-Diverse-Best-of-N-Reasoning-Optimization)
* [Magic Mushroom A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](#Magic-Mushroom-A-Customizable-Benchmark-for-Fine-grained-Analysis-of-Retrieval-Noise-Erosion-in-RAG-Systems)
* [STAR Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization](#STAR-Learning-Diverse-Robot-Skill-Abstractions-through-Rotation-Augmented-Vector-Quantization)
* [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](#Unifying-Uniform-and-Binary-coding-Quantization-for-Accurate-Compression-of-Large-Language-Models)
* [AhaKV Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](#AhaKV-Adaptive-Holistic-Attention-Driven-KV-Cache-Eviction-for-Efficient-Inference-of-Large-Language-Models)
* [FSHNet Fully Sparse Hybrid Network for 3D Object Detection](#FSHNet-Fully-Sparse-Hybrid-Network-for-3D-Object-Detection)
* [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](#Learning-at-Criticality-in-Large-Language-Models-for-Quantum-Field-Theory-and-Beyond)
* [AdaDecode Accelerating LLM Decoding with Adaptive Layer Parallelism](#AdaDecode-Accelerating-LLM-Decoding-with-Adaptive-Layer-Parallelism)
* [Scaling Transformers for Discriminative Recommendation via Generative Pretraining](#Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining)
* [DSSAU-NetU-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation](#DSSAU-NetU-Shaped-Hybrid-Network-for-Pubic-Symphysis-and-Fetal-Head-Segmentation)
* [Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI](#Analyzing-Transformer-Models-and-Knowledge-Distillation-Approaches-for-Image-Captioning-on-Edge-AI)
* [POSS Position Specialist Generates Better Draft for Speculative Decoding](#POSS-Position-Specialist-Generates-Better-Draft-for-Speculative-Decoding)
* [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](#Accurate-Sublayer-Pruning-for-Large-Language-Models-by-Exploiting-Latency-and-Tunability-Information)
* [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](#Mitigating-Non-IID-Drift-in-Zeroth-Order-Federated-LLM-Fine-Tuning-with-Transferable-Sparsity)
* [Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs](#Parallel-CPU-GPU-Execution-for-LLM-Inference-on-Constrained-GPUs)
* [Chipmunk Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](#Chipmunk-Training-Free-Acceleration-of-Diffusion-Transformers-with-Dynamic-Column-Sparse-Deltas)
* [HumanRAM Feed-forward Human Reconstruction and Animation Model using Transformers](#HumanRAM-Feed-forward-Human-Reconstruction-and-Animation-Model-using-Transformers)
* [Controllable Human-centric Keyframe Interpolation with Generative Prior](#Controllable-Human-centric-Keyframe-Interpolation-with-Generative-Prior)
* [TalkingMachines Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](#TalkingMachines-Real-Time-Audio-Driven-FaceTime-Style-Video-via-Autoregressive-Diffusion-Models)
* [StreamBP Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](#StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs)
* [Sparse-vDiT Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](#Sparse-vDiT-Unleashing-the-Power-of-Sparse-Attention-to-Accelerate-Video-Diffusion-Transformers)
* [Towards Analyzing and Understanding the Limitations of VAPO A Theoretical Perspective](#Towards-Analyzing-and-Understanding-the-Limitations-of-VAPO-A-Theoretical-Perspective)
* [Hybrid deep learning and iterative methods for accelerated solutions of viscous incompressible flow](#Hybrid-deep-learning-and-iterative-methods-for-accelerated-solutions-of-viscous-incompressible-flow)
* [Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich Representation](#Controllable-Text-to-Speech-Synthesis-with-Masked-Autoencoded-Style-Rich-Representation)
* [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](#Computation--and-Communication-Efficient-Online-FL-for-Resource-Constrained-Aerial-Vehicles)
* [PC-MoE Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](#PC-MoE-Memory-Efficient-and-Privacy-Preserving-Collaborative-Training-for-Mixture-of-Experts-LLMs)
* [Adaptive Graph Pruning for Multi-Agent Communication](#Adaptive-Graph-Pruning-for-Multi-Agent-Communication)
* [Memory-Efficient Split Federated Learning for LLM Fine-Tuning on Heterogeneous Mobile Devices](#Memory-Efficient-Split-Federated-Learning-for-LLM-Fine-Tuning-on-Heterogeneous-Mobile-Devices)
* [CLONE Customizing LLMs for Efficient Latency-Aware Inference at the Edge](#CLONE-Customizing-LLMs-for-Efficient-Latency-Aware-Inference-at-the-Edge)
* [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](#Rethinking-Dynamic-Networks-and-Heterogeneous-Computing-with-Automatic-Parallelization)
* [Reuse or Generate? Accelerating Code Editing via Edit-Oriented Speculative Decoding](#Reuse-or-Generate?-Accelerating-Code-Editing-via-Edit-Oriented-Speculative-Decoding)
* [XicorAttention Time Series Transformer Using Attention with Nonlinear Correlation](#XicorAttention-Time-Series-Transformer-Using-Attention-with-Nonlinear-Correlation)
* [Human-In-The-Loop Workflow for Neuro- Symbolic Scholarly Knowledge Organization](#Human-In-The-Loop-Workflow-for-Neuro--Symbolic-Scholarly-Knowledge-Organization)
* [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](#A-Pretrained-Probabilistic-Transformer-for-City-Scale-Traffic-Volume-Prediction)
* [KVCache Cache in the Wild Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](#KVCache-Cache-in-the-Wild-Characterizing-and-Optimizing-KVCache-Cache-at-a-Large-Cloud-Provider)
* [HATA Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](#HATA-Trainable-and-Hardware-Efficient-Hash-Aware-Top-k-Attention-for-Scalable-Large-Model-Inference)
* [Pruning General Large Language Models into Customized Expert Models](#Pruning-General-Large-Language-Models-into-Customized-Expert-Models)
* [Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention](#Hardware-Centric-Analysis-of-DeepSeek's-Multi-Head-Latent-Attention)
* [AURA Agentic Upskilling via Reinforced Abstractions](#AURA-Agentic-Upskilling-via-Reinforced-Abstractions)
* [Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning Quantization-Assisted Min-Max Fair Scheduling](#Enhancing-Convergence,-Privacy-and-Fairness-for-Wireless-Personalized-Federated-Learning-Quantization-Assisted-Min-Max-Fair-Scheduling)
* [Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology](#Revisiting-End-to-End-Learning-with-Slide-level-Supervision-in-Computational-Pathology)
* [Random at First, Fast at Last NTK-Guided Fourier Pre-Processing for Tabular DL](#Random-at-First,-Fast-at-Last-NTK-Guided-Fourier-Pre-Processing-for-Tabular-DL)
* [OThink-R1 Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](#OThink-R1-Intrinsic-Fast/Slow-Thinking-Mode-Switching-for-Over-Reasoning-Mitigation)
* [Consultant Decoding Yet Another Synergistic Mechanism](#Consultant-Decoding-Yet-Another-Synergistic-Mechanism)
* [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](#Reconciling-Hessian-Informed-Acceleration-and-Scalar-Only-Communication-for-Efficient-Federated-Zeroth-Order-Fine-Tuning)
* [A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer](#A-TRPCA-Inspired-Deep-Unfolding-Network-for-Hyperspectral-Image-Denoising-via-Thresholded-t-SVD-and-Top-K-Sparse-Transformer)
* [Angles Don't Lie Unlocking Training-Efficient RL Through the Model's Own Signals](#Angles-Don't-Lie-Unlocking-Training-Efficient-RL-Through-the-Model's-Own-Signals)
* [Improving compiler support for SIMD offload using Arm Streaming SVE](#Improving-compiler-support-for-SIMD-offload-using-Arm-Streaming-SVE)
* [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](#A-Dynamic-Framework-for-Semantic-Grouping-of-Common-Data-Elements-(CDE)-Using-Embeddings-and-Clustering)
* [HENT-SRT Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](#HENT-SRT-Hierarchical-Efficient-Neural-Transducer-with-Self-Distillation-for-Joint-Speech-Recognition-and-Translation)
* [GLoSS Generative Language Models with Semantic Search for Sequential Recommendation](#GLoSS-Generative-Language-Models-with-Semantic-Search-for-Sequential-Recommendation)
* [Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts](#Memory-Access-Characterization-of-Large-Language-Models-in-CPU-Environment-and-its-Potential-Impacts)
* [Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming](#Efficient-Learning-of-Balanced-Signed-Graphs-via-Sparse-Linear-Programming)
* [A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](#A-Study-on-the-MCP-x-A2A-Framework-for-Enhancing-Interoperability-of-LLM-based-Autonomous-Agents)
* [Science Prospects for the Southern Wide-field Gamma-ray Observatory SWGO](#Science-Prospects-for-the-Southern-Wide-field-Gamma-ray-Observatory-SWGO)
* [Benford's Curse Tracing Digit Bias to Numerical Hallucination in LLMs](#Benford's-Curse-Tracing-Digit-Bias-to-Numerical-Hallucination-in-LLMs)
* [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](#Enhancing-Speech-Emotion-Recognition-with-Graph-Based-Multimodal-Fusion-and-Prosodic-Features-for-the-Speech-Emotion-Recognition-in-Naturalistic-Conditions-Challenge-at-Interspeech-2025)
* [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](#Assigning-Distinct-Roles-to-Quantized-and-Low-Rank-Matrices-Toward-Optimal-Weight-Decomposition)
* [Sparse Imagination for Efficient Visual World Model Planning](#Sparse-Imagination-for-Efficient-Visual-World-Model-Planning)
* [ThinkEval Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs](#ThinkEval-Practical-Evaluation-of-Knowledge-Preservation-and-Consistency-in-LLM-Editing-with-Thought-based-Knowledge-Graphs)
* [Generative Next POI Recommendation with Semantic ID](#Generative-Next-POI-Recommendation-with-Semantic-ID)
* [TAH-QUANT Effective Activation Quantization in Pipeline Parallelism over Slow Network](#TAH-QUANT-Effective-Activation-Quantization-in-Pipeline-Parallelism-over-Slow-Network)
* [Evaluating Large Language Models in Crisis Detection A Real-World Benchmark from Psychological Support Hotlines](#Evaluating-Large-Language-Models-in-Crisis-Detection-A-Real-World-Benchmark-from-Psychological-Support-Hotlines)
* [Energy Considerations for Large Pretrained Neural Networks](#Energy-Considerations-for-Large-Pretrained-Neural-Networks)
* [Mamba Drafters for Speculative Decoding](#Mamba-Drafters-for-Speculative-Decoding)
* [ReTern Exploiting Natural Redundancy and Sign Transformations for Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs](#ReTern-Exploiting-Natural-Redundancy-and-Sign-Transformations-for-Enhanced-Fault-Tolerance-in-Compute-in-Memory-based-Ternary-LLMs)
* [TRUST -- Transformer-Driven U-Net for Sparse Target Recovery](#TRUST----Transformer-Driven-U-Net-for-Sparse-Target-Recovery)
* [Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](#Generic-Token-Compression-in-Multimodal-Large-Language-Models-from-an-Explainability-Perspective)
* [Probing Neural Topology of Large Language Models](#Probing-Neural-Topology-of-Large-Language-Models)
* [Graph Neural Networks for Jamming Source Localization](#Graph-Neural-Networks-for-Jamming-Source-Localization)
* [Autoregressive Images Watermarking through Lexical Biasing An Approach Resistant to Regeneration Attack](#Autoregressive-Images-Watermarking-through-Lexical-Biasing-An-Approach-Resistant-to-Regeneration-Attack)
* [FedQuad Adaptive Layer-wise LoRA Deployment and Activation Quantization for Federated Fine-Tuning](#FedQuad-Adaptive-Layer-wise-LoRA-Deployment-and-Activation-Quantization-for-Federated-Fine-Tuning)
* [Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO](#Pilot-Contamination-Aware-Graph-Attention-Network-for-Power-Control-in-CFmMIMO)
* [Unlocking Personalized Knowledge in Federated Large Language Model The Power of Mixture of Experts](#Unlocking-Personalized-Knowledge-in-Federated-Large-Language-Model-The-Power-of-Mixture-of-Experts)
* [Speaking Beyond Language A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](#Speaking-Beyond-Language-A-Large-Scale-Multimodal-Dataset-for-Learning-Nonverbal-Cues-from-Video-Grounded-Dialogues)
* [MedBookVQA A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book](#MedBookVQA-A-Systematic-and-Comprehensive-Medical-Benchmark-Derived-from-Open-Access-Book)
* [LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery](#LLM-Cannot-Discover-Causality,-and-Should-Be-Restricted-to-Non-Decisional-Support-in-Causal-Discovery)
* [LIFT the Veil for the Truth Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](#LIFT-the-Veil-for-the-Truth-Principal-Weights-Emerge-after-Rank-Reduction-for-Reasoning-Focused-Supervised-Fine-Tuning)
* [Beyond Attention Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies](#Beyond-Attention-Learning-Spatio-Temporal-Dynamics-with-Emergent-Interpretable-Topologies)
* [Vector fields as a framework for modelling the mobility of commodities](#Vector-fields-as-a-framework-for-modelling-the-mobility-of-commodities)
* [Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers](#Blending-Complementary-Memory-Systems-in-Hybrid-Quadratic-Linear-Transformers)
* [Assortment of Attention Heads Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](#Assortment-of-Attention-Heads-Accelerating-Federated-PEFT-with-Head-Pruning-and-Strategic-Client-Selection)
* [From Local Cues to Global Percepts Emergent Gestalt Organization in Self-Supervised Vision Models](#From-Local-Cues-to-Global-Percepts-Emergent-Gestalt-Organization-in-Self-Supervised-Vision-Models)
* [Optimizing Sensory Neurons Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](#Optimizing-Sensory-Neurons-Nonlinear-Attention-Mechanisms-for-Accelerated-Convergence-in-Permutation-Invariant-Neural-Networks-for-Reinforcement-Learning)
* [SafeTuneBed A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](#SafeTuneBed-A-Toolkit-for-Benchmarking-LLM-Safety-Alignment-in-Fine-Tuning)
* [Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings](#Permutation-Invariant-Transformer-Neural-Architectures-for-Set-Based-Indoor-Localization-Using-Learned-RSSI-Embeddings)
* [Qualitative analysis of a quasi-magnetic universe](#Qualitative-analysis-of-a-quasi-magnetic-universe)
* [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](#Prompt-Tuned-LLM-Augmented-DRL-for-Dynamic-O-RAN-Network-Slicing)
* [ARIA Training Language Agents with Intention-Driven Reward Aggregation](#ARIA-Training-Language-Agents-with-Intention-Driven-Reward-Aggregation)
* [FLoE Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](#FLoE-Fisher-Based-Layer-Selection-for-Efficient-Sparse-Adaptation-of-Low-Rank-Experts)
* [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](#Accelerating-Diffusion-LLMs-via-Adaptive-Parallel-Decoding)
* [Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively](#Speculative-Reward-Model-Boosts-Decision-Making-Ability-of-LLMs-Cost-Effectively)
* [Power-of-Two (PoT) Weights in Large Language Models (LLMs)](#Power-of-Two-(PoT)-Weights-in-Large-Language-Models-(LLMs))
* [MIR Methodology Inspiration Retrieval for Scientific Research Problems](#MIR-Methodology-Inspiration-Retrieval-for-Scientific-Research-Problems)
* [MOFGPT Generative Design of Metal-Organic Frameworks using Language Models](#MOFGPT-Generative-Design-of-Metal-Organic-Frameworks-using-Language-Models)
* [TalkingHeadBench A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection](#TalkingHeadBench-A-Multi-Modal-Benchmark-&-Analysis-of-Talking-Head-DeepFake-Detection)
* [Towards Secure MLOps Surveying Attacks, Mitigation Strategies, and Research Challenges](#Towards-Secure-MLOps-Surveying-Attacks,-Mitigation-Strategies,-and-Research-Challenges)
* [LegalEval-Q A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](#LegalEval-Q-A-New-Benchmark-for-The-Quality-Evaluation-of-LLM-Generated-Legal-Text)
* [TC-GS A Faster Gaussian Splatting Module Utilizing Tensor Cores](#TC-GS-A-Faster-Gaussian-Splatting-Module-Utilizing-Tensor-Cores)
* [Gated Multimodal Graph Learning for Personalized Recommendation](#Gated-Multimodal-Graph-Learning-for-Personalized-Recommendation)
* [EXP-Bench Can AI Conduct AI Research Experiments?](#EXP-Bench-Can-AI-Conduct-AI-Research-Experiments?)
* [AFLoRA Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](#AFLoRA-Adaptive-Federated-Fine-Tuning-of-Large-Language-Models-with-Resource-Aware-Low-Rank-Adaption)
* [SUMO Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](#SUMO-Subspace-Aware-Moment-Orthogonalization-for-Accelerating-Memory-Efficient-LLM-Training)
* [HELM Hyperbolic Large Language Models via Mixture-of-Curvature Experts](#HELM-Hyperbolic-Large-Language-Models-via-Mixture-of-Curvature-Experts)
* [A Simple Linear Patch Revives Layer-Pruned Large Language Models](#A-Simple-Linear-Patch-Revives-Layer-Pruned-Large-Language-Models)
* [Decoding Knowledge Attribution in Mixture-of-Experts A Framework of Basic-Refinement Collaboration and Efficiency Analysis](#Decoding-Knowledge-Attribution-in-Mixture-of-Experts-A-Framework-of-Basic-Refinement-Collaboration-and-Efficiency-Analysis)
* [CREFT Sequential Multi-Agent LLM for Character Relation Extraction](#CREFT-Sequential-Multi-Agent-LLM-for-Character-Relation-Extraction)
* [Cross-Attention Speculative Decoding](#Cross-Attention-Speculative-Decoding)
* [LPASS Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs](#LPASS-Linear-Probes-as-Stepping-Stones-for-vulnerability-detection-using-compressed-LLMs)
* [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](#Model-Unlearning-via-Sparse-Autoencoder-Subspace-Guided-Projections)
* [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning)
* [ReCalKV Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](#ReCalKV-Low-Rank-KV-Cache-Compression-via-Head-Reordering-and-Offline-Calibration)
* [STAR-Net An Interpretable Model-Aided Network for Remote Sensing Image Denoising](#STAR-Net-An-Interpretable-Model-Aided-Network-for-Remote-Sensing-Image-Denoising)
* [GradPower Powering Gradients for Faster Language Model Pre-Training](#GradPower-Powering-Gradients-for-Faster-Language-Model-Pre-Training)
* [Reasoning Can Hurt the Inductive Abilities of Large Language Models](#Reasoning-Can-Hurt-the-Inductive-Abilities-of-Large-Language-Models)
* [CLaSp In-Context Layer Skip for Self-Speculative Decoding](#CLaSp-In-Context-Layer-Skip-for-Self-Speculative-Decoding)
* [Beyond Exponential Decay Rethinking Error Accumulation in Large Language Models](#Beyond-Exponential-Decay-Rethinking-Error-Accumulation-in-Large-Language-Models)
* [SALE  Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](#SALE--Low-bit-Estimation-for-Efficient-Sparse-Attention-in-Long-context-LLM-Prefilling)
* [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](#Adaptive-LoRA-Merge-with-Parameter-Pruning-for-Low-Resource-Generation)
* [S4-Driver Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](#S4-Driver-Scalable-Self-Supervised-Driving-Multimodal-Large-Language-Modelwith-Spatio-Temporal-Visual-Representation)
* [Symmetry-Breaking Magneto-Optical Effects in Altermagnets](#Symmetry-Breaking-Magneto-Optical-Effects-in-Altermagnets)
* [SkyLB A Locality-Aware Cross-Region Load Balancer for LLM Inference](#SkyLB-A-Locality-Aware-Cross-Region-Load-Balancer-for-LLM-Inference)


## EPiC Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation

>Authors: Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu

>2025-06-04

> http://arxiv.org/abs/2506.04205v1

Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at **pruning**
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.


## SkipGPT Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling

>Authors: Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen

>2025-06-04

> http://arxiv.org/abs/2506.04179v1

Large language models (LLMs) achieve remarkable performance across tasks but
incur substantial computational costs due to their deep, multi-layered
architectures. Layer **pruning** has emerged as a strategy to alleviate these
inefficiencies, but conventional static **pruning** methods overlook two critical
dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level
heterogeneity demands context-aware **pruning** decisions, and (2) vertical
dynamics, where the distinct functional roles of MLP and self-attention layers
necessitate component-specific **pruning** policies. We introduce SkipGPT, a
dynamic layer **pruning** framework designed to optimize computational resource
allocation through two core innovations: (1) global token-aware routing to
prioritize critical tokens, and (2) decoupled **pruning** policies for MLP and
self-attention components. To mitigate training instability, we propose a
two-stage optimization paradigm: first, a disentangled training phase that
learns routing strategies via soft parameterization to avoid premature **pruning**
decisions, followed by parameter-efficient LoRA fine-tuning to restore
performance impacted by layer removal. Extensive experiments demonstrate that
SkipGPT reduces over 40% of model parameters while matching or exceeding the
performance of the original dense model across benchmarks. By harmonizing
dynamic efficiency with preserved expressivity, SkipGPT advances the practical
deployment of scalable, resource-aware LLMs. Our code is publicly available at:
https://github.com/EIT-NLP/SkipGPT.


## FlexGS Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting

>Authors: Hengyu Liu, Yuehao Wang, Chenxin Li, Ruisi Cai, Kevin Wang, Wuyang Li, Pavlo Molchanov, Peihao Wang, Zhangyang Wang

>2025-06-04

> http://arxiv.org/abs/2506.04174v1

3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on **pruning** less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.


## Rectified Sparse Attention

>Authors: Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei

>2025-06-04

> http://arxiv.org/abs/2506.04108v2

Efficient long-sequence generation is a critical challenge for Large Language
Models. While recent **sparse** decoding methods improve efficiency, they suffer
from **KV** cache misalignment, where approximation errors accumulate and degrade
generation quality. In this work, we propose Rectified Sparse Attention (ReSA),
a simple yet effective method that combines block-**sparse** attention with
periodic dense rectification. By refreshing the **KV** cache at fixed intervals
using a dense forward pass, ReSA bounds error accumulation and preserves
alignment with the pretraining distribution. Experiments across math reasoning,
language modeling, and retrieval tasks demonstrate that ReSA achieves
near-lossless generation quality with significantly improved efficiency.
Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at
256K sequence length, making it a practical solution for scalable long-context
inference. Code is available at https://aka.ms/ReSA-LM.


## Structured Pruning for Diverse Best-of-N Reasoning Optimization

>Authors: Hieu Trung Nguyen, Bao Nguyen, Viet Anh Nguyen

>2025-06-04

> http://arxiv.org/abs/2506.03978v1

Model **pruning** in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
**pruning** of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.


## Magic Mushroom A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems

>Authors: Yuxin Zhang, Yan Wang, Yongrui Chen, Shenyu Zhang, Xinbang Dai, Sheng Bi, Guilin Qi

>2025-06-04

> http://arxiv.org/abs/2506.03901v2

Retrieval-Augmented Generation (RAG) systems enhance Large Language Models
(LLMs) by incorporating external retrieved information, mitigating issues such
as hallucination and outdated knowledge. However, RAG systems are highly
sensitive to retrieval noise prevalent in real-world scenarios. Existing
benchmarks fail to emulate the complex and heterogeneous noise distributions
encountered in real-world retrieval environments, undermining reliable
robustness assessment. In this paper, we define four categories of retrieval
noise based on linguistic properties and noise characteristics, aiming to
reflect the heterogeneity of noise in real-world scenarios. Building on this,
we introduce Magic Mushroom, a benchmark for replicating "magic mushroom"
noise: contexts that appear relevant on the surface but covertly mislead RAG
systems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop
question-answer pairs. More importantly, Magic Mushroom enables researchers to
flexibly configure combinations of retrieval noise according to specific
research objectives or application scenarios, allowing for highly controlled
evaluation setups. We evaluate LLM generators of varying parameter scales and
classic RAG denoising strategies under diverse noise distributions to
investigate their performance dynamics during progressive noise encroachment.
Our analysis reveals that both generators and denoising strategies have
significant room for improvement and exhibit extreme sensitivity to noise
distributions. Magic Mushroom emerges as a promising tool for evaluating and
advancing noise-robust RAG systems, accelerating their widespread deployment in
real-world applications. The Magic Mushroom benchmark is available at
https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.


## STAR Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization

>Authors: Hao Li, Qi Lv, Rui Shao, Xiang Deng, Yinchuan Li, Jianye Hao, Liqiang Nie

>2025-06-04

> http://arxiv.org/abs/2506.03863v1

Transforming complex actions into discrete skill abstractions has
demonstrated strong potential for robotic manipulation. Existing approaches
mainly leverage latent variable models, e.g., VQ-VAE, to learn skill
abstractions through learned vectors (codebooks), while they suffer from
codebook collapse and modeling the causal relationship between learned skills.
To address these limitations, we present \textbf{S}kill \textbf{T}raining with
\textbf{A}ugmented \textbf{R}otation (\textbf{STAR}), a framework that advances
both skill learning and composition to complete complex behaviors.
Specifically, to prevent codebook collapse, we devise rotation-augmented
residual skill **quantization** (RaRSQ). It encodes relative angles between encoder
outputs into the gradient flow by rotation-based gradient mechanism. Points
within the same skill code are forced to be either pushed apart or pulled
closer together depending on gradient directions. Further, to capture the
causal relationship between skills, we present causal skill transformer (CST)
which explicitly models dependencies between skill representations through an
autoregressive mechanism for coherent action generation. Extensive experiments
demonstrate the superiority of STAR on both LIBERO benchmark and realworld
tasks, with around 12\% improvement over the baselines.


## Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models

>Authors: Seungcheol Park, Jeongin Bae, Beomseok Kwon, Minjun Kim, Byeongwook Kim, Se Jung Kwon, U Kang, Dongsoo Lee

>2025-06-04

> http://arxiv.org/abs/2506.03781v1

How can we **quantize** large language models while preserving accuracy?
Quantization is essential for deploying large language models (LLMs)
efficiently. Binary-coding **quantization** (BCQ) and uniform **quantization** (UQ) are
promising **quantization** schemes that have strong expressiveness and
optimizability, respectively. However, neither scheme leverages both
advantages. In this paper, we propose UniQuanF (Unified Quantization with
Flexible Mapping), an accurate **quantization** method for LLMs. UniQuanF harnesses
both strong expressiveness and optimizability by unifying the flexible mapping
technique in UQ and non-uniform **quantization** levels of BCQ. We propose unified
initialization, and local and periodic mapping techniques to optimize the
parameters in UniQuanF precisely. After optimization, our unification theorem
removes computational and memory overhead, allowing us to utilize the superior
accuracy of UniQuanF without extra deployment costs induced by the unification.
Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ
methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.


## AhaKV Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models

>Authors: Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu

>2025-06-04

> http://arxiv.org/abs/2506.03762v1

Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) **KV** cache consumes a lot of memory during inference. While several
works propose reducing the **KV** cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention **KV** (Aha**KV**), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
Aha**KV** utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed Aha**KV** on different models with a
fixed cache budget. Experiments show that Aha**KV** successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.


## FSHNet Fully Sparse Hybrid Network for 3D Object Detection

>Authors: Shuai Liu, Mingyue Cui, Boyang Li, Quanmin Liang, Tinghe Hong, Kai Huang, Yunxiao Shan, Kai Huang

>2025-06-04

> http://arxiv.org/abs/2506.03714v1

Fully **sparse** 3D detectors have recently gained significant attention due to
their efficiency in long-range detection. However, **sparse** 3D detectors extract
features only from non-empty voxels, which impairs long-range interactions and
causes the center feature missing. The former weakens the feature extraction
capability, while the latter hinders network optimization. To address these
challenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet
incorporates a proposed SlotFormer block to enhance the long-range feature
extraction capability of existing **sparse** encoders. The SlotFormer divides
**sparse** voxels using a slot partition approach, which, compared to traditional
window partition, provides a larger receptive field. Additionally, we propose a
dynamic **sparse** label assignment strategy to deeply optimize the network by
providing more high-quality positive samples. To further enhance performance,
we introduce a **sparse** upsampling module to refine downsampled voxels,
preserving fine-grained details crucial for detecting small objects. Extensive
experiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the
effectiveness of FSHNet. The code is available at
https://github.com/Say2L/FSHNet.


## Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond

>Authors: Xiansheng Cai, Sihan Hu, Tao Wang, Yuan Huang, Pan Zhang, Youjin Deng, Kun Chen

>2025-06-04

> http://arxiv.org/abs/2506.03703v1

Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-**sparse**
challenges in fundamental physics.


## AdaDecode Accelerating LLM Decoding with Adaptive Layer Parallelism

>Authors: Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng

>2025-06-04

> http://arxiv.org/abs/2506.03700v1

Large language models (LLMs) are increasingly used for long-content
generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency
becomes a critical bottleneck: Autoregressive decoding is inherently limited by
its sequential token generation process, where each token must be generated
before the next can be processed. This sequential dependency restricts the
ability to fully leverage modern hardware's parallel processing capabilities.
Existing methods like speculative decoding and layer skipping offer potential
speedups but have notable drawbacks: speculative decoding relies on an
auxiliary "drafter" model, which can be challenging to acquire and increases
memory overhead, while layer skipping may introduce discrepancies in the
outputs due to the missing key-value cache at skipped layers. In this work, we
propose AdaDecode, which accelerates LLM decoding without requiring auxiliary
models or changes to the original model parameters, while ensuring output
consistency. AdaDecode leverages the insight that many tokens can accurately be
generated at intermediate layers, as further layers often do not significantly
alter predictions once the model reaches a certain confidence. By adaptively
generating tokens at intermediate layers when confidence is high, AdaDecode
enables the next token's computation to begin immediately. The remaining layer
computations for early-predicted tokens are deferred and executed in parallel
with subsequent tokens when needed, maximizing hardware utilization and
reducing decoding latency. A final verification step ensures that early
predictions match the results of standard autoregressive decoding, preserving
output parity. Experiments across diverse generation tasks shows that AdaDecode
consistently achieves superior decoding throughput with up to 1.73x speedup,
while guaranteeing output parity with standard autoregressive decoding.


## Scaling Transformers for Discriminative Recommendation via Generative Pretraining

>Authors: Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng

>2025-06-04

> http://arxiv.org/abs/2506.03699v1

Discriminative recommendation tasks, such as CTR (click-through rate) and CVR
(conversion rate) prediction, play critical roles in the ranking stage of
large-scale industrial recommender systems. However, training a discriminative
model encounters a significant overfitting issue induced by data **sparsity**.
Moreover, this overfitting issue worsens with larger models, causing them to
underperform smaller ones. To address the overfitting issue and enhance model
scalability, we propose a framework named GPSD (\textbf{G}enerative
\textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative
Recommendation), drawing inspiration from generative training, which exhibits
no evident signs of overfitting. GPSD leverages the parameters learned from a
pretrained generative model to initialize a discriminative model, and
subsequently applies a **sparse** parameter freezing strategy. Extensive
experiments conducted on both industrial-scale and publicly available datasets
demonstrate the superior performance of GPSD. Moreover, it delivers remarkable
improvements in online A/B tests. GPSD offers two primary advantages: 1) it
substantially narrows the generalization gap in model training, resulting in
better test performance; and 2) it leverages the scalability of Transformers,
delivering consistent performance gains as models are scaled up. Specifically,
we observe consistent performance improvements as the model dense parameters
scale from 13K to 0.3B, closely adhering to power laws. These findings pave the
way for unifying the architectures of recommendation models and language
models, enabling the direct application of techniques well-established in large
language models to recommendation models.


## DSSAU-NetU-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation

>Authors: Zunhui Xia, Hongxing Li, Libin Lan

>2025-06-04

> http://arxiv.org/abs/2506.03684v1

In the childbirth process, traditional methods involve invasive vaginal
examinations, but research has shown that these methods are both subjective and
inaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way
to assess fetal head position via two key parameters: Angle of Progression
(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal
head (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth
delivery process. Therefore, accurate segmentation of FH and PS is crucial. In
this work, we propose a **sparse** self-attention network architecture with good
performance and high computational efficiency, named DSSAU-Net, for the
segmentation of FH and PS. Specifically, we stack varying numbers of Dual
Sparse Selection Attention (DSSA) blocks at each stage to form a symmetric
U-shaped encoder-decoder network architecture. For a given query, DSSA is
designed to explicitly perform one **sparse** token selection at both the region
and pixel levels, respectively, which is beneficial for further reducing
computational complexity while extracting the most relevant features. To
compensate for the information loss during the upsampling process, skip
connections with convolutions are designed. Additionally, multiscale feature
fusion is employed to enrich the model's global and local information. The
performance of DSSAU-Net has been validated using the Intrapartum Ultrasound
Grand Challenge (IUGC) 2024 \textit{test set} provided by the organizer in the
MICCAI IUGC 2024
competition\footnote{\href{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}},
where we win the fourth place on the tasks of classification and segmentation,
demonstrating its effectiveness. The codes will be available at
https://github.com/XiaZunhui/DSSAU-Net.


## Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI

>Authors: Wing Man Casca Kwok, Yip Chiu Tung, Kunal Bhagchandani

>2025-06-04

> http://arxiv.org/abs/2506.03607v1

Edge computing decentralizes processing power to network edge, enabling
real-time AI-driven decision-making in IoT applications. In industrial
automation such as robotics and rugged edge AI, real-time perception and
intelligence are critical for autonomous operations. Deploying
transformer-based image captioning models at the edge can enhance machine
perception, improve scene understanding for autonomous robots, and aid in
industrial inspection.
  However, these edge or IoT devices are often constrained in computational
resources for physical agility, yet they have strict response time
requirements. Traditional deep learning models can be too large and
computationally demanding for these devices. In this research, we present
findings of transformer-based models for image captioning that operate
effectively on edge devices. By evaluating resource-effective transformer
models and applying knowledge distillation techniques, we demonstrate inference
can be accelerated on resource-constrained devices while maintaining model
performance using these techniques.


## POSS Position Specialist Generates Better Draft for Speculative Decoding

>Authors: Langlin Huang, Chengsong Huang, Jixuan Leng, Di Huang, Jiaxin Huang

>2025-06-04

> http://arxiv.org/abs/2506.03566v1

Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.


## Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information

>Authors: Seungcheol Park, Sojin Lee, Jongjin Kim, Jinsik Lee, Hyunjik Jo, U Kang

>2025-06-04

> http://arxiv.org/abs/2506.03510v1

How can we accelerate large language models(LLMs) without sacrificing
accuracy? The slow inference speed of LLMs hinders us to benefit from their
remarkable performance in diverse applications. This is mainly because numerous
sublayers are stacked together in LLMs. Sublayer **pruning** compresses and
expedites LLMs via removing unnecessary sublayers. However, existing sublayer
**pruning** algorithms are limited in accuracy since they naively select sublayers
to prune, overlooking the different characteristics of each sublayer. In this
paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability
Information), an accurate sublayer **pruning** method for LLMs. SPRINT accurately
selects a target sublayer to prune by considering 1) the amount of latency
reduction after **pruning** and 2) the tunability of sublayers. SPRINT iteratively
prunes redundant sublayers and swiftly tunes the parameters of remaining
sublayers. Experiments show that SPRINT achieves the best accuracy-speedup
trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense
reasoning benchmarks compared to existing **pruning** algorithms.


## Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity

>Authors: Yide Ran, Wentao Guo, Jingwei Sun, Yanzhou Pan, Xiaodong Yu, Hao Wang, Jianwen Xie, Yiran Chen, Denghui Zhang, Zhaozhuo Xu

>2025-06-03

> http://arxiv.org/abs/2506.03337v1

Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a **sparse**
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely **sparse** subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing **sparsity** baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.


## Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs

>Authors: Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos

>2025-06-03

> http://arxiv.org/abs/2506.03296v1

Deploying large language models (LLMs) for online inference is often
constrained by limited GPU memory, particularly due to the growing **KV** cache
during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a
promising solution by offloading **KV** cache management and parts of attention
computation to the CPU. However, a key bottleneck remains: existing schedulers
fail to effectively overlap CPU-offloaded tasks with GPU execution during the
latency-critical, bandwidth-bound decode phase. This particularly penalizes
real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)
which are currently underserved by existing systems, especially under memory
pressure typical of edge or low-cost deployments.
  We present APEX, a novel, profiling-informed scheduling strategy that
maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems
relying on static rules or purely heuristic approaches, APEX dynamically
dispatches compute across heterogeneous resources by predicting execution times
of CPU and GPU subtasks to maximize overlap while avoiding scheduling
overheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA
T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only
schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%
on A10 GPUs, while preserving latency. Against the best existing hybrid
schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in
long-output settings.APEX significantly advances hybrid LLM inference
efficiency on such memory-constrained hardware and provides a blueprint for
scheduling in heterogeneous AI systems, filling a critical gap for efficient
real-time LLM applications.


## Chipmunk Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas

>Authors: Austin Silveria, Soham V. Govande, Daniel Y. Fu

>2025-06-03

> http://arxiv.org/abs/2506.03275v1

Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
**sparsity** at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic **sparsity** introduces two systems
challenges: (1) **sparse** attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic **sparsity** patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise **sparsity**. We implement column-**sparse** kernels utilizing efficient
**sparse** gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% **sparsity** compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of **sparsity** patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.


## HumanRAM Feed-forward Human Reconstruction and Animation Model using Transformers

>Authors: Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, Xiaowei Zhou

>2025-06-03

> http://arxiv.org/abs/2506.03118v1

3D human reconstruction and animation are long-standing topics in computer
graphics and vision. However, existing methods typically rely on sophisticated
dense-view capture and/or time-consuming per-subject optimization procedures.
To address these limitations, we propose HumanRAM, a novel feed-forward
approach for generalizable human reconstruction and animation from monocular or
**sparse** human images. Our approach integrates human reconstruction and animation
into a unified framework by introducing explicit pose conditions, parameterized
by a shared SMPL-X neural texture, into transformer-based large reconstruction
models (LRM). Given monocular or **sparse** input images with associated camera
parameters and SMPL-X poses, our model employs scalable transformers and a
DPT-based decoder to synthesize realistic human renderings under novel
viewpoints and novel poses. By leveraging the explicit pose conditions, our
model simultaneously enables high-quality human reconstruction and
high-fidelity pose-controlled animation. Experiments show that HumanRAM
significantly surpasses previous methods in terms of reconstruction accuracy,
animation fidelity, and generalization performance on real-world datasets.
Video results are available at https://zju3dv.github.io/humanram/.


## Controllable Human-centric Keyframe Interpolation with Generative Prior

>Authors: Zujin Guo, Size Wu, Zhongang Cai, Wei Li, Chen Change Loy

>2025-06-03

> http://arxiv.org/abs/2506.03119v1

Existing interpolation methods use pre-trained video diffusion priors to
generate intermediate frames between **sparse**ly sampled keyframes. In the absence
of 3D geometric guidance, these methods struggle to produce plausible results
for complex, articulated human motions and offer limited control over the
synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe
Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human
guidance signals into the diffusion process for Controllable Human-centric
Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for
interpolation, our PoseFuse3D, a 3D-informed control model, features a novel
SMPL-X encoder that transforms 3D geometry and shape into the 2D latent
conditioning space, alongside a fusion network that integrates these 3D cues
with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset
annotated with both 2D poses and 3D SMPL-X parameters. We show that
PoseFuse3D-KI consistently outperforms state-of-the-art baselines on
CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.
Comprehensive ablations demonstrate that our PoseFuse3D model improves
interpolation fidelity.


## TalkingMachines Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models

>Authors: Chetwin Low, Weimin Wang

>2025-06-03

> http://arxiv.org/abs/2506.03099v1

In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a **sparse** causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/


## StreamBP Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs

>Authors: Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li

>2025-06-03

> http://arxiv.org/abs/2506.03077v1

Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.


## Sparse-vDiT Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers

>Authors: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, Tao Chen

>2025-06-03

> http://arxiv.org/abs/2506.03065v1

While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring **sparsity** patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a **sparsity**
**acceleration** framework for vDiT comprising: 1) Pattern-optimized **sparse** kernels
that replace dense attention with computationally efficient implementations for
each identified **sparsity** pattern. 2) An offline **sparse** diffusion search
algorithm that selects the optimal **sparse** computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural **sparsity** in vDiTs can be systematically exploited for long video
synthesis.


## Towards Analyzing and Understanding the Limitations of VAPO A Theoretical Perspective

>Authors: Jintian Shao, Yiming Cheng

>2025-06-03

> http://arxiv.org/abs/2506.03038v1

Reinforcement learning (RL) enhances large language models (LLMs) in complex,
long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,
despite sophisticated mechanisms like Decoupled GAE, theoretically faces
fundamental limitations in comprehensively modeling and leveraging deep,
long-term value for fine-grained, step-by-step policy guidance in extended
reasoning chains. We argue these limitations stem from inherent difficulties in
credit assignment, value function representational capacity with temporally
abstracted goals, and translating global value signals into local policy
improvements, especially with **sparse** rewards. Our theoretical analysis examines
these aspects to illuminate VAPO's boundaries in long-term value modeling,
aiming to deepen understanding of current RL for advanced reasoning and suggest
future research for more robust LLM agents.


## Hybrid deep learning and iterative methods for accelerated solutions of viscous incompressible flow

>Authors: Heming Bai, Xin Bian

>2025-06-03

> http://arxiv.org/abs/2506.03016v1

The pressure Poisson equation, central to the fractional step method in
incompressible flow simulations, incurs high computational costs due to the
iterative solution of large-scale linear systems. To address this challenge, we
introduce HyDEA, a novel framework that synergizes deep learning with classical
iterative solvers. It leverages the complementary strengths of a DeepONet -
capable of capturing large-scale features of the solution - and the CG or a PCG
method, which efficiently resolves fine-scale errors. Specifically, within the
framework of line-search methods, the DeepONet predicts search directions to
accelerate convergence in solving **sparse**, symmetric-positive-definite linear
systems, while the CG/ PCG method ensures robustness through iterative
refinement. The framework seamlessly extends to flows over solid structures via
the decoupled immersed boundary projection method. Crucially, the DeepONet is
trained on fabricated linear systems rather than flow specific data, endowing
it with inherent generalization across geometric complexities and Reynolds
numbers without retraining. Benchmarks demonstrate superior efficiency and
accuracy of HyDEA over the CG/PCG methods for flows with no obstacles, single
or multiple stationary obstacles, and one moving obstacle - using fixed network
weights. Remarkably, HyDEA also exhibits super-resolution capability: although
the DeepONet is trained on a 128*128 grid for Re=1000, the hybrid solver
delivers accurate solutions on a 512*512 grid for Re=10000 via interpolation,
despite discretizations mismatch. In contrast, a purely data-driven DeepONet
fails for complex flows, underscoring the necessity of hybridizing deep
learning with iterative methods. Robustness, efficiency, and generalization
across geometries, resolutions, and Reynolds numbers of HyDEA highlight its
potential as a transformative solver for real world fluid dynamics problems.


## Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich Representation

>Authors: Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu

>2025-06-03

> http://arxiv.org/abs/2506.02997v1

Controllable TTS models with natural language prompts often lack the ability
for fine-grained control and face a scarcity of high-quality data. We propose a
two-stage style-controllable TTS system with language models, utilizing a
**quantize**d masked-autoencoded style-rich representation as an intermediary. In
the first stage, an autoregressive transformer is used for the conditional
generation of these style-rich tokens from text and control signals. The second
stage generates codec tokens from both text and sampled style-rich tokens.
Experiments show that training the first-stage model on extensive datasets
enhances the content robustness of the two-stage model as well as control
capabilities over multiple attributes. By selectively combining discrete labels
and speaker embeddings, we explore fully controlling the speaker's timbre and
other stylistic information, and adjusting attributes like emotion for a
specified speaker. Audio samples are available at
https://style-ar-tts.github.io.


## Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles

>Authors: Md-Ferdous Pervej, Richeng Jin, Md Moin Uddin Chowdhury, Simran Singh, smail Gven, Huaiyu Dai

>2025-06-03

> http://arxiv.org/abs/2506.02972v1

Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically **quantize** and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and non**quantize**d, hence, computation- and communication-inefficient
counterparts.


## PC-MoE Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs

>Authors: Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low

>2025-06-03

> http://arxiv.org/abs/2506.02965v2

Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the **sparsity** of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.


## Adaptive Graph Pruning for Multi-Agent Communication

>Authors: Boyi Li, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang

>2025-06-03

> http://arxiv.org/abs/2506.02951v1

Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-**pruning**) and communication topology (soft-**pruning**).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-**pruning** networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-**pruning** and
soft-**pruning** within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.


## Memory-Efficient Split Federated Learning for LLM Fine-Tuning on Heterogeneous Mobile Devices

>Authors: Xiaopei Chen, Liang Li, Fei Ji, Wen Wu

>2025-06-03

> http://arxiv.org/abs/2506.02940v1

In this paper, we propose an edge-assisted split federated learning framework
to facilitate large language model (LLM) fine-tuning on heterogeneous mobile
devices while alleviating memory pressures on both mobile devices and the edge
server. Specifically, mobile devices perform low-rank adaptation (LoRA)
fine-tuning on only a subset of lower layers of the pre-trained LLM, tailored
to their individual capacities. On the server, a full LLM is maintained, and
the corresponding LoRA modules are selectively fine-tuned in a sequential
manner for each device. To further enhance training efficiency, we propose a
server-side training scheduling method that optimizes the processing order of
devices for accelerating fine-tuning. Extensive experiments demonstrate that
compared to the baselines, our scheme can reduce 79\% memory footprint and 6\%
training time while achieving comparable performance.


## CLONE Customizing LLMs for Efficient Latency-Aware Inference at the Edge

>Authors: Chunlin Tian, Xinpeng Qin, Kahou Tam, Li Li, Zijian Wang, Yuanzhe Zhao, Minglei Zhang, Chengzhong Xu

>2025-06-03

> http://arxiv.org/abs/2506.02847v1

Deploying large language models (LLMs) on edge devices is crucial for
delivering fast responses and ensuring data privacy. However, the limited
storage, weight, and power of edge devices make it difficult to deploy
LLM-powered applications. These devices must balance latency requirements with
energy consumption and model accuracy. In this paper, we first quantify the
challenges of deploying LLMs on off-the-shelf edge devices and then we present
CLONE, an in-depth algorithm-hardware co-design at both the model- and
system-level that intelligently integrates real-time, energy optimization while
maintaining robust generality. In order to maximize the synergistic benefits of
these algorithms in always-on and intermediate edge computing settings, we
specialize in a 28nm scalable hardware accelerator system. We implement and
extensively evaluate CLONE on two off-the-shelf edge platforms. Experiments
show that CLONE effectively accelerates the inference process up to 11.92x, and
saves energy up to 7.36x, while maintaining high-generation.


## Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization

>Authors: Ruilong Wu, Xinjiao Li, Yisu Wang, Xinyu Chen, Dirk Kutscher

>2025-06-03

> http://arxiv.org/abs/2506.02787v1

Hybrid parallelism techniques are essential for efficiently training large
language models (LLMs). Nevertheless, current automatic parallel planning
frameworks often overlook the simultaneous consideration of node heterogeneity
and dynamic network topology changes, limiting their effectiveness in practical
applications. In this paper, we address these limitations by modeling
heterogeneous nodes within dynamically changing network environments and
leveraging simulation-based strategies to determine optimal parallel
configurations. Our approach enables fine-grained workload allocation tailored
for heterogeneous nodes and complex network scenarios, achieving performance
competitive with state-of-the-art methods under regular and stable network
conditions. Additionally, we introduce a strategy **pruning** technique to rapidly
discard infeasible parallel configurations, substantially reducing the search
space and accelerating the search process through parallel execution within the
simulator. Preliminary evaluations confirm that our method notably enhances
training performance on heterogeneous nodes and demonstrates improved
adaptability in complex, dynamic scenarios such as cloud computing
environments.


## Reuse or Generate? Accelerating Code Editing via Edit-Oriented Speculative Decoding

>Authors: Peiding Wang, Li Zhang, Fang Liu, Yinghao Zhu, Wang Xu, Lin Shi, Xiaoli Lian, Minxiao Li, Bo Shen, An Fu

>2025-06-03

> http://arxiv.org/abs/2506.02780v1

Large Language Models (LLMs) have demonstrated remarkable capabilities in
code editing, substantially enhancing software development productivity.
However, the inherent complexity of code editing tasks forces existing
approaches to rely on LLMs' autoregressive end-to-end generation, where
decoding speed plays a critical role in efficiency. While inference
**acceleration** techniques like speculative decoding are applied to improve the
decoding efficiency, these methods fail to account for the unique
characteristics of code editing tasks where changes are typically localized and
existing code segments are reused. To address this limitation, we propose
EfficientEdit, a novel method that improves LLM-based code editing efficiency
through two key mechanisms based on speculative decoding: (1) effective reuse
of original code segments while identifying potential edit locations, and (2)
efficient generate edit content via high-quality drafts from edit-oriented
draft models and a dynamic verification mechanism that balances quality and
**acceleration**. Experimental results show that EfficientEdit can achieve up to
10.38$\times$ and 13.09$\times$ speedup compared to standard autoregressive
decoding in CanItEdit and CodeIF-Bench, respectively, outperforming
state-of-the-art inference **acceleration** approaches by up to 90.6%.


## XicorAttention Time Series Transformer Using Attention with Nonlinear Correlation

>Authors: Daichi Kimura, Tomonori Izumitani, Hisashi Kashima

>2025-06-03

> http://arxiv.org/abs/2506.02694v1

Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations **sparse**, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.


## Human-In-The-Loop Workflow for Neuro- Symbolic Scholarly Knowledge Organization

>Authors: Lena John, Tim Wittenborg, Sren Auer, Oliver Karras

>2025-06-03

> http://arxiv.org/abs/2506.03221v1

As the volume of scientific literature continues to grow, efficient knowledge
organization is an increasingly challenging task. Traditional structuring of
scientific content is time-consuming and requires significant domain expertise,
increasing the need for tool support. Our goal is to create a Human-in-the-Loop
(HITL) workflow that supports researchers in creating and structuring
scientific knowledge, leveraging neural models and knowledge graphs,
exemplified using the Open Research Knowledge Graph (ORKG). The workflow aims
to automate key steps, including data extraction and knowledge structuring,
while keeping user oversight through human validation. We developed a modular
framework implementing the workflow and evaluated it along the Quality
Improvement Paradigm (QIP) with participants from the ORKG user community. The
evaluation indicated that the framework is highly usable and provides practical
support. It significantly reduces the time and effort required to transition
from a research interest to literature-based answers by streamlining the import
of information into a knowledge graph. Participants evaluated the framework
with an average System Usability Scale (SUS) score of 84.17, an A+ -- the
highest achievable rating. They also reported that it improved their time
spent, previously between 4 hours and two weeks, down to an average of 24:40
minutes. The tool streamlines the creation of scientific corpora and extraction
of structured knowledge for KG integration by leveraging LLMs and user-defined
models, significantly accelerating the review process. However, human
validation remains essential throughout the extraction process, and future work
is needed to improve extraction accuracy and entity linking to existing
knowledge resources.


## A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction

>Authors: Shiyu Shen, Bin Pan, Guirong Xue

>2025-06-03

> http://arxiv.org/abs/2506.02654v1

City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
**sparsity**. Code will be open.


## KVCache Cache in the Wild Characterizing and Optimizing KVCache Cache at a Large Cloud Provider

>Authors: Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen

>2025-06-03

> http://arxiv.org/abs/2506.02634v1

Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (**KV**\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from **KV**\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the **KV**\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: **KV**\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.


## HATA Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference

>Authors: Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li

>2025-06-03

> http://arxiv.org/abs/2506.02572v1

Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like **KV**Cache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent **sparsity** of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.


## Pruning General Large Language Models into Customized Expert Models

>Authors: Yirao Zhao, Guizhen Chen, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang

>2025-06-03

> http://arxiv.org/abs/2506.02561v1

Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing **pruning** methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained **pruning**. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and **pruning** irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.


## Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention

>Authors: Robin Geens, Marian Verhelst

>2025-06-03

> http://arxiv.org/abs/2506.02523v1

Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the
efficiency of large language models by projecting query, key, and value tensors
into a compact latent space. This architectural change reduces the **KV**-cache
size and significantly lowers memory bandwidth demands, particularly in the
autoregressive decode phase. This letter presents the first hardware-centric
analysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and
evaluating its implications for accelerator performance. We identify two
alternative execution schemes of MLA--reusing, resp. recomputing latent
projection matrices--which offer distinct trade-offs between compute and memory
access. Using the Stream design space exploration framework, we model their
throughput and energy cost across a range of hardware platforms and find that
MLA can shift attention workloads toward the compute-bound regime.
  Our results show that MLA not only reduces bandwidth usage but also enables
adaptable execution strategies aligned with hardware constraints. Compared to
MHA, it provides more stable and efficient performance, particularly on
bandwidth-limited hardware platforms. These findings emphasize MLA's relevance
as a co-design opportunity for future AI accelerators.


## AURA Agentic Upskilling via Reinforced Abstractions

>Authors: Alvin Zhu, Yusuke Tanaka, Dennis Hong

>2025-06-03

> http://arxiv.org/abs/2506.02507v1

We study the combinatorial explosion involved in translating high-level task
prompts into deployable control policies for agile robots through multi-stage
reinforcement learning. We introduce AURA (Agentic Upskilling via Reinforced
Abstractions), a schema-centric curriculum RL framework that leverages Large
Language Models (LLMs) as autonomous designers of multi-stage curricula. AURA
transforms user prompts into YAML workflows that encode full reward functions,
domain randomization strategies, and training configurations. All files are
statically validated against a schema before any GPU time is consumed, ensuring
reliable and efficient execution without human intervention. A
retrieval-augmented feedback loop allows specialized LLM agents to design,
execute, and refine staged curricula based on prior training results stored in
a vector database, supporting continual improvement over time. Ablation studies
highlight the importance of retrieval for curriculum quality and convergence
stability. Quantitative experiments show that AURA consistently outperforms
LLM-guided baselines on GPU-accelerated training frameworks. In qualitative
tests, AURA successfully trains end-to-end policies directly from user prompts
and deploys them zero-shot on a custom humanoid robot across a range of
environments. By abstracting away the complexity of curriculum design, AURA
enables scalable and adaptive policy learning pipelines that would be
prohibitively complex to construct by hand.


## Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning Quantization-Assisted Min-Max Fair Scheduling

>Authors: Xiyu Zhao, Qimei Cui, Ziqiang Du, Weicai Li, Xi Yu, Wei Ni, Ji Zhang, Xiaofeng Tao, Ping Zhang

>2025-06-03

> http://arxiv.org/abs/2506.02422v1

Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits **quantization** errors to enhance the privacy of WPFL and proposes a
novel **quantization**-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., **quantization** errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.


## Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology

>Authors: Wenhao Tang, Rong Qin, Heng Fang, Fengtao Zhou, Hao Chen, Xiang Li, Ming-Ming Cheng

>2025-06-03

> http://arxiv.org/abs/2506.02408v1

Pre-trained encoders for offline feature extraction followed by multiple
instance learning (MIL) aggregators have become the dominant paradigm in
computational pathology (CPath), benefiting cancer diagnosis and prognosis.
However, performance limitations arise from the absence of encoder fine-tuning
for downstream tasks and disjoint optimization with MIL. While slide-level
supervised end-to-end (E2E) learning is an intuitive solution to this issue, it
faces challenges such as high computational demands and suboptimal results.
These limitations motivate us to revisit E2E learning. We argue that prior work
neglects inherent E2E optimization challenges, leading to performance
disparities compared to traditional two-stage methods. In this paper, we
pioneer the elucidation of optimization challenge caused by **sparse**-attention
MIL and propose a novel MIL called ABMILX. It mitigates this problem through
global correlation-based attention refinement and multi-head mechanisms. With
the efficient multi-scale random patch sampling strategy, an E2E trained ResNet
with ABMILX surpasses SOTA foundation models under the two-stage paradigm
across multiple challenging benchmarks, while remaining computationally
efficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath
and calls for greater research focus in this area. The code is
https://github.com/DearCaat/E2E-WSI-ABMILX.


## Random at First, Fast at Last NTK-Guided Fourier Pre-Processing for Tabular DL

>Authors: Renat Sergazinov, Jing Wu, Shao-An Yin

>2025-06-03

> http://arxiv.org/abs/2506.02406v1

While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.


## OThink-R1 Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation

>Authors: Shengjia Zhang, Junjie Wu, Jiawei Chen, Changwang Zhang, Xingyu Lou, Wangchunshu Zhou, Sheng Zhou, Can Wang, Jun Wang

>2025-06-03

> http://arxiv.org/abs/2506.02397v1

Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.


## Consultant Decoding Yet Another Synergistic Mechanism

>Authors: Chuanghao Ding, Jiaping Wang, Ziqing Yang, Xiaoliang Wang, Dahua Lin, Cam-Tu Nguyen, Fei Tan

>2025-06-03

> http://arxiv.org/abs/2506.02391v1

The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.


## Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning

>Authors: Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang

>2025-06-03

> http://arxiv.org/abs/2506.02370v1

Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.


## A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer

>Authors: Liang Li, Jianli Zhao, Sheng Fang, Siyu Chen, Hui Sun

>2025-06-03

> http://arxiv.org/abs/2506.02364v1

Hyperspectral images (HSIs) are often degraded by complex mixed noise during
acquisition and transmission, making effective denoising essential for
subsequent analysis. Recent hybrid approaches that bridge model-driven and
data-driven paradigms have shown great promise. However, most of these
approaches lack effective alternation between different priors or modules,
resulting in loosely coupled regularization and insufficient exploitation of
their complementary strengths. Inspired by tensor robust principal component
analysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that
enforces stage-wise alternation between two tightly integrated modules:
low-rank and **sparse**. The low-rank module employs thresholded tensor singular
value decomposition (t-SVD), providing a widely adopted convex surrogate for
tensor low-rankness and has been demonstrated to effectively capture the global
spatial-spectral structure of HSIs. The Top-K **sparse** transformer module
adaptively imposes **sparse** constraints, directly matching the **sparse**
regularization in TRPCA and enabling effective removal of localized outliers
and complex noise. This tightly coupled architecture preserves the stage-wise
alternation between low-rank approximation and **sparse** refinement inherent in
TRPCA, while enhancing representational capacity through attention mechanisms.
Extensive experiments on synthetic and real-world HSIs demonstrate that
DU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while
offering interpretability benefits and stable denoising dynamics inspired by
iterative optimization. Code is available at
https://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising.


## Angles Don't Lie Unlocking Training-Efficient RL Through the Model's Own Signals

>Authors: Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, Yiran Chen

>2025-06-02

> http://arxiv.org/abs/2506.02281v1

Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
**acceleration** in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.


## Improving compiler support for SIMD offload using Arm Streaming SVE

>Authors: Mohamed Husain Noor Mohamed, Adarsh Patil, Latchesar Ionkov, Eric Van Hensbergen

>2025-06-02

> http://arxiv.org/abs/2506.02233v1

The wider adoption of tightly coupled core-adjacent accelerators, such as Arm
Scalable Matrix Extension (SME), hinges on lowering software programming
complexity. In this paper, we focus on enabling the use of SME architecture in
Streaming Scalable Vector Extension (SSVE) mode for workloads written in C/C++.
While current compilers optimize loops for all types of SIMD instructions,
these techniques primarily target vector units within the core and falter when
applied to disaggregated, core-adjacent SIMD accelerators. Our goal is to
enable the compiler to automatically generate code for such accelerators only
when profitable.
  To this end, we investigate a path towards performant, precise, and
repeatable computation offloading through two compiler ecosystems. We revisit
LLVM compiler passes, MLIR transforms and their associated cost models, and
heuristics. We hope that these insights can provide directions for evolving
compiler capabilities towards automatic code generation for this
next-generation vector processing paradigm.


## A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering

>Authors: Madan Krishnamurthy, Daniel Korn, Melissa A Haendel, Christopher J Mungall, Anne E Thessen

>2025-06-02

> http://arxiv.org/abs/2506.02160v1

This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.


## HENT-SRT Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation

>Authors: Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur

>2025-06-02

> http://arxiv.org/abs/2506.02157v1

Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.


## GLoSS Generative Language Models with Semantic Search for Sequential Recommendation

>Authors: Krishna Acharya, Aleksandr V. Petrov, Juba Ziani

>2025-06-02

> http://arxiv.org/abs/2506.01910v1

We propose Generative Low-rank language model with Semantic Search (GLoSS), a
generative recommendation framework that combines large language models with
dense retrieval for sequential recommendation. Unlike prior methods such as
GPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to
retrieve relevant items beyond lexical matching. For query generation, we
employ 4-bit **quantize**d LlaMA-3 models fine-tuned with low-rank adaptation
(LoRA), enabling efficient training and inference on modest hardware. We
evaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and
Sports, and find that it achieves state-of-the-art performance. Compared to
traditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and
15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms
LLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5
gains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show
that GLoSS performs particularly well for cold-start users in the Amazon Toys
and Sports datasets, and benefits from longer user histories in Amazon Beauty
dataset, demonstrating robustness across different levels of interaction
lengths.


## Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts

>Authors: Spencer Banasik

>2025-06-02

> http://arxiv.org/abs/2506.01827v1

As machine learning algorithms are shown to be an increasingly valuable tool,
the demand for their access has grown accordingly. Oftentimes, it is infeasible
to run inference with larger models without an accelerator, which may be
unavailable in environments that have constraints such as energy consumption,
security, or cost. To increase the availability of these models, we aim to
improve the LLM inference speed on a CPU-only environment by modifying the
cache architecture. To determine what improvements could be made, we conducted
two experiments using Llama.cpp and the QWEN model: running various cache
configurations and evaluating their performance, and outputting a trace of the
memory footprint. Using these experiments, we investigate the memory access
patterns and performance characteristics to identify potential optimizations.


## Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming

>Authors: Haruki Yokota, Hiroshi Higashi, Yuichi Tanaka, Gene Cheung

>2025-06-02

> http://arxiv.org/abs/2506.01826v1

Signed graphs are equipped with both positive and negative edge weights,
encoding pairwise correlations as well as anti-correlations in data. A balanced
signed graph is a signed graph with no cycles containing an odd number of
negative edges. Laplacian of a balanced signed graph has eigenvectors that map
via a simple linear transform to ones in a corresponding positive graph
Laplacian, thus enabling reuse of spectral filtering tools designed for
positive graphs. We propose an efficient method to learn a balanced signed
graph Laplacian directly from data. Specifically, extending a previous linear
programming (LP) based **sparse** inverse covariance estimation method called
CLIME, we formulate a new LP problem for each Laplacian column $i$, where the
linear constraints restrict weight signs of edges stemming from node $i$, so
that nodes of same / different polarities are connected by positive / negative
edges. Towards optimal model selection, we derive a suitable CLIME parameter
$\rho$ based on a combination of the Hannan-Quinn information criterion and a
minimum feasibility criterion. We solve the LP problem efficiently by tailoring
a **sparse** LP method based on ADMM. We theoretically prove local solution
convergence of our proposed iterative algorithm. Extensive experimental results
on synthetic and real-world datasets show that our balanced graph learning
method outperforms competing methods and enables reuse of spectral filters,
wavelets, and graph convolutional nets (GCN) constructed for positive graphs.


## A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents

>Authors: Cheonsu Jeong

>2025-06-02

> http://arxiv.org/abs/2506.01804v1

This paper provides an in-depth technical analysis and implementation
methodology of the open-source Agent-to-Agent (A2A) protocol developed by
Google and the Model Context Protocol (MCP) introduced by Anthropic. While the
evolution of LLM-based autonomous agents is rapidly accelerating, efficient
interactions among these agents and their integration with external systems
remain significant challenges. In modern AI systems, collaboration between
autonomous agents and integration with external tools have become essential
elements for building practical AI applications. A2A offers a standardized
communication method that enables agents developed in heterogeneous
environments to collaborate effectively, while MCP provides a structured I/O
framework for agents to connect with external tools and resources. Prior
studies have focused primarily on the features and applications of either A2A
or MCP individually. In contrast, this study takes an integrated approach,
exploring how the two protocols can complement each other to address
interoperability issues and facilitate efficient collaboration within complex
agent ecosystems.


## Science Prospects for the Southern Wide-field Gamma-ray Observatory SWGO

>Authors: SWGO Collaboration, P. Abreu, R. Alfaro, A. Alfonso, M. Andrade, E. O. Angner, E. A. Anita-Rangel, O. Aquines-Gutirrez, C. Arcaro, R. Arceo, J. C. Arteaga-Velzquez, P. Assis, H. A. Ayala Solares, A. Bakalova, E. M. Bandeira, P. Bangale, U. Barres de Almeida, P. Batista, I. Batkovi, J. Bazo, E. Belmont, J. Bennemann, S. Y. BenZvi, A. Bernal, W. Bian, C. Bigongiari, E. Bottacini, R. Branada, P. Brogueira, A. M. Brown, T. Bulik, K. S. Caballero-Mora, P. Camarri, W. Cao, Z. Cao, Z. Cao, T. Capistrn, M. Cardillo, C. Casentini, C. Castromonte, P. M. Chadwick, J. Chaname, J. Chang, S. Chen, M. Chianese, A. Chiavassa, L. Chytka, R. Colallillo, R. Conceio, G. Consolati, R. Cordero, P. J. Costa, R. Covarelli, X. Cui, X. Cui, A. De Angelis, E. de Gouveia Dal Pino, R. de Menezes, P. Desiati, N. Di Lalla, F. Di Pierro, G. Di Sciascio, J. C. Daz Vlez, C. Dib, B. Dingus, J. Djuvsland, C. Dobrigkeit, L. M. Domingues Mendes, T. Dorigo, M. Doro, A. C. dos Reis, M. Du Vernois, D. Elsaesser, K. Engel, T. Ergin, M. Errando, K. Fang, A. Fazzi, C. Feng, M. Feroci, C. N. Ferreira, N. Fraija, S. Fraija, A. Franceschini, G. F. Franco, S. Funk, R. Galleguillos, B. Gao, C. Gao, A. M. Garcia Reyes, S. Garcia, F. Garfias, G. Giacinti, L. Gibilisco, B. Giovanni, J. Glombitza, H. Goksu, G. Gong, B. S. Gonzlez, M. M. Gonzalez, J. Goodman, V. M. Grieco, M. Gu, F. Guarino, G. P. Guedes, J. Gyeong, F. Haist, G. Han, P. Hansen, J. P. Harding, S. Hernandez Cadena, I. Herzog, J. A. Hinton, W. Hofmann, C. Hou, Hou C., K. Hu, D. Huang, P. Huentemeyer, A. Iriarte, J. Isakovi, A. Jardin-Blicq, L. I. Junoy, J. Juryek, S. Kaci, B. Khelifi, D. Kieda, F. La Monaca, G. La Mura, R. G. Lang, J. S. Lapington, R. Laspiur, L. Lavitola, J. Lee, F. Leitl, M. Lemoine-Goumard, L. Lessio, T. Lewis, C. Li, J. Li, K. Li, T. Li, B. Liberti, S. Lin, R. A. Lineros, D. Liu, J. Liu, R. Liu, F. Longo, Y. Luo, J. Lv, E. Macerata, G. Magugliani, K. Malone, A. Mancilla, D. Mandat, M. Manganaro, M. Mariani, A. Mariazzi, M. Mariotti, T. Marrodan, H. Martnez-Huerta, I. Martins, S. Medina, D. Melo, L. F. Mendes, E. Meza, R. Micali, D. Miceli, S. Miozzi, P. E. Mirn Enriquez, A. Mitchell, A. Molinario, A. Montero, O. G. Morales-Olivares, A. Morselli, E. Mossini, M. Mostaf, W. Muhammad, F. Muleri, F. Nardi, A. Negro, L. Nellen, M. Nisa, V. Novotny, L. Olivera-Nieto, N. Omodei, E. Orlando, S. Ortolani, M. Osorio-Archila, T. Ota, L. Otiniano, L. Palacios, A. Paoloni, I. M. Pepe, R. Perca, M. Peresano, Y. Prez Araujo, T. Petrosillo, G. Piano, D. Piccolo, A. Pichel, M. Pimenta, M. Pirke, A. Porcelli, T. Porter, E. Prandini, A. Pratts, R. Pretsch, A. Qi, J. Qin, S. Rain, L. Recabarren, A. Reisenegger, Q. Remy, H. X. Ren, F. Rescic, B. Reville, M. Reyes, C. D. Rho, M. Riquelme, J. I. Rivadeneira, G. Rodriguez Fernandez, B. Rossi, A. C. Rovero, A. Ruina, G. Salazar, C. Salotto, F. Sanchez, A. Sandoval, F. Sansone, M. Santander, R. Santonico, G. L. P. Santos, A. Santos-Guevara, D. Sartirana, N. Saviano, M. Schneider, M. Schneider, H. Schoorlemmer, F. Schussler, H. Schutte, J. Serna-Franco, M. Shoaib, A. Smith, A. J. Smith, Y. Son, O. Soto, R. W. Springer, J. Stewart, L. A. Stuani, H. Sun, M. Tambone, R. Tang, Z. Tang, S. Tapia, M. Tavani, R. Terrier, T. Terzi, K. Tollefson, B. Tom, I. Torres, R. Torres-Escobedo, G. C. Trinchero, R. Turner, P. Ulloa, L. Valore, C. van Eldik, J. Vega, I. D. Vergara Quispe, A. Viana, J. Vcha, C. F. Vigorito, V. Vittorini, B. Wang, L. Wang, X. Wang, X. Wang, X. Wang, Z. Wang, Z. Wang, M. Waqas, I. J. Watson, F. Werner, R. White, C. Wiebusch, F. Wohlleben, S. Xi, G. Xiao, H. Xiao, H. Xiao, L. Yang, R. Yang, Z. Yang, Z. Yang, R. Yanyachi, Z. Yao, D. Zavrtanik, H. Zhang, H. Zhang, J. Zhang, J. Zhang, S. Zhang, S. Zhang, X. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, J. Zhao, L. Zhao, H. Zhou, C. Zhu, H. Zhu, H. Zhu, P. Zhu, X. Zuo, P. Zyla

>2025-06-02

> http://arxiv.org/abs/2506.01786v1

Ground-based gamma-ray astronomy is now well established as a key
observational approach to address critical topics at the frontiers of
astroparticle physics and high-energy astrophysics. Whilst the field of TeV
astronomy was once dominated by arrays of atmospheric Cherenkov Telescopes,
ground-level particle detection has now been demonstrated to be an equally
viable and strongly complementary approach. Ground-level particle detection
provides continuous monitoring of the overhead sky, critical for the mapping of
extended structures and capturing transient phenomena. As demonstrated by HAWC
and LHAASO, the technique provides the best available sensitivity above a few
tens of TeV, and for the first time access to the PeV energy range. Despite the
success of this approach, there is so far no major ground-level particle-based
observatory with access to the Southern sky. HESS, located in Namibia, is the
only major gamma-ray instrument in the Southern Hemisphere, and has shown the
extraordinary richness of the inner galaxy in the TeV band, but is limited in
terms of field of view and energy reach.
  SWGO is an international effort to construct the first wide-field instrument
in the south with deep sensitivity from 100s of GeV into the PeV domain. The
project is now close to the end of its development phase and planning for
construction of the array in Chile has begun. Here we describe the baseline
design, expected sensitivity and resolution, and describe in detail the main
scientific topics that will be addressed by this new facility and its initial
phase SWGO-A. We show that SWGO will have a transformational impact on a wide
range of topics from cosmic-ray **acceleration** and transport to the nature of
dark matter. SWGO represents a key piece of infrastructure for multi-messenger
astronomy in the next decade, with strong scientific synergies with the nearby
CTA Observatory.


## Benford's Curse Tracing Digit Bias to Numerical Hallucination in LLMs

>Authors: Jiandong Shao, Yao Lu, Jianfei Yang

>2025-06-02

> http://arxiv.org/abs/2506.01734v1

Large Language Models (LLMs) exhibit impressive performance on complex
reasoning tasks, yet they frequently fail on basic numerical problems,
producing incorrect outputs. Inspired by Benford's Law -- a statistical pattern
where lower digits occur more frequently as leading digits -- we hypothesize
that the long-tailed digit distributions in web-collected corpora may be
learned by LLMs during pretraining, leading to biased numerical generation. To
investigate the hypothesis, we first examine whether digits frequencies in
pretraining corpus (OLMo2) follows Benford's law. We then construct an
evaluation benchmark with uniformly distributed ground-truth digits across
seven numerical reasoning tasks. Our evaluation results demonstrate that
leading open-source LLMs show a consistent pattern of digit bias that resembles
Benford's law. Through logit-lens tracing and neuron-level dissection, we
identify that this bias arises predominantly from a small subset of highly
digit-selective feed-forward network (FFN) neurons in the deeper layers.
Finally, we demonstrate that **pruning** these neurons mitigates imbalanced
overgeneration and partially corrects erroneous outputs, providing causal
evidence that fine-grained pretraining digit bias can propagate into model
behavior. Our findings reveal a fundamental connection between corpus-level
statistics and symbolic failure modes in LLMs, offering a new lens for
diagnosing and mitigating hallucinations in numerical tasks.


## Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025

>Authors: Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas lives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galvo Filho

>2025-06-02

> http://arxiv.org/abs/2506.02088v1

Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
**quantization** and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.


## Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition

>Authors: Yoonjun Cho, Soeun Kim, Dongjae Jeon, Kyelim Lee, Beomsoo Lee, Albert No

>2025-06-02

> http://arxiv.org/abs/2506.02077v1

Decomposing weight matrices into **quantization** and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between **quantization** and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on **quantization**,
enabling more effective balance between **quantization** and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes **quantization** scale, and
improves perplexity and zero-shot accuracy in **low-bit** settings.


## Sparse Imagination for Efficient Visual World Model Planning

>Authors: Junha Chun, Youngjoon Jeong, Taesup Kim

>2025-06-02

> http://arxiv.org/abs/2506.01392v1

World model based planning has significantly improved decision-making in
complex environments by enabling agents to simulate future states and make
informed choices. However, ensuring the prediction accuracy of world models
often demands substantial computational resources, posing a major challenge for
real-time applications. This computational burden is particularly restrictive
in robotics, where resources are severely constrained. To address this
limitation, we propose a Sparse Imagination for Efficient Visual World Model
Planning, which enhances computational efficiency by reducing the number of
tokens processed during forward prediction. Our method leverages a **sparse**ly
trained vision-based world model based on transformers with randomized grouped
attention strategy, allowing the model to adaptively adjust the number of
tokens processed based on the computational resource. By enabling **sparse**
imagination (rollout), our approach significantly accelerates planning while
maintaining high control fidelity. Experimental results demonstrate that **sparse**
imagination preserves task performance while dramatically improving inference
efficiency, paving the way for the deployment of world models in real-time
decision-making scenarios.


## ThinkEval Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs

>Authors: Manit Baser, Dinil Mon Divakaran, Mohan Gurusamy

>2025-06-02

> http://arxiv.org/abs/2506.01386v1

Model editing has become an important tool for addressing privacy, bias, and
misinformation in large language models (LLMs) by enabling updates to knowledge
without the need for retraining from scratch. However, existing editing
techniques often target isolated facts, ignoring ripple effects on related
knowledge, allowing edited facts to remain deducible and compromising broader
contextual integrity. For example, changing Harry Potter's school from Hogwarts
to Ilvermorny requires reassigning his house from Gryffindor to a suitable
alternative while preserving Gryffindor's relationship with Hogwarts. In this
work, we present a new model-editing setting, deep editing, to show: (1) how
editing techniques fail to handle connected facts, evaluating how original
knowledge sneaks through unchanged causal links, and (2) their impact on
broader contextual knowledge. We introduce ThinkEval, a framework to
systematically evaluate model-editing techniques by building model-specific
knowledge graphs to analyze pre- and post-edit effects on fact persistence and
catastrophic forgetting. We present KnowGIC, a benchmark created with
ThinkEval, consisting of sequentially linked queries to measure these effects.
We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE
across multiple LLMs. We find that these techniques struggle to balance
indirect fact suppression with the preservation of related knowledge. Our
dataset is available at: https://anonymous.4open.science/r/KnowGIC.


## Generative Next POI Recommendation with Semantic ID

>Authors: Dongsheng Wang, Yuxi Huang, Shen Gao, Yifan Wang, Chengrui Huang, Shuo Shang

>2025-06-02

> http://arxiv.org/abs/2506.01375v1

Point-of-interest (POI) recommendation systems aim to predict the next
destinations of user based on their preferences and historical check-ins.
Existing generative POI recommendation methods usually employ random numeric
IDs for POIs, limiting the ability to model semantic relationships between
similar locations. In this paper, we propose Generative Next POI Recommendation
with Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel
semantic POI ID (SID) representation method that enhances the semantic
understanding of POI modeling. There are two key components in our GNPR-SID:
(1) a Semantic ID Construction module that generates semantically rich POI IDs
based on semantic and collaborative features, and (2) a Generative POI
Recommendation module that fine-tunes LLMs to predict the next POI using these
semantic IDs. By incorporating user interaction patterns and POI semantic
features into the semantic ID generation, our method improves the
recommendation accuracy and generalization of the model. To construct
semantically related SIDs, we propose a POI **quantization** method based on
residual **quantize**d variational autoencoder, which maps POIs into a discrete
semantic space. We also propose a diversity loss to ensure that SIDs are
uniformly distributed across the semantic space. Extensive experiments on three
benchmark datasets demonstrate that GNPR-SID substantially outperforms
state-of-the-art methods, achieving up to 16% improvement in recommendation
accuracy.


## TAH-QUANT Effective Activation Quantization in Pipeline Parallelism over Slow Network

>Authors: Guangxin He, Yuan Cao, Yutong He, Tianyi Bai, Kun Yuan, Binhang Yuan

>2025-06-02

> http://arxiv.org/abs/2506.01352v1

Decentralized training of large language models offers the opportunity to
pool computational resources across geographically distributed participants but
faces significant network communication bottlenecks, particularly in
pipeline-parallel settings. While pipeline parallelism partitions model layers
across devices to handle large-scale models, it necessitates frequent
communication of intermediate activations, creating challenges when network
bandwidth is limited. Existing activation compression methods, such as AQ-SGD,
mitigate **quantization**-induced errors through error compensation but impose
prohibitive memory overhead by requiring storage of previous activations. To
address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard
Quantization), a novel activation **quantization** framework designed specifically
for pipeline parallelism. Our approach integrates fine-grained tile-wise
**quantization** for precise control, entropy-guided token-level adaptive bit
allocation for optimal bit usage, and a Hadamard-based transform with pivot
element swapping to effectively suppress **quantization** outliers. We further
provide a theoretical analysis, proving that pipeline parallel training
equipped with TAH-Quant maintains a convergence rate of
$\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient
descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant
achieves aggressive activation **quantization** (3-4 bits) ratio, which provides up
to 4.3$\times$ end-to-end speedup without compromising training convergence,
matches state-of-the-art methods, incurs no extra memory overhead, and
generalizes well across different training scenarios.


## Evaluating Large Language Models in Crisis Detection A Real-World Benchmark from Psychological Support Hotlines

>Authors: Guifeng Deng, Shuyin Rao, Tianyu Lin, Anlu Dai, Pan Wang, Junyi Xie, Haidong Song, Ke Zhao, Dongwu Xu, Zhengdong Cheng, Tao Li, Haiteng Jiang

>2025-06-02

> http://arxiv.org/abs/2506.01329v1

Psychological support hotlines are critical for crisis intervention but face
significant challenges due to rising demand. Large language models (LLMs) could
support crisis assessments, yet their capabilities in emotionally sensitive
contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540
annotated transcripts from the Hangzhou Psychological Assistance Hotline,
assessing four tasks: mood status recognition, suicidal ideation detection,
suicide plan identification, and risk assessment. We evaluated 64 LLMs across
15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,
few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with
statistical comparisons via Welch's t-tests. LLMs performed strongly on
suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood
status recognition was more challenging (max F1=0.709), likely due to lost
vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)
surpassed larger models on mood and suicidal ideation. Open-source models like
QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though
closed models retained an edge in mood detection (p=0.007). Performance scaled
with size up to a point; **quantization** (AWQ) reduced GPU memory by 70% with
minimal F1 degradation. LLMs show substantial promise in structured
psychological crisis assessments, especially with fine-tuning. Mood recognition
remains limited due to contextual complexity. The narrowing gap between open-
and closed-source models, combined with efficient **quantization**, suggests
feasible integration. PsyCrisisBench offers a robust evaluation framework to
guide model development and ethical deployment in mental health.


## Energy Considerations for Large Pretrained Neural Networks

>Authors: Leo Mei, Mark Stamp

>2025-06-02

> http://arxiv.org/abs/2506.01311v1

Increasingly complex neural network architectures have achieved phenomenal
performance. However, these complex models require massive computational
resources that consume substantial amounts of electricity, which highlights the
potential environmental impact of such models. Previous studies have
demonstrated that substantial redundancies exist in large pre-trained models.
However, previous work has primarily focused on compressing models while
retaining comparable model performance, and the direct impact on electricity
consumption appears to have received relatively little attention. By
quantifying the energy usage associated with both uncompressed and compressed
models, we investigate compression as a means of reducing electricity
consumption. We consider nine different pre-trained models, ranging in size
from 8M parameters to 138M parameters. To establish a baseline, we first train
each model without compression and record the electricity usage and time
required during training, along with other relevant statistics. We then apply
three compression techniques: Steganographic capacity reduction, **pruning**, and
low-rank factorization. In each of the resulting cases, we again measure the
electricity usage, training time, model accuracy, and so on. We find that
**pruning** and low-rank factorization offer no significant improvements with
respect to energy usage or other related statistics, while steganographic
capacity reduction provides major benefits in almost every case. We discuss the
significance of these findings.


## Mamba Drafters for Speculative Decoding

>Authors: Daewon Choi, Seunghyuk Oh, Saket Dingliwal, Jihoon Tack, Kyuyoung Kim, Woomin Song, Seojin Kim, Insu Han, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati

>2025-06-01

> http://arxiv.org/abs/2506.01206v1

Speculative decoding has emerged as a promising approach to accelerating
large language model (LLM) generation using a fast drafter while maintaining
alignment with the target model's distribution. However, existing approaches
face a trade-off: external drafters offer flexibility but can suffer from
slower drafting, while self-speculation methods use drafters tailored to the
target model but require re-training. In this paper, we introduce novel
drafters based on Mamba, a state-of-the-art state space model (SSM), as a
solution that combines the best aspects of both approaches. By leveraging the
linear structure of SSMs, our approach avoids the quadratic complexity inherent
in traditional Transformer-based methods, enabling faster drafting and lower
memory usage while maintaining the flexibility to work across different target
models. We further enhance efficiency with a novel test-time tree search
algorithm for generating high-quality draft candidates. Our empirical
evaluation demonstrates that Mamba-based drafters not only outperform existing
external drafting methods but are also comparable to state-of-the-art
self-speculation approaches while using less memory and maintaining their
cross-model adaptability.


## ReTern Exploiting Natural Redundancy and Sign Transformations for Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs

>Authors: Akul Malhotra, Sumeet Kumar Gupta

>2025-06-01

> http://arxiv.org/abs/2506.01140v1

Ternary large language models (LLMs), which utilize ternary precision weights
and 8-bit activations, have demonstrated competitive performance while
significantly reducing the high computational and memory requirements of
full-precision LLMs. The energy efficiency and performance of Ternary LLMs can
be further improved by deploying them on ternary computing-in-memory (TCiM)
accelerators, thereby alleviating the von-Neumann bottleneck. However, TCiM
accelerators are prone to memory stuck-at faults (SAFs) leading to degradation
in the model accuracy. This is particularly severe for LLMs due to their low
weight **sparsity**. To boost the SAF tolerance of TCiM accelerators, we propose
ReTern that is based on (i) fault-aware sign transformations (FAST) and (ii)
TCiM bit-cell reprogramming exploiting their natural redundancy. The key idea
is to utilize FAST to minimize computations errors due to SAFs in +1/-1
weights, while the natural bit-cell redundancy is exploited to target SAFs in 0
weights (zero-fix). Our experiments on BitNet b1.58 700M and 3B ternary LLMs
show that our technique furnishes significant fault tolerance, notably 35%
reduction in perplexity on the Wikitext dataset in the presence of faults.
These benefits come at the cost of < 3%, < 7%, and < 1% energy, latency and
area overheads respectively.


## TRUST -- Transformer-Driven U-Net for Sparse Target Recovery

>Authors: Di An, Dylan Poppert, Jiayue Li, Mark Foster, Trac D. Tran

>2025-06-01

> http://arxiv.org/abs/2506.01112v1

In the context of inverse problems $\bf y = Ax$, **sparse** recovery offers a
powerful paradigm shift by enabling the stable solution of ill-posed or
underdetermined systems through the exploitation of structure, particularly
**sparsity**. Sparse regularization techniques via $\ell_0$- or $\ell_1$-norm
minimization encourage solutions $\bf x$ that are both consistent with
observations $\bf y$ and parsimonious in representation, often yielding
physically meaningful interpretations. In this work, we address the classical
inverse problem under the challenging condition where the sensing operator $\bf
A$ is unknown and only a limited set of observation-target pairs $\{ \bf x,\bf
y \}$ is available. We propose a novel neural architecture, TRUST, that
integrates the attention mechanism of Transformers with the decoder pathway of
a UNet to simultaneously learn the sensing operator and reconstruct the **sparse**
signal. The TRUST model incorporates a Transformer-based encoding branch to
capture long-range dependencies and estimate **sparse** support, which then guides
a U-Net-style decoder to refine reconstruction through multiscale feature
integration. The skip connections between the transformer stages and the
decoder not only enhance image quality but also enable the decoder to access
image features at different levels of abstraction. This hybrid architecture
enables more accurate and robust recovery by combining global context with
local details. Experimental results demonstrate that TRUST significantly
outperforms traditional **sparse** recovery methods and standalone U-Net models,
achieving superior performance in SSIM and PSNR metrics while effectively
suppressing hallucination artifacts that commonly plague deep learning-based
inverse solvers.


## Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective

>Authors: Lei Lei, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Tong Xu

>2025-06-01

> http://arxiv.org/abs/2506.01097v1

Existing Multimodal Large Language Models (MLLMs) process a large number of
visual tokens, leading to significant computational costs and inefficiency.
Previous works generally assume that all visual tokens are necessary in the
shallow layers of LLMs, and therefore token compression typically occurs in
intermediate layers. In contrast, our study reveals an interesting insight:
with proper selection, token compression is feasible at the input stage of LLM
with negligible performance loss. Specifically, we reveal that explainability
methods can effectively evaluate the importance of each visual token with
respect to the given instruction, which can well guide the token compression.
Furthermore, we propose to learn a mapping from the attention map of the first
LLM layer to the explanation results, thereby avoiding the need for a full
inference pass and facilitating practical deployment. Interestingly, this
mapping can be learned using a simple and lightweight convolutional network,
whose training is efficient and independent of MLLMs. Extensive experiments on
10 image and video benchmarks across three leading MLLMs (Qwen2-VL,
LLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,
e.g., **pruning** 50% visual tokens while retaining more than 96% of the original
performance across all benchmarks for all these three MLLMs. It also exhibits
strong generalization, even when the number of tokens in inference far exceeds
that used in training.


## Probing Neural Topology of Large Language Models

>Authors: Yu Zheng, Yuan Yuan, Yong Li, Paolo Santi

>2025-06-01

> http://arxiv.org/abs/2506.01042v1

Probing large language models (LLMs) has yielded valuable insights into their
internal mechanisms by linking neural representations to interpretable
semantics. However, how neurons functionally co-activate with each other to
give rise to emergent capabilities remains largely unknown, hindering a deeper
understanding and safer development of LLMs. In this work, we introduce graph
probing, a method for uncovering the functional connectivity topology of LLM
neurons and relating it to language generation performance. By analyzing
internal neural graphs across diverse LLM families and scales, we discover a
universal predictability of next-token prediction performance using only neural
topology. This predictability is robust even when retaining just 1% of neuron
connections or probing models after only 8 pretraining steps, highlighting the
**sparsity** and early emergence of topological patterns. Further graph matching
analysis suggests that, despite significant distinctions in architectures,
parameters, and training data, different LLMs develop intricate and consistent
neural topological structures that may form the foundation for their language
generation abilities. Codes and data for the graph probing toolbox are released
at https://github.com/DavyMorgan/llm-graph-probing.


## Graph Neural Networks for Jamming Source Localization

>Authors: Dania Herzalla, Willian T. Lunardi, Martin Andreoni

>2025-06-01

> http://arxiv.org/abs/2506.03196v1

Graph-based learning has emerged as a transformative approach for modeling
complex relationships across diverse domains, yet its potential in wireless
security remains largely unexplored. In this work, we introduce the first
application of graph-based learning for jamming source localization, addressing
the imminent threat of jamming attacks in wireless networks. Unlike geometric
optimization techniques that struggle under environmental uncertainties and
dense interference, we reformulate localization as an inductive graph
regression task. Our approach integrates structured node representations that
encode local and global signal aggregation, ensuring spatial coherence and
adaptive signal fusion. To enhance robustness, we incorporate an
attention-based graph neural network that adaptively refines neighborhood
influence and introduces a confidence-guided estimation mechanism that
dynamically balances learned predictions with domain-informed priors. We
evaluate our approach under complex radio frequency environments with varying
sampling densities and signal propagation conditions, conducting comprehensive
ablation studies on graph construction, feature selection, and pooling
strategies. Results demonstrate that our novel graph-based learning framework
significantly outperforms established localization baselines, particularly in
challenging scenarios with **sparse** and obfuscated signal information. Code is
available at
[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).


## Autoregressive Images Watermarking through Lexical Biasing An Approach Resistant to Regeneration Attack

>Authors: Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, Jinjun Wang

>2025-06-01

> http://arxiv.org/abs/2506.01011v1

Autoregressive (AR) image generation models have gained increasing attention
for their breakthroughs in synthesis quality, highlighting the need for robust
watermarking to prevent misuse. However, existing in-generation watermarking
techniques are primarily designed for diffusion models, where watermarks are
embedded within diffusion latent states. This design poses significant
challenges for direct adaptation to AR models, which generate images
sequentially through token prediction. Moreover, diffusion-based regeneration
attacks can effectively erase such watermarks by perturbing diffusion latent
states. To address these challenges, we propose Lexical Bias Watermarking
(LBW), a novel framework designed for AR models that resists regeneration
attacks. LBW embeds watermarks directly into token maps by biasing token
selection toward a predefined green list during generation. This approach
ensures seamless integration with existing AR models and extends naturally to
post-hoc watermarking. To increase the security against white-box attacks,
instead of using a single green list, the green list for each image is randomly
sampled from a pool of green lists. Watermark detection is performed via
**quantization** and statistical analysis of the token distribution. Extensive
experiments demonstrate that LBW achieves superior watermark robustness,
particularly in resisting regeneration attacks.


## FedQuad Adaptive Layer-wise LoRA Deployment and Activation Quantization for Federated Fine-Tuning

>Authors: Rukuo Li, Jianchun Liu, Hongli Xu, Liusheng Huang

>2025-06-01

> http://arxiv.org/abs/2506.01001v1

Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning
large language models (LLMs) in privacy-sensitive scenarios. However, practical
deployment remains challenging due to the limited resources on end devices.
Existing methods typically utilize parameter-efficient fine-tuning (PEFT)
techniques, such as Low-Rank Adaptation (LoRA), to substantially reduce
communication overhead. Nevertheless, significant memory usage for activation
storage and computational demands from full backpropagation remain major
barriers to efficient deployment on resource-constrained end devices. Moreover,
substantial resource heterogeneity across devices results in severe
synchronization bottlenecks, diminishing the overall fine-tuning efficiency. To
address these issues, we propose FedQuad, a novel LoRA-based FedFT framework
that adaptively adjusts the LoRA depth (the number of consecutive tunable LoRA
layers from the output) according to device computational capabilities, while
employing activation **quantization** to reduce memory overhead, thereby enabling
efficient deployment on resource-constrained devices. Specifically, FedQuad
first identifies the feasible and efficient combinations of LoRA depth and the
number of activation **quantization** layers based on device-specific resource
constraints. Subsequently, FedQuad employs a greedy strategy to select the
optimal configurations for each device, effectively accommodating system
heterogeneity. Extensive experiments demonstrate that FedQuad achieves a
1.4-5.3x convergence **acceleration** compared to state-of-the-art baselines when
reaching target accuracy, highlighting its efficiency and deployability in
resource-constrained and heterogeneous end-device environments.


## Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO

>Authors: Tingting Zhang, Sergiy A. Vorobyov, David J. Love, Taejoon Kim, Kai Dong

>2025-06-01

> http://arxiv.org/abs/2506.00967v1

Optimization-based power control algorithms are predominantly iterative with
high computational complexity, making them impractical for real-time
applications in cell-free massive multiple-input multiple-output (CFmMIMO)
systems. Learning-based methods have emerged as a promising alternative, and
among them, graph neural networks (GNNs) have demonstrated their excellent
performance in solving power control problems. However, all existing GNN-based
approaches assume ideal orthogonality among pilot sequences for user equipments
(UEs), which is unrealistic given that the number of UEs exceeds the available
orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based
methods assume a fixed number of UEs, whereas the number of active UEs varies
over time in practice. Additionally, supervised training necessitates costly
computational resources for computing the target power control solutions for a
large volume of training samples. To address these issues, we propose a graph
attention network for downlink power control in CFmMIMO systems that operates
in a self-supervised manner while effectively handling pilot contamination and
adapting to a dynamic number of UEs. Experimental results show its
effectiveness, even in comparison to the optimal accelerated projected gradient
method as a baseline.


## Unlocking Personalized Knowledge in Federated Large Language Model The Power of Mixture of Experts

>Authors: Fan Liu, Bikang Pan, Zhongyi Wang, Xi Yao, Xiaoying Tang, Jingya Wang, Ye Shi

>2025-06-01

> http://arxiv.org/abs/2506.00965v1

The Mixture of Experts (MoE) architecture has emerged as a prominent strategy
for scaling large language models (LLMs), effectively leveraging **sparse**
activation and facilitating task-specific personalization. However, current
federated learning (FL) approaches are primarily designed for dense models,
making them unable to directly exploit the **sparsity** inherent in MoE
architectures. Treating MoE models as dense networks in federated scenarios
results in excessive communication overhead and computational costs,
undermining the potential for personalized knowledge sharing. To address these
challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel
federated learning framework explicitly tailored for MoE-based LLMs. FLEx
efficiently personalizes by **pruning** the global MoE model to keep only one
expert per client, and employs an adaptive gating mechanism to reintegrate
these personalized experts into the pre-trained MoE layers, ensuring the
original backbone architecture remains unchanged. These personalized experts
are trained with local data and stored locally on each client, while the shared
modules are aggregated globally. Extensive evaluations on diverse
instruction-based datasets under non-IID conditions consistently demonstrate
that FLEx outperforms existing federated baselines. Our code is available at
https://anonymous.4open.science/r/FLEx-8F12.


## Speaking Beyond Language A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues

>Authors: Youngmin Kim, Jiwan Chung, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu

>2025-06-01

> http://arxiv.org/abs/2506.00958v1

Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-**quantize**d nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.


## MedBookVQA A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book

>Authors: Sau Lai Yip, Sunan He, Yuxiang Nie, Shu Pui Chan, Yilin Ye, Sum Ying Lam, Hao Chen

>2025-06-01

> http://arxiv.org/abs/2506.00855v1

The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.


## LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery

>Authors: Xingyu Wu, Kui Yu, Jibin Wu, Kay Chen Tan

>2025-06-01

> http://arxiv.org/abs/2506.00844v1

This paper critically re-evaluates LLMs' role in causal discovery and argues
against their direct involvement in determining causal relationships. We
demonstrate that LLMs' autoregressive, correlation-driven modeling inherently
lacks the theoretical grounding for causal reasoning and introduces
unreliability when used as priors in causal discovery algorithms. Through
empirical studies, we expose the limitations of existing LLM-based methods and
reveal that deliberate prompt engineering (e.g., injecting ground-truth
knowledge) could overstate their performance, helping to explain the
consistently favorable results reported in much of the current literature.
Based on these findings, we strictly confined LLMs' role to a non-decisional
auxiliary capacity: LLMs should not participate in determining the existence or
directionality of causal relationships, but can assist the search process for
causal graphs (e.g., LLM-based heuristic search). Experiments across various
settings confirm that, by strictly isolating LLMs from causal decision-making,
LLM-guided heuristic search can accelerate the convergence and outperform both
traditional and LLM-based methods in causal structure learning. We conclude
with a call for the community to shift focus from naively applying LLMs to
developing specialized models and training method that respect the core
principles of causal discovery.


## LIFT the Veil for the Truth Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning

>Authors: Zihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, Shiwei Liu

>2025-06-01

> http://arxiv.org/abs/2506.00772v1

Recent studies have shown that supervised fine-tuning of LLMs on a small
number of high-quality datasets can yield strong reasoning capabilities.
However, full fine-tuning (Full FT), while powerful, is computationally
expensive and susceptible to overfitting and catastrophic forgetting,
particularly when data is limited. Sparse fine-tuning, which previously
achieved notable success by updating only a small subset of model parameters,
offers a promising trade-off between efficiency and effectiveness. Yet, it has
lagged behind in the LLM era due to the difficulty of identifying parameters
truly critical for reasoning. In this work, we state that weights with the
largest magnitude after low-rank approximation are critical weights for
fine-tuning, which we call Principal Weights. Surprisingly, while
magnitude-based **sparse** fine-tuning performs poorly as a baseline on LLM
fine-tuning, it becomes highly effective after rank reduction. These insights
motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only
updates the top 5% Principal Weights throughout training and consistently
achieves better performance on reasoning tasks than Full FT, while maintaining
memory efficiency on par with popular parameter-efficient fine-tuning methods.
In addition to strong performance on target domains such as arithmetic
reasoning, LIFT also retains up to 20% more source-domain knowledge, compared
to Full FT and LoRA. Our code is available at:
https://github.com/zihanghliu/LIFT.


## Beyond Attention Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies

>Authors: Sai Vamsi Alisetti, Vikas Kalagi, Sanjukta Krishnagopal

>2025-06-01

> http://arxiv.org/abs/2506.00770v1

Spatio-temporal forecasting is critical in applications such as traffic
prediction, energy demand modeling, and weather monitoring. While Graph
Attention Networks (GATs) are popular for modeling spatial dependencies, they
rely on predefined adjacency structures and dynamic attention scores,
introducing inductive biases and computational overhead that can obscure
interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked
attention with a fully learnable, symmetric node interaction matrix, capturing
latent spatial relationships without relying on fixed graph topologies. Our
framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,
outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a
21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop
dataset across all forecasting horizons (15 to 60 minutes). Additionally, we
observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it
recovers **sparse**, topology-aware attention patterns that align with community
structure. Spectral and clustering analyses show that the model captures both
localized and global dynamics, offering insights into the functional topology
driving predictions. This highlights how structure learning can simultaneously
support prediction, computational efficiency, and topological interpretabil-ity
in dynamic graph-based domains.


## Vector fields as a framework for modelling the mobility of commodities

>Authors: Sima Farokhnejad, Anglica S. da Mata, Mariana Macedo, Ronaldo Menezes

>2025-05-31

> http://arxiv.org/abs/2506.02047v1

Commodities, including livestock, flow through trade networks globally, with
trajectories that can be effectively captured using mobility pattern modelling
approaches similar to those used in human mobility studies. However,
documenting these movements comprehensively presents significant challenges; it
can be unrealistic, costly, and may conflict with data protection regulations.
As a result, mobility datasets typically contain uncertainties due to **sparsity**
and limitations in data collection. Origin-destination (OD) representations
offer a powerful framework for modelling movement patterns and are widely
adopted in mobility studies. However, these matrices possess inherent
limitations: locations absent from the OD framework lack spatial information on
potential mobility directions and intensities. This spatial incompleteness
creates analytical gaps across different geographical scales, constraining our
ability to characterise movement patterns in underrepresented areas. In this
study, we introduce a vector-field-based method to address these data
challenges, transforming OD data into vector fields capturing spatial flow
patterns comprehensively enabling us to study mobility directions solidly. We
use cattle trade data from Minas Gerais, Brazil, as our case study for
commodity flows. This region's large livestock trading network makes it an
ideal test case. Cattle movements are significant as they affect disease
transmission, including foot-and-mouth disease. Accurately modelling these
flows allows better surveillance and control strategies. Our vector-field
approach reveals fundamental patterns in commodity mobility and can infer
movement information for unrepresented locations. Our approach offers an
alternative to traditional network-based models, enhancing our capacity to
infer mobility patterns from incomplete datasets and advancing our
understanding of large-scale commodity trades.


## Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers

>Authors: Kazuki Irie, Morris Yau, Samuel J. Gershman

>2025-05-31

> http://arxiv.org/abs/2506.00744v1

We develop hybrid memory architectures for general-purpose sequence
processing neural networks, that combine key-value memory using softmax
attention (**KV**-memory) with dynamic synaptic memory through fast-weight
programming (FW-memory) -- the core principles of quadratic and linear
transformers, respectively. These two memory systems have complementary but
individually limited properties: **KV**-memory offers precise retrieval but is
constrained by quadratic complexity in sequence length, while FW-memory
supports arbitrarily long sequences and enables more expressive computation but
sacrifices precise recall. We propose and compare three methods to blend these
two systems into a single memory system to leverage the strengths of both. We
conduct experiments on general language modeling and retrieval tasks by
training 340M- and 1.3B-parameter models from scratch, as well as on synthetic
algorithmic tasks designed to precisely illustrate the benefits of certain
hybrid methods over others. We also evaluate our hybrid memory systems on
reinforcement learning in partially observable environments. Overall, we
demonstrate how a well-designed hybrid can overcome the limitations of its
individual components, offering new insights into the design principle of
neural memory systems.


## Assortment of Attention Heads Accelerating Federated PEFT with Head Pruning and Strategic Client Selection

>Authors: Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda

>2025-05-31

> http://arxiv.org/abs/2506.00743v1

Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in
adapting Large Language Models (LLMs) for downstream tasks in Natural Language
Processing. However, its adoption in privacy-preserving distributed learning
frameworks, such as Federated Learning (FL), remains relatively limited. This
is mainly due to challenges specific to FL, such as resource-constrained
devices and diverse data distributions among clients. In this paper, we propose
an efficient method to perform PEFT within the FL framework for Multi-Head
Attention (MHA) based language models. We address the challenges through head
**pruning**, a novel head-specific weighted aggregation mechanism, and a client
selection strategy. Head **pruning** minimizes training complexity within the
clients, guided by the importance score computed based on the confidence of the
attention head. Weighted aggregation of heads ensures the global model captures
crucial updates from diverse clients complementing our client selection
strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,
XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model
with LoRA as our PEFT method, attaining **sparsity** levels of up to 90%, resulting
in a communication advantage of up to 1.8x and a reduction in training OPs of
3.9x while maintaining the accuracy drop under 2%.


## From Local Cues to Global Percepts Emergent Gestalt Organization in Self-Supervised Vision Models

>Authors: Tianqin Li, Ziqi Wen, Leiran Song, Jun Liu, Zhi Jing, Tai Sing Lee

>2025-05-31

> http://arxiv.org/abs/2506.00718v1

Human vision organizes local cues into coherent global forms using Gestalt
principles like closure, proximity, and figure-ground assignment -- functions
reliant on global spatial structure. We investigate whether modern vision
models show similar behaviors, and under what training conditions these emerge.
We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)
exhibit activation patterns consistent with Gestalt laws, including illusory
contour completion, convexity preference, and dynamic figure-ground
segregation. To probe the computational basis, we hypothesize that modeling
global dependencies is necessary for Gestalt-like organization. We introduce
the Distorted Spatial Relationship Testbench (DiSRT), which evaluates
sensitivity to global spatial perturbations while preserving local textures.
Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform
supervised baselines and sometimes even exceed human performance. ConvNeXt
models trained with MAE also exhibit Gestalt-compatible representations,
suggesting such sensitivity can arise without attention architectures. However,
classification finetuning degrades this ability. Inspired by biological vision,
we show that a Top-K activation **sparsity** mechanism can restore global
sensitivity. Our findings identify training conditions that promote or suppress
Gestalt-like perception and establish DiSRT as a diagnostic for global
structure sensitivity across models.


## Optimizing Sensory Neurons Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning

>Authors: Junaid Muzaffar, Khubaib Ahmed, Ingo Frommholz, Zeeshan Pervez, Ahsan ul Haq

>2025-05-31

> http://arxiv.org/abs/2506.00691v2

Training reinforcement learning (RL) agents often requires significant
computational resources and extended training times. To address this, we build
upon the foundation laid by Google Brain's Sensory Neuron, which introduced a
novel neural architecture for reinforcement learning tasks that maintained
permutation in-variance in the sensory neuron system. While the baseline model
demonstrated significant performance improvements over traditional approaches,
we identified opportunities to enhance the efficiency of the learning process
further. We propose a modified attention mechanism incorporating a non-linear
transformation of the key vectors (K) using a mapping function, resulting in a
new set of key vectors (K'). This non-linear mapping enhances the
representational capacity of the attention mechanism, allowing the model to
encode more complex feature interactions and accelerating convergence without
compromising performance. Our enhanced model demonstrates significant
improvements in learning efficiency, showcasing the potential for non-linear
attention mechanisms in advancing reinforcement learning algorithms.


## SafeTuneBed A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning

>Authors: Saad Hossain, Samanvay Vajpayee, Sirisha Rambhatla

>2025-05-31

> http://arxiv.org/abs/2506.00676v1

As large language models (LLMs) become ubiquitous, parameter-efficient
fine-tuning methods and safety-first defenses have proliferated rapidly.
However, the number of approaches and their recent increase have resulted in
diverse evaluations-varied datasets, metrics, and inconsistent threat
settings-making it difficult to fairly compare safety, utility, and robustness
across methods. To address this, we introduce SafeTuneBed, a benchmark and
toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a
diverse repository of multiple fine-tuning datasets spanning sentiment
analysis, question-answering, multi-step reasoning, and open-ended instruction
tasks, and allows for the generation of harmful-variant splits; (ii) enables
integration of state-of-the-art defenses, including alignment-stage
immunization, in-training safeguards, and post-tuning repair; and (iii)
provides evaluators for safety (attack success rate, refusal consistency) and
utility. Built on Python-first, dataclass-driven configs and plugins,
SafeTuneBed requires minimal additional code to specify any fine-tuning regime,
defense method, and metric suite, while ensuring end-to-end reproducibility. We
showcase its value by benchmarking representative defenses across varied
poisoning scenarios and tasks. By standardizing data, code, and metrics,
SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and
comparable research in safe LLM fine-tuning. Code is available at:
https://github.com/criticalml-uw/SafeTuneBed


## Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings

>Authors: Aris J. Aristorenas

>2025-05-31

> http://arxiv.org/abs/2506.00656v1

We propose a permutation-invariant neural architecture for indoor
localization using RSSI scans from Wi-Fi access points. Each scan is modeled as
an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned
embeddings and concatenated with signal strength. These are processed by a Set
Transformer, enabling the model to handle variable-length, **sparse** inputs while
learning attention-based representations over access point relationships. We
evaluate the model on a dataset collected across a campus environment
consisting of six buildings. Results show that the model accurately recovers
fine-grained spatial structure and maintains performance across physically
distinct domains. In our experiments, a simple LSTM consistently outperformed
all other models, achieving the lowest mean localization error across three
tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer
performed competitively, ranking second in every experiment and outperforming
the MLP, RNN, and basic attention models, particularly in scenarios involving
multiple buildings (E2) and multiple floors (E3). Performance degraded most in
E2, where signal conditions varied substantially across buildings, highlighting
the importance of architectural robustness to domain diversity. This work
demonstrates that set-based neural models are a natural fit for signal-based
localization, offering a principled approach to handling **sparse**, unordered
inputs in real-world positioning tasks.


## Qualitative analysis of a quasi-magnetic universe

>Authors: Alan G. Cesar, Mario Novello, Eduardo Bittencourt, Fernando A. Franco

>2025-05-31

> http://arxiv.org/abs/2506.00640v1

We investigate the cosmological dynamics induced by nonlinear electrodynamics
(NLED) in a homogeneous and isotropic universe, focusing on the role of
primordial electromagnetic fields with random spatial orientations. Building
upon a generalization of the Tolman-Ehrenfest averaging procedure, we derive a
modified energy-momentum tensor consistent with FLRW symmetry, incorporating
the influence of the dual invariant G and its statistical contributions. A
specific NLED model with quatratic corrections to Maxwell's Lagrangian is
considered, giving rise to what we define as quasi-magnetic universe (qMU),
interpolating between purely magnetic and statistically null field
configurations. We analyze the resulting cosmological dynamics through
qualitative methods. By casting the equations into autonomous dynamical
systems, we identify the equilibrium points, determine their stability, and
study the behavior of solutions under various spatial curvatures. Our findings
reveal the existence of bouncing and cyclic solutions, regions where energy
conditions are violated, and scenarios of accelerated expansion. Special
attention is given to two limiting cases: the Magnetic Universe (MU) and the
Statistical Null Universe (SNU), both of which exhibit qualitatively distinct
phase portraits and energy-condition behavior. This work provides a
comprehensive framework for understanding the influence of nonlinear
electromagnetic fields in the early universe and opens avenues for exploring
their observational consequences.


## Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing

>Authors: Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah

>2025-05-31

> http://arxiv.org/abs/2506.00574v1

Modern wireless networks must adapt to dynamic conditions while efficiently
managing diverse service demands. Traditional deep reinforcement learning (DRL)
struggles in these environments, as scattered and evolving feedback makes
optimal decision-making challenging. Large Language Models (LLMs) offer a
solution by structuring unorganized network feedback into meaningful latent
representations, helping RL agents recognize patterns more effectively. For
example, in O-RAN slicing, concepts like SNR, power levels and throughput are
semantically related, and LLMs can naturally cluster them, providing a more
interpretable state representation. To leverage this capability, we introduce a
contextualization-based adaptation method that integrates learnable prompts
into an LLM-augmented DRL framework. Instead of relying on full model
fine-tuning, we refine state representations through task-specific prompts that
dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained
on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)
framework. Learnable prompts optimize both semantic clustering and RL
objectives, allowing RL agents to achieve higher rewards in fewer iterations
and adapt more efficiently. By incorporating prompt-augmented learning, our
approach enables faster, more scalable, and adaptive resource allocation in
O-RAN slicing. Experimental results show that it accelerates convergence and
outperforms other baselines.


## ARIA Training Language Agents with Intention-Driven Reward Aggregation

>Authors: Ruihan Yang, Yikai Zhang, Aili Chen, Xintao Wang, Siyu Yuan, Jiangjie Chen, Deqing Yang, Yanghua Xiao

>2025-05-31

> http://arxiv.org/abs/2506.00539v2

Large language models (LLMs) have enabled agents to perform complex reasoning
and decision-making through free-form language interactions. However, in
open-ended language action environments (e.g., negotiation or question-asking
games), the action space can be formulated as a joint distribution over tokens,
resulting in an exponentially large action space. Sampling actions in such a
space can lead to extreme reward **sparsity**, which brings large reward variance,
hindering effective reinforcement learning (RL). To address this, we propose
ARIA, a method that Aggregates Rewards in Intention space to enable efficient
and effective language Agents training. ARIA aims to project natural language
actions from the high-dimensional joint token distribution space into a
low-dimensional intention space, where semantically similar actions are
clustered and assigned shared rewards. This intention-aware reward aggregation
reduces reward variance by densifying reward signals, fostering better policy
optimization. Extensive experiments demonstrate that ARIA not only
significantly reduces policy gradient variance, but also delivers substantial
performance gains of an average of 9.95% across four downstream tasks,
consistently outperforming offline and online RL baselines.


## FLoE Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts

>Authors: Xinyi Wang, Lirong Gao, Haobo Wang, Yiming Zhang, Junbo Zhao

>2025-05-31

> http://arxiv.org/abs/2506.00495v1

Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely
adopted strategy for adapting pre-trained Large Language Models (LLMs) to
downstream tasks, significantly reducing memory and computational costs.
However, most existing PEFT techniques uniformly deploy LoRA adapters across
all layers, disregarding the intrinsic heterogeneity of layer contributions and
task-specific rank requirements. This uniform paradigm leads to redundant
parameter allocation and suboptimal adaptation efficiency. To address these
limitations, we propose FLoE, a novel PEFT framework that introduces two key
innovations: (i) a Fisher information-guided importance scoring mechanism to
dynamically identify task-critical transformer layers for MoE-based low-rank
adaptation, enabling **sparse** adapter deployment; and (ii) a Bayesian
optimization-driven rank allocator that automatically determines optimal LoRA
ranks on specific datasets without exhaustive grid search. Extensive
experiments across diverse LLMs and benchmarks reveal that FLoE achieves
impressive efficiency-accuracy trade-offs, making FLoE particularly
advantageous in resource-constrained environments that necessitate rapid
adaptation.


## Accelerating Diffusion LLMs via Adaptive Parallel Decoding

>Authors: Daniel Israel, Guy Van den Broeck, Aditya Grover

>2025-05-31

> http://arxiv.org/abs/2506.00413v1

The generation speed of LLMs are bottlenecked by autoregressive decoding,
where tokens are predicted sequentially one by one. Alternatively, diffusion
large language models (dLLMs) theoretically allow for parallel token
generation, but in practice struggle to achieve the speed of autoregressive
models without significantly sacrificing quality. We therefore introduce
adaptive parallel decoding (APD), a novel method that dynamically adjusts the
number of tokens sampled in parallel. We achieve this by defining a
multiplicative mixture between the dLLM marginal probabilities and the joint
probability of sequences under a small auxiliary autoregressive model. This
inverts the standard setup of speculative decoding, where the goal is to sample
from a large autoregressive verifier by drafting from a smaller model. We
further optimize APD by enabling **KV** caching and limiting the size of the masked
input. Altogether, our method puts forward three tunable parameters to flexibly
tradeoff throughput and quality. We show that APD provides markedly higher
throughput with minimal quality degradations on downstream benchmarks.


## Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively

>Authors: Jiawei Gu, Shangsong Liang

>2025-05-31

> http://arxiv.org/abs/2506.00396v1

Effective decision-making in Large Language Models (LLMs) is essential for
handling intricate tasks. However, existing approaches prioritize performance
but often overlook the balance between effectiveness and computational cost. To
address this, we first introduce the 3E Criteria to systematically assess the
cost-effectiveness of search strategies, revealing that existing methods often
trade significant efficiency for marginal performance gains. To improve LLM
decision-making while maintaining efficiency, we propose the Speculative Reward
Model (SRM), a plug-and-play framework that seamlessly integrates with existing
search strategies. Specifically, SRM employs an external reward assigner to
predict optimal actions, reducing reliance on LLMs' internal self-evaluation.
And a speculative verification mechanism is used to prune suboptimal choices
and guide the search toward more promising steps. We evaluate SRM on several
complex decision-making tasks including mathematical reasoning, planning and
numerical reasoning in specialized domains. Experimental results show that SRM
reduces costs to 1/10 of the original search framework on average while
maintaining effectiveness.


## Power-of-Two (PoT) Weights in Large Language Models (LLMs)

>Authors: Mahmoud Elgenedy

>2025-05-31

> http://arxiv.org/abs/2506.00315v1

Complexity of Neural Networks is increasing rapidly due to the massive
increase in model parameters. Specifically, in Large Language Models (LLMs),
the number of model parameters has grown exponentially in the past few years,
for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This
raises a significant challenge for implementation, especially for Edge devices
where memory and processing power are very limited. In this work, we
investigate reducing LLM complexity with special type of **quantization**, power of
two (PoT), for linear layers weights and transformer tables. PoT not only
provides memory reduction but more importantly provides significant
computational reduction through converting multiplication to bit shifting. We
obtained preliminary results of PoT **quantization** on Nano-GPT implementation
using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The
PoT **quantization** results are shown to be very promising with cross entropy loss
degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent
power levels.


## MIR Methodology Inspiration Retrieval for Scientific Research Problems

>Authors: Aniketh Garikaparthi, Manasi Patwardhan, Aditya Sanjiv Kanade, Aman Hassan, Lovekesh Vig, Arman Cohan

>2025-05-30

> http://arxiv.org/abs/2506.00249v1

There has been a surge of interest in harnessing the reasoning capabilities
of Large Language Models (LLMs) to accelerate scientific discovery. While
existing approaches rely on grounding the discovery process within the relevant
literature, effectiveness varies significantly with the quality and nature of
the retrieved literature. We address the challenge of retrieving prior work
whose concepts can inspire solutions for a given research problem, a task we
define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset
tailored for training and evaluating retrievers on MIR, and establish
baselines. To address MIR, we build the Methodology Adjacency Graph (MAG);
capturing methodological lineage through citation relationships. We leverage
MAG to embed an "intuitive prior" into dense retrievers for identifying
patterns of methodological inspiration beyond superficial semantic similarity.
This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average
Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking
strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and
+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we
exhibit the promise of MIR in enhancing automated scientific discovery and
outline avenues for advancing inspiration-driven retrieval.


## MOFGPT Generative Design of Metal-Organic Frameworks using Language Models

>Authors: Srivathsan Badrinarayanan, Rishikesh Magar, Akshay Antony, Radheesh Sharma Meda, Amir Barati Farimani

>2025-05-30

> http://arxiv.org/abs/2506.00198v1

The discovery of Metal-Organic Frameworks (MOFs) with application-specific
properties remains a central challenge in materials chemistry, owing to the
immense size and complexity of their structural design space. Conventional
computational screening techniques such as molecular simulations and density
functional theory (DFT), while accurate, are computationally prohibitive at
scale. Machine learning offers an exciting alternative by leveraging
data-driven approaches to accelerate materials discovery. The complexity of
MOFs, with their extended periodic structures and diverse topologies, creates
both opportunities and challenges for generative modeling approaches. To
address these challenges, we present a reinforcement learning-enhanced,
transformer-based framework for the de novo design of MOFs. Central to our
approach is MOFid, a chemically-informed string representation encoding both
connectivity and topology, enabling scalable generative modeling. Our pipeline
comprises three components: (1) a generative GPT model trained on MOFid
sequences, (2) MOFormer, a transformer-based property predictor, and (3) a
reinforcement learning (RL) module that optimizes generated candidates via
property-guided reward functions. By integrating property feedback into
sequence generation, our method drives the model toward synthesizable,
topologically valid MOFs with desired functional attributes. This work
demonstrates the potential of large language models, when coupled with
reinforcement learning, to accelerate inverse design in reticular chemistry and
unlock new frontiers in computational MOF discovery.


## TalkingHeadBench A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection

>Authors: Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu, Roni Sengupta

>2025-05-30

> http://arxiv.org/abs/2505.24866v1

The rapid advancement of talking-head deepfake generation fueled by advanced
generative models has elevated the realism of synthetic videos to a level that
poses substantial risks in domains such as media, politics, and finance.
However, current benchmarks for deepfake talking-head detection fail to reflect
this progress, relying on outdated generators and offering limited insight into
model robustness and generalization. We introduce TalkingHeadBench, a
comprehensive multi-model multi-generator benchmark and curated dataset
designed to evaluate the performance of state-of-the-art detectors on the most
advanced generators. Our dataset includes deepfakes synthesized by leading
academic and commercial models and features carefully constructed protocols to
assess generalization under distribution shifts in identity and generator
characteristics. We benchmark a diverse set of existing detection methods,
including CNNs, vision transformers, and temporal models, and analyze their
robustness and generalization capabilities. In addition, we provide error
analysis using Grad-CAM visualizations to expose common failure modes and
detector biases. TalkingHeadBench is hosted on
https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to
all data splits and protocols. Our benchmark aims to accelerate research
towards more robust and generalizable detection models in the face of rapidly
evolving generative techniques.


## Towards Secure MLOps Surveying Attacks, Mitigation Strategies, and Research Challenges

>Authors: Raj Patel, Himanshu Tripathi, Jasper Stone, Noorbakhsh Amiri Golilarz, Sudip Mittal, Shahram Rahimi, Vini Chaudhary

>2025-05-30

> http://arxiv.org/abs/2506.02032v1

The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.


## LegalEval-Q A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text

>Authors: Li yunhan, Wu gengshen

>2025-05-30

> http://arxiv.org/abs/2505.24826v1

As large language models (LLMs) are increasingly used in legal applications,
current evaluation benchmarks tend to focus mainly on factual accuracy while
largely neglecting important linguistic quality aspects such as clarity,
coherence, and terminology. To address this gap, we propose three steps: First,
we develop a regression model to evaluate the quality of legal texts based on
clarity, coherence, and terminology. Second, we create a specialized set of
legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off
at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at
72 billion parameters. Second, engineering choices such as **quantization** and
context length have a negligible impact, as indicated by statistical
significance thresholds above 0.016. Third, reasoning models consistently
outperform base architectures. A significant outcome of our research is the
release of a ranking list and Pareto analysis, which highlight the Qwen3 series
as the optimal choice for cost-performance tradeoffs. This work not only
establishes standardized evaluation protocols for legal LLMs but also uncovers
fundamental limitations in current training data refinement approaches. Code
and models are available at: https://github.com/lyxx3rd/LegalEval-Q.


## TC-GS A Faster Gaussian Splatting Module Utilizing Tensor Cores

>Authors: Zimu Liao, Jifeng Ding, Rong Fu, Siwei Cui, Ruixuan Gong, Li Wang, Boni Hu, Yi Wang, Hengjie Li, XIngcheng Zhang, Hui Wang

>2025-05-30

> http://arxiv.org/abs/2505.24796v1

3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where conditional alpha-blending dominates the time cost in the
rendering pipeline. This paper proposes TC-GS, an algorithm-independent
universal module that expands Tensor Core (TCU) applicability for 3DGS, leading
to substantial speedups and seamless integration into existing 3DGS
optimization frameworks. The key innovation lies in mapping alpha computation
to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS
implementations. TC-GS provides plug-and-play **acceleration** for existing
top-tier **acceleration** algorithms tightly coupled with rendering pipeline
designs, like Gaussian compression and redundancy elimination algorithms.
Additionally, we introduce a global-to-local coordinate transformation to
mitigate rounding errors from quadratic terms of pixel coordinates caused by
Tensor Core half-precision computation. Extensive experiments demonstrate that
our method maintains rendering quality while providing an additional 2.18x
speedup over existing Gaussian **acceleration** algorithms, thus reaching up to a
total 5.6x **acceleration**. The code is currently available at anonymous
\href{https://github.com/TensorCore3DGS/3DGSTensorCore}


## Gated Multimodal Graph Learning for Personalized Recommendation

>Authors: Sibei Liu, Yuanzhe Zhang, Xiang Li, Yunbo Liu, Chengwei Feng, Hao Yang

>2025-05-30

> http://arxiv.org/abs/2506.00107v1

Multimodal recommendation has emerged as a promising solution to alleviate
the cold-start and **sparsity** problems in collaborative filtering by
incorporating rich content information, such as product images and textual
descriptions. However, effectively integrating heterogeneous modalities into a
unified recommendation framework remains a challenge. Existing approaches often
rely on fixed fusion strategies or complex architectures , which may fail to
adapt to modality quality variance or introduce unnecessary computational
overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular
recommendation framework that combines graph-based user modeling with adaptive
multimodal item encoding. The model employs a gated fusion module to
dynamically balance the contribution of visual and textual modalities, enabling
fine-grained and content-aware item representations. Meanwhile, a two-layer
LightGCN encoder captures high-order collaborative signals by propagating
embeddings over the user-item interaction graph without relying on nonlinear
transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain.
Experimental results demonstrate that RLMultimodalRec consistently outperforms
several competitive baselines, including collaborative filtering, visual-aware,
and multimodal GNN-based methods. The proposed approach achieves significant
improvements in top-K recommendation metrics while maintaining scalability and
interpretability, making it suitable for practical deployment.


## EXP-Bench Can AI Conduct AI Research Experiments?

>Authors: Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen

>2025-05-30

> http://arxiv.org/abs/2505.24785v2

Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.


## AFLoRA Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption

>Authors: Yajie Zhou, Xiaoyi Pang, Zhibo Wang

>2025-05-30

> http://arxiv.org/abs/2505.24773v1

Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank **pruning** to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.


## SUMO Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training

>Authors: Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum

>2025-05-30

> http://arxiv.org/abs/2505.24749v1

Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential **acceleration** in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.


## HELM Hyperbolic Large Language Models via Mixture-of-Curvature Experts

>Authors: Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying

>2025-05-30

> http://arxiv.org/abs/2505.24722v1

Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-**KV**-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.


## A Simple Linear Patch Revives Layer-Pruned Large Language Models

>Authors: Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan

>2025-05-30

> http://arxiv.org/abs/2505.24680v1

Layer **pruning** has become a popular technique for compressing large language
models (LLMs) due to its simplicity. However, existing layer **pruning** methods
often suffer from significant performance drops. We identify that this
degradation stems from the mismatch of activation magnitudes across layers and
tokens at the **pruning** interface. To address this, we propose LinearPatch, a
simple plug-and-play technique to revive the layer-pruned LLMs. The proposed
method adopts Hadamard transformation to suppress massive outliers in
particular tokens, and channel-wise scaling to align the activation magnitudes.
These operations can be fused into a single matrix, which functions as a patch
to bridge the **pruning** interface with negligible inference overhead. LinearPatch
retains up to 94.15% performance of the original model when **pruning** 5 layers of
LLaMA-3-8B on the question answering benchmark, surpassing existing
state-of-the-art methods by 4%. In addition, the patch matrix can be further
optimized with memory efficient offline knowledge distillation. With only 5K
samples, the retained performance of LinearPatch can be further boosted to
95.16% within 30 minutes on a single computing card.


## Decoding Knowledge Attribution in Mixture-of-Experts A Framework of Basic-Refinement Collaboration and Efficiency Analysis

>Authors: Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu

>2025-05-30

> http://arxiv.org/abs/2505.24593v1

The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in **sparse**
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze **sparse** MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.


## CREFT Sequential Multi-Agent LLM for Character Relation Extraction

>Authors: Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim

>2025-05-30

> http://arxiv.org/abs/2505.24553v1

Understanding complex character relations is crucial for narrative analysis
and efficient script evaluation, yet existing extraction methods often fail to
handle long-form narratives with nuanced interactions. To address this
challenge, we present CREFT, a novel sequential framework leveraging
specialized Large Language Model (LLM) agents. First, CREFT builds a base
character graph through knowledge distillation, then iteratively refines
character composition, relation extraction, role identification, and group
assignments. Experiments on a curated Korean drama dataset demonstrate that
CREFT significantly outperforms single-agent LLM baselines in both accuracy and
completeness. By systematically visualizing character networks, CREFT
streamlines narrative comprehension and accelerates script review -- offering
substantial benefits to the entertainment, publishing, and educational sectors.


## Cross-Attention Speculative Decoding

>Authors: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee

>2025-05-30

> http://arxiv.org/abs/2505.24544v1

Speculative decoding (SD) is a widely adopted approach for accelerating
inference in large language models (LLMs), particularly when the draft and
target models are well aligned. However, state-of-the-art SD methods typically
rely on tightly coupled, self-attention-based Transformer decoders, often
augmented with auxiliary pooling or fusion layers. This coupling makes them
increasingly complex and harder to generalize across different models. We
present Budget EAGLE (Beagle), the first, to our knowledge,
cross-attention-based Transformer decoder SD model that achieves performance on
par with leading self-attention SD models (EAGLE-v2) while eliminating the need
for pooling or auxiliary components, simplifying the architecture, improving
training efficiency, and maintaining stable memory usage during training-time
simulation. To enable effective training of this novel architecture, we propose
Two-Stage Block-Attention Training, a new method that achieves training
stability and convergence efficiency in block-level attention scenarios.
Extensive experiments across multiple LLMs and datasets show that Beagle
achieves competitive inference speedups and higher training efficiency than
EAGLE-v2, offering a strong alternative for architectures in speculative
decoding.


## LPASS Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs

>Authors: Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux

>2025-05-30

> http://arxiv.org/abs/2505.24451v1

Large Language Models (LLMs) are being extensively used for cybersecurity
purposes. One of them is the detection of vulnerable codes. For the sake of
efficiency and effectiveness, compression and fine-tuning techniques are being
developed, respectively. However, they involve spending substantial
computational efforts. In this vein, we analyse how Linear Probes (LPs) can be
used to provide an estimation on the performance of a compressed LLM at an
early phase -- before fine-tuning. We also show their suitability to set the
cut-off point when applying layer **pruning** compression. Our approach, dubbed
$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25
most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in
142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be
removed, respectively, with no precision loss; (2) they provide an early
estimate of the post-fine-tuning and post-compression model effectiveness, with
3\% and 8.68\% as the lowest and average precision errors, respectively.
$LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy
in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed
versions of Gemma outperform the original ones by 1.6\% of F1-score at a
maximum while saving 29.4 \% and 23.8\% of training and inference time and
42.98\% of model size.


## Model Unlearning via Sparse Autoencoder Subspace Guided Projections

>Authors: Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou

>2025-05-30

> http://arxiv.org/abs/2505.24428v1

Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to **sparse** autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.


## Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning

>Authors: Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy

>2025-05-30

> http://arxiv.org/abs/2505.24360v2

Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.


## ReCalKV Low-Rank KV Cache Compression via Head Reordering and Offline Calibration

>Authors: Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang

>2025-05-30

> http://arxiv.org/abs/2505.24357v2

Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (**KV**) cache. This makes **KV** cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the **KV** cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCal**KV**, a post-training **KV** cache
compression method that reduces the hidden dimensions of the **KV** cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCal**KV** outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. The code and models will be available at:
https://github.com/XIANGLONGYAN/ReCal**KV**.


## STAR-Net An Interpretable Model-Aided Network for Remote Sensing Image Denoising

>Authors: Jingjing Liu, Jiashun Jin, Xianchao Xiu, Jianhua Zhang, Wanquan Liu

>2025-05-30

> http://arxiv.org/abs/2505.24327v1

Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named **sparse** tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
**sparse** variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.


## GradPower Powering Gradients for Faster Language Model Pre-Training

>Authors: Mingze Wang, Jinbo Wang, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu

>2025-05-30

> http://arxiv.org/abs/2505.24275v1

We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.


## Reasoning Can Hurt the Inductive Abilities of Large Language Models

>Authors: Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang

>2025-05-30

> http://arxiv.org/abs/2505.24225v1

Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
**sparse** examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.


## CLaSp In-Context Layer Skip for Self-Speculative Decoding

>Authors: Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang

>2025-05-30

> http://arxiv.org/abs/2505.24196v1

Speculative decoding (SD) is a promising method for accelerating the decoding
process of Large Language Models (LLMs). The efficiency of SD primarily hinges
on the consistency between the draft model and the verify model. However,
existing drafting approaches typically require additional modules to be
trained, which can be challenging to implement and ensure compatibility across
various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping
strategy for self-speculative decoding. Unlike prior methods, CLaSp does not
require additional drafting modules or extra training. Instead, it employs a
plug-and-play mechanism by skipping intermediate layers of the verify model to
construct a compressed draft model. Specifically, we develop a dynamic
programming algorithm that optimizes the layer-skipping process by leveraging
the complete hidden states from the last verification stage as an objective.
This enables CLaSp to dynamically adjust its layer-skipping strategy after each
verification stage, without relying on pre-optimized sets of skipped layers.
Experimental results across diverse downstream tasks demonstrate that CLaSp
achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the
original distribution of the generated text.


## Beyond Exponential Decay Rethinking Error Accumulation in Large Language Models

>Authors: Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir

>2025-05-30

> http://arxiv.org/abs/2505.24187v1

The prevailing assumption of an exponential decay in large language model
(LLM) reliability with sequence length, predicated on independent per-token
error probabilities, posits an inherent limitation for long autoregressive
outputs. Our research fundamentally challenges this view by synthesizing
emerging evidence that LLM errors are not uniformly distributed but are
concentrated at **sparse** "key tokens" ($5-10\%$ of total tokens) representing
critical decision junctions. By distinguishing these high-impact tokens from
the increasingly predictable majority, we introduce a new reliability formula
explaining the sustained coherence of modern LLMs over thousands of tokens.
Converging research streams reveal that long-context performance primarily
depends on accurately navigating a few crucial semantic decision points rather
than on uniform token-level accuracy, enabling targeted strategies that
significantly outperform brute-force approaches. We thus propose a framework
for next-generation systems centered on selective preservation of semantically
vital tokens, dynamic computational allocation at uncertain decision
boundaries, multi-path exploration at ambiguities, and architectures aligned
with natural semantic domains. This marks a fundamental shift from raw scaling
to strategic reasoning, promising breakthrough performance without
proportionate computational scaling and offering a more nuanced understanding
that supersedes the exponential decay hypothesis, thereby opening pathways
toward substantially more powerful and efficient language systems.


## SALE  Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling

>Authors: Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui

>2025-05-30

> http://arxiv.org/abs/2505.24179v1

Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing **sparse** attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained **sparse** attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
**quantize**d query-key products, followed by block-**sparse** attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.


## Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation

>Authors: Ryota Miyano, Yuki Arase

>2025-05-30

> http://arxiv.org/abs/2505.24174v1

This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.


## S4-Driver Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation

>Authors: Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan

>2025-05-30

> http://arxiv.org/abs/2505.24139v2

The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel **sparse** volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.


## Symmetry-Breaking Magneto-Optical Effects in Altermagnets

>Authors: Jiuyu Sun, Yongping Du, Erjun Kan

>2025-05-30

> http://arxiv.org/abs/2505.24124v1

The recently discovered altermagnets (AMs), hosting momentum-dependent spin
splitting and vanishing net magnetization, have attracted intensive attention
for their promising application in novel spintronics. However, limited by
facility and material constraints, experimentally distinguishing them from
conventional antiferromagnets (AFMs) remains a challenge, which hinders the
high-throughput screening for AM candidates. Here, we predict strain-mediated
magneto-optical responses in AMs, which can serve as a universal and
experimentally accessible strategy for efficient identification of AMs.
Symmetry analysis reveals that uniaxial strain can selectively breaks rotation
or mirror symmetries in AMs while preserving $PT$ symmetry in AFMs, thereby
activating distinct linear magneto-optical responses (e.g., optical absorption
and Kerr rotation) unique to AMs. First-principles calculations across
prototypical systems -- including semiconducting V$_2$Se$_2$O monolayer and
metallic CrSb bulk -- show that the strain-induced optical signatures are
significant enough for conventional optical measurements. Our work establishes
a rapid, non-invasive characterization methodology for altermagnetism across
material platforms, accelerating its exploration for spin-based technologies.


## SkyLB A Locality-Aware Cross-Region Load Balancer for LLM Inference

>Authors: Tian Xia, Ziming Mao, Jamison Kerney, Ethan J. Jackson, Zhifei Li, Jiarong Xing, Scott Shenker, Ion Stoica

>2025-05-30

> http://arxiv.org/abs/2505.24095v1

Serving Large Language Models (LLMs) efficiently in multi-region setups
remains a challenge. Due to cost and GPU availability concerns, providers
typically deploy LLMs in multiple regions using instance with long-term
commitments, like reserved instances or on-premise clusters, which are often
underutilized due to their region-local traffic handling and diurnal traffic
variance. In this paper, we introduce SkyLB, a locality-aware multi-region load
balancer for LLM inference that aggregates regional diurnal patterns through
cross-region traffic handling. By doing so, SkyLB enables providers to reserve
instances based on expected global demand, rather than peak demand in each
individual region. Meanwhile, SkyLB preserves **KV**-Cache locality and a balanced
load, ensuring cost efficiency without sacrificing performance. SkyLB achieves
this with a cache-aware cross-region traffic handler and a selective pushing
load balancing mechanism based on checking pending requests. Our evaluation on
real-world workloads shows that it achieves 1.12-2.06x higher throughput and
1.74-6.30x lower latency compared to existing load balancers, while reducing
total serving cost by 25%.

