# 2024-11-29

# Table of Contents
* [Bumblebee cosmology Tests using distance- and time-redshift probes](#Bumblebee-cosmology-Tests-using-distance--and-time-redshift-probes)
* [Perturbation Ontology based Graph Attention Networks](#Perturbation-Ontology-based-Graph-Attention-Networks)
* [FastSwitch Optimizing Context Switching Efficiency in Fairness-aware Large Language Model Serving](#FastSwitch-Optimizing-Context-Switching-Efficiency-in-Fairness-aware-Large-Language-Model-Serving)
* [Neural Image Unfolding Flattening Sparse Anatomical Structures using Neural Fields](#Neural-Image-Unfolding-Flattening-Sparse-Anatomical-Structures-using-Neural-Fields)
* [Preserving Deep Representations In One-Shot Pruning A Hessian-Free Second-Order Optimization Framework](#Preserving-Deep-Representations-In-One-Shot-Pruning-A-Hessian-Free-Second-Order-Optimization-Framework)
* [Break the ID-Language Barrier An Adaption Framework for Sequential Recommendation](#Break-the-ID-Language-Barrier-An-Adaption-Framework-for-Sequential-Recommendation)
* [A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs](#A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs)
* [SALMONN-omni A Codec-free LLM for Full-duplex Speech Understanding and Generation](#SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation)
* [Training Noise Token Pruning](#Training-Noise-Token-Pruning)
* [Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache](#Pushing-the-Limits-of-LLM-Inference-via-2-Bit-Layer-Discriminative-KV-Cache)
* [FlexiBit Fully Flexible Precision Bit-parallel Accelerator Architecture for Arbitrary Mixed Precision AI](#FlexiBit-Fully-Flexible-Precision-Bit-parallel-Accelerator-Architecture-for-Arbitrary-Mixed-Precision-AI)
* [Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification](#Heterogeneous-Relationships-of-Subjects-and-Shapelets-for-Semi-supervised-Multivariate-Series-Classification)
* [Manual-PA Learning 3D Part Assembly from Instruction Diagrams](#Manual-PA-Learning-3D-Part-Assembly-from-Instruction-Diagrams)
* [HAAT Hybrid Attention Aggregation Transformer for Image Super-Resolution](#HAAT-Hybrid-Attention-Aggregation-Transformer-for-Image-Super-Resolution)
* [Hybrid Beamforming Design for Covert mmWave MIMO with Finite-Resolution DACs](#Hybrid-Beamforming-Design-for-Covert-mmWave-MIMO-with-Finite-Resolution-DACs)
* [Engineering Trustworthy Software A Mission for LLMs](#Engineering-Trustworthy-Software-A-Mission-for-LLMs)
* [SoftmAP Software-Hardware Co-design for Integer-Only Softmax on Associative Processors](#SoftmAP-Software-Hardware-Co-design-for-Integer-Only-Softmax-on-Associative-Processors)
* [Low-Bit Quantization Favors Undertrained LLMs Scaling Laws for Quantized LLMs with 100T Training Tokens](#Low-Bit-Quantization-Favors-Undertrained-LLMs-Scaling-Laws-for-Quantized-LLMs-with-100T-Training-Tokens)
* [Attamba Attending To Multi-Token States](#Attamba-Attending-To-Multi-Token-States)
* [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](#Enhancing-Character-Level-Understanding-in-LLMs-through-Token-Internal-Structure-Learning)
* [Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset](#Explainable-AI-for-Classifying-UTI-Risk-Groups-Using-a-Real-World-Linked-EHR-and-Pathology-Lab-Dataset)
* [DapPep Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction](#DapPep-Domain-Adaptive-Peptide-agnostic-Learning-for-Universal-T-cell-Receptor-antigen-Binding-Affinity-Prediction)
* [Scalable iterative pruning of large language and vision models using block coordinate descent](#Scalable-iterative-pruning-of-large-language-and-vision-models-using-block-coordinate-descent)
* [An Ensemble Approach for Brain Tumor Segmentation and Synthesis](#An-Ensemble-Approach-for-Brain-Tumor-Segmentation-and-Synthesis)
* [Accelerating Vision Diffusion Transformers with Skip Branches](#Accelerating-Vision-Diffusion-Transformers-with-Skip-Branches)
* [Scaling Speech-Text Pre-training with Synthetic Interleaved Data](#Scaling-Speech-Text-Pre-training-with-Synthetic-Interleaved-Data)
* [Pushing the Limits of Large Language Model Quantization via the Linearity Theorem](#Pushing-the-Limits-of-Large-Language-Model-Quantization-via-the-Linearity-Theorem)
* [Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient](#Collaborative-Decoding-Makes-Visual-Auto-Regressive-Modeling-Efficient)
* [Fast and Exact Similarity Search in less than a Blink of an Eye](#Fast-and-Exact-Similarity-Search-in-less-than-a-Blink-of-an-Eye)
* [Efficient Deployment of Transformer Models in Analog In-Memory Computing Hardware](#Efficient-Deployment-of-Transformer-Models-in-Analog-In-Memory-Computing-Hardware)
* [Probing vector gravitational atom with eccentric intermediate mass-ratio inspirals](#Probing-vector-gravitational-atom-with-eccentric-intermediate-mass-ratio-inspirals)
* [MAT Multi-Range Attention Transformer for Efficient Image Super-Resolution](#MAT-Multi-Range-Attention-Transformer-for-Efficient-Image-Super-Resolution)
* [An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models](#An-In-depth-Investigation-of-Sparse-Rate-Reduction-in-Transformer-like-Models)
* [LiteVAR Compressing Visual Autoregressive Modelling with Efficient Attention and Quantization](#LiteVAR-Compressing-Visual-Autoregressive-Modelling-with-Efficient-Attention-and-Quantization)
* [Star Attention Efficient LLM Inference over Long Sequences](#Star-Attention-Efficient-LLM-Inference-over-Long-Sequences)
* [PassionSR Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution](#PassionSR-Post-Training-Quantization-with-Adaptive-Scale-in-One-Step-Diffusion-based-Image-Super-Resolution)
* [Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation](#Efficient-LLM-Inference-with-I/O-Aware-Partial-KV-Cache-Recomputation)
* [Dynamic Programming-Based Redundancy Resolution for Path Planning of Redundant Manipulators Considering Breakpoints](#Dynamic-Programming-Based-Redundancy-Resolution-for-Path-Planning-of-Redundant-Manipulators-Considering-Breakpoints)
* [SAR3D Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE](#SAR3D-Autoregressive-3D-Object-Generation-and-Understanding-via-Multi-scale-3D-VQVAE)
* [Factorized Visual Tokenization and Generation](#Factorized-Visual-Tokenization-and-Generation)
* [Quark Real-time, High-resolution, and General Neural View Synthesis](#Quark-Real-time,-High-resolution,-and-General-Neural-View-Synthesis)
* [StructFormer Document Structure-based Masked Attention and its Impact on Language Model Pre-Training](#StructFormer-Document-Structure-based-Masked-Attention-and-its-Impact-on-Language-Model-Pre-Training)
* [DetailGen3D Generative 3D Geometry Enhancement via Data-Dependent Flow](#DetailGen3D-Generative-3D-Geometry-Enhancement-via-Data-Dependent-Flow)
* [Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency](#Efficient-Video-Face-Enhancement-with-Enhanced-Spatial-Temporal-Consistency)
* [VQ-SGen A Vector Quantized Stroke Representation for Sketch Generation](#VQ-SGen-A-Vector-Quantized-Stroke-Representation-for-Sketch-Generation)
* [WTDUN Wavelet Tree-Structured Sampling and Deep Unfolding Network for Image Compressed Sensing](#WTDUN-Wavelet-Tree-Structured-Sampling-and-Deep-Unfolding-Network-for-Image-Compressed-Sensing)
* [Even Sparser Graph Transformers](#Even-Sparser-Graph-Transformers)
* [MH-MoE Multi-Head Mixture-of-Experts](#MH-MoE-Multi-Head-Mixture-of-Experts)
* [NovelGS Consistent Novel-view Denoising via Large Gaussian Reconstruction Model](#NovelGS-Consistent-Novel-view-Denoising-via-Large-Gaussian-Reconstruction-Model)
* [MixPE Quantization and Hardware Co-design for Efficient LLM Inference](#MixPE-Quantization-and-Hardware-Co-design-for-Efficient-LLM-Inference)
* [SKQVC One-Shot Voice Conversion by K-Means Quantization with Self-Supervised Speech Representations](#SKQVC-One-Shot-Voice-Conversion-by-K-Means-Quantization-with-Self-Supervised-Speech-Representations)
* [DF-GNN Dynamic Fusion Framework for Attention Graph Neural Networks on GPUs](#DF-GNN-Dynamic-Fusion-Framework-for-Attention-Graph-Neural-Networks-on-GPUs)
* [Soft-TransFormers for Continual Learning](#Soft-TransFormers-for-Continual-Learning)
* [Anda Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format](#Anda-Unlocking-Efficient-LLM-Inference-with-a-Variable-Length-Grouped-Activation-Data-Format)
* [Towards the LLM-Based Generation of Formal Specifications from Natural-Language Contracts Early Experiments with Symboleo](#Towards-the-LLM-Based-Generation-of-Formal-Specifications-from-Natural-Language-Contracts-Early-Experiments-with-Symboleo)
* [Evaluating Large Language Models for Causal Modeling](#Evaluating-Large-Language-Models-for-Causal-Modeling)
* [LibraGrad Balancing Gradient Flow for Universally Better Vision Transformer Attributions](#LibraGrad-Balancing-Gradient-Flow-for-Universally-Better-Vision-Transformer-Attributions)
* [Variable-size Symmetry-based Graph Fourier Transforms for image compression](#Variable-size-Symmetry-based-Graph-Fourier-Transforms-for-image-compression)
* [A Method for Building Large Language Models with Predefined KV Cache Capacity](#A-Method-for-Building-Large-Language-Models-with-Predefined-KV-Cache-Capacity)
* [PriorDiffusion Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](#PriorDiffusion-Leverage-Language-Prior-in-Diffusion-Models-for-Monocular-Depth-Estimation)
* [Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems](#Task-Scheduling-for-Efficient-Inference-of-Large-Language-Models-on-Single-Moderate-GPU-Systems)
* [LLaMA-MoE v2 Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training](#LLaMA-MoE-v2-Exploring-Sparsity-of-LLaMA-from-Perspective-of-Mixture-of-Experts-with-Post-Training)
* [DrugAgent Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration](#DrugAgent-Automating-AI-aided-Drug-Discovery-Programming-through-LLM-Multi-Agent-Collaboration)
* [Machine Learning-based sEMG Signal Classification for Hand Gesture Recognition](#Machine-Learning-based-sEMG-Signal-Classification-for-Hand-Gesture-Recognition)
* [Reassessing Layer Pruning in LLMs New Insights and Methods](#Reassessing-Layer-Pruning-in-LLMs-New-Insights-and-Methods)
* [freePruner A Training-free Approach for Large Multimodal Model Acceleration](#freePruner-A-Training-free-Approach-for-Large-Multimodal-Model-Acceleration)
* [SPRINT Enables Interpretable and Ultra-Fast Virtual Screening against Thousands of Proteomes](#SPRINT-Enables-Interpretable-and-Ultra-Fast-Virtual-Screening-against-Thousands-of-Proteomes)
* [Efficient Online Inference of Vision Transformers by Training-Free Tokenization](#Efficient-Online-Inference-of-Vision-Transformers-by-Training-Free-Tokenization)
* [ChatBCI A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios](#ChatBCI-A-P300-Speller-BCI-Leveraging-Large-Language-Models-for-Improved-Sentence-Composition-in-Realistic-Scenarios)
* [On the Impact of Fine-Tuning on Chain-of-Thought Reasoning](#On-the-Impact-of-Fine-Tuning-on-Chain-of-Thought-Reasoning)
* [SafeLight Enhancing Security in Optical Convolutional Neural Network Accelerators](#SafeLight-Enhancing-Security-in-Optical-Convolutional-Neural-Network-Accelerators)
* [The Seiberg-Witten Axion](#The-Seiberg-Witten-Axion)
* [XGrammar Flexible and Efficient Structured Generation Engine for Large Language Models](#XGrammar-Flexible-and-Efficient-Structured-Generation-Engine-for-Large-Language-Models)
* [Design-o-meter Towards Evaluating and Refining Graphic Designs](#Design-o-meter-Towards-Evaluating-and-Refining-Graphic-Designs)
* [Sources and Radiations of the Fermi Bubbles](#Sources-and-Radiations-of-the-Fermi-Bubbles)
* [J-Invariant Volume Shuffle for Self-Supervised Cryo-Electron Tomogram Denoising on Single Noisy Volume](#J-Invariant-Volume-Shuffle-for-Self-Supervised-Cryo-Electron-Tomogram-Denoising-on-Single-Noisy-Volume)
* [TEXGen a Generative Diffusion Model for Mesh Textures](#TEXGen-a-Generative-Diffusion-Model-for-Mesh-Textures)
* [IRLab@iKAT24 Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search](#IRLab@iKAT24-Learned-Sparse-Retrieval-with-Multi-aspect-LLM-Query-Generation-for-Conversational-Search)
* [FLARE FP-Less PTQ and Low-ENOB ADC Based AMS-PiM for Error-Resilient, Fast, and Efficient Transformer Acceleration](#FLARE-FP-Less-PTQ-and-Low-ENOB-ADC-Based-AMS-PiM-for-Error-Resilient,-Fast,-and-Efficient-Transformer-Acceleration)
* [A Lightweight Edge-CNN-Transformer Model for Detecting Coordinated Cyber and Digital Twin Attacks in Cooperative Smart Farming](#A-Lightweight-Edge-CNN-Transformer-Model-for-Detecting-Coordinated-Cyber-and-Digital-Twin-Attacks-in-Cooperative-Smart-Farming)
* [BrightVAE Luminosity Enhancement in Underexposed Endoscopic Images](#BrightVAE-Luminosity-Enhancement-in-Underexposed-Endoscopic-Images)
* [VQalAttent a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space](#VQalAttent-a-Transparent-Speech-Generation-Pipeline-based-on-Transformer-learned-VQ-VAE-Latent-Space)


## Bumblebee cosmology Tests using distance- and time-redshift probes

>Authors: Xincheng Zhu, Rui Xu, Dandan Xu

>2024-11-27

> http://arxiv.org/abs/2411.18559v1

In modern cosmology, the discovery of the Universe's accelerated expansion
has significantly transformed our understanding of cosmic evolution and
expansion history. The unknown properties of dark energy, the driver of this
**acceleration**, have not only prompted extensive studies on its nature but also
spurred interest in modified gravity theories that might serve as alternatives.
In this paper, we adopt a bumblebee vector-tensor modified gravity theory to
model the cosmic expansion history and derive predictions for the Hubble
parameter. We constrain the bumblebee model parameters using observational data
from established probes, including the Pantheon+ SH0ES Type Ia Supernovae and
BAO measurements from DESI DR1, as well as recently included Cosmic
Chronometers and Gamma Ray Bursts data. The Markov Chain Monte Carlo sampling
of Bayesian posterior distribution enables us to rigorously constrain the
bumblebee model and compare it with the standard LCDM cosmology. Our results
indicate that the bumblebee theory provides a compatible fit with current
observational data across a range of its parameters, suggesting it as a viable
alternative to the LCDM model.


## Perturbation Ontology based Graph Attention Networks

>Authors: Yichen Wang, Jie Wang, Fulin Wang, Xiang Li, Hao Yin, Bhiksha Raj

>2024-11-27

> http://arxiv.org/abs/2411.18520v1

In recent years, graph representation learning has undergone a paradigm
shift, driven by the emergence and proliferation of graph neural networks
(GNNs) and their heterogeneous counterparts. Heterogeneous GNNs have shown
remarkable success in extracting low-dimensional embeddings from complex graphs
that encompass diverse entity types and relationships. While meta-path-based
techniques have long been recognized for their ability to capture semantic
affinities among nodes, their dependence on manual specification poses a
significant limitation. In contrast, matrix-focused methods accelerate
processing by utilizing structural cues but often overlook contextual richness.
In this paper, we challenge the current paradigm by introducing ontology as a
fundamental semantic primitive within complex graphs. Our goal is to integrate
the strengths of both matrix-centric and meta-path-based approaches into a
unified framework. We propose perturbation Ontology-based Graph Attention
Networks (POGAT), a novel methodology that combines ontology subgraphs with an
advanced self-supervised learning paradigm to achieve a deep contextual
understanding. The core innovation of POGAT lies in our enhanced homogeneous
perturbing scheme designed to generate rigorous negative samples, encouraging
the model to explore minimal contextual features more thoroughly. Through
extensive empirical evaluations, we demonstrate that POGAT significantly
outperforms state-of-the-art baselines, achieving a groundbreaking improvement
of up to 10.78\% in F1-score for the critical task of link prediction and
12.01\% in Micro-F1 for the critical task of node classification.


## FastSwitch Optimizing Context Switching Efficiency in Fairness-aware Large Language Model Serving

>Authors: Ao Shen, Zhiyao Li, Mingyu Gao

>2024-11-27

> http://arxiv.org/abs/2411.18424v1

Serving numerous users and requests concurrently requires good fairness in
Large Language Models (LLMs) serving system. This ensures that, at the same
cost, the system can meet the Service Level Objectives (SLOs) of more users ,
such as time to first token (TTFT) and time between tokens (TBT), rather than
allowing a few users to experience performance far exceeding the SLOs. To
achieve better fairness, the preemption-based scheduling policy dynamically
adjusts the priority of each request to maintain balance during runtime.
However, existing systems tend to overly prioritize throughput, overlooking the
overhead caused by preemption-induced context switching, which is crucial for
maintaining fairness through priority adjustments. In this work, we identify
three main challenges that result in this overhead. 1) Inadequate I/O
utilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn
conversations. Our key insight is that the block-based **KV** cache memory policy
in existing systems, while achieving near-zero memory waste, leads to
discontinuity and insufficient granularity in the **KV** cache memory. To respond,
we introduce FastSwitch, a fairness-aware serving system that not only aligns
with existing **KV** cache memory allocation policy but also mitigates context
switching overhead. Our evaluation shows that FastSwitch outperforms the
state-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across
different tail TTFT and TBT.


## Neural Image Unfolding Flattening Sparse Anatomical Structures using Neural Fields

>Authors: Leonhard Rist, Pluvio Stephan, Noah Maul, Linda Vorberg, Hendrik Ditt, Michael Sühling, Andreas Maier, Bernhard Egger, Oliver Taubmann

>2024-11-27

> http://arxiv.org/abs/2411.18415v1

Tomographic imaging reveals internal structures of 3D objects and is crucial
for medical diagnoses. Visualizing the morphology and appearance of non-planar
**sparse** anatomical structures that extend over multiple 2D slices in tomographic
volumes is inherently difficult but valuable for decision-making and reporting.
Hence, various organ-specific unfolding techniques exist to map their densely
sampled 3D surfaces to a distortion-minimized 2D representation. However, there
is no versatile framework to flatten complex **sparse** structures including
vascular, duct or bone systems. We deploy a neural field to fit the
transformation of the anatomy of interest to a 2D overview image. We further
propose distortion regularization strategies and combine geometric with
intensity-based loss formulations to also display non-annotated and auxiliary
targets. In addition to improved versatility, our unfolding technique
outperforms mesh-based baselines for **sparse** structures w.r.t. peak distortion
and our regularization scheme yields smoother transformations compared to
Jacobian formulations from neural field-based image registration.


## Preserving Deep Representations In One-Shot Pruning A Hessian-Free Second-Order Optimization Framework

>Authors: Ryan Lucas, Rahul Mazumder

>2024-11-27

> http://arxiv.org/abs/2411.18376v1

We present SNOWS, a one-shot post-training **pruning** framework aimed at
reducing the cost of vision network inference without retraining. Current
leading one-shot **pruning** methods minimize layer-wise least squares
reconstruction error which does not take into account deeper network
representations. We propose to optimize a more global reconstruction objective.
This objective accounts for nonlinear activations deep in the network to obtain
a better proxy for the network loss. This nonlinear objective leads to a more
challenging optimization problem -- we demonstrate it can be solved efficiently
using a specialized second-order optimization framework. A key innovation of
our framework is the use of Hessian-free optimization to compute exact Newton
descent steps without needing to compute or store the full Hessian matrix. A
distinct advantage of SNOWS is that it can be readily applied on top of any
**sparse** mask derived from prior methods, readjusting their weights to exploit
nonlinearities in deep feature representations. SNOWS obtains state-of-the-art
results on various one-shot **pruning** benchmarks including residual networks and
Vision Transformers (ViT/B-16 and ViT/L-16, 86m and 304m parameters
respectively).


## Break the ID-Language Barrier An Adaption Framework for Sequential Recommendation

>Authors: Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang

>2024-11-27

> http://arxiv.org/abs/2411.18262v1

The recent breakthrough of large language models (LLMs) in natural language
processing has sparked exploration in recommendation systems, however, their
limited domain-specific knowledge remains a critical bottleneck. Specifically,
LLMs lack key pieces of information crucial for sequential recommendations,
such as user behavior patterns. To address this critical gap, we propose
IDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich
in domain-specific knowledge, into LLMs to improve recommendation accuracy.
IDLE-Adapter acts as a bridge, transforming **sparse** user-item interaction data
into dense, LLM-compatible representations through a Pre-trained ID Sequential
Model, Dimensionality Alignment, Layer-wise Embedding Refinement, and
Layer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates
remarkable flexibility by seamlessly integrating ID embeddings from diverse
ID-based sequential models and LLM architectures. Extensive experiments across
various datasets demonstrate the superiority of IDLE-Adapter, achieving over
10\% and 20\% improvements in HitRate@5 and NDCG@5 metrics, respectively,
compared to state-of-the-art methods.


## A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs

>Authors: Ehsan Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang

>2024-11-27

> http://arxiv.org/abs/2411.18148v1

Transformer neural networks (TNN) excel in natural language processing (NLP),
machine translation, and computer vision (CV) without relying on recurrent or
convolutional layers. However, they have high computational and memory demands,
particularly on resource-constrained devices like FPGAs. Moreover, transformer
models vary in processing time across applications, requiring custom models
with specific parameters. Designing custom accelerators for each model is
complex and time-intensive. Some custom accelerators exist with no runtime
adaptability, and they often rely on **sparse** matrices to reduce latency.
However, hardware designs become more challenging due to the need for
application-specific **sparsity** patterns. This paper introduces ADAPTOR, a
runtime-adaptive accelerator for dense matrix computations in transformer
encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing
elements and on-chip memory, enhancing parallelism and reducing latency. It
incorporates efficient matrix tiling to distribute resources across FPGA
platforms and is fully **quantize**d for computational efficiency and portability.
Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like
VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more
power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively.
Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some
state-of-the-art FPGA-based accelerators.


## SALMONN-omni A Codec-free LLM for Full-duplex Speech Understanding and Generation

>Authors: Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang

>2024-11-27

> http://arxiv.org/abs/2411.18138v1

Full-duplex multimodal large language models (LLMs) provide a unified
framework for addressing diverse speech understanding and generation tasks,
enabling more natural and seamless human-machine conversations. Unlike
traditional modularised conversational AI systems, which separate speech
recognition, understanding, and text-to-speech generation into distinct
components, multimodal LLMs operate as single end-to-end models. This
streamlined design eliminates error propagation across components and fully
leverages the rich non-verbal information embedded in input speech signals. We
introduce SALMONN-omni, a codec-free, full-duplex speech understanding and
generation model capable of simultaneously listening to its own generated
speech and background sounds while speaking. To support this capability, we
propose a novel duplex spoken dialogue framework incorporating a ``thinking''
mechanism that facilitates asynchronous text and speech generation relying on
embeddings instead of codecs (**quantize**d speech and audio tokens). Experimental
results demonstrate SALMONN-omni's versatility across a broad range of
streaming speech tasks, including speech recognition, speech enhancement, and
spoken question answering. Additionally, SALMONN-omni excels at managing
turn-taking, barge-in, and echo cancellation scenarios, establishing its
potential as a robust prototype for full-duplex conversational AI systems. To
the best of our knowledge, SALMONN-omni is the first codec-free model of its
kind. A full technical report along with model checkpoints will be released
soon.


## Training Noise Token Pruning

>Authors: Mingxing Rao, Bohan Jiang, Daniel Moyer

>2024-11-27

> http://arxiv.org/abs/2411.18092v1

In the present work we present Training Noise Token (TNT) Pruning for vision
transformers. Our method relaxes the discrete token dropping condition to
continuous additive noise, providing smooth optimization in training, while
retaining discrete dropping computational gains in deployment settings. We
provide theoretical connections to Rate-Distortion literature, and empirical
evaluations on the ImageNet dataset using ViT and DeiT architectures
demonstrating TNT's advantages over previous **pruning** methods.


## Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache

>Authors: Akshat Sharma, Hangliang Ding, Jianping Li, Neel Dani, Minjia Zhang

>2024-11-27

> http://arxiv.org/abs/2411.18077v1

How to efficiently serve LLMs in practice has become exceptionally
challenging due to their prohibitive memory and computation requirements. In
this study, we investigate optimizing the **KV** cache, whose memory footprint
poses a critical bottleneck in LLM inference, especially when dealing with long
context tasks. To tackle the challenge, we introduce Mini**KV**, a **KV** cache
optimization method that simultaneously preserves long context task accuracy
while significantly reducing **KV** cache size via a novel 2-bit
layer-discriminative **KV** cache. More importantly, we develop specialized CUDA
kernels to make Mini**KV** compatible with FlashAttention. Experiments on a wide
range of long context tasks show that Mini**KV** effectively achieves 86% **KV** cache
compression ratio while recovering over 98.5% of accuracy, outperforming
state-of-the-art methods while achieving excellent measured system performance
improvements.


## FlexiBit Fully Flexible Precision Bit-parallel Accelerator Architecture for Arbitrary Mixed Precision AI

>Authors: Faraz Tahmasebi, Yian Wang, Benji Y. H. Huang, Hyoukjun Kwon

>2024-11-27

> http://arxiv.org/abs/2411.18065v1

Recent research has shown that large language models (LLMs) can utilize
low-precision floating point (FP) **quantization** to deliver high efficiency while
maintaining original model accuracy. In particular, recent works have shown the
effectiveness of non-power-of-two precisions, such as FP6 and FP5, and diverse
sensitivity to low-precision arithmetic of LLM layers, which motivates mixed
precision arithmetic including non-power-of-two precisions in LLMs. Although
low-precision algorithmically leads to low computational overheads, such
benefits cannot be fully exploited due to hardware constraints that support a
limited set of power-of-two precisions (e.g., FP8, 16, 32, and 64 in NVIDIA
H100 Tensor Core). In addition, the hardware compute units are designed to
support standard formats (e.g., E4M3 and E5M2 for FP8). Such practices require
re-designing the hardware whenever new precision and format emerge, which leads
to high hardware replacement costs to exploit the benefits of new precisions
and formats. Therefore, in this paper, we propose a new accelerator
architecture, FlexiBit, which efficiently supports FP and INT arithmetic in
arbitrary precisions and formats. Unlike previous bit-serial designs, which
also provide flexibility but at the cost of performance due to its bit-wise
temporal processing nature, FlexiBit's architecture enables bit-parallel
processing of any precision and format without compute unit underutilization.
FlexiBit's new capability to exploit non-power of two precision and format led
to 1.66x and 1.62x higher performance per area on GPT-3 in FP6 targeting a
cloud-scale accelerator, compared to a Tensor Core-like architecture and a
state-of-the-art bit-parallel flexible precision accelerator, BitFusion,
respectively. Also, the bit-parallel nature of FlexiBit's architecture led to
3.9x higher performance/area compared to a state-of-the-art bit-serial
architecture.


## Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification

>Authors: Mingsen Du, Meng Chen, Yongjian Li, Cun Ji, Shoushui Wei

>2024-11-27

> http://arxiv.org/abs/2411.18043v1

Multivariate time series (MTS) classification is widely applied in fields
such as industry, healthcare, and finance, aiming to extract key features from
complex time series data for accurate decision-making and prediction. However,
existing methods for MTS often struggle due to the challenges of effectively
modeling high-dimensional data and the lack of labeled data, resulting in poor
classification performance. To address this issue, we propose a heterogeneous
relationships of subjects and shapelets method for semi-supervised MTS
classification. This method offers a novel perspective by integrating various
types of additional information while capturing the relationships between them.
Specifically, we first utilize a contrast temporal self-attention module to
obtain **sparse** MTS representations, and then model the similarities between
these representations using soft dynamic time warping to construct a similarity
graph. Secondly, we learn the shapelets for different subject types,
incorporating both the subject features and their shapelets as additional
information to further refine the similarity graph, ultimately generating a
heterogeneous graph. Finally, we use a dual level graph attention network to
get prediction. Through this method, we successfully transform dataset into a
heterogeneous graph, integrating multiple additional information and achieving
precise semi-supervised node classification. Experiments on the Human Activity
Recognition, sleep stage classification and University of East Anglia datasets
demonstrate that our method outperforms current state-of-the-art methods in MTS
classification tasks, validating its superiority.


## Manual-PA Learning 3D Part Assembly from Instruction Diagrams

>Authors: Jiahao Zhang, Anoop Cherian, Cristian Rodriguez, Weijian Deng, Stephen Gould

>2024-11-27

> http://arxiv.org/abs/2411.18011v1

Assembling furniture amounts to solving the discrete-continuous optimization
task of selecting the furniture parts to assemble and estimating their
connecting poses in a physically realistic manner. The problem is hampered by
its combinatorially large yet **sparse** solution space thus making learning to
assemble a challenging task for current machine learning models. In this paper,
we attempt to solve this task by leveraging the assembly instructions provided
in diagrammatic manuals that typically accompany the furniture parts. Our key
insight is to use the cues in these diagrams to split the problem into discrete
and continuous phases. Specifically, we present Manual-PA, a transformer-based
instruction Manual-guided 3D Part Assembly framework that learns to
semantically align 3D parts with their illustrations in the manuals using a
contrastive learning backbone towards predicting the assembly order and infers
the 6D pose of each part via relating it to the final furniture depicted in the
manual. To validate the efficacy of our method, we conduct experiments on the
benchmark PartNet dataset. Our results show that using the diagrams and the
order of the parts lead to significant improvements in assembly performance
against the state of the art. Further, Manual-PA demonstrates strong
generalization to real-world IKEA furniture assembly on the IKEA-Manual
dataset.


## HAAT Hybrid Attention Aggregation Transformer for Image Super-Resolution

>Authors: Song-Jiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kai-wen Xue, Kin-Man Lama

>2024-11-27

> http://arxiv.org/abs/2411.18003v1

In the research area of image super-resolution, Swin-transformer-based models
are favored for their global spatial modeling and shifting window attention
mechanism. However, existing methods often limit self-attention to non
overlapping windows to cut costs and ignore the useful information that exists
across channels. To address this issue, this paper introduces a novel model,
the Hybrid Attention Aggregation Transformer (HAAT), designed to better
leverage feature information. HAAT is constructed by integrating
Swin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks
(HGAB). SDRCB expands the receptive field while maintaining a streamlined
architecture, resulting in enhanced performance. HGAB incorporates channel
attention, **sparse** attention, and window attention to improve nonlocal feature
fusion and achieve more visually compelling results. Experimental evaluations
demonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.
  Keywords: Image super-resolution, Computer vision, Attention mechanism,
Transformer


## Hybrid Beamforming Design for Covert mmWave MIMO with Finite-Resolution DACs

>Authors: Wei Ci, Chenhao Qi, Xiaohu You

>2024-11-27

> http://arxiv.org/abs/2411.17986v1

We investigate hybrid beamforming design for covert millimeter wave
multiple-input multiple-output systems with finite-resolution digital-to-analog
converters (DACs), which impose practical hardware constraints not yet
considered by the existing works and have negative impact on the covertness.
Based on the additive **quantization** noise model, we derive the detection error
probability of the warden considering finite-resolution DACs. Aiming at
maximizing the sum covert rate (SCR) between the transmitter and legitimate
users, we design hybrid beamformers subject to power and covertness
constraints. To solve this nonconvex joint optimization problem, we propose an
alternating optimization (AO) scheme based on fractional programming, quadratic
transformation, and inner majorization-minimization methods to iteratively
optimize the analog and digital beamformers. To reduce the computational
complexity of the AO scheme, we propose a vector-space based heuristic (VSH)
scheme to design the hybrid beamformer. We prove that as the number of antennas
grows to be infinity, the SCR in the VSH scheme can approach the channel mutual
information. Simulation results show that the AO and VSH schemes outperform the
existing schemes and the VSH scheme can be used to obtain an initialization for
the AO scheme to speed up its convergence.


## Engineering Trustworthy Software A Mission for LLMs

>Authors: Marco Vieira

>2024-11-27

> http://arxiv.org/abs/2411.17981v1

LLMs are transforming software engineering by accelerating development,
reducing complexity, and cutting costs. When fully integrated into the software
lifecycle they will drive design, development and deployment while facilitating
early bug detection, continuous improvement, and rapid resolution of critical
issues. However, trustworthy LLM-driven software engineering requires
addressing multiple challenges such as accuracy, scalability, bias, and
explainability.


## SoftmAP Software-Hardware Co-design for Integer-Only Softmax on Associative Processors

>Authors: Mariam Rakka, Jinhao Li, Guohao Dai, Ahmed Eltawil, Mohammed E. Fouda, Fadi Kurdahi

>2024-11-26

> http://arxiv.org/abs/2411.17847v1

Recent research efforts focus on reducing the computational and memory
overheads of Large Language Models (LLMs) to make them feasible on
resource-constrained devices. Despite advancements in compression techniques,
non-linear operators like Softmax and Layernorm remain bottlenecks due to their
sensitivity to **quantization**. We propose SoftmAP, a software-hardware co-design
methodology that implements an integer-only low-precision Softmax using
In-Memory Compute (IMC) hardware. Our method achieves up to three orders of
magnitude improvement in the energy-delay product compared to A100 and RTX3090
GPUs, making LLMs more deployable without compromising performance.


## Low-Bit Quantization Favors Undertrained LLMs Scaling Laws for Quantized LLMs with 100T Training Tokens

>Authors: Xu Ouyang, Tao Ge, Thomas Hartvigsen, Zhisong Zhang, Haitao Mi, Dong Yu

>2024-11-26

> http://arxiv.org/abs/2411.17691v2

We reveal that **low-bit** **quantization** favors undertrained large language models
(LLMs) by observing that models with larger sizes or fewer training tokens
experience less **quantization**-induced degradation (QiD) when applying **low-bit**
**quantization**, whereas smaller models with extensive training tokens suffer
significant QiD. To gain deeper insights into this trend, we study over 1500
**quantize**d LLM checkpoints of various sizes and at different training levels
(undertrained or fully trained) in a controlled setting, deriving scaling laws
for understanding the relationship between QiD and factors such as the number
of training tokens, model size and bit width.
  With the derived scaling laws, we propose a novel perspective that we can use
QiD to measure an LLM's training levels and determine the number of training
tokens required for fully training LLMs of various sizes. Moreover, we use the
scaling laws to predict the **quantization** performance of different-sized LLMs
trained with 100 trillion tokens. Our projection shows that the **low-bit**
**quantization** performance of future models, which are expected to be trained
with over 100 trillion tokens, may NOT be desirable. This poses a potential
challenge for **low-bit** **quantization** in the future and highlights the need for
awareness of a model's training level when evaluating **low-bit** **quantization**
research. To facilitate future research on this problem, we release all the
1500+ **quantize**d checkpoints used in this work at
https://huggingface.co/Xu-Ouyang.


## Attamba Attending To Multi-Token States

>Authors: Yash Akhauri, Safeen Huda, Mohamed S. Abdelfattah

>2024-11-26

> http://arxiv.org/abs/2411.17685v1

When predicting the next token in a sequence, vanilla transformers compute
attention over all previous tokens, resulting in quadratic scaling of compute
with sequence length. State-space models compress the entire sequence of tokens
into a fixed-dimensional representation to improve efficiency, while other
architectures achieve sub-quadratic complexity via low-rank projections or
**sparse** attention patterns over the sequence. In this paper, we introduce
Attamba, a novel architecture that uses state-space models to compress chunks
of tokens and applies attention on these compressed key-value representations.
We find that replacing key and value projections in a transformer with SSMs can
improve model quality and enable flexible token chunking, resulting in 24%
improved perplexity with transformer of similar **KV**-Cache and attention
footprint, and ~4 times smaller **KV**-Cache and Attention FLOPs for 5% perplexity
trade-off. Attamba can perform attention on chunked-sequences of variable
length, enabling a smooth transition between quadratic and linear scaling,
offering adaptable efficiency gains.


## Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning

>Authors: Zhu Xu, Zhiqiang Zhao, Zihan Zhang, Yuchi Liu, Quanwei Shen, Fei Liu, Yu Kuang

>2024-11-26

> http://arxiv.org/abs/2411.17679v1

Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE
(BBPE) have significantly improved the computational efficiency and vocabulary
representation stability of large language models (LLMs) by segmenting text
into tokens. However, this segmentation often obscures the internal character
structures and sequences within tokens, preventing models from fully learning
these intricate details during training. Consequently, LLMs struggle to
comprehend the character compositions and positional relationships within
tokens, especially when fine-tuned on downstream tasks with limited data. In
this paper, we introduce Token Internal Position Awareness (TIPA), a novel
approach that enhances LLMs' understanding of internal token structures by
training them on reverse character prediction tasks using the tokenizer's own
vocabulary. This method enables models to effectively learn and generalize
character positions and internal structures. Experimental results demonstrate
that LLMs trained with TIPA outperform baseline models in predicting character
positions at the token level. Furthermore, when applied to the downstream task
of Chinese Spelling Correction (CSC), TIPA not only accelerates model
convergence but also significantly improves task performance.


## Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset

>Authors: Yujie Dai, Brian Sullivan, Axel Montout, Amy Dillon, Chris Waller, Peter Acs, Rachel Denholm, Philip Williams, Alastair D Hay, Raul Santos-Rodriguez, Andrew Dowsey

>2024-11-26

> http://arxiv.org/abs/2411.17645v1

The use of machine learning and AI on electronic health records (EHRs) holds
substantial potential for clinical insight. However, this approach faces
significant challenges due to data heterogeneity, **sparsity**, temporal
misalignment, and limited labeled outcomes. In this context, we leverage a
linked EHR dataset of approximately one million de-identified individuals from
Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary
tract infections (UTIs) and develop predictive models focused on data quality,
fairness and transparency. A comprehensive data pre-processing and curation
pipeline transforms the raw EHR data into a structured format suitable for AI
modeling. Given the limited availability and biases of ground truth UTI
outcomes, we introduce a UTI risk estimation framework informed by clinical
expertise to estimate UTI risk across individual patient timelines. Using this
framework, we built pairwise XGBoost models to differentiate UTI risk
categories with explainable AI techniques to identify key predictors while
ensuring interpretability. Our findings reveal differences in clinical and
demographic factors across risk groups, offering insights into UTI risk
stratification and progression. This study demonstrates the added value of
AI-driven insights into UTI clinical decision-making while prioritizing
interpretability, transparency, and fairness, underscoring the importance of
sound data practices in advancing health outcomes.


## DapPep Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction

>Authors: Jiangbin Zheng, Qianhui Xu, Ruichen Xia, Stan Z. Li

>2024-11-26

> http://arxiv.org/abs/2411.17798v1

Identifying T-cell receptors (TCRs) that interact with antigenic peptides
provides the technical basis for developing vaccines and immunotherapies. The
emergent deep learning methods excel at learning antigen binding patterns from
known TCRs but struggle with novel or **sparse**ly represented antigens. However,
binding specificity for unseen antigens or exogenous peptides is critical. We
introduce a domain-adaptive peptide-agnostic learning framework DapPep for
universal TCR-antigen binding affinity prediction to address this challenge.
The lightweight self-attention architecture combines a pre-trained protein
language model with an inner-loop self-supervised regime to enable robust
TCR-peptide representations. Extensive experiments on various benchmarks
demonstrate that DapPep consistently outperforms existing tools, showcasing
robust generalization capability, especially for data-scarce settings and
unseen peptides. Moreover, DapPep proves effective in challenging clinical
tasks such as sorting reactive T cells in tumor neoantigen therapy and
identifying key positions in 3D structures.


## Scalable iterative pruning of large language and vision models using block coordinate descent

>Authors: Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Elton Yechao Zhu, Serdar Kadıoğlu, Sima E. Borujeni, Helmut G. Katzgraber

>2024-11-26

> http://arxiv.org/abs/2411.17796v1

Pruning neural networks, which involves removing a fraction of their weights,
can often maintain high accuracy while significantly reducing model complexity,
at least up to a certain limit. We present a neural network **pruning** technique
that builds upon the Combinatorial Brain Surgeon, but solves an optimization
problem over a subset of the network weights in an iterative, block-wise manner
using block coordinate descent. The iterative, block-based nature of this
**pruning** technique, which we dub ``iterative Combinatorial Brain Surgeon''
(iCBS) allows for scalability to very large models, including large language
models (LLMs), that may not be feasible with a one-shot combinatorial
optimization approach. When applied to large models like Mistral and DeiT, iCBS
achieves higher performance metrics at the same density levels compared to
existing **pruning** methods such as Wanda. This demonstrates the effectiveness of
this iterative, block-wise **pruning** method in compressing and optimizing the
performance of large deep learning models, even while optimizing over only a
small fraction of the weights. Moreover, our approach allows for a quality-time
(or cost) tradeoff that is not available when using a one-shot **pruning**
technique alone. The block-wise formulation of the optimization problem enables
the use of hardware accelerators, potentially offsetting the increased
computational costs compared to one-shot **pruning** methods like Wanda. In
particular, the optimization problem solved for each block is quantum-amenable
in that it could, in principle, be solved by a quantum computer.


## An Ensemble Approach for Brain Tumor Segmentation and Synthesis

>Authors: Juampablo E. Heras Rivera, Agamdeep S. Chopra, Tianyi Ren, Hitender Oswal, Yutong Pan, Zineb Sordo, Sophie Walters, William Henry, Hooman Mohammadi, Riley Olson, Fargol Rezayaraghi, Tyson Lam, Akshay Jaikanth, Pavan Kancharla, Jacob Ruzevick, Daniela Ushizima, Mehmet Kurt

>2024-11-26

> http://arxiv.org/abs/2411.17617v1

The integration of machine learning in magnetic resonance imaging (MRI),
specifically in neuroimaging, is proving to be incredibly effective, leading to
better diagnostic accuracy, accelerated image analysis, and data-driven
insights, which can potentially transform patient care. Deep learning models
utilize multiple layers of processing to capture intricate details of complex
data, which can then be used on a variety of tasks, including brain tumor
classification, segmentation, image synthesis, and registration. Previous
research demonstrates high accuracy in tumor segmentation using various model
architectures, including nn-UNet and Swin-UNet. U-Mamba, which uses state space
modeling, also achieves high accuracy in medical image segmentation. To
leverage these models, we propose a deep learning framework that ensembles
these state-of-the-art architectures to achieve accurate segmentation and
produce finely synthesized images.


## Accelerating Vision Diffusion Transformers with Skip Branches

>Authors: Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Tianlong Chen, Cheng Yu

>2024-11-26

> http://arxiv.org/abs/2411.17616v1

Diffusion Transformers (DiT), an emerging image and video generation model
architecture, has demonstrated great potential because of its high generation
quality and scalability properties. Despite the impressive performance, its
practical deployment is constrained by computational complexity and redundancy
in the sequential denoising process. While feature caching across timesteps has
proven effective in accelerating diffusion models, its application to DiT is
limited by fundamental architectural differences from U-Net-based approaches.
Through empirical analysis of DiT feature dynamics, we identify that
significant feature variation between DiT blocks presents a key challenge for
feature reusability. To address this, we convert standard DiT into Skip-DiT
with skip branches to enhance feature smoothness. Further, we introduce
Skip-Cache which utilizes the skip branches to cache DiT features across
timesteps at the inference time. We validated effectiveness of our proposal on
different DiT backbones for video and image generation, showcasing skip
branches to help preserve generation quality and achieve higher speedup.
Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost for
free and a 2.2x speedup with only a minor reduction in quantitative metrics.
Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.


## Scaling Speech-Text Pre-training with Synthetic Interleaved Data

>Authors: Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang

>2024-11-26

> http://arxiv.org/abs/2411.17607v1

Speech language models (SpeechLMs) accept speech input and produce speech
output, allowing for more natural human-computer interaction compared to
text-based large language models (LLMs). Traditional approaches for developing
SpeechLMs are constrained by the limited availability of unsupervised speech
data and parallel speech-text data, which are significantly less abundant than
text pre-training data, thereby limiting their scalability as LLMs. We propose
a novel approach to scaling speech-text pre-training by leveraging large-scale
synthetic interleaved data derived from text corpora, eliminating the need for
parallel speech-text datasets. Our method efficiently constructs speech-text
interleaved data by sampling text spans from existing text corpora and
synthesizing corresponding speech spans using a text-to-token model, bypassing
the need to generate actual speech. We also employ a supervised speech
tokenizer derived from an automatic speech recognition (ASR) model by
incorporating a vector-**quantize**d bottleneck into the encoder. This supervised
training approach results in discrete speech tokens with strong semantic
preservation even at lower sampling rates (e.g. 12.5Hz), while still
maintaining speech reconstruction quality. Starting from a pre-trained language
model and scaling our pre-training to 1 trillion tokens (with 600B synthetic
interleaved speech-text data), we achieve state-of-the-art performance in
speech language modeling and spoken question answering, improving performance
on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We
further demonstrate that by fine-tuning the pre-trained model with speech
dialogue data, we can develop an end-to-end spoken chatbot that achieves
competitive performance comparable to existing baselines in both conversational
abilities and speech quality, even operating exclusively in the speech domain.


## Pushing the Limits of Large Language Model Quantization via the Linearity Theorem

>Authors: Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtárik, Dan Alistarh

>2024-11-26

> http://arxiv.org/abs/2411.17525v1

Quantizing large language models has become a standard way to reduce their
memory and computational costs. Typically, existing methods focus on breaking
down the problem into individual layer-wise sub-problems, and minimizing
per-layer error, measured via various metrics. Yet, this approach currently
lacks theoretical justification and the metrics employed may be sub-optimal. In
this paper, we present a "linearity theorem" establishing a direct relationship
between the layer-wise $\ell_2$ reconstruction error and the model perplexity
increase due to **quantization**. This insight enables two novel applications: (1)
a simple data-free LLM **quantization** method using Hadamard rotations and
MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free
approaches such as the extremely popular NF4 **quantize**d format, and (2) an
optimal solution to the problem of finding non-uniform per-layer **quantization**
levels which match a given compression constraint in the medium-bitwidth
regime, obtained by reduction to dynamic programming. On the practical side, we
demonstrate improved accuracy-compression trade-offs on Llama-3.1 and
3.2-family models, as well as on Qwen-family models. Further, we show that our
method can be efficiently supported in terms of GPU kernels at various batch
sizes, advancing both data-free and non-uniform **quantization** for LLMs.


## Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient

>Authors: Zigeng Chen, Xinyin Ma, Gongfan Fang, Xinchao Wang

>2024-11-26

> http://arxiv.org/abs/2411.17787v1

In the rapidly advancing field of image generation, Visual Auto-Regressive
(VAR) modeling has garnered considerable attention for its innovative
next-scale prediction approach. This paradigm offers substantial improvements
in efficiency, scalability, and zero-shot generalization. Yet, the inherently
coarse-to-fine nature of VAR introduces a prolonged token sequence, leading to
prohibitive memory consumption and computational redundancies. To address these
bottlenecks, we propose Collaborative Decoding (CoDe), a novel efficient
decoding strategy tailored for the VAR framework. CoDe capitalizes on two
critical observations: the substantially reduced parameter demands at larger
scales and the exclusive generation patterns across different scales. Based on
these insights, we partition the multi-scale inference process into a seamless
collaboration between a large model and a small model. The large model serves
as the 'drafter', specializing in generating low-frequency content at smaller
scales, while the smaller model serves as the 'refiner', solely focusing on
predicting high-frequency details at larger scales. This collaboration yields
remarkable efficiency with minimal impact on quality: CoDe achieves a 1.7x
speedup, slashes memory usage by around 50%, and preserves image quality with
only a negligible FID increase from 1.95 to 1.98. When drafting steps are
further decreased, CoDe can achieve an impressive 2.9x **acceleration** ratio,
reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU, while
preserving a commendable FID of 2.27. The code is available at
https://github.com/czg1225/CoDe


## Fast and Exact Similarity Search in less than a Blink of an Eye

>Authors: Patrick Schäfer, Jakob Brand, Ulf Leser, Peng Botao, Themis Palpanas

>2024-11-26

> http://arxiv.org/abs/2411.17483v1

Similarity search is a fundamental operation for analyzing data series (DS),
which are ordered sequences of real values. To enhance efficiency,
summarization techniques are employed that reduce the dimensionality of DS.
SAX-based approaches are the state-of-the-art for exact similarity queries, but
their performance degrades for high-frequency signals, such as noisy data, or
for high-frequency DS. In this work, we present the SymbOlic Fourier
Approximation index (SOFA), which implements fast, exact similarity queries.
SOFA is based on two building blocks: a tree index (inspired by MESSI) and the
SFA symbolic summarization. It makes use of a learned summarization method
called Symbolic Fourier Approximation (SFA), which is based on the Fourier
transform and utilizes a data-adaptive **quantization** of the frequency domain. To
better capture relevant information in high-frequency signals, SFA selects the
Fourier coefficients by highest variance, resulting in a larger value range,
thus larger **quantization** bins. The tree index solution employed by SOFA makes
use of the GEMINI-approach to answer exact similarity search queries using
lower bounding distance measures, and an efficient SIMD implementation. We
further propose a novel benchmark comprising $17$ diverse datasets,
encompassing 1 billion DS. Our experimental results demonstrate that SOFA
outperforms existing methods on exact similarity queries: it is up to 10 times
faster than a parallel sequential scan, 3-4 times faster than FAISS, and 2
times faster on average than MESSI. For high-frequency datasets, we observe a
remarkable 38-fold performance improvement.


## Efficient Deployment of Transformer Models in Analog In-Memory Computing Hardware

>Authors: Chen Li, Corey Lammie, Manuel Le Gallo, Bipin Rajendran

>2024-11-26

> http://arxiv.org/abs/2411.17367v1

Analog in-memory computing (AIMC) has emerged as a promising solution to
overcome the von Neumann bottleneck, accelerating neural network computations
and improving computational efficiency. While AIMC has demonstrated success
with architectures such as CNNs, MLPs, and RNNs, deploying transformer-based
models using AIMC presents unique challenges. Transformers are expected to
handle diverse downstream tasks and adapt to new user data or instructions
after deployment, which requires more flexible approaches to suit AIMC
constraints.
  In this paper, we propose a novel method for deploying pre-trained
transformer models onto AIMC hardware. Unlike traditional approaches requiring
hardware-aware training, our technique allows direct deployment without the
need for retraining the original model. Instead, we utilize lightweight,
low-rank adapters -- compact modules stored in digital cores -- to adapt the
model to hardware constraints. We validate our approach on MobileBERT,
demonstrating accuracy on par with, or even exceeding, a traditional
hardware-aware training approach. Our method is particularly appealing in
multi-task scenarios, as it enables a single analog model to be reused across
multiple tasks. Moreover, it supports on-chip adaptation to new hardware
constraints and tasks without updating analog weights, providing a flexible and
versatile solution for real-world AI applications. Code is available.


## Probing vector gravitational atom with eccentric intermediate mass-ratio inspirals

>Authors: Yan Cao, Ya-Ze Cheng, Gen-Liang Li, Yong Tang

>2024-11-26

> http://arxiv.org/abs/2411.17247v1

Ultralight bosons, proposed as candidates for dark matter, are predicted by
various new physics models. In the presence of bosons with suitable masses,
superradiant (SR) instability can naturally transform a spinning black hole
(BH) into a gravitational atom (GA). Here we study the dynamics of intermediate
mass-ratio inspirals (IMRIs) around a GA formed by ultralight vector field
saturated in its SR ground state. We employ a perturbative model at the leading
Newtonian order to consistently account for both the conservative effect of
cloud gravity and the dissipative effect of cloud ionization. We find the cloud
can make a sizable negative contribution to the secular periastron precession
at binary separations comparable to the gravitational Bohr radius. Meanwhile,
the backreaction of ionization could significantly accelerate the process of
orbital decay and circularization. Considering reasonably small vector boson
masses, we investigate the adiabatic orbital evolution and gravitational
waveforms of eccentric inspirals. The results indicate that vector GAs might be
detectable through observations of low-frequency IMRIs by future space-based
gravitational-wave detectors, such as LISA and Taiji.


## MAT Multi-Range Attention Transformer for Efficient Image Super-Resolution

>Authors: Chengxing Xie, Xiaoming Zhang, Kai Zhang, Linze Li, Yuqian Fu, Biao Gong, Tianrui Li

>2024-11-26

> http://arxiv.org/abs/2411.17214v1

Recent advances in image super-resolution (SR) have significantly benefited
from the incorporation of Transformer architectures. However, conventional
techniques aimed at enlarging the self-attention window to capture broader
contexts come with inherent drawbacks, especially the significantly increased
computational demands. Moreover, the feature perception within a fixed-size
window of existing models restricts the effective receptive fields and the
intermediate feature diversity. This study demonstrates that a flexible
integration of attention across diverse spatial extents can yield significant
performance enhancements. In line with this insight, we introduce Multi-Range
Attention Transformer (MAT) tailored for SR tasks. MAT leverages the
computational advantages inherent in dilation operation, in conjunction with
self-attention mechanism, to facilitate both multi-range attention (MA) and
**sparse** multi-range attention (SMA), enabling efficient capture of both regional
and **sparse** global features. Further coupled with local feature extraction, MAT
adeptly capture dependencies across various spatial ranges, improving the
diversity and efficacy of its feature representations. We also introduce the
MSConvStar module, which augments the model's ability for multi-range
representation learning. Comprehensive experiments show that our MAT exhibits
superior performance to existing state-of-the-art SR models with remarkable
efficiency (~3.3 faster than SRFormer-light).


## An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models

>Authors: Yunzhe Hu, Difan Zou, Dong Xu

>2024-11-26

> http://arxiv.org/abs/2411.17182v1

Deep neural networks have long been criticized for being black-box. To unveil
the inner workings of modern neural architectures, a recent work
\cite{yu2024white} proposed an information-theoretic objective function called
Sparse Rate Reduction (SRR) and interpreted its unrolled optimization as a
Transformer-like model called Coding Rate Reduction Transformer (CRATE).
However, the focus of the study was primarily on the basic implementation, and
whether this objective is optimized in practice and its causal relationship to
generalization remain elusive. Going beyond this study, we derive different
implementations by analyzing layer-wise behaviors of CRATE, both theoretically
and empirically. To reveal the predictive power of SRR on generalization, we
collect a set of model variants induced by varied implementations and
hyperparameters and evaluate SRR as a complexity measure based on its
correlation with generalization. Surprisingly, we find out that SRR has a
positive correlation coefficient and outperforms other baseline measures, such
as path-norm and sharpness-based ones. Furthermore, we show that generalization
can be improved using SRR as regularization on benchmark image classification
datasets. We hope this paper can shed light on leveraging SRR to design
principled models and study their generalization ability.


## LiteVAR Compressing Visual Autoregressive Modelling with Efficient Attention and Quantization

>Authors: Rui Xie, Tianchen Zhao, Zhihang Yuan, Rui Wan, Wenxi Gao, Zhenhua Zhu, Xuefei Ning, Yu Wang

>2024-11-26

> http://arxiv.org/abs/2411.17178v1

Visual Autoregressive (VAR) has emerged as a promising approach in image
generation, offering competitive potential and performance comparable to
diffusion-based models. However, current AR-based visual generation models
require substantial computational resources, limiting their applicability on
resource-constrained devices. To address this issue, we conducted analysis and
identified significant redundancy in three dimensions of the VAR model: (1) the
attention map, (2) the attention outputs when using classifier free guidance,
and (3) the data precision. Correspondingly, we proposed efficient attention
mechanism and **low-bit** **quantization** method to enhance the efficiency of VAR
models while maintaining performance. With negligible performance lost (less
than 0.056 FID increase), we could achieve 85.2% reduction in attention
computation, 50% reduction in overall memory and 1.5x latency reduction. To
ensure deployment feasibility, we developed efficient training-free compression
techniques and analyze the deployment feasibility and efficiency gain of each
technique.


## Star Attention Efficient LLM Inference over Long Sequences

>Authors: Shantanu Acharya, Fei Jia, Boris Ginsburg

>2024-11-26

> http://arxiv.org/abs/2411.17116v1

Inference with Transformer-based Large Language Models (LLMs) on long
sequences is both costly and slow due to the quadratic complexity of the
self-attention mechanism. We introduce Star Attention, a two-phase block-**sparse**
approximation that improves computational efficiency by sharding attention
across multiple hosts while minimizing communication overhead. In the first
phase, the context is processed using blockwise-local attention across hosts,
in parallel. In the second phase, query and response tokens attend to all prior
cached tokens through sequence-global attention. Star Attention integrates
seamlessly with most Transformer-based LLMs trained with global attention,
reducing memory requirements and inference time by up to 11x while preserving
95-100% of accuracy.


## PassionSR Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution

>Authors: Libo Zhu, Jianze Li, Haotong Qin, Yulun Zhang, Yong Guo, Xiaokang Yang

>2024-11-26

> http://arxiv.org/abs/2411.17106v1

Diffusion-based image super-resolution (SR) models have shown superior
performance at the cost of multiple denoising steps. However, even though the
denoising step has been reduced to one, they require high computational costs
and storage requirements, making it difficult for deployment on hardware
devices. To address these issues, we propose a novel post-training **quantization**
approach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.
First, we simplify OSD model to two core components, UNet and Variational
Autoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable
Boundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to
optimize the **quantization** process and manipulate activation distributions for
better **quantization**. Finally, we design a Distributed Quantization Calibration
(DQC) strategy that stabilizes the training of **quantize**d parameters for rapid
convergence. Comprehensive experiments demonstrate that PassionSR with 8-bit
and 6-bit obtains comparable visual results with full-precision model.
Moreover, our PassionSR achieves significant advantages over recent leading
**low-bit** **quantization** methods for image SR. Our code will be at
https://github.com/libozhu03/PassionSR.


## Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation

>Authors: Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, Murali Annavaram

>2024-11-26

> http://arxiv.org/abs/2411.17089v1

Inference for Large Language Models (LLMs) is computationally demanding. To
reduce the cost of auto-regressive decoding, Key-Value (**KV**) caching is used to
store intermediate activations, enabling GPUs to perform only the incremental
computation required for each new token. This approach significantly lowers the
computational overhead for token generation. However, the memory required for
**KV** caching grows rapidly, often exceeding the capacity of GPU memory. A
cost-effective alternative is to offload **KV** cache to CPU memory, which
alleviates GPU memory pressure but shifts the bottleneck to the limited
bandwidth of the PCIe connection between the CPU and GPU. Existing methods
attempt to address these issues by overlapping GPU computation with I/O or
employing CPU-GPU heterogeneous execution, but they are hindered by excessive
data movement and dependence on CPU capabilities. In this paper, we introduce
an efficient CPU-GPU I/O-aware LLM inference method that avoids transferring
the entire **KV** cache from CPU to GPU by recomputing partial **KV** cache from
activations while concurrently transferring the remaining **KV** cache via PCIe
bus. This approach overlaps GPU recomputation with data transfer to minimize
idle GPU time and maximize inference performance. Our method is fully automated
by integrating a profiler module that utilizes input characteristics and system
hardware information, a scheduler module to optimize the distribution of
computation and communication workloads, and a runtime module to efficiently
execute the derived execution plan. Experimental results show that our method
achieves up to 35.8% lower latency and 46.2% higher throughput during decoding
compared to state-of-the-art approaches.


## Dynamic Programming-Based Redundancy Resolution for Path Planning of Redundant Manipulators Considering Breakpoints

>Authors: Zhihang Yin, Fa Wu, Ruofan Bian, Ziqian Wang, Jianmin Yang, Jiyong Tan, Dexing Kong

>2024-11-26

> http://arxiv.org/abs/2411.17034v1

This paper proposes a redundancy resolution algorithm for a redundant
manipulator based on dynamic programming. This algorithm can compute the
desired joint angles at each point on a pre-planned discrete path in Cartesian
space, while ensuring that the angles, velocities, and **acceleration**s of each
joint do not exceed the manipulator's constraints. We obtain the analytical
solution to the inverse kinematics problem of the manipulator using a
parameterization method, transforming the redundancy resolution problem into an
optimization problem of determining the parameters at each path point. The
constraints on joint velocity and **acceleration** serve as constraints for the
optimization problem. Then all feasible inverse kinematic solutions for each
pose under the joint angle constraints of the manipulator are obtained through
parameterization methods, and the globally optimal solution to this problem is
obtained through the dynamic programming algorithm. On the other hand, if a
feasible joint-space path satisfying the constraints does not exist, the
proposed algorithm can compute the minimum number of breakpoints required for
the path and partition the path with as few breakpoints as possible to
facilitate the manipulator's operation along the path. The algorithm can also
determine the optimal selection of breakpoints to minimize the global cost
function, rather than simply interrupting when the manipulator is unable to
continue operating. The proposed algorithm is tested using a manipulator
produced by a certain manufacturer, demonstrating the effectiveness of the
algorithm.


## SAR3D Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE

>Authors: Yongwei Chen, Yushi Lan, Shangchen Zhou, Tengfei Wang, XIngang Pan

>2024-11-25

> http://arxiv.org/abs/2411.16856v1

Autoregressive models have demonstrated remarkable success across various
fields, from large language models (LLMs) to large multimodal models (LMMs) and
2D content generation, moving closer to artificial general intelligence (AGI).
Despite these advances, applying autoregressive approaches to 3D object
generation and understanding remains largely unexplored. This paper introduces
Scale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale
3D vector-**quantize**d variational autoencoder (VQVAE) to tokenize 3D objects for
efficient autoregressive generation and detailed understanding. By predicting
the next scale in a multi-scale latent representation instead of the next
single token, SAR3D reduces generation time significantly, achieving fast 3D
object generation in just 0.82 seconds on an A6000 GPU. Additionally, given the
tokens enriched with hierarchical 3D-aware information, we finetune a
pretrained LLM on them, enabling multimodal comprehension of 3D content. Our
experiments show that SAR3D surpasses current 3D generation methods in both
speed and quality and allows LLMs to interpret and caption 3D models
comprehensively.


## Factorized Visual Tokenization and Generation

>Authors: Zechen Bai, Jianxiong Gao, Ziteng Gao, Pichao Wang, Zheng Zhang, Tong He, Mike Zheng Shou

>2024-11-25

> http://arxiv.org/abs/2411.16681v2

Visual tokenizers are fundamental to image generation. They convert visual
data into discrete tokens, enabling transformer-based models to excel at image
generation. Despite their success, VQ-based tokenizers like VQGAN face
significant limitations due to constrained vocabulary sizes. Simply expanding
the codebook often leads to training instability and diminishing performance
gains, making scalability a critical challenge. In this work, we introduce
Factorized Quantization (FQ), a novel approach that revitalizes VQ-based
tokenizers by decomposing a large codebook into multiple independent
sub-codebooks. This factorization reduces the lookup complexity of large
codebooks, enabling more efficient and scalable visual tokenization. To ensure
each sub-codebook captures distinct and complementary information, we propose a
disentanglement regularization that explicitly reduces redundancy, promoting
diversity across the sub-codebooks. Furthermore, we integrate representation
learning into the training process, leveraging pretrained vision models like
CLIP and DINO to infuse semantic richness into the learned representations.
This design ensures our tokenizer captures diverse semantic levels, leading to
more expressive and disentangled representations. Experiments show that the
proposed FQGAN model substantially improves the reconstruction quality of
visual tokenizers, achieving state-of-the-art performance. We further
demonstrate that this tokenizer can be effectively adapted into auto-regressive
image generation. https://showlab.github.io/FQGAN


## Quark Real-time, High-resolution, and General Neural View Synthesis

>Authors: John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck

>2024-11-25

> http://arxiv.org/abs/2411.16680v1

We present a novel neural algorithm for performing high-quality,
high-resolution, real-time novel view synthesis. From a **sparse** set of input RGB
images or videos streams, our network both reconstructs the 3D scene and
renders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our
feed-forward network generalizes across a wide variety of datasets and scenes
and produces state-of-the-art quality for a real-time method. Our quality
approaches, and in some cases surpasses, the quality of some of the top offline
methods. In order to achieve these results we use a novel combination of
several key concepts, and tie them together into a cohesive and effective
algorithm. We build on previous works that represent the scene using
semi-transparent layers and use an iterative learned render-and-refine approach
to improve those layers. Instead of flat layers, our method reconstructs
layered depth maps (LDMs) that efficiently represent scenes with complex depth
and occlusions. The iterative update steps are embedded in a multi-scale,
UNet-style architecture to perform as much compute as possible at reduced
resolution. Within each update step, to better aggregate the information from
multiple input views, we use a specialized Transformer-based network component.
This allows the majority of the per-input image processing to be performed in
the input image space, as opposed to layer space, further increasing
efficiency. Finally, due to the real-time nature of our reconstruction and
rendering, we dynamically create and discard the internal 3D geometry for each
frame, generating the LDM for each view. Taken together, this produces a novel
and effective algorithm for view synthesis. Through extensive evaluation, we
demonstrate that we achieve state-of-the-art quality at real-time rates.
Project page: https://quark-3d.github.io/


## StructFormer Document Structure-based Masked Attention and its Impact on Language Model Pre-Training

>Authors: Kaustubh Ponkshe, Venkatapathy Subramanian, Natwar Modani, Ganesh Ramakrishnan

>2024-11-25

> http://arxiv.org/abs/2411.16618v1

Most state-of-the-art techniques for Language Models (LMs) today rely on
transformer-based architectures and their ubiquitous attention mechanism.
However, the exponential growth in computational requirements with longer input
sequences confines Transformers to handling short passages. Recent efforts have
aimed to address this limitation by introducing selective attention mechanisms,
notably local and global attention. While **sparse** attention mechanisms, akin to
full attention in being Turing-complete, have been theoretically established,
their practical impact on pre-training remains unexplored. This study focuses
on empirically assessing the influence of global attention on BERT
pre-training. The primary steps involve creating an extensive corpus of
structure-aware text through arXiv data, alongside a text-only counterpart. We
carry out pre-training on these two datasets, investigate shifts in attention
patterns, and assess their implications for downstream tasks. Our analysis
underscores the significance of incorporating document structure into LM
models, demonstrating their capacity to excel in more abstract tasks, such as
document understanding.


## DetailGen3D Generative 3D Geometry Enhancement via Data-Dependent Flow

>Authors: Ken Deng, Yuanchen Guo, Jingxiang Sun, Zixin Zou, Yangguang Li, Xin Cai, Yanpei Cao, Yebin Liu, Ding Liang

>2024-11-25

> http://arxiv.org/abs/2411.16820v2

Modern 3D generation methods can rapidly create shapes from **sparse** or single
views, but their outputs often lack geometric detail due to computational
constraints. We present DetailGen3D, a generative approach specifically
designed to enhance these generated 3D shapes. Our key insight is to model the
coarse-to-fine transformation directly through data-dependent flows in latent
space, avoiding the computational overhead of large-scale 3D generative models.
We introduce a token matching strategy that ensures accurate spatial
correspondence during refinement, enabling local detail synthesis while
preserving global structure. By carefully designing our training data to match
the characteristics of synthesized coarse shapes, our method can effectively
enhance shapes produced by various 3D generation and reconstruction approaches,
from single-view to **sparse** multi-view inputs. Extensive experiments demonstrate
that DetailGen3D achieves high-fidelity geometric detail synthesis while
maintaining efficiency in training.


## Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency

>Authors: Yutong Wang, Jiajie Teng, Jiajiong Cao, Yuming Li, Chenguang Ma, Hongteng Xu, Dixin Luo

>2024-11-25

> http://arxiv.org/abs/2411.16468v1

As a very common type of video, face videos often appear in movies, talk
shows, live broadcasts, and other scenes. Real-world online videos are often
plagued by degradations such as blurring and **quantization** noise, due to the
high compression ratio caused by high communication costs and limited
transmission bandwidth. These degradations have a particularly serious impact
on face videos because the human visual system is highly sensitive to facial
details. Despite the significant advancement in video face enhancement, current
methods still suffer from $i)$ long processing time and $ii)$ inconsistent
spatial-temporal visual effects (e.g., flickering). This study proposes a novel
and efficient blind video face enhancement method to overcome the above two
challenges, restoring high-quality videos from their compressed low-quality
versions with an effective de-flickering mechanism. In particular, the proposed
method develops upon a 3D-VQGAN backbone associated with spatial-temporal
codebooks recording high-quality portrait features and residual-based temporal
information. We develop a two-stage learning framework for the model. In Stage
\Rmnum{1}, we learn the model with a regularizer mitigating the codebook
collapse problem. In Stage \Rmnum{2}, we learn two transformers to lookup code
from the codebooks and further update the encoder of low-quality videos.
Experiments conducted on the VFHQ-Test dataset demonstrate that our method
surpasses the current state-of-the-art blind face video restoration and
de-flickering methods on both efficiency and effectiveness. Code is available
at \url{https://github.com/Dixin-Lab/BFVR-STC}.


## VQ-SGen A Vector Quantized Stroke Representation for Sketch Generation

>Authors: Jiawei Wang, Zhiming Cui, Changjian Li

>2024-11-25

> http://arxiv.org/abs/2411.16446v1

This paper presents VQ-SGen, a novel algorithm for high-quality sketch
generation. Recent approaches have often framed the task as pixel-based
generation either as a whole or part-by-part, neglecting the intrinsic and
contextual relationships among individual strokes, such as the shape and
spatial positioning of both proximal and distant strokes. To overcome these
limitations, we propose treating each stroke within a sketch as an entity and
introducing a vector-**quantize**d (VQ) stroke representation for fine-grained
sketch generation. Our method follows a two-stage framework - in the first
stage, we decouple each stroke's shape and location information to ensure the
VQ representation prioritizes stroke shape learning. In the second stage, we
feed the precise and compact representation into an auto-decoding Transformer
to incorporate stroke semantics, positions, and shapes into the generation
process. By utilizing tokenized stroke representation, our approach generates
strokes with high fidelity and facilitates novel applications, such as
conditional generation and semantic-aware stroke editing. Comprehensive
experiments demonstrate our method surpasses existing state-of-the-art
techniques, underscoring its effectiveness. The code and model will be made
publicly available upon publication.


## WTDUN Wavelet Tree-Structured Sampling and Deep Unfolding Network for Image Compressed Sensing

>Authors: Kai Han, Jin Wang, Yunhui Shi, Hanqin Cai, Nam Ling, Baocai Yin

>2024-11-25

> http://arxiv.org/abs/2411.16336v1

Deep unfolding networks have gained increasing attention in the field of
compressed sensing (CS) owing to their theoretical interpretability and
superior reconstruction performance. However, most existing deep unfolding
methods often face the following issues: 1) they learn directly from
single-channel images, leading to a simple feature representation that does not
fully capture complex features; and 2) they treat various image components
uniformly, ignoring the characteristics of different components. To address
these issues, we propose a novel wavelet-domain deep unfolding framework named
WTDUN, which operates directly on the multi-scale wavelet subbands. Our method
utilizes the intrinsic **sparsity** and multi-scale structure of wavelet
coefficients to achieve a tree-structured sampling and reconstruction,
effectively capturing and highlighting the most important features within
images. Specifically, the design of tree-structured reconstruction aims to
capture the inter-dependencies among the multi-scale subbands, enabling the
identification of both fine and coarse features, which can lead to a marked
improvement in reconstruction quality. Furthermore, a wavelet domain adaptive
sampling method is proposed to greatly improve the sampling capability, which
is realized by assigning measurements to each wavelet subband based on its
importance. Unlike pure deep learning methods that treat all components
uniformly, our method introduces a targeted focus on important subbands,
considering their energy and **sparsity**. This targeted strategy lets us capture
key information more efficiently while discarding less important information,
resulting in a more effective and detailed reconstruction. Extensive
experimental results on various datasets validate the superior performance of
our proposed method.


## Even Sparser Graph Transformers

>Authors: Hamed Shirzad, Honghao Lin, Balaji Venkatachalam, Ameya Velingker, David Woodruff, Danica Sutherland

>2024-11-25

> http://arxiv.org/abs/2411.16278v1

Graph Transformers excel in long-range dependency modeling, but generally
require quadratic memory complexity in the number of nodes in an input graph,
and hence have trouble scaling to large graphs. Sparse attention variants such
as Exphormer can help, but may require high-degree augmentations to the input
graph for good performance, and do not attempt to sparsify an already-dense
input graph. As the learned attention mechanisms tend to use few of these
edges, such high-degree connections may be unnecessary. We show (empirically
and with theoretical backing) that attention scores on graphs are usually quite
consistent across network widths, and use this observation to propose a
two-stage procedure, which we call Spexphormer: first, train a narrow network
on the full augmented graph. Next, use only the active connections to train a
wider network on a much **sparse**r graph. We establish theoretical conditions when
a narrow network's attention scores can match those of a wide network, and show
that Spexphormer achieves good performance with drastically reduced memory
requirements on various graph datasets.


## MH-MoE Multi-Head Mixture-of-Experts

>Authors: Shaohan Huang, Xun Wu, Shuming Ma, Furu Wei

>2024-11-25

> http://arxiv.org/abs/2411.16205v2

Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by
using the multi-head mechanism to collectively attend to information from
various representation spaces within different experts. In this paper, we
present a novel implementation of MH-MoE that maintains both FLOPs and
parameter parity with **sparse** Mixture of Experts models. Experimental results on
language models show that the new implementation yields quality improvements
over both vanilla MoE and fine-grained MoE models. Additionally, our
experiments demonstrate that MH-MoE is compatible with 1-bit Large Language
Models (LLMs) such as BitNet.


## NovelGS Consistent Novel-view Denoising via Large Gaussian Reconstruction Model

>Authors: Jinpeng Liu, Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Ying Shan, Yansong Tang

>2024-11-25

> http://arxiv.org/abs/2411.16779v1

We introduce NovelGS, a diffusion model for Gaussian Splatting (GS) given
**sparse**-view images. Recent works leverage feed-forward networks to generate
pixel-aligned Gaussians, which could be fast rendered. Unfortunately, the
method was unable to produce satisfactory results for areas not covered by the
input images due to the formulation of these methods. In contrast, we leverage
the novel view denoising through a transformer-based network to generate 3D
Gaussians. Specifically, by incorporating both conditional views and noisy
target views, the network predicts pixel-aligned Gaussians for each view.
During training, the rendered target and some additional views of the Gaussians
are supervised. During inference, the target views are iteratively rendered and
denoised from pure noise. Our approach demonstrates state-of-the-art
performance in addressing the multi-view image reconstruction challenge. Due to
generative modeling of unseen regions, NovelGS effectively reconstructs 3D
objects with consistent and sharp textures. Experimental results on publicly
available datasets indicate that NovelGS substantially surpasses existing
image-to-3D frameworks, both qualitatively and quantitatively. We also
demonstrate the potential of NovelGS in generative tasks, such as text-to-3D
and image-to-3D, by integrating it with existing multiview diffusion models. We
will make the code publicly accessible.


## MixPE Quantization and Hardware Co-design for Efficient LLM Inference

>Authors: Yu Zhang, Mingzi Wang, Lancheng Zou, Wulong Liu, Hui-Ling Zhen, Mingxuan Yuan, Bei Yu

>2024-11-25

> http://arxiv.org/abs/2411.16158v1

Transformer-based large language models (LLMs) have achieved remarkable
success as model sizes continue to grow, yet their deployment remains
challenging due to significant computational and memory demands. Quantization
has emerged as a promising solution, and state-of-the-art **quantization**
algorithms for LLMs introduce the need for mixed-precision matrix
multiplication (mpGEMM), where lower-precision weights are multiplied with
higher-precision activations. Despite its benefits, current hardware
accelerators such as GPUs and TPUs lack native support for efficient mpGEMM,
leading to inefficient de**quantization** operations in the main sequential loop.
To address this limitation, we introduce MixPE, a specialized mixed-precision
processing element designed for efficient **low-bit** **quantization** in LLM
inference. MixPE leverages two key innovations to minimize de**quantization**
overhead and unlock the full potential of **low-bit** **quantization**. First,
recognizing that scale and zero point are shared within each **quantization**
group, we propose performing de**quantization** after per-group mpGEMM,
significantly reducing de**quantization** overhead. Second, instead of relying on
conventional multipliers, MixPE utilizes efficient shift\&add operations for
multiplication, optimizing both computation and energy efficiency. Our
experimental results demonstrate that MixPE surpasses the state-of-the-art
**quantization** accelerators by $2.6\times$ speedup and $1.4\times$ energy
reduction.


## SKQVC One-Shot Voice Conversion by K-Means Quantization with Self-Supervised Speech Representations

>Authors: Youngjun Sim, Jinsung Yoon, Young-Joo Suh

>2024-11-25

> http://arxiv.org/abs/2411.16147v1

One-shot voice conversion (VC) is a method that enables the transformation
between any two speakers using only a single target speaker utterance. Existing
methods often rely on complex architectures and pre-trained speaker
verification (SV) models to improve the fidelity of converted speech. Recent
works utilizing K-means **quantization** (KQ) with self-supervised learning (SSL)
features have proven capable of capturing content information from speech.
However, they often struggle to preserve speaking variation, such as prosodic
detail and phonetic variation, particularly with smaller codebooks. In this
work, we propose a simple yet effective one-shot VC model that utilizes the
characteristics of SSL features and speech attributes. Our approach addresses
the issue of losing speaking variation, enabling high-fidelity voice conversion
trained with only reconstruction losses, without requiring external speaker
embeddings. We demonstrate the performance of our model across 6 evaluation
metrics, with results highlighting the benefits of the speaking variation
compensation method.


## DF-GNN Dynamic Fusion Framework for Attention Graph Neural Networks on GPUs

>Authors: Jiahui Liu, Zhenkun Cai, Zhiyong Chen, Minjie Wang

>2024-11-25

> http://arxiv.org/abs/2411.16127v1

Attention Graph Neural Networks (AT-GNNs), such as GAT and Graph Transformer,
have demonstrated superior performance compared to other GNNs. However,
existing GNN systems struggle to efficiently train AT-GNNs on GPUs due to their
intricate computation patterns. The execution of AT-GNN operations without
kernel fusion results in heavy data movement and significant kernel launch
overhead, while fixed thread scheduling in existing GNN kernel fusion
strategies leads to sub-optimal performance, redundant computation and
unbalanced workload. To address these challenges, we propose a dynamic kernel
fusion framework, DF-GNN, for the AT-GNN family. DF-GNN introduces a dynamic
bi-level thread scheduling strategy, enabling flexible adjustments to thread
scheduling while retaining the benefits of shared memory within the fused
kernel. DF-GNN tailors specific thread scheduling for operations in AT-GNNs and
considers the performance bottleneck shift caused by the presence of super
nodes. Additionally, DF-GNN is integrated with the PyTorch framework for high
programmability. Evaluations across diverse GNN models and multiple datasets
reveal that DF-GNN surpasses existing GNN kernel optimization works like
cuGraph and dgNN, with speedups up to $7.0\times$ over the state-of-the-art
non-fusion DGL **sparse** library. Moreover, it achieves an average speedup of
$2.16\times$ in end-to-end training compared to the popular GNN computing
framework DGL.


## Soft-TransFormers for Continual Learning

>Authors: Haeyong Kang, Chang D. Yoo

>2024-11-25

> http://arxiv.org/abs/2411.16073v1

Inspired by Well-initialized Lottery Ticket Hypothesis (WLTH), which provides
suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual
learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF
sequentially learns and selects an optimal soft-network or subnetwork for each
task. During sequential training in CL, Soft-TF jointly optimizes the weights
of **sparse** layers to obtain task-adaptive soft (real-valued) networks or
subnetworks (binary masks), while keeping the well-pre-trained layer parameters
frozen. In inference, the identified task-adaptive network of Soft-TF masks the
parameters of the pre-trained network, mapping to an optimal solution for each
task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves
the knowledge of the pre-trained network. Extensive experiments on Vision
Transformer (ViT) and CLIP demonstrate the effectiveness of Soft-TF, achieving
state-of-the-art performance across various CL scenarios, including
Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL), supported
by convergence theory.


## Anda Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format

>Authors: Chao Fang, Man Shi, Robin Geens, Arne Symons, Zhongfeng Wang, Marian Verhelst

>2024-11-24

> http://arxiv.org/abs/2411.15982v1

The widely-used, weight-only **quantize**d large language models (LLMs), which
leverage **low-bit** integer (INT) weights and retain floating-point (FP)
activations, reduce storage requirements while maintaining accuracy. However,
this shifts the energy and latency bottlenecks towards the FP activations that
are associated with costly memory accesses and computations. Existing LLM
accelerators focus primarily on computation optimizations, overlooking the
potential of jointly optimizing FP computations and data movement, particularly
for the dominant FP-INT GeMM operations in LLM inference.
  To address these challenges, we investigate the sensitivity of activation
precision across various LLM modules and its impact on overall model accuracy.
Based on our findings, we first propose the Anda data type: an adaptive data
format with group-shared exponent bits and dynamic mantissa bit allocation.
Secondly, we develop an iterative post-training adaptive precision search
algorithm that optimizes the bit-width for different LLM modules to balance
model accuracy, energy efficiency, and inference speed. Lastly, a suite of
hardware optimization techniques is proposed to maximally exploit the benefits
of the Anda format. These include a bit-plane-based data organization scheme,
Anda-enhanced processing units with bit-serial computation, and a runtime
bit-plane Anda compressor to simultaneously optimize storage, computation, and
memory footprints. Our evaluations on FPINT GeMM operations show that Anda
achieves a 2.4x speedup, 4.0x area efficiency, and 3.1x energy efficiency
improvement on average for popular LLMs including OPT, LLaMA, and LLaMA-2
series over the GPU-like FP-FP baseline. Anda demonstrates strong adaptability
across various application scenarios, accuracy requirements, and system
performance, enabling efficient LLM inference across a wide range of deployment
scenarios.


## Towards the LLM-Based Generation of Formal Specifications from Natural-Language Contracts Early Experiments with Symboleo

>Authors: Mounira Nihad Zitouni, Amal Ahmed Anda, Sahil Rajpal, Daniel Amyot, John Mylopoulos

>2024-11-24

> http://arxiv.org/abs/2411.15898v1

Over the past decade, different domain-specific languages (DSLs) were
proposed to formally specify requirements stated in legal contracts, mainly for
analysis but also for code generation. Symboleo is a promising language in that
area. However, writing formal specifications from natural-language contracts is
a complex task, especial for legal experts who do not have formal language
expertise. This paper reports on an exploratory experiment targeting the
automated generation of Symboleo specifications from business contracts in
English using Large Language Models (LLMs). Combinations (38) of prompt
components are investigated (with/without the grammar, semantics explanations,
0 to 3 examples, and emotional prompts), mainly on GPT-4o but also to a lesser
extent on 4 other LLMs. The generated specifications are manually assessed
against 16 error types grouped into 3 severity levels. Early results on all
LLMs show promising outcomes (even for a little-known DSL) that will likely
accelerate the specification of legal contracts. However, several observed
issues, especially around grammar/syntax adherence and environment variable
identification (49%), suggest many areas where potential improvements should be
investigated.


## Evaluating Large Language Models for Causal Modeling

>Authors: Houssam Razouk, Leonie Benischke, Georg Niess, Roman Kern

>2024-11-24

> http://arxiv.org/abs/2411.15888v1

In this paper, we consider the process of transforming causal domain
knowledge into a representation that aligns more closely with guidelines from
causal data science. To this end, we introduce two novel tasks related to
distilling causal domain knowledge into causal variables and detecting
interaction entities using LLMs. We have determined that contemporary LLMs are
helpful tools for conducting causal modeling tasks in collaboration with human
experts, as they can provide a wider perspective. Specifically, LLMs, such as
GPT-4-turbo and Llama3-70b, perform better in distilling causal domain
knowledge into causal variables compared to **sparse** expert models, such as
Mixtral-8x22b. On the contrary, **sparse** expert models such as Mixtral-8x22b
stand out as the most effective in identifying interaction entities. Finally,
we highlight the dependency between the domain where the entities are generated
and the performance of the chosen LLM for causal modeling.


## LibraGrad Balancing Gradient Flow for Universally Better Vision Transformer Attributions

>Authors: Faridoun Mehri, Mahdieh Soleymani Baghshah, Mohammad Taher Pilehvar

>2024-11-24

> http://arxiv.org/abs/2411.16760v1

Why do gradient-based explanations struggle with Transformers, and how can we
improve them? We identify gradient flow imbalances in Transformers that violate
FullGrad-completeness, a critical property for attribution faithfulness that
CNNs naturally possess. To address this issue, we introduce LibraGrad -- a
theoretically grounded post-hoc approach that corrects gradient imbalances
through **pruning** and scaling of backward paths, without changing the forward
pass or adding computational overhead. We evaluate LibraGrad using three metric
families: Faithfulness, which quantifies prediction changes under perturbations
of the most and least relevant features; Completeness Error, which measures
attribution conservation relative to model outputs; and Segmentation AP, which
assesses alignment with human perception. Extensive experiments across 8
architectures, 4 model sizes, and 4 datasets show that LibraGrad universally
enhances gradient-based methods, outperforming existing white-box methods --
including Transformer-specific approaches -- across all metrics. We demonstrate
superior qualitative results through two complementary evaluations: precise
text-prompted region highlighting on CLIP models and accurate class
discrimination between co-occurring animals on ImageNet-finetuned models -- two
settings on which existing methods often struggle. LibraGrad is effective even
on the attention-free MLP-Mixer architecture, indicating potential for
extension to other modern architectures. Our code is freely available at
https://github.com/NightMachinery/LibraGrad.


## Variable-size Symmetry-based Graph Fourier Transforms for image compression

>Authors: Alessandro Gnutti, Fabrizio Guerrini, Riccardo Leonardi, Antonio Ortega

>2024-11-24

> http://arxiv.org/abs/2411.15824v1

Modern compression systems use linear transformations in their encoding and
decoding processes, with transforms providing compact signal representations.
While multiple data-dependent transforms for image/video coding can adapt to
diverse statistical characteristics, assembling large datasets to learn each
transform is challenging. Also, the resulting transforms typically lack fast
implementation, leading to significant computational costs. Thus, despite many
papers proposing new transform families, the most recent compression standards
predominantly use traditional separable sinusoidal transforms. This paper
proposes integrating a new family of Symmetry-based Graph Fourier Transforms
(SBGFTs) of variable sizes into a coding framework, focusing on the extension
from our previously introduced 8x8 SBGFTs to the general case of NxN grids.
SBGFTs are non-separable transforms that achieve **sparse** signal representation
while maintaining low computational complexity thanks to their symmetry
properties. Their design is based on our proposed algorithm, which generates
symmetric graphs on the grid by adding specific symmetrical connections between
nodes and does not require any data-dependent adaptation. Furthermore, for
video intra-frame coding, we exploit the correlations between optimal graphs
and prediction modes to reduce the cardinality of the transform sets, thus
proposing a low-complexity framework. Experiments show that SBGFTs outperform
the primary transforms integrated in the explicit Multiple Transform Selection
(MTS) used in the latest VVC intra-coding, providing a bit rate saving
percentage of 6.23%, with only a marginal increase in average complexity. A
MATLAB implementation of the proposed algorithm is available online at [1].


## A Method for Building Large Language Models with Predefined KV Cache Capacity

>Authors: Zhonghua Yi, Ge Niu, Lei Wang, Wei Tang, Liqiu Zhang

>2024-11-24

> http://arxiv.org/abs/2411.15785v2

This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),
for building large language models with a predefined Key-Value (**KV**) cache
capacity. The BCT addresses the excessive memory consumption issue in
traditional **KV** caches by implementing a bounded-length **KV** cache, which is
particularly suitable for the attention layers in Transformer decode-only
architectures. By dynamically updating the key-value vector sequences, the BCT
achieves efficient inference within limited cache capacity, significantly
reducing memory usage while maintaining model performance and system
throughput. Experimental results demonstrate that the BCT significantly reduces
memory usage while maintaining the model's inference quality, offering a new
solution for efficient inference in large language models.


## PriorDiffusion Leverage Language Prior in Diffusion Models for Monocular Depth Estimation

>Authors: Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong

>2024-11-24

> http://arxiv.org/abs/2411.16750v1

This paper explores the potential of leveraging language priors learned by
text-to-image diffusion models to address ambiguity and visual nuisance in
monocular depth estimation. Particularly, traditional monocular depth
estimation suffers from inherent ambiguity due to the absence of stereo or
multi-view depth cues, and nuisance due to lack of robustness of vision. We
argue that language prior in diffusion models can enhance monocular depth
estimation by leveraging the geometric prior aligned with the language
description, which is learned during text-to-image pre-training. To generate
images that reflect the text properly, the model must comprehend the size and
shape of specified objects, their spatial relationship, and the scale of the
scene. Thus, we propose PriorDiffusion, using a pre-trained text-to-image
diffusion model that takes both image and text description that aligned with
the scene to infer affine-invariant depth through a denoising process. We also
show that language priors can guide the model's attention to specific regions
and help it perceive the 3D scene in alignment with user intent.
Simultaneously, it acts as a constraint to accelerate the convergence of the
diffusion trajectory, since learning 3D properties from a condensed,
low-dimensional language feature is more efficient compared with learning from
a redundant, high-dimensional image feature. By training on HyperSim and
Virtual KITTI, we achieve state-of-the-art zero-shot performance and a faster
convergence speed, compared with other diffusion-based depth estimators, across
NYUv2, KITTI, ETH3D, and ScanNet.


## Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems

>Authors: Wenxiang Lin, Xinglin Pan, Shaohuai Shi, Xuan Wang, Xiaowen Chu

>2024-11-24

> http://arxiv.org/abs/2411.15715v1

Large language models~(LLMs) are known for their high demand on computing
resources and memory due to their substantial model size, which leads to
inefficient inference on moderate GPU systems. Techniques like **quantization** or
**pruning** can shrink model sizes but often impair accuracy, making them
unsuitable for practical applications. In this work, we introduce \modelname{},
a high-performance inference engine designed to speed up LLM inference without
compromising model accuracy. \modelname{} incorporates three innovative methods
to increase inference efficiency: 1) model partitioning to allow asynchronous
processing of tasks across CPU computation, GPU computation, and CPU-GPU
communication, 2) an adaptive partition algorithm to optimize the use of CPU,
GPU, and PCIe communication capabilities, and 3) a token assignment strategy to
handle diverse prompt and generation tasks during LLM inference. Comprehensive
experiments were conducted with various LLMs such as Mixtral, LLaMA-2, Qwen,
and PhiMoE across three test environments featuring different CPUs and GPUs.
The experimental findings demonstrate that \modelname{} achieves speeds between
$1.11\times$ to $1.80\times$ faster in decoding and $1.69\times$ to
$6.33\times$ faster in pre-filling, leading to an overall speedup ranging from
$1.25\times$ to $2.04\times$ compared to state-of-the-art solutions, llama.cpp
and Fiddler.


## LLaMA-MoE v2 Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training

>Authors: Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, Yu Cheng

>2024-11-24

> http://arxiv.org/abs/2411.15708v1

Recently, inspired by the concept of **sparsity**, Mixture-of-Experts (MoE)
models have gained increasing popularity for scaling model size while keeping
the number of activated parameters constant. In this study, we thoroughly
investigate the **sparsity** of the dense LLaMA model by constructing MoE for both
the attention (i.e., Attention MoE) and MLP (i.e., MLP MoE) modules in the
transformer blocks. Specifically, we investigate different expert construction
methods and granularities under the same activation conditions to analyze the
impact of sparsifying the model. Additionally, to comprehensively evaluate the
model's capabilities across various domains (e.g., conversation, code, math)
after sparsification, we apply **sparsity** to the instructed large language models
(LLMs) and construct instructed MoE models. To counteract the performance
degradation resulting from increased **sparsity**, we design a two-stage
post-training strategy to enhance model performance. Experiments on the LLaMA3
model demonstrate the potential effectiveness of this approach for future
developments of instructed MoE models. The source codes and models are
available at: \url{https://github.com/OpenSparseLLMs/LLaMA-MoE-v2}.


## DrugAgent Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration

>Authors: Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Tianfan Fu, Yue Zhao

>2024-11-24

> http://arxiv.org/abs/2411.15692v1

Recent advancements in Large Language Models (LLMs) have opened new avenues
for accelerating drug discovery processes. Despite their potential, several
critical challenges remain unsolved, particularly in translating theoretical
ideas into practical applications within the highly specialized field of
pharmaceutical research, limiting practitioners from leveraging the latest AI
development in drug discovery. To this end, we introduce DrugAgent, a
multi-agent framework aimed at automating machine learning (ML) programming in
drug discovery. DrugAgent incorporates domain expertise by identifying specific
requirements and building domain-specific tools, while systematically exploring
different ideas to find effective solutions. A preliminary case study
demonstrates DrugAgent's potential to overcome key limitations LLMs face in
drug discovery, moving toward AI-driven innovation. For example, DrugAgent is
able to complete the ML programming pipeline end-to-end, from data acquisition
to performance evaluation for the ADMET prediction task, and finally select the
best model, where the random forest model achieves an F1 score of 0.92 when
predicting absorption using the PAMPA dataset.


## Machine Learning-based sEMG Signal Classification for Hand Gesture Recognition

>Authors: Parshuram N. Aarotale, Ajita Rattani

>2024-11-23

> http://arxiv.org/abs/2411.15655v1

EMG-based hand gesture recognition uses electromyographic~(EMG) signals to
interpret and classify hand movements by analyzing electrical activity
generated by muscle contractions. It has wide applications in prosthesis
control, rehabilitation training, and human-computer interaction. Using
electrodes placed on the skin, the EMG sensor captures muscle signals, which
are processed and filtered to reduce noise. Numerous feature extraction and
machine learning algorithms have been proposed to extract and classify muscle
signals to distinguish between various hand gestures. This paper aims to
benchmark the performance of EMG-based hand gesture recognition using novel
feature extraction methods, namely, fused time-domain descriptors,
temporal-spatial descriptors, and wavelet transform-based features, combined
with the state-of-the-art machine and deep learning models. Experimental
investigations on the Grabmyo dataset demonstrate that the 1D Dilated CNN
performed the best with an accuracy of $97\%$ using fused time-domain
descriptors such as power spectral moments, **sparsity**, irregularity factor and
waveform length ratio. Similarly, on the FORS-EMG dataset, random forest
performed the best with an accuracy of $94.95\%$ using temporal-spatial
descriptors (which include time domain features along with additional features
such as coefficient of variation (COV), and Teager-Kaiser energy operator
(TKEO)).


## Reassessing Layer Pruning in LLMs New Insights and Methods

>Authors: Yao Lu, Hao Cheng, Yujie Fang, Zeyu Wang, Jiaheng Wei, Dongwei Xu, Qi Xuan, Xiaoniu Yang, Zhaowei Zhu

>2024-11-23

> http://arxiv.org/abs/2411.15558v1

Although large language models (LLMs) have achieved remarkable success across
various domains, their considerable scale necessitates substantial
computational resources, posing significant challenges for deployment in
resource-constrained environments. Layer **pruning**, as a simple yet effective
compression method, removes layers of a model directly, reducing computational
overhead. However, what are the best practices for layer **pruning** in LLMs? Are
sophisticated layer selection metrics truly effective? Does the LoRA (Low-Rank
Approximation) family, widely regarded as a leading method for pruned model
fine-tuning, truly meet expectations when applied to post-**pruning** fine-tuning?
To answer these questions, we dedicate thousands of GPU hours to benchmarking
layer **pruning** in LLMs and gaining insights across multiple dimensions. Our
results demonstrate that a simple approach, i.e., **pruning** the final 25\% of
layers followed by fine-tuning the \texttt{lm\_head} and the remaining last
three layer, yields remarkably strong performance. Following this guide, we
prune Llama-3.1-8B-It and obtain a model that outperforms many popular LLMs of
similar size, such as ChatGLM2-6B, Vicuna-7B-v1.5, Qwen1.5-7B and Baichuan2-7B.
We release the optimal model weights on Huggingface, and the code is available
on GitHub.


## freePruner A Training-free Approach for Large Multimodal Model Acceleration

>Authors: Bingxin Xu, Yuzhang Shang, Yunhao Ge, Qian Lou, Yan Yan

>2024-11-23

> http://arxiv.org/abs/2411.15446v1

Large Multimodal Models (LMMs) have demonstrated impressive capabilities in
visual-language tasks but face significant deployment challenges due to their
high computational demands. While recent token reduction methods show promise
for accelerating LMMs, they typically require extensive retraining or
fine-tuning, making them impractical for many state-of-the-art models,
especially those with proprietary training data. We propose freePruner, a
training-free token reduction approach that can be directly applied to any
open-source LMM without additional training. Unlike existing methods that rely
heavily on token merging operations, freePruner employs a two-stage token
selection strategy: (1) identifying pivotal tokens that capture high-level
semantic information using our designed contribution degree metric, and (2)
selecting complementary tokens that preserve essential low-level visual details
through attention pattern analysis. Extensive experiments demonstrate that
freePruner achieves 2x **acceleration** while maintaining comparable performance
across mainstream visual question-answering benchmarks in the training-free
setting. Moreover, freePruner is orthogonal to and can be combined with other
post-training **acceleration** techniques, such as post-training **quantization**,
providing a practical solution for efficient LMM deployment.


## SPRINT Enables Interpretable and Ultra-Fast Virtual Screening against Thousands of Proteomes

>Authors: Andrew T. McNutt, Abhinav K. Adduri, Caleb N. Ellington, Monica T. Dayao, Eric P. Xing, Hosein Mohimani, David R. Koes

>2024-11-23

> http://arxiv.org/abs/2411.15418v1

Virtual screening of small molecules against protein targets can accelerate
drug discovery and development by predicting drug-target interactions (DTIs).
However, structure-based methods like molecular docking are too slow to allow
for broad proteome-scale screens, limiting their application in screening for
off-target effects or new molecular mechanisms. Recently, vector-based methods
using protein language models (PLMs) have emerged as a complementary approach
that bypasses explicit 3D structure modeling. Here, we develop SPRINT, a
vector-based approach for screening entire chemical libraries against whole
proteomes for DTIs and novel mechanisms of action. SPRINT improves on prior
work by using a self-attention based architecture and structure-aware PLMs to
learn drug-target co-embeddings for binder prediction, search, and retrieval.
SPRINT achieves SOTA enrichment factors in virtual screening on LIT-PCBA and
DTI classification benchmarks, while providing interpretability in the form of
residue-level attention maps. In addition to being both accurate and
interpretable, SPRINT is ultra-fast: querying the whole human proteome against
the ENAMINE Real Database (6.7B drugs) for the 100 most likely binders per
protein takes 16 minutes. SPRINT promises to enable virtual screening at an
unprecedented scale, opening up new opportunities for in silico drug
repurposing and development. SPRINT is available on the web as ColabScreen:
https://bit.ly/colab-screen


## Efficient Online Inference of Vision Transformers by Training-Free Tokenization

>Authors: Leonidas Gee, Wing Yan Li, Viktoriia Sharmanska, Novi Quadrianto

>2024-11-23

> http://arxiv.org/abs/2411.15397v1

The cost of deploying vision transformers increasingly represents a barrier
to wider industrial adoption. Existing compression requires additional
end-to-end fine-tuning or incurs a significant drawback to runtime, thus making
them ill-suited for online inference. We introduce the $\textbf{Visual Word
Tokenizer}$ (VWT), a training-free method for reducing energy costs while
retaining performance and runtime. The VWT groups patches (visual subwords)
that are frequently used into visual words while infrequent ones remain intact.
To do so, intra-image or inter-image statistics are leveraged to identify
similar visual concepts for compression. Experimentally, we demonstrate a
reduction in wattage of up to 19% with only a 20% increase in runtime at most.
Comparative approaches of 8-bit **quantization** and token merging achieve a lower
or similar energy efficiency but exact a higher toll on runtime (up to
$2\times$ or more). Our results indicate that VWTs are well-suited for
efficient online inference with a marginal compromise on performance.


## ChatBCI A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios

>Authors: Jiazhen Hong, Weinan Wang, Laleh Najafizadeh

>2024-11-23

> http://arxiv.org/abs/2411.15395v1

P300 speller BCIs allow users to compose sentences by selecting target keys
on a GUI through the detection of P300 component in their EEG signals following
visual stimuli. Most P300 speller BCIs require users to spell words letter by
letter, or the first few initial letters, resulting in high keystroke demands
that increase time, cognitive load, and fatigue. This highlights the need for
more efficient, user-friendly methods for faster sentence composition. In this
work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot
learning capabilities of large language models (LLMs) to suggest words from
user-spelled initial letters or predict the subsequent word(s), reducing
keystrokes and accelerating sentence composition. ChatBCI retrieves word
suggestions through remote queries to the GPT-3.5 API. A new GUI, displaying
GPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300
classification. Seven subjects completed two online spelling tasks: 1)
copy-spelling a self-composed sentence using ChatBCI, and 2) improvising a
sentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,
on average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time
and keystrokes by 62.14% and 53.22%, respectively, and increasing information
transfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings
and a record 8.53 characters/min for typing speed. Overall, ChatBCI, by
employing remote LLM queries, enhances sentence composition in realistic
scenarios, significantly outperforming traditional spellers without requiring
local model training or storage. ChatBCI's (multi-) word predictions, combined
with its new GUI, pave the way for developing next-generation speller BCIs that
are efficient and effective for real-time communication, especially for users
with communication and motor disabilities.


## On the Impact of Fine-Tuning on Chain-of-Thought Reasoning

>Authors: Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju

>2024-11-22

> http://arxiv.org/abs/2411.15382v1

Large language models have emerged as powerful tools for general
intelligence, showcasing advanced natural language processing capabilities that
find applications across diverse domains. Despite their impressive performance,
recent studies have highlighted the potential for significant enhancements in
LLMs' task-specific performance through fine-tuning strategies like
Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning
(SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works
have shown that while fine-tuning offers significant performance gains, it also
leads to challenges such as catastrophic forgetting and privacy and safety
risks. To this end, there has been little to no work in \textit{understanding
the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research
investigates the effect of fine-tuning on the reasoning abilities of LLMs,
addressing critical questions regarding the impact of task-specific fine-tuning
on overall reasoning capabilities, the influence of fine-tuning on
Chain-of-Thought (CoT) reasoning performance, and the implications for the
faithfulness of CoT reasonings. By exploring these dimensions, our study shows
the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness
of CoT reasoning, on average across four datasets, decreases, highlighting
potential shifts in internal mechanisms of the LLMs resulting from fine-tuning
processes.


## SafeLight Enhancing Security in Optical Convolutional Neural Network Accelerators

>Authors: Salma Afifi, Ishan Thakkar, Sudeep Pasricha

>2024-11-22

> http://arxiv.org/abs/2411.16712v1

The rapid proliferation of deep learning has revolutionized computing
hardware, driving innovations to improve computationally expensive
multiply-and-accumulate operations in deep neural networks. Among these
innovations are integrated silicon-photonic systems that have emerged as
energy-efficient platforms capable of achieving light speed computation and
communication, positioning optical neural network (ONN) platforms as a
transformative technology for accelerating deep learning models such as
convolutional neural networks (CNNs). However, the increasing complexity of
optical hardware introduces new vulnerabilities, notably the risk of hardware
trojan (HT) attacks. Despite the growing interest in ONN platforms, little
attention has been given to how HT-induced threats can compromise performance
and security. This paper presents an in-depth analysis of the impact of such
attacks on the performance of CNN models accelerated by ONN accelerators.
Specifically, we show how HTs can compromise microring resonators (MRs) in a
state-of-the-art non-coherent ONN accelerator and reduce classification
accuracy across CNN models by up to 7.49% to 80.46% by just targeting 10% of
MRs. We then propose techniques to enhance ONN accelerator robustness against
these attacks and show how the best techniques can effectively recover the
accuracy drops.


## The Seiberg-Witten Axion

>Authors: Csaba Csáki, Rotem Ovadia, Maximilian Ruhdorfer, Ofri Telem, John Terning

>2024-11-22

> http://arxiv.org/abs/2411.15312v1

We present a fully calculable UV complete toy model of a Peccei-Quinn (PQ)
axion coupled to magnetic monopoles as well as electric charges. The theory has
manifest electric-magnetic duality built in. We find that the axion-photon
coupling contains the usual anomaly term, plus periodic corrections which can
also become large if the monopole is light, without violating the discrete
axion shift symmetry. These additional periodic terms can be identified as the
non-perturbative corrections due to the monopoles (and other BPS states), but
can also be interpreted as a sum over instanton corrections. The key aspect
helping reconcile axion coupling **quantization** with electric-magnetic duality is
the fact that the axion itself undergoes a non-linear transformation under
electric-magnetic duality. The theory analyzed here is just the original $N=2$
supersymmetric $SU(2)$ Seiberg-Witten theory, which contains a PQ axion due to
an anomalous spontaneously broken global $R$-symmetry, as well as massless
fermionic monopoles and dyons at special points in the moduli space. Hence the
entire machinery of the Seiberg-Witten solution can be applied to reliably
calculate the photon-axion coupling in different duality frames. We show
explicitly that the physically observable axion-photon amplitude is duality
invariant, as it had to be.


## XGrammar Flexible and Efficient Structured Generation Engine for Large Language Models

>Authors: Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen

>2024-11-22

> http://arxiv.org/abs/2411.15100v2

The applications of LLM Agents are becoming increasingly complex and diverse,
leading to a high demand for structured outputs that can be parsed into code,
structured function calls, and embodied agent commands. These developments
bring significant demands for structured generation in LLM inference.
Context-free grammar is a flexible approach to enable structured generation via
constrained decoding. However, executing context-free grammar requires going
through several stack states over all tokens in vocabulary during runtime,
bringing non-negligible overhead for structured generation. In this paper, we
propose XGrammar, a flexible and efficient structure generation engine for
large language models. XGrammar accelerates context-free grammar execution by
dividing the vocabulary into context-independent tokens that can be prechecked
and context-dependent tokens that need to be interpreted during runtime. We
further build transformations to expand the grammar context and reduce the
number of context-independent tokens. Additionally, we build an efficient
persistent stack to accelerate the context-dependent token checks. Finally, we
co-design the grammar engine with LLM inference engine to overlap grammar
computation with GPU executions. Evaluation results show that XGrammar can
achieve up to 100x speedup over existing solutions. Combined with an LLM
inference engine, it can generate near-zero overhead structure generation in
end-to-end low-LLM serving.


## Design-o-meter Towards Evaluating and Refining Graphic Designs

>Authors: Sahil Goyal, Abhinav Mahajan, Swasti Mishra, Prateksha Udhayanan, Tripti Shukla, K J Joseph, Balaji Vasan Srinivasan

>2024-11-22

> http://arxiv.org/abs/2411.14959v1

Graphic designs are an effective medium for visual communication. They range
from greeting cards to corporate flyers and beyond. Off-late, machine learning
techniques are able to generate such designs, which accelerates the rate of
content production. An automated way of evaluating their quality becomes
critical. Towards this end, we introduce Design-o-meter, a data-driven
methodology to quantify the goodness of graphic designs. Further, our approach
can suggest modifications to these designs to improve its visual appeal. To the
best of our knowledge, Design-o-meter is the first approach that scores and
refines designs in a unified framework despite the inherent subjectivity and
ambiguity of the setting. Our exhaustive quantitative and qualitative analysis
of our approach against baselines adapted for the task (including recent
Multimodal LLM-based approaches) brings out the efficacy of our methodology. We
hope our work will usher more interest in this important and pragmatic problem
setting.


## Sources and Radiations of the Fermi Bubbles

>Authors: Vladimir A. Dogiel, Chung-Ming Ko

>2024-11-22

> http://arxiv.org/abs/2411.14916v1

Two enigmatic gamma-ray features in the Galactic central region, known as
Fermi Bubbles (FBs), were found from Fermi-LAT data. An energy release (e.g.,
by tidal disruption events in the Galactic center, GC), generates a cavity with
a shock that expands into the local ambient medium of the Galactic halo. A
decade or so ago, a phenomenological model of the FBs was suggested as a result
of routine star disruptions by the supermassive black hole in the GC which
might provide enough energy for large-scale structures, like the FBs. In 2020,
analytical and numerical models of the FBs as a process of routine tidal
disruption of stars near the GC were developed, which can provide enough
cumulative energy to form and maintain large scale structures like the FBs. The
disruption events are expected to be ten to hundred events per million years,
providing the average power of energy release from the GC into the halo of 3E41
erg/s, which is needed to support the FBs. Analysis of the evolution of
superbubbles in exponentially stratified disks concluded that the FB envelope
would be destroyed by the Rayleigh-Taylor (RT) instabilities at late stages.
The shell is composed of a swept-up gas of the bubble, whose thickness is much
thinner in comparison to the size of the envelope. We assume that hydrodynamic
turbulence is excited in the FB envelope by the RT instability. In this case,
the universal energy spectrum of turbulence may be developed in the inertial
range of wavenumbers of fluctuations (the Kolmogorov-Obukhov spectrum). From
our model we suppose the power of the FBs is transformed partly into the energy
of hydrodynamic turbulence in the envelope. If so, hydrodynamic turbulence may
generate MHD-fluctuations, which accelerate cosmic rays there and generate
gamma-ray and radio emission from the FBs. We hope that this model may
interpret the observed nonthermal emission from the bubbles.


## J-Invariant Volume Shuffle for Self-Supervised Cryo-Electron Tomogram Denoising on Single Noisy Volume

>Authors: Xiwei Liu, Mohamad Kassab, Min Xu, Qirong Ho

>2024-11-22

> http://arxiv.org/abs/2411.15248v1

Cryo-Electron Tomography (Cryo-ET) enables detailed 3D visualization of
cellular structures in near-native states but suffers from low signal-to-noise
ratio due to imaging constraints. Traditional denoising methods and supervised
learning approaches often struggle with complex noise patterns and the lack of
paired datasets. Self-supervised methods, which utilize noisy input itself as a
target, have been studied; however, existing Cryo-ET self-supervised denoising
methods face significant challenges due to losing information during training
and the learned incomplete noise patterns. In this paper, we propose a novel
self-supervised learning model that denoises Cryo-ET volumetric images using a
single noisy volume. Our method features a U-shape J-invariant blind spot
network with **sparse** centrally masked convolutions, dilated channel attention
blocks, and volume unshuffle/shuffle technique. The volume-unshuffle/shuffle
technique expands receptive fields and utilizes multi-scale representations,
significantly improving noise reduction and structural preservation.
Experimental results demonstrate that our approach achieves superior
performance compared to existing methods, advancing Cryo-ET data processing for
structural biology research


## TEXGen a Generative Diffusion Model for Mesh Textures

>Authors: Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, JianHui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, Xiaojuan Qi

>2024-11-22

> http://arxiv.org/abs/2411.14740v1

While high-quality texture maps are essential for realistic 3D asset
rendering, few studies have explored learning directly in the texture space,
especially on large-scale datasets. In this work, we depart from the
conventional approach of relying on pre-trained 2D diffusion models for
test-time optimization of 3D textures. Instead, we focus on the fundamental
problem of learning in the UV texture space itself. For the first time, we
train a large diffusion model capable of directly generating high-resolution
texture maps in a feed-forward manner. To facilitate efficient learning in
high-resolution UV spaces, we propose a scalable network architecture that
interleaves convolutions on UV maps with attention layers on point clouds.
Leveraging this architectural design, we train a 700 million parameter
diffusion model that can generate UV texture maps guided by text prompts and
single-view images. Once trained, our model naturally supports various extended
applications, including text-guided texture inpainting, **sparse**-view texture
completion, and text-driven texture synthesis. Project page is at
http://cvmi-lab.github.io/TEXGen/.


## IRLab@iKAT24 Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search

>Authors: Simon Lupart, Zahra Abbasiantaeb, Mohammad Aliannejadi

>2024-11-22

> http://arxiv.org/abs/2411.14739v1

The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing
conversational assistants, able to adapt their interaction and responses from
personalized user knowledge. The track incorporates a Personal Textual
Knowledge Base (PTKB) alongside Conversational AI tasks, such as passage
ranking and response generation. Query Rewrite being an effective approach for
resolving conversational context, we explore Large Language Models (LLMs), as
query rewriters. Specifically, our submitted runs explore multi-aspect query
generation using the MQ4CS framework, which we further enhance with Learned
Sparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder
models. We also propose an alternative to the previous interleaving strategy,
aggregating multiple aspects during the reranking phase. Our findings indicate
that multi-aspect query generation is effective in enhancing performance when
integrated with advanced retrieval and reranking models. Our results also lead
the way for better personalization in Conversational Search, relying on LLMs to
integrate personalization within query rewrite, and outperforming human rewrite
performance.


## FLARE FP-Less PTQ and Low-ENOB ADC Based AMS-PiM for Error-Resilient, Fast, and Efficient Transformer Acceleration

>Authors: Donghyeon Yi, Seoyoung Lee, Jongho Kim, Junyoung Kim, Sohmyung Ha, Ik Joon Chang, Minkyu Je

>2024-11-22

> http://arxiv.org/abs/2411.14733v1

Encoder-based transformers, powered by self-attention layers, have
revolutionized machine learning with their context-aware representations.
However, their quadratic growth in computational and memory demands presents
significant bottlenecks. Analog-Mixed-Signal Process-in-Memory (AMS-PiM)
architectures address these challenges by enabling efficient on-chip
processing. Traditionally, AMS-PiM relies on Quantization-Aware Training (QAT),
which is hardware-efficient but requires extensive retraining to adapt models
to AMS-PiMs, making it increasingly impractical for transformer models.
Post-Training Quantization (PTQ) mitigates this training overhead but
introduces significant hardware inefficiencies. PTQ relies on
de**quantization**-**quantization** (DQ-Q) processes, floating-point units (FPUs), and
high-ENOB (Effective Number of Bits) analog-to-digital converters (ADCs).
Particularly, High-ENOB ADCs scale exponentially in area and energy
($2^{ENOB}$), reduce sensing margins, and increase susceptibility to process,
voltage, and temperature (PVT) variations, further compounding PTQ's challenges
in AMS-PiM systems. To overcome these limitations, we propose RAP, an AMS-PiM
architecture that eliminates DQ-Q processes, introduces FPU- and division-free
nonlinear processing, and employs a low-ENOB-ADC-based **sparse** Matrix Vector
multiplication technique. Using the proposed techniques, RAP improves error
resiliency, area/energy efficiency, and computational speed while preserving
numerical stability. Experimental results demonstrate that RAP outperforms
state-of-the-art GPUs and conventional PiM architectures in energy efficiency,
latency, and accuracy, making it a scalable solution for the efficient
deployment of transformers.


## A Lightweight Edge-CNN-Transformer Model for Detecting Coordinated Cyber and Digital Twin Attacks in Cooperative Smart Farming

>Authors: Lopamudra Praharaj, Deepti Gupta, Maanak Gupta

>2024-11-22

> http://arxiv.org/abs/2411.14729v1

The agriculture sector is increasingly adopting innovative technologies to
meet the growing food demands of the global population. To optimize resource
utilization and minimize crop losses, farmers are joining cooperatives to share
their data and resources among member farms. However, while farmers benefit
from this data sharing and interconnection, it exposes them to cybersecurity
threats and privacy concerns. A cyberattack on one farm can have widespread
consequences, affecting the targeted farm as well as all member farms within a
cooperative. In this research, we address existing gaps by proposing a novel
and secure architecture for Cooperative Smart Farming (CSF). First, we
highlight the role of edge-based DTs in enhancing the efficiency and resilience
of agricultural operations. To validate this, we develop a test environment for
CSF, implementing various cyberattacks on both the DTs and their physical
counterparts using different attack vectors. We collect two smart farming
network datasets to identify potential threats. After identifying these
threats, we focus on preventing the transmission of malicious data from
compromised farms to the central cloud server. To achieve this, we propose a
CNN-Transformer-based network anomaly detection model, specifically designed
for deployment at the edge. As a proof of concept, we implement this model and
evaluate its performance by varying the number of encoder layers. Additionally,
we apply Post-Quantization to compress the model and demonstrate the impact of
compression on its performance in edge environments. Finally, we compare the
model's performance with traditional machine learning approaches to assess its
overall effectiveness.


## BrightVAE Luminosity Enhancement in Underexposed Endoscopic Images

>Authors: Farzaneh Koohestani, Zahra Nabizadeh, Nader Karimi, Shahram Shirani, Shadrokh Samavi

>2024-11-22

> http://arxiv.org/abs/2411.14663v1

The enhancement of image luminosity is especially critical in endoscopic
images. Underexposed endoscopic images often suffer from reduced contrast and
uneven brightness, significantly impacting diagnostic accuracy and treatment
planning. Internal body imaging is challenging due to uneven lighting and
shadowy regions. Enhancing such images is essential since precise image
interpretation is crucial for patient outcomes. In this paper, we introduce
BrightVAE, an architecture based on the hierarchical Vector Quantized
Variational Autoencoder (hierarchical VQ-VAE) tailored explicitly for enhancing
luminosity in low-light endoscopic images. Our architecture is meticulously
designed to tackle the unique challenges inherent in endoscopic imaging, such
as significant variations in illumination and obscured details due to poor
lighting conditions. The proposed model emphasizes advanced feature extraction
from three distinct viewpoints-incorporating various receptive fields, skip
connections, and feature attentions to robustly enhance image quality and
support more accurate medical diagnoses. Through rigorous experimental
analysis, we demonstrate the effectiveness of these techniques in enhancing
low-light endoscopic images. To evaluate the performance of our architecture,
we employ three widely recognized metrics-SSIM, PSNR, and LPIPS-specifically on
Endo4IE dataset, which consists of endoscopic images. We evaluated our method
using the Endo4IE dataset, which consists exclusively of endoscopic images, and
showed significant advancements over the state-of-the-art methods for enhancing
luminosity in endoscopic imaging.


## VQalAttent a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space

>Authors: Armani Rodriguez, Silvija Kokalj-Filipovic

>2024-11-22

> http://arxiv.org/abs/2411.14642v1

Generating high-quality speech efficiently remains a key challenge for
generative models in speech synthesis. This paper introduces VQalAttent, a
lightweight model designed to generate fake speech with tunable performance and
interpretability. Leveraging the AudioMNIST dataset, consisting of human
utterances of decimal digits (0-9), our method employs a two-step architecture:
first, a scalable vector **quantize**d autoencoder (VQ-VAE) that compresses audio
spectrograms into discrete latent representations, and second, a decoder-only
transformer that learns the probability model of these latents. Trained
transformer generates similar latent sequences, convertible to audio
spectrograms by the VQ-VAE decoder, from which we generate fake utterances.
Interpreting statistical and perceptual quality of the fakes, depending on the
dimension and the extrinsic information of the latent space, enables guided
improvements in larger, commercial generative models. As a valuable tool for
understanding and refining audio synthesis, our results demonstrate
VQalAttent's capacity to generate intelligible speech samples with limited
computational resources, while the modularity and transparency of the training
pipeline helps easily correlate the analytics with modular modifications, hence
providing insights for the more complex models.

