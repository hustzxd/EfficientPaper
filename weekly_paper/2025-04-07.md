# 2025-04-07

# Table of Contents
* [Nemotron-H A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](#Nemotron-H-A-Family-of-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Models)
* [ZFusion An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving](#ZFusion-An-Effective-Fuser-of-Camera-and-4D-Radar-for-3D-Object-Perception-in-Autonomous-Driving)
* [Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning](#Online-Difficulty-Filtering-for-Reasoning-Oriented-Reinforcement-Learning)
* [Sustainable LLM Inference for Edge AI Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency](#Sustainable-LLM-Inference-for-Edge-AI-Evaluating-Quantized-LLMs-for-Energy-Efficiency,-Output-Accuracy,-and-Inference-Latency)
* [Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale Interval-Valued Time Series Forecasting](#Block-Toeplitz-Sparse-Precision-Matrix-Estimation-for-Large-Scale-Interval-Valued-Time-Series-Forecasting)
* [A Survey of Quantum Transformers Approaches, Advantages, Challenges, and Future Directions](#A-Survey-of-Quantum-Transformers-Approaches,-Advantages,-Challenges,-and-Future-Directions)
* [Model Reveals What to Cache Profiling-Based Feature Reuse for Video Diffusion Models](#Model-Reveals-What-to-Cache-Profiling-Based-Feature-Reuse-for-Video-Diffusion-Models)
* [NuWa Deriving Lightweight Task-Specific Vision Transformers for Edge Devices](#NuWa-Deriving-Lightweight-Task-Specific-Vision-Transformers-for-Edge-Devices)
* [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](#Sparse-Autoencoders-Learn-Monosemantic-Features-in-Vision-Language-Models)
* [Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition](#Multi-Head-Adaptive-Graph-Convolution-Network-for-Sparse-Point-Cloud-Based-Human-Activity-Recognition)
* [HyperRAG Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse](#HyperRAG-Enhancing-Quality-Efficiency-Tradeoffs-in-Retrieval-Augmented-Generation-with-Reranker-KV-Cache-Reuse)
* [Echoes of the hidden Uncovering coordination beyond network structure](#Echoes-of-the-hidden-Uncovering-coordination-beyond-network-structure)
* [HQViT Hybrid Quantum Vision Transformer for Image Classification](#HQViT-Hybrid-Quantum-Vision-Transformer-for-Image-Classification)
* [ERPO Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization](#ERPO-Advancing-Safety-Alignment-via-Ex-Ante-Reasoning-Preference-Optimization)
* [GPTQv2 Efficient Finetuning-Free Quantization for Asymmetric Calibration](#GPTQv2-Efficient-Finetuning-Free-Quantization-for-Asymmetric-Calibration)
* [RoSMM A Robust and Secure Multi-Modal Watermarking Framework for Diffusion Models](#RoSMM-A-Robust-and-Secure-Multi-Modal-Watermarking-Framework-for-Diffusion-Models)
* [APHQ-ViT Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers](#APHQ-ViT-Post-Training-Quantization-with-Average-Perturbation-Hessian-Based-Reconstruction-for-Vision-Transformers)
* [Cognitive Memory in Large Language Models](#Cognitive-Memory-in-Large-Language-Models)
* [Inducing contractions of the mother of all continued fractions](#Inducing-contractions-of-the-mother-of-all-continued-fractions)
* [Toward General and Robust LLM-enhanced Text-attributed Graph Learning](#Toward-General-and-Robust-LLM-enhanced-Text-attributed-Graph-Learning)
* [Riemannian Optimization for Sparse Tensor CCA](#Riemannian-Optimization-for-Sparse-Tensor-CCA)
* [LearNAT Learning NL2SQL with AST-guided Task Decomposition for Large Language Models](#LearNAT-Learning-NL2SQL-with-AST-guided-Task-Decomposition-for-Large-Language-Models)
* [MegaScale-Infer Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](#MegaScale-Infer-Serving-Mixture-of-Experts-at-Scale-with-Disaggregated-Expert-Parallelism)
* [WonderTurbo Generating Interactive 3D World in 0.72 Seconds](#WonderTurbo-Generating-Interactive-3D-World-in-0.72-Seconds)
* [LLM-Augmented Graph Neural Recommenders Integrating User Reviews](#LLM-Augmented-Graph-Neural-Recommenders-Integrating-User-Reviews)
* [MDP Multidimensional Vision Model Pruning with Latency Constraint](#MDP-Multidimensional-Vision-Model-Pruning-with-Latency-Constraint)
* [LakeVisage Towards Scalable, Flexible and Interactive Visualization Recommendation for Data Discovery over Data Lakes](#LakeVisage-Towards-Scalable,-Flexible-and-Interactive-Visualization-Recommendation-for-Data-Discovery-over-Data-Lakes)
* [LL4G Self-Supervised Dynamic Optimization for Graph-Based Personality Detection](#LL4G-Self-Supervised-Dynamic-Optimization-for-Graph-Based-Personality-Detection)
* [LLMPi Optimizing LLMs for High-Throughput on Raspberry Pi](#LLMPi-Optimizing-LLMs-for-High-Throughput-on-Raspberry-Pi)
* [Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis](#Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis)
* [Laboratory evaluation of a wearable instrumented headband for rotational head kinematics measurement](#Laboratory-evaluation-of-a-wearable-instrumented-headband-for-rotational-head-kinematics-measurement)
* [Review, Refine, Repeat Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection](#Review,-Refine,-Repeat-Understanding-Iterative-Decoding-of-AI-Agents-with-Dynamic-Evaluation-and-Selection)
* [Architect Your Landscape Approach (AYLA) for Optimizations in Deep Learning](#Architect-Your-Landscape-Approach-(AYLA)-for-Optimizations-in-Deep-Learning)
* [Quattro Transformer-Accelerated Iterative Linear Quadratic Regulator Framework for Fast Trajectory Optimization](#Quattro-Transformer-Accelerated-Iterative-Linear-Quadratic-Regulator-Framework-for-Fast-Trajectory-Optimization)
* [UniViTAR Unified Vision Transformer with Native Resolution](#UniViTAR-Unified-Vision-Transformer-with-Native-Resolution)
* [Token Pruning in Audio Transformers Optimizing Performance and Decoding Patch Importance](#Token-Pruning-in-Audio-Transformers-Optimizing-Performance-and-Decoding-Patch-Importance)
* [Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams the Case of Luxembourgish](#Testing-Low-Resource-Language-Support-in-LLMs-Using-Language-Proficiency-Exams-the-Case-of-Luxembourgish)
* [Generative Retrieval and Alignment Model A New Paradigm for E-commerce Retrieval](#Generative-Retrieval-and-Alignment-Model-A-New-Paradigm-for-E-commerce-Retrieval)
* [Attention Mamba Time Series Modeling with Adaptive Pooling Acceleration and Receptive Field Enhancements](#Attention-Mamba-Time-Series-Modeling-with-Adaptive-Pooling-Acceleration-and-Receptive-Field-Enhancements)
* [When Reasoning Meets Compression Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks](#When-Reasoning-Meets-Compression-Benchmarking-Compressed-Large-Reasoning-Models-on-Complex-Reasoning-Tasks)
* [Global Rice Multi-Class Segmentation Dataset (RiceSEG) A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms](#Global-Rice-Multi-Class-Segmentation-Dataset-(RiceSEG)-A-Comprehensive-and-Diverse-High-Resolution-RGB-Annotated-Images-for-the-Development-and-Benchmarking-of-Rice-Segmentation-Algorithms)
* [ThinkPrune Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning](#ThinkPrune-Pruning-Long-Chain-of-Thought-of-LLMs-via-Reinforcement-Learning)
* [PolygoNet Leveraging Simplified Polygonal Representation for Effective Image Classification](#PolygoNet-Leveraging-Simplified-Polygonal-Representation-for-Effective-Image-Classification)
* [Efficient n-body simulations using physics informed graph neural networks](#Efficient-n-body-simulations-using-physics-informed-graph-neural-networks)
* [MaLAware Automating the Comprehension of Malicious Software Behaviours using Large Language Models (LLMs)](#MaLAware-Automating-the-Comprehension-of-Malicious-Software-Behaviours-using-Large-Language-Models-(LLMs))
* [Near-energy-free Photonic Fourier Transformation for Convolution Operation Acceler](#Near-energy-free-Photonic-Fourier-Transformation-for-Convolution-Operation-Acceler)
* [MergeVQ A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization](#MergeVQ-A-Unified-Framework-for-Visual-Generation-and-Representation-with-Disentangled-Token-Merging-and-Quantization)
* [Accelerating drug discovery with Artificial a whole-lab orchestration and scheduling system for self-driving labs](#Accelerating-drug-discovery-with-Artificial-a-whole-lab-orchestration-and-scheduling-system-for-self-driving-labs)
* [DropGaussian Structural Regularization for Sparse-view Gaussian Splatting](#DropGaussian-Structural-Regularization-for-Sparse-view-Gaussian-Splatting)
* [Accelerated Inorganic Materials Design with Generative AI Agents](#Accelerated-Inorganic-Materials-Design-with-Generative-AI-Agents)
* [ToReMi Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection](#ToReMi-Topic-Aware-Data-Reweighting-for-Dynamic-Pre-Training-Data-Selection)
* [Coca-Splat Collaborative Optimization for Camera Parameters and 3D Gaussians](#Coca-Splat-Collaborative-Optimization-for-Camera-Parameters-and-3D-Gaussians)
* [Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features](#Efficient-LLaMA-3.2-Vision-by-Trimming-Cross-attended-Visual-Features)
* [Preconditioned Additive Gaussian Processes with Fourier Acceleration](#Preconditioned-Additive-Gaussian-Processes-with-Fourier-Acceleration)
* [Spatiotemporal Airy rings wavepackets](#Spatiotemporal-Airy-rings-wavepackets)
* [HawkeyeEfficient Reasoning with Model Collaboration](#HawkeyeEfficient-Reasoning-with-Model-Collaboration)
* [Spatiotemporal Attention Learning Framework for Event-Driven Object Recognition](#Spatiotemporal-Attention-Learning-Framework-for-Event-Driven-Object-Recognition)
* [VNJPTranslate A comprehensive pipeline for Vietnamese-Japanese translation](#VNJPTranslate-A-comprehensive-pipeline-for-Vietnamese-Japanese-translation)
* [Co-design Optimization of Moving Parts for Compliance and Collision Avoidance](#Co-design-Optimization-of-Moving-Parts-for-Compliance-and-Collision-Avoidance)
* [PIM-LLM A High-Throughput Hybrid PIM Architecture for 1-bit LLMs](#PIM-LLM-A-High-Throughput-Hybrid-PIM-Architecture-for-1-bit-LLMs)
* [ERUPT Efficient Rendering with Unposed Patch Transformer](#ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer)
* [SQuat Subspace-orthogonal KV Cache Quantization](#SQuat-Subspace-orthogonal-KV-Cache-Quantization)
* [Pyrometheus Symbolic abstractions for XPU and automatically differentiated computation of combustion kinetics and thermodynamics](#Pyrometheus-Symbolic-abstractions-for-XPU-and-automatically-differentiated-computation-of-combustion-kinetics-and-thermodynamics)
* [Style Quantization for Data-Efficient GAN Training](#Style-Quantization-for-Data-Efficient-GAN-Training)
* [Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality](#Evaluating-and-Designing-Sparse-Autoencoders-by-Approximating-Quasi-Orthogonality)
* [Text2Tracks Prompt-based Music Recommendation via Generative Retrieval](#Text2Tracks-Prompt-based-Music-Recommendation-via-Generative-Retrieval)
* [ReaLM Reliable and Efficient Large Language Model Inference with Statistical Algorithm-Based Fault Tolerance](#ReaLM-Reliable-and-Efficient-Large-Language-Model-Inference-with-Statistical-Algorithm-Based-Fault-Tolerance)
* [Towards Scientific Intelligence A Survey of LLM-based Scientific Agents](#Towards-Scientific-Intelligence-A-Survey-of-LLM-based-Scientific-Agents)
* [CITRAS Covariate-Informed Transformer for Time Series Forecasting](#CITRAS-Covariate-Informed-Transformer-for-Time-Series-Forecasting)
* [Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving](#Rethinking-Key-Value-Cache-Compression-Techniques-for-Large-Language-Model-Serving)
* [DenseFormer Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model](#DenseFormer-Learning-Dense-Depth-Map-from-Sparse-Depth-and-Image-via-Conditional-Diffusion-Model)
* [AirCache Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](#AirCache-Activating-Inter-modal-Relevancy-KV-Cache-Compression-for-Efficient-Large-Vision-Language-Model-Inference)
* [Choco-Q Commute Hamiltonian-based QAOA for Constrained Binary Optimization](#Choco-Q-Commute-Hamiltonian-based-QAOA-for-Constrained-Binary-Optimization)
* [Model Hemorrhage and the Robustness Limits of Large Language Models](#Model-Hemorrhage-and-the-Robustness-Limits-of-Large-Language-Models)
* [An End-to-End Comprehensive Gear Fault Diagnosis Method Based on Multi-Scale Feature-Level Fusion Strategy](#An-End-to-End-Comprehensive-Gear-Fault-Diagnosis-Method-Based-on-Multi-Scale-Feature-Level-Fusion-Strategy)
* [MVDRAM Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration](#MVDRAM-Enabling-GeMV-Execution-in-Unmodified-DRAM-for-Low-Bit-LLM-Acceleration)
* [Adaptive Layer-skipping in Pre-trained LLMs](#Adaptive-Layer-skipping-in-Pre-trained-LLMs)
* [Performing Path Integral Molecular Dynamics Using Artificial Intelligence Enhanced Molecular Simulation Framework](#Performing-Path-Integral-Molecular-Dynamics-Using-Artificial-Intelligence-Enhanced-Molecular-Simulation-Framework)
* [A Conceptual Framework for Human-AI Collaborative Genome Annotation](#A-Conceptual-Framework-for-Human-AI-Collaborative-Genome-Annotation)
* [Uni-Render A Unified Accelerator for Real-Time Rendering Across Diverse Neural Renderers](#Uni-Render-A-Unified-Accelerator-for-Real-Time-Rendering-Across-Diverse-Neural-Renderers)
* [Stochastic analysis of impulsive thrust uncertainties in the CR3BP](#Stochastic-analysis-of-impulsive-thrust-uncertainties-in-the-CR3BP)
* [Using Artificial Neural Networks to Optimize Acceleration Due to Gravity g Measurement in a Compound Pendulum Experiment](#Using-Artificial-Neural-Networks-to-Optimize-Acceleration-Due-to-Gravity-g-Measurement-in-a-Compound-Pendulum-Experiment)
* [Reinforcement Learning-based Token Pruning in Vision Transformers A Markov Game Approach](#Reinforcement-Learning-based-Token-Pruning-in-Vision-Transformers-A-Markov-Game-Approach)
* [Efficient Token Compression for Vision Transformer with Spatial Information Preserved](#Efficient-Token-Compression-for-Vision-Transformer-with-Spatial-Information-Preserved)
* [Filtering with Time-frequency Analysis An Adaptive and Lightweight Model for Sequential Recommender Systems Based on Discrete Wavelet Transform](#Filtering-with-Time-frequency-Analysis-An-Adaptive-and-Lightweight-Model-for-Sequential-Recommender-Systems-Based-on-Discrete-Wavelet-Transform)
* [Representations of knot groups in $\textrm{AGL}_{1}(\mathbb{C})$ and Alexander invariants](#Representations-of-knot-groups-in-$\textrm{AGL}_{1}(\mathbb{C})$-and-Alexander-invariants)
* [Solve sparse PCA problem by employing Hamiltonian system and leapfrog method](#Solve-sparse-PCA-problem-by-employing-Hamiltonian-system-and-leapfrog-method)
* [HiPART Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation](#HiPART-Hierarchical-Pose-AutoRegressive-Transformer-for-Occluded-3D-Human-Pose-Estimation)
* [AI Agents in Engineering Design A Multi-Agent Framework for Aesthetic and Aerodynamic Car Design](#AI-Agents-in-Engineering-Design-A-Multi-Agent-Framework-for-Aesthetic-and-Aerodynamic-Car-Design)
* [Cocktail Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference](#Cocktail-Chunk-Adaptive-Mixed-Precision-Quantization-for-Long-Context-LLM-Inference)
* [SketchVideo Sketch-based Video Generation and Editing](#SketchVideo-Sketch-based-Video-Generation-and-Editing)
* [Vacuum polarization current in presence of intense Sauter field](#Vacuum-polarization-current-in-presence-of-intense-Sauter-field)
* [TRA Better Length Generalisation with Threshold Relative Attention](#TRA-Better-Length-Generalisation-with-Threshold-Relative-Attention)
* [Reasoning-SQL Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL](#Reasoning-SQL-Reinforcement-Learning-with-SQL-Tailored-Partial-Rewards-for-Reasoning-Enhanced-Text-to-SQL)
* [SupertonicTTS Towards Highly Scalable and Efficient Text-to-Speech System](#SupertonicTTS-Towards-Highly-Scalable-and-Efficient-Text-to-Speech-System)
* [RL2Grid Benchmarking Reinforcement Learning in Power Grid Operations](#RL2Grid-Benchmarking-Reinforcement-Learning-in-Power-Grid-Operations)
* [A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support](#A-Retrieval-Augmented-Knowledge-Mining-Method-with-Deep-Thinking-LLMs-for-Biomedical-Research-and-Clinical-Support)
* [Federated Semantic Learning for Privacy-preserving Cross-domain Recommendation](#Federated-Semantic-Learning-for-Privacy-preserving-Cross-domain-Recommendation)
* [DAT Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation](#DAT-Dynamic-Alpha-Tuning-for-Hybrid-Retrieval-in-Retrieval-Augmented-Generation)
* [Multimodal machine learning with large language embedding model for polymer property prediction](#Multimodal-machine-learning-with-large-language-embedding-model-for-polymer-property-prediction)
* [Adaptive Interactive Navigation of Quadruped Robots using Large Language Models](#Adaptive-Interactive-Navigation-of-Quadruped-Robots-using-Large-Language-Models)
* [SSM-RDU A Reconfigurable Dataflow Unit for Long-Sequence State-Space Models](#SSM-RDU-A-Reconfigurable-Dataflow-Unit-for-Long-Sequence-State-Space-Models)
* [Token-Driven GammaTune Adaptive Calibration for Enhanced Speculative Decoding](#Token-Driven-GammaTune-Adaptive-Calibration-for-Enhanced-Speculative-Decoding)
* [Quamba2 A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](#Quamba2-A-Robust-and-Scalable-Post-training-Quantization-Framework-for-Selective-State-Space-Models)
* [A Pilot Study on Tunable Precision Emulation via Automatic BLAS Offloading](#A-Pilot-Study-on-Tunable-Precision-Emulation-via-Automatic-BLAS-Offloading)
* [MCRB for Parameter Estimation from One-Bit Quantized and Oversampled Measurements](#MCRB-for-Parameter-Estimation-from-One-Bit-Quantized-and-Oversampled-Measurements)
* [DiTFastAttnV2 Head-wise Attention Compression for Multi-Modality Diffusion Transformers](#DiTFastAttnV2-Head-wise-Attention-Compression-for-Multi-Modality-Diffusion-Transformers)
* [Unicorn Text-Only Data Synthesis for Vision Language Model Training](#Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training)
* [Learnable cut flow](#Learnable-cut-flow)
* [Movable Antenna Enhanced Downlink Multi-User Integrated Sensing and Communication System](#Movable-Antenna-Enhanced-Downlink-Multi-User-Integrated-Sensing-and-Communication-System)
* [Missing Components in ΛCDM from DESI Y1 BAO Measurements Insights from Redshift Remapping](#Missing-Components-in-ΛCDM-from-DESI-Y1-BAO-Measurements-Insights-from-Redshift-Remapping)
* [STADE Standard Deviation as a Pruning Metric](#STADE-Standard-Deviation-as-a-Pruning-Metric)
* [CoSIL Software Issue Localization via LLM-Driven Code Repository Graph Searching](#CoSIL-Software-Issue-Localization-via-LLM-Driven-Code-Repository-Graph-Searching)
* [GAITGen Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain](#GAITGen-Disentangled-Motion-Pathology-Impaired-Gait-Generative-Model----Bringing-Motion-Generation-to-the-Clinical-Domain)
* [MASCOTS Model-Agnostic Symbolic COunterfactual explanations for Time Series](#MASCOTS-Model-Agnostic-Symbolic-COunterfactual-explanations-for-Time-Series)
* [A Refined Analysis of Massive Activations in LLMs](#A-Refined-Analysis-of-Massive-Activations-in-LLMs)
* [Make Some Noise Towards LLM audio reasoning and generation using sound tokens](#Make-Some-Noise-Towards-LLM-audio-reasoning-and-generation-using-sound-tokens)
* [FLAM Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation](#FLAM-Foundation-Model-Based-Body-Stabilization-for-Humanoid-Locomotion-and-Manipulation)
* [EdgeInfinite A Memory-Efficient Infinite-Context Transformer for Edge Devices](#EdgeInfinite-A-Memory-Efficient-Infinite-Context-Transformer-for-Edge-Devices)
* [Enhanced Charging in Multi-Battery Systems by Nonreciprocity](#Enhanced-Charging-in-Multi-Battery-Systems-by-Nonreciprocity)
* [PharmAgents Building a Virtual Pharma with Large Language Model Agents](#PharmAgents-Building-a-Virtual-Pharma-with-Large-Language-Model-Agents)
* [Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data](#Correlation-Attention-Masked-Temporal-Transformer-for-User-Identity-Linkage-Using-Heterogeneous-Mobility-Data)
* [Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition](#Low-Rank-and-Sparse-Fourier-Structure-in-Recurrent-Networks-Trained-on-Modular-Addition)


## Nemotron-H A Family of Accurate and Efficient Hybrid Mamba-Transformer Models

>Authors: NVIDIA, :, Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl, Marta Stepniewska-Dziubinska, Matthieu Le, Matvei Novikov, Mehrzad Samadi, Michael Andersch, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mike Ranzinger, Mikolaj Blaz, Misha Smelyanskiy, Mohamed Fawzy, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Nima Tajbakhsh, Ning Xu, Oleg Rybakov, Oleksii Kuchaiev, Olivier Delalleau, Osvald Nitski, Parth Chadha, Pasha Shamis, Paulius Micikevicius, Pavlo Molchanov, Peter Dykas, Philipp Fischer, Pierre-Yves Aquilanti, Piotr Bialecki, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi, Rahul Kandu, Ran El-Yaniv, Raviraj Joshi, Roger Waleffe, Ruoxi Zhang, Sabrina Kavanaugh, Sahil Jain, Samuel Kriman, Sangkug Lym, Sanjeev Satheesh, Saurav Muralidharan, Sean Narenthiran, Selvaraj Anandaraj, Seonmyeong Bak, Sergey Kashirsky, Seungju Han, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Sharon Clay, Shelby Thomas, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shyamala Prayaga, Siddhartha Jain, Sirshak Das, Slawek Kierat, Somshubra Majumdar, Song Han, Soumye Singhal, Sriharsha Niverty, Stefania Alborghetti, Suseella Panguluri, Swetha Bhendigeri, Syeda Nahida Akter, Szymon Migacz, Tal Shiri, Terry Kong, Timo Roman, Tomer Ronen, Trisha Saar, Tugrul Konuk, Tuomas Rintamaki, Tyler Poon, Ushnish De, Vahid Noroozi, Varun Singh, Vijay Korthikanti, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenliang Dai, Wonmin Byeon, Xiaowei Ren, Yao Xu, Yejin Choi, Yian Zhang, Ying Lin, Yoshi Suhara, Zhiding Yu, Zhiqi Li, Zhiyu Li, Zhongbo Zhu, Zhuolin Yang, Zijia Chen

>2025-04-04

> http://arxiv.org/abs/2504.03624v1

As inference-time scaling becomes critical for enhanced reasoning
capabilities, it is increasingly becoming important to build models that are
efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid
Mamba-Transformer models designed to reduce inference cost for a given accuracy
level. To achieve this goal, we replace the majority of self-attention layers
in the common Transformer model architecture with Mamba layers that perform
constant computation and require constant memory per generated token. We show
that Nemotron-H models offer either better or on-par accuracy compared to other
similarly-sized state-of-the-art open-sourced Transformer models (e.g.,
Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at
inference. To further increase inference speed and reduce the memory required
at inference time, we created Nemotron-H-47B-Base from the 56B model using a
new compression via **pruning** and distillation technique called MiniPuzzle.
Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%
faster to infer. In addition, we introduce an FP8-based training recipe and
show that it can achieve on par results with BF16-based training. This recipe
is used to train the 56B model. All Nemotron-H models will be released, with
support in Hugging Face, NeMo, and Megatron-LM.


## ZFusion An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving

>Authors: Sheng Yang, Tong Zhan, Shichen Qiao, Jicheng Gong, Qing Yang, Yanfeng Lu, Jian Wang

>2025-04-04

> http://arxiv.org/abs/2504.03438v1

Reliable 3D object perception is essential in autonomous driving. Owing to
its sensing capabilities in all weather conditions, 4D radar has recently
received much attention. However, compared to LiDAR, 4D radar provides much
**sparse**r point cloud. In this paper, we propose a 3D object detection method,
termed ZFusion, which fuses 4D radar and vision modality. As the core of
ZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross
Attention) fuser complements the (**sparse**) radar information and (dense) vision
information, effectively. Specifically, with a feature-pyramid structure, the
FP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal
features at different scales, thus enhancing perception accuracy. In addition,
we utilize the Depth-Context-Split view transformation module due to the
physical properties of 4D radar. Considering that 4D radar has a much lower
cost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.
In typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments
show that with reasonable inference speed, ZFusion achieved the
state-of-the-art mAP (mean average precision) in the region of interest, while
having competitive mAP in the entire area compared to the baseline methods,
which demonstrates performance close to LiDAR and greatly outperforms those
camera-only methods.


## Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning

>Authors: Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak

>2025-04-04

> http://arxiv.org/abs/2504.03380v1

Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning
ability of Large Language Models (LLMs). However, due to the **sparsity** of
rewards in RORL, effective training is highly dependent on the selection of
problems of appropriate difficulty. Although curriculum learning attempts to
address this by adjusting difficulty, it often relies on static schedules, and
even recent online filtering methods lack theoretical grounding and a
systematic understanding of their effectiveness. In this work, we theoretically
and empirically show that curating the batch with the problems that the
training model achieves intermediate accuracy on the fly can maximize the
effectiveness of RORL training, namely balanced online difficulty filtering. We
first derive that the lower bound of the KL divergence between the initial and
the optimal policy can be expressed with the variance of the sampled accuracy.
Building on those insights, we show that balanced filtering can maximize the
lower bound, leading to better performance. Experimental results across five
challenging math reasoning benchmarks show that balanced online filtering
yields an additional 10% in AIME and 4% improvements in average over plain
GRPO. Moreover, further analysis shows the gains in sample efficiency and
training time efficiency, exceeding the maximum reward of plain GRPO within 60%
training time and the volume of the training set.


## Sustainable LLM Inference for Edge AI Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency

>Authors: Erik Johannes Husom, Arda Goknil, Merve Astekin, Lwin Khin Shar, Andre Kåsen, Sagar Sen, Benedikt Andreas Mithassel, Ahmet Soylu

>2025-04-04

> http://arxiv.org/abs/2504.03360v1

Deploying Large Language Models (LLMs) on edge devices presents significant
challenges due to computational constraints, memory limitations, inference
speed, and energy consumption. Model **quantization** has emerged as a key
technique to enable efficient LLM inference by reducing model size and
computational overhead. In this study, we conduct a comprehensive analysis of
28 **quantize**d LLMs from the Ollama library, which applies by default
Post-Training Quantization (PTQ) and weight-only **quantization** techniques,
deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy
efficiency, inference performance, and output accuracy across multiple
**quantization** levels and task types. Models are benchmarked on five standardized
datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and
we employ a high-resolution, hardware-based energy measurement tool to capture
real-world power consumption. Our findings reveal the trade-offs between energy
efficiency, inference speed, and accuracy in different **quantization** settings,
highlighting configurations that optimize LLM deployment for
resource-constrained environments. By integrating hardware-level energy
profiling with LLM benchmarking, this study provides actionable insights for
sustainable AI, bridging a critical gap in existing research on energy-aware
LLM deployment.


## Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale Interval-Valued Time Series Forecasting

>Authors: Wan Tian, Zhongfeng Qin

>2025-04-04

> http://arxiv.org/abs/2504.03322v1

Modeling and forecasting interval-valued time series (ITS) have attracted
considerable attention due to their growing presence in various contexts. To
the best of our knowledge, there have been no efforts to model large-scale ITS.
In this paper, we propose a feature extraction procedure for large-scale ITS,
which involves key steps such as auto-segmentation and clustering, and feature
transfer learning. This procedure can be seamlessly integrated with any
suitable prediction models for forecasting purposes. Specifically, we transform
the automatic segmentation and clustering of ITS into the estimation of
Toeplitz **sparse** precision matrices and assignment set. The
majorization-minimization algorithm is employed to convert this highly
non-convex optimization problem into two subproblems. We derive efficient
dynamic programming and alternating direction method to solve these two
subproblems alternately and establish their convergence properties. By
employing the Joint Recurrence Plot (JRP) to image subsequence and assigning a
class label to each cluster, an image dataset is constructed. Then, an
appropriate neural network is chosen to train on this image dataset and used to
extract features for the next step of forecasting. Real data applications
demonstrate that the proposed method can effectively obtain invariant
representations of the raw data and enhance forecasting performance.


## A Survey of Quantum Transformers Approaches, Advantages, Challenges, and Future Directions

>Authors: Hui Zhang, Qinglin Zhao

>2025-04-04

> http://arxiv.org/abs/2504.03192v1

Quantum Transformer models represent a significant research direction in
quantum machine learning (QML), leveraging the parallelism and entanglement
properties of quantum computing to overcome the computational complexity and
expressive limitations of classical Transformers. Parameterized quantum circuit
(PQC)-based Transformer models are the primary focus of current research,
employing PQCs to achieve varying degrees of quantumization, including
strategies such as Q**KV**-only Quantum mapping, Quantum Pairwise Attention,
Quantum Global Attention, and Quantum-Assisted Acceleration. These approaches
are well-suited to Noisy Intermediate-Scale Quantum (NISQ) devices,
demonstrating potential in small-scale tasks to reduce complexity or enhance
performance. The strength of PQC-based methods lies in their compatibility with
existing quantum hardware, positioning them as the main pathway toward the
practical implementation of quantum Transformers. However, these methods face
challenges such as limited scalability, the absence of standardized testing
benchmarks, and the "barren plateau" problem during training. As a
complementary approach, Quantum Linear Algebra (QLA)-based Transformer models
rely on future fault-tolerant quantum computing, utilizing techniques like
block-encoding and Quantum Singular Value Transformation (QSVT) to achieve
efficient matrix operations and theoretically significant complexity
reductions, though they remain in the theoretical exploration stage. Future
research should prioritize optimizing PQC-based hybrid architectures and
quantum global attention models, establishing unified evaluation frameworks,
and addressing training difficulties, while also exploring hybrid PQC-QLA
approaches to advance the development of quantum Transformers.


## Model Reveals What to Cache Profiling-Based Feature Reuse for Video Diffusion Models

>Authors: Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, Harry Yang

>2025-04-04

> http://arxiv.org/abs/2504.03140v1

Recent advances in diffusion models have demonstrated remarkable capabilities
in video generation. However, the computational intensity remains a significant
challenge for practical applications. While feature caching has been proposed
to reduce the computational burden of diffusion models, existing methods
typically overlook the heterogeneous significance of individual blocks,
resulting in suboptimal reuse and degraded output quality. To this end, we
address this gap by introducing ProfilingDiT, a novel adaptive caching strategy
that explicitly disentangles foreground and background-focused blocks. Through
a systematic analysis of attention distributions in diffusion models, we reveal
a key observation: 1) Most layers exhibit a consistent preference for either
foreground or background regions. 2) Predicted noise shows low inter-step
similarity initially, which stabilizes as denoising progresses. This finding
inspires us to formulate a selective caching strategy that preserves full
computation for dynamic foreground elements while efficiently caching static
background features. Our approach substantially reduces computational overhead
while preserving visual fidelity. Extensive experiments demonstrate that our
framework achieves significant **acceleration** (e.g., 2.01 times speedup for
Wan2.1) while maintaining visual fidelity across comprehensive quality metrics,
establishing a viable method for efficient video generation.


## NuWa Deriving Lightweight Task-Specific Vision Transformers for Edge Devices

>Authors: Ziteng Wei, Qiang He, Bing Li, Feifei Chen, Yun Yang

>2025-04-04

> http://arxiv.org/abs/2504.03118v1

Vision Transformers (ViTs) excel in computer vision tasks but lack
flexibility for edge devices' diverse needs. A vital issue is that ViTs
pre-trained to cover a broad range of tasks are \textit{over-qualified} for
edge devices that usually demand only part of a ViT's knowledge for specific
tasks. Their task-specific accuracy on these edge devices is suboptimal. We
discovered that small ViTs that focus on device-specific tasks can improve
model accuracy and in the meantime, accelerate model inference. This paper
presents NuWa, an approach that derives small ViTs from the base ViT for edge
devices with specific task requirements. NuWa can transfer task-specific
knowledge extracted from the base ViT into small ViTs that fully leverage
constrained resources on edge devices to maximize model accuracy with inference
latency assurance. Experiments with three base ViTs on three public datasets
demonstrate that compared with state-of-the-art solutions, NuWa improves model
accuracy by up to $\text{11.83}\%$ and accelerates model inference by
1.29$\times$ - 2.79$\times$. Code for reproduction is available at
https://anonymous.4open.science/r/Task_Specific-3A5E.


## Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models

>Authors: Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata

>2025-04-03

> http://arxiv.org/abs/2504.02821v1

Sparse Autoencoders (SAEs) have recently been shown to enhance
interpretability and steerability in Large Language Models (LLMs). In this
work, we extend the application of SAEs to Vision-Language Models (VLMs), such
as CLIP, and introduce a comprehensive framework for evaluating monosemanticity
in vision representations. Our experimental results reveal that SAEs trained on
VLMs significantly enhance the monosemanticity of individual neurons while also
exhibiting hierarchical representations that align well with expert-defined
structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that
applying SAEs to intervene on a CLIP vision encoder, directly steer output from
multimodal LLMs (e.g., LLaVA) without any modifications to the underlying
model. These findings emphasize the practicality and efficacy of SAEs as an
unsupervised approach for enhancing both the interpretability and control of
VLMs.


## Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition

>Authors: Vincent Gbouna Zakka, Luis J. Manso, Zhuangzhuang Dai

>2025-04-03

> http://arxiv.org/abs/2504.02778v1

Human activity recognition is increasingly vital for supporting independent
living, particularly for the elderly and those in need of assistance. Domestic
service robots with monitoring capabilities can enhance safety and provide
essential support. Although image-based methods have advanced considerably in
the past decade, their adoption remains limited by concerns over privacy and
sensitivity to low-light or dark conditions. As an alternative, millimetre-wave
(mmWave) radar can produce point cloud data which is privacy-preserving.
However, processing the **sparse** and noisy point clouds remains a long-standing
challenge. While graph-based methods and attention mechanisms show promise,
they predominantly rely on "fixed" kernels; kernels that are applied uniformly
across all neighbourhoods, highlighting the need for adaptive approaches that
can dynamically adjust their kernels to the specific geometry of each local
neighbourhood in point cloud data. To overcome this limitation, we introduce an
adaptive approach within the graph convolutional framework. Instead of a single
shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates
multiple dynamic kernels, each capturing different aspects of the local feature
space. By progressively refining local features while maintaining global
spatial context, our method enables convolution kernels to adapt to varying
local features. Experimental results on benchmark datasets confirm the
effectiveness of our approach, achieving state-of-the-art performance in human
activity recognition. Our source code is made publicly available at:
https://github.com/Gbouna/MAK-GCN


## HyperRAG Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse

>Authors: Yuwei An, Yihua Cheng, Seo Jin Park, Junchen Jiang

>2025-04-03

> http://arxiv.org/abs/2504.02921v1

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing the performance of large language models (LLMs) by integrating
external knowledge into the generation process. A key component of RAG
pipelines is the reranker, which selects the most relevant documents from a
pool of retrieved candidates and significantly improves the quality of the
generated responses. While rerankers refine the selection of retrieved
documents in RAG pipelines, they introduce computational challenges that hinder
high throughput and low latency. To address this problem, we propose HyperRAG,
a system that optimizes the trade-off between quality and efficiency in RAG
pipelines by leveraging **KV**-cache reuse for efficient reranker inference. By
reusing document-side **KV**-cache, HyperRAG achieves both high-quality generation
and system-level efficiency. To fully realize the benefits of **KV**-cache reuse,
HyperRAG incorporates a range of system-level optimizations designed to enhance
efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3
throughput improvement with decoder-only rerankers while also delivering higher
downstream performance compared with traditional RAG service.


## Echoes of the hidden Uncovering coordination beyond network structure

>Authors: Shahar Somin, Tom Cohen, Jeremy Kepner, Alex Pentland

>2025-04-03

> http://arxiv.org/abs/2504.02757v1

The study of connectivity and coordination has drawn increasing attention in
recent decades due to their central role in driving markets, shaping societal
dynamics, and influencing biological systems. Traditionally, observable
connections, such as phone calls, financial transactions, or social media
connections, have been used to infer coordination and connectivity. However,
incomplete, encrypted, or fragmented data, alongside the ubiquity of
communication platforms and deliberate obfuscation, often leave many real-world
connections hidden. In this study, we demonstrate that coordinating individuals
exhibit shared bursty activity patterns, enabling their detection even when
observable links between them are **sparse** or entirely absent. We further propose
a generative model based on the network of networks formalism to account for
the mechanisms driving this collaborative burstiness, attributing it to shock
propagation across networks rather than isolated individual behavior. Model
simulations demonstrate that when observable connection density is below 70\%,
burstiness significantly improves coordination detection compared to
state-of-the-art temporal and structural methods. This work provides a new
perspective on community and coordination dynamics, advancing both theoretical
understanding and practical detection. By laying the foundation for identifying
hidden connections beyond observable network structures, it enables detection
across different platforms, alongside enhancing system behavior understanding,
informed decision-making, and risk mitigation.


## HQViT Hybrid Quantum Vision Transformer for Image Classification

>Authors: Hui Zhang, Qinglin Zhao, Mengchu Zhou, Li Feng

>2025-04-03

> http://arxiv.org/abs/2504.02730v1

Transformer-based architectures have revolutionized the landscape of deep
learning. In computer vision domain, Vision Transformer demonstrates remarkable
performance on par with or even surpassing that of convolutional neural
networks. However, the quadratic computational complexity of its self-attention
mechanism poses challenges for classical computing, making model training with
high-dimensional input data, e.g., images, particularly expensive. To address
such limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that
leverages the principles of quantum computing to accelerate model training
while enhancing model performance. HQViT introduces whole-image processing with
amplitude encoding to better preserve global image information without
additional positional encoding. By leveraging quantum computation on the most
critical steps and selectively handling other components in a classical way, we
lower the cost of quantum resources for HQViT. The qubit requirement is
minimized to $O(log_2N)$ and the number of parameterized quantum gates is only
$O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum
devices. By offloading the computationally intensive attention coefficient
matrix calculation to the quantum framework, HQViT reduces the classical
computational load by $O(T^2d)$. Extensive experiments across various computer
vision datasets demonstrate that HQViT outperforms existing models, achieving a
maximum improvement of up to $10.9\%$ (on the MNIST 10-classification task)
over the state of the art. This work highlights the great potential to combine
quantum and classical computing to cope with complex image classification
tasks.


## ERPO Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization

>Authors: Kehua Feng, Keyan Ding, Jing Yu, Menghan Li, Yuhao Wang, Tong Xu, Xinda Wang, Qiang Zhang, Huajun Chen

>2025-04-03

> http://arxiv.org/abs/2504.02725v1

Recent advancements in large language models (LLMs) have accelerated progress
toward artificial general intelligence, yet their potential to generate harmful
content poses critical safety challenges. Existing alignment methods often
struggle to cover diverse safety scenarios and remain vulnerable to adversarial
attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization
(ERPO), a novel safety alignment framework that equips LLMs with explicit
preemptive reasoning through Chain-of-Thought and provides clear evidence for
safety judgments by embedding predefined safety rules. Specifically, our
approach consists of three stages: first, equipping the model with Ex-Ante
reasoning through supervised fine-tuning (SFT) using a constructed reasoning
module; second, enhancing safety, usefulness, and efficiency via Direct
Preference Optimization (DPO); and third, mitigating inference latency with a
length-controlled iterative preference optimization strategy. Experiments on
multiple open-source LLMs demonstrate that ERPO significantly enhances safety
performance while maintaining response efficiency.


## GPTQv2 Efficient Finetuning-Free Quantization for Asymmetric Calibration

>Authors: Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda

>2025-04-03

> http://arxiv.org/abs/2504.02692v2

We introduce GPTQv2, a novel finetuning-free **quantization** method for
compressing large-scale transformer architectures. Unlike the previous GPTQ
method, which independently calibrates each layer, we always match the
**quantize**d layer's output to the exact output in the full-precision model,
resulting in a scheme that we call asymmetric calibration. Such a scheme can
effectively reduce the **quantization** error accumulated in previous layers. We
analyze this problem using optimal brain compression to derive a close-formed
solution. The new solution explicitly minimizes the **quantization** error as well
as the accumulated asymmetry error. Furthermore, we utilize various techniques
to parallelize the solution calculation, including channel parallelization,
neuron decomposition, and Cholesky reformulation for matrix fusion. As a
result, GPTQv2 is easy to implement, simply using 20 more lines of code than
GPTQ but improving its performance under **low-bit** **quantization**. Remarkably, on a
single GPU, we **quantize** a 405B language transformer as well as EVA-02 the rank
first vision transformer that achieves 90% pretraining Imagenet accuracy. Code
is available at github.com/Intelligent-Computing-Lab-Yale/GPTQv2.


## RoSMM A Robust and Secure Multi-Modal Watermarking Framework for Diffusion Models

>Authors: ZhongLi Fang, Yu Xie, Ping Chen

>2025-04-03

> http://arxiv.org/abs/2504.02640v1

Current image watermarking technologies are predominantly categorized into
text watermarking techniques and image steganography; however, few methods can
simultaneously handle text and image-based watermark data, which limits their
applicability in complex digital environments. This paper introduces an
innovative multi-modal watermarking approach, drawing on the concept of vector
discretization in encoder-based vector **quantization**. By constructing adjacency
matrices, the proposed method enables the transformation of text watermarks
into robust image-based representations, providing a novel multi-modal
watermarking paradigm for image generation applications. Additionally, this
study presents a newly designed image restoration module to mitigate image
degradation caused by transmission losses and various noise interferences,
thereby ensuring the reliability and integrity of the watermark. Experimental
results validate the robustness of the method under multiple noise attacks,
providing a secure, scalable, and efficient solution for digital image
copyright protection.


## APHQ-ViT Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers

>Authors: Zhuguanyu Wu, Jiayi Zhang, Jiaxin Chen, Jinyang Guo, Di Huang, Yunhong Wang

>2025-04-03

> http://arxiv.org/abs/2504.02508v1

Vision Transformers (ViTs) have become one of the most commonly used
backbones for vision tasks. Despite their remarkable performance, they often
suffer significant accuracy drops when **quantize**d for practical deployment,
particularly by post-training **quantization** (PTQ) under ultra-low bits.
Recently, reconstruction-based PTQ methods have shown promising performance in
quantizing Convolutional Neural Networks (CNNs). However, they fail when
applied to ViTs, primarily due to the inaccurate estimation of output
importance and the substantial accuracy degradation in quantizing post-GELU
activations. To address these issues, we propose \textbf{APHQ-ViT}, a novel PTQ
approach based on importance estimation with Average Perturbation Hessian
(APH). Specifically, we first thoroughly analyze the current approximation
approaches with Hessian loss, and propose an improved average perturbation
Hessian loss. To deal with the **quantization** of the post-GELU activations, we
design an MLP Reconstruction (MR) method by replacing the GELU function in MLP
with ReLU and reconstructing it by the APH loss on a small unlabeled
calibration set. Extensive experiments demonstrate that APHQ-ViT using linear
**quantize**rs outperforms existing PTQ methods by substantial margins in 3-bit and
4-bit across different vision tasks. The source code is available at
https://github.com/GoatWu/APHQ-ViT.


## Cognitive Memory in Large Language Models

>Authors: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu

>2025-04-03

> http://arxiv.org/abs/2504.02441v1

This paper examines memory mechanisms in Large Language Models (LLMs),
emphasizing their importance for context-rich responses, reduced
hallucinations, and improved efficiency. It categorizes memory into sensory,
short-term, and long-term, with sensory memory corresponding to input prompts,
short-term memory processing immediate context, and long-term memory
implemented via external databases or structures. The text-based memory section
covers acquisition (selection and summarization), management (updating,
accessing, storing, and resolving conflicts), and utilization (full-text
search, SQL queries, semantic search). The **KV** cache-based memory section
discusses selection methods (regularity-based summarization, score-based
approaches, special token embeddings) and compression techniques (low-rank
compression, **KV** merging, multimodal compression), along with management
strategies like offloading and shared attention mechanisms. Parameter-based
memory methods (LoRA, TTT, MoE) transform memories into model parameters to
enhance efficiency, while hidden-state-based memory approaches (chunk
mechanisms, recurrent transformers, Mamba model) improve long-text processing
by combining RNN hidden states with current methods. Overall, the paper offers
a comprehensive analysis of LLM memory mechanisms, highlighting their
significance and future research directions.


## Inducing contractions of the mother of all continued fractions

>Authors: Karma Dajani, Cor Kraaikamp, Slade Sanderson

>2025-04-03

> http://arxiv.org/abs/2504.02350v1

We introduce a new, large class of continued fraction algorithms producing
what are called contracted Farey expansions. These algorithms are defined by
coupling two **acceleration** techniques -- induced transformations and contraction
-- in the setting of Shunji Ito's natural extension of the Farey tent map,
which generates `slow' continued fraction expansions. In addition to defining
new algorithms, we also realise several existing continued fraction algorithms
in our unifying setting. In particular, we find regular continued fractions,
the second-named author's $S$-expansions, and Nakada's parameterised family of
$\alpha$-continued fractions for all $0<\alpha\le 1$ as examples of contracted
Farey expansions. Moreover, we give a new description of a planar natural
extension for each of the $\alpha$-continued fraction transformations as an
explicit induced transformation of Ito's natural extension.


## Toward General and Robust LLM-enhanced Text-attributed Graph Learning

>Authors: Zihao Zhang, Xunkai Li, Rong-Hua Li, Bing Zhou, Zhenjun Li, Guoren Wang

>2025-04-03

> http://arxiv.org/abs/2504.02343v1

Recent advancements in Large Language Models (LLMs) and the proliferation of
Text-Attributed Graphs (TAGs) across various domains have positioned
LLM-enhanced TAG learning as a critical research area. By utilizing rich graph
descriptions, this paradigm leverages LLMs to generate high-quality embeddings,
thereby enhancing the representational capacity of Graph Neural Networks
(GNNs). However, the field faces significant challenges: (1) the absence of a
unified framework to systematize the diverse optimization perspectives arising
from the complex interactions between LLMs and GNNs, and (2) the lack of a
robust method capable of handling real-world TAGs, which often suffer from
texts and edge **sparsity**, leading to suboptimal performance.
  To address these challenges, we propose UltraTAG, a unified pipeline for
LLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and
domain-adaptive framework that not only organizes existing methodologies but
also paves the way for future advancements in the field. Building on this
framework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed
to tackle the inherent **sparsity** issues in real-world TAGs. UltraTAG-S employs
LLM-based text propagation and text augmentation to mitigate text **sparsity**,
while leveraging LLM-augmented node selection techniques based on PageRank and
edge reconfiguration strategies to address edge **sparsity**. Our extensive
experiments demonstrate that UltraTAG-S significantly outperforms existing
baselines, achieving improvements of 2.12\% and 17.47\% in ideal and **sparse**
settings, respectively. Moreover, as the data **sparsity** ratio increases, the
performance improvement of UltraTAG-S also rises, which underscores the
effectiveness and robustness of UltraTAG-S.


## Riemannian Optimization for Sparse Tensor CCA

>Authors: Yanjiao Zhu, Xianchao Xiu, Qilin Li

>2025-04-03

> http://arxiv.org/abs/2504.02339v1

Tensor canonical correlation analysis (TCCA) has received significant
attention due to its ability to effectively preserve the geometric structure of
high-order data. However, existing methods generally rely on tensor
decomposition techniques with high computational complexity, which severely
limits their application in large-scale datasets. In this paper, a modified
method, TCCA-L, is proposed, which integrates **sparse** regularization and
Laplacian regularization. An alternating manifold proximal gradient algorithm
is designed based on Riemannian manifold theory. The algorithm avoids the
traditional tensor decomposition and combines with the semi-smooth Newton
algorithm to solve the subproblem, thus significantly improving the
computational efficiency. Furthermore, the global convergence of the sequence
generated by the algorithm is established, providing a solid theoretical
foundation for its convergence. Numerical experiments demonstrate that TCCA-L
outperforms traditional methods in both classification accuracy and running
time.


## LearNAT Learning NL2SQL with AST-guided Task Decomposition for Large Language Models

>Authors: Weibin Liao, Xin Gao, Tianyu Jia, Rihong Qiu, Yifan Zhu, Yang Lin, Xu Chu, Junfeng Zhao, Yasha Wang

>2025-04-03

> http://arxiv.org/abs/2504.02327v1

Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling
seamless interaction with databases. Recent advancements in Large Language
Models (LLMs) have demonstrated remarkable performance in this domain. However,
existing NL2SQL methods predominantly rely on closed-source LLMs leveraging
prompt engineering, while open-source models typically require fine-tuning to
acquire domain-specific knowledge. Despite these efforts, open-source LLMs
struggle with complex NL2SQL tasks due to the indirect expression of user query
objectives and the semantic gap between user queries and database schemas.
Inspired by the application of reinforcement learning in mathematical
problem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT
(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that
improves the performance of open-source LLMs on complex NL2SQL tasks through
task decomposition and reinforcement learning. LearNAT introduces three key
components: (1) a Decomposition Synthesis Procedure that leverages Abstract
Syntax Trees (ASTs) to guide efficient search and **pruning** strategies for task
decomposition, (2) Margin-aware Reinforcement Learning, which employs
fine-grained step-level optimization via DPO with AST margins, and (3) Adaptive
Demonstration Reasoning, a mechanism for dynamically selecting relevant
examples to enhance decomposition capabilities. Extensive experiments on two
benchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a
7B-parameter open-source LLM to achieve performance comparable to GPT-4, while
offering improved efficiency and accessibility.


## MegaScale-Infer Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism

>Authors: Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A. Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, Jianzhe Xiao, Xinyi Zhang, Lingjun Liu, Haibin Lin, Li-Wen Chang, Jianxi Ye, Xiao Yu, Xuanzhe Liu, Xin Jin, Xin Liu

>2025-04-03

> http://arxiv.org/abs/2504.02263v1

Mixture-of-Experts (MoE) showcases tremendous potential to scale large
language models (LLMs) with enhanced performance and reduced computational
complexity. However, its **sparse**ly activated architecture shifts feed-forward
networks (FFNs) from being compute-intensive to memory-intensive during
inference, leading to substantially lower GPU utilization and increased
operational costs. We present MegaScale-Infer, an efficient and cost-effective
system for serving large-scale MoE models. MegaScale-Infer disaggregates
attention and FFN modules within each model layer, enabling independent
scaling, tailored parallelism strategies, and heterogeneous deployment for both
modules. To fully exploit disaggregation in the presence of MoE's **sparsity**,
MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a
request batch into micro-batches and shuttles them between attention and FFNs
for inference. Combined with distinct model parallelism for each module,
MegaScale-Infer effectively hides communication overhead and maximizes GPU
utilization. To adapt to disaggregated attention and FFN modules and minimize
data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a
high-performance M2N communication library that eliminates unnecessary
GPU-to-CPU data copies, group initialization overhead, and GPU synchronization.
Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher
per-GPU throughput than state-of-the-art solutions.


## WonderTurbo Generating Interactive 3D World in 0.72 Seconds

>Authors: Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, Wenjun Mei

>2025-04-03

> http://arxiv.org/abs/2504.02261v1

Interactive 3D generation is gaining momentum and capturing extensive
attention for its potential to create immersive virtual experiences. However, a
critical challenge in current 3D generation technologies lies in achieving
real-time interactivity. To address this issue, we introduce WonderTurbo, the
first real-time interactive 3D scene generation framework capable of generating
novel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo
accelerates both geometric and appearance modeling in 3D scene generation. In
terms of geometry, we propose StepSplat, an innovative method that constructs
efficient 3D geometric representations through dynamic updates, each taking
only 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth
completion module that provides consistent depth input for StepSplat, further
enhancing geometric accuracy. For appearance modeling, we develop FastPaint, a
2-steps diffusion model tailored for instant inpainting, which focuses on
maintaining spatial appearance consistency. Experimental results demonstrate
that WonderTurbo achieves a remarkable 15X speedup compared to baseline
methods, while preserving excellent spatial consistency and delivering
high-quality output.


## LLM-Augmented Graph Neural Recommenders Integrating User Reviews

>Authors: Hiroki Kanezashi, Toyotaro Suzumura, Cade Reid, Md Mostafizur Rahman, Yu Hirate

>2025-04-03

> http://arxiv.org/abs/2504.02195v1

Recommender systems increasingly aim to combine signals from both user
reviews and purchase (or other interaction) behaviors. While user-written
comments provide explicit insights about preferences, merging these textual
representations from large language models (LLMs) with graph-based embeddings
of user actions remains a challenging task. In this work, we propose a
framework that employs both a Graph Neural Network (GNN)-based model and an LLM
to produce review-aware representations, preserving review semantics while
mitigating textual noise. Our approach utilizes a hybrid objective that
balances user-item interactions against text-derived features, ensuring that
user's both behavioral and linguistic signals are effectively captured. We
evaluate this method on multiple datasets from diverse application domains,
demonstrating consistent improvements over a baseline GNN-based recommender
model. Notably, our model achieves significant gains in recommendation accuracy
when review data is **sparse** or unevenly distributed. These findings highlight
the importance of integrating LLM-driven textual feedback with GNN-derived user
behavioral patterns to develop robust, context-aware recommender systems.


## MDP Multidimensional Vision Model Pruning with Latency Constraint

>Authors: Xinglong Sun, Barath Lakshmanan, Maying Shen, Shiyi Lan, Jingde Chen, Jose M. Alvarez

>2025-04-02

> http://arxiv.org/abs/2504.02168v1

Current structural **pruning** methods face two significant limitations: (i) they
often limit **pruning** to finer-grained levels like channels, making aggressive
parameter reduction challenging, and (ii) they focus heavily on parameter and
FLOP reduction, with existing latency-aware methods frequently relying on
simplistic, suboptimal linear models that fail to generalize well to
transformers, where multiple interacting dimensions impact latency. In this
paper, we address both limitations by introducing Multi-Dimensional Pruning
(MDP), a novel paradigm that jointly optimizes across a variety of **pruning**
granularities-including channels, query, key, heads, embeddings, and blocks.
MDP employs an advanced latency modeling technique to accurately capture
latency variations across all prunable dimensions, achieving an optimal balance
between latency and accuracy. By reformulating **pruning** as a Mixed-Integer
Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned
structure across all prunable dimensions while respecting latency constraints.
This versatile framework supports both CNNs and transformers. Extensive
experiments demonstrate that MDP significantly outperforms previous methods,
especially at high **pruning** ratios. On ImageNet, MDP achieves a 28% speed
increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for
ResNet50 **pruning**. Against the latest transformer **pruning** method, Isomorphic,
MDP delivers an additional 37% **acceleration** with a +0.7 Top-1 accuracy
improvement.


## LakeVisage Towards Scalable, Flexible and Interactive Visualization Recommendation for Data Discovery over Data Lakes

>Authors: Yihao Hu, Jin Wang, Sajjadur Rahman

>2025-04-02

> http://arxiv.org/abs/2504.02150v1

Data discovery from data lakes is an essential application in modern data
science. While many previous studies focused on improving the efficiency and
effectiveness of data discovery, little attention has been paid to the
usability of such applications. In particular, exploring data discovery results
can be cumbersome due to the cognitive load involved in understanding raw
tabular results and identifying insights to draw conclusions. To address this
challenge, we introduce a new problem -- visualization recommendation for data
discovery over data lakes -- which aims at automatically identifying
visualizations that highlight relevant or desired trends in the results
returned by data discovery engines. We propose LakeVisage, an end-to-end
framework as the first solution to this problem. Given a data lake, a data
discovery engine, and a user-specified query table, LakeVisage intelligently
explores the space of visualizations and recommends the most useful and
``interesting'' visualization plans. To this end, we developed (i) approaches
to smartly construct the candidate visualization plans from the results of the
data discovery engine and (ii) effective **pruning** strategies to filter out less
interesting plans so as to accelerate the visual analysis. Experimental results
on real data lakes show that our proposed techniques can lead to an order of
magnitude speedup in visualization recommendation. We also conduct a
comprehensive user study to demonstrate that LakeVisage offers convenience to
users in real data analysis applications by enabling them seamlessly get
started with the tasks and performing explorations flexibly.


## LL4G Self-Supervised Dynamic Optimization for Graph-Based Personality Detection

>Authors: Lingzhi Shen, Yunfei Long, Xiaohao Cai, Guanming Chen, Yuhan Wang, Imran Razzak, Shoaib Jameel

>2025-04-02

> http://arxiv.org/abs/2504.02146v1

Graph-based personality detection constructs graph structures from textual
data, particularly social media posts. Current methods often struggle with
**sparse** or noisy data and rely on static graphs, limiting their ability to
capture dynamic changes between nodes and relationships. This paper introduces
LL4G, a self-supervised framework leveraging large language models (LLMs) to
optimize graph neural networks (GNNs). LLMs extract rich semantic features to
generate node representations and to infer explicit and implicit relationships.
The graph structure adaptively adds nodes and edges based on input data,
continuously optimizing itself. The GNN then uses these optimized
representations for joint training on node reconstruction, edge prediction, and
contrastive learning tasks. This integration of semantic and structural
information generates robust personality profiles. Experimental results on
Kaggle and Pandora datasets show LL4G outperforms state-of-the-art models.


## LLMPi Optimizing LLMs for High-Throughput on Raspberry Pi

>Authors: Mahsa Ardakani, Jinendra Malekar, Ramtin Zand

>2025-04-02

> http://arxiv.org/abs/2504.02118v1

Deploying Large Language Models (LLMs) on resource-constrained edge devices
like the Raspberry Pi presents challenges in computational efficiency, power
consumption, and response latency. This paper explores **quantization**-based
optimization techniques to enable high-throughput, energy-efficient execution
of LLMs on low-power embedded systems. Our approach leverages k-**quantization**, a
Post-Training Quantization (PTQ) method designed for different bit-widths,
enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight **quantization**.
Additionally, we employ ternary **quantization** using Quantization-Aware Training
(QAT) for BitNet models, allowing for more effective adaptation to lower-bit
representations while preserving accuracy.
  Our findings highlight the potential of **quantize**d LLMs for real-time
conversational AI on edge devices, paving the way for low-power,
high-efficiency AI deployment in mobile and embedded applications. This study
demonstrates that aggressive **quantization** strategies can significantly reduce
energy consumption while maintaining inference quality, making LLMs practical
for resource-limited environments.


## Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis

>Authors: Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar

>2025-04-02

> http://arxiv.org/abs/2504.01960v1

Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance
Fields (NeRF) have achieved impressive results in real-time 3D reconstruction
and novel view synthesis. However, these methods struggle in large-scale,
unconstrained environments where **sparse** and uneven input coverage, transient
occlusions, appearance variability, and inconsistent camera settings lead to
degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a
multi-view diffusion model to address these limitations. By generating
pseudo-observations conditioned on multi-view inputs, our method transforms
under-constrained 3D reconstruction problems into well-posed ones, enabling
robust optimization even with **sparse** data. GS-Diff further integrates several
enhancements, including appearance embedding, monocular depth priors, dynamic
object modeling, anisotropy regularization, and advanced rasterization
techniques, to tackle geometric and photometric challenges in real-world
settings. Experiments on four benchmarks demonstrate that GS-Diff consistently
outperforms state-of-the-art baselines by significant margins.


## Laboratory evaluation of a wearable instrumented headband for rotational head kinematics measurement

>Authors: Anu Tripathi, Yang Wan, Sushant Malave, Sheila Turcsanyi, Alice Lux Fawzi, Alison Brooks, Haneesh Kesari, Traci Snedden, Peter Ferrazzano, Christian Franck, Rika Carlsen

>2025-04-02

> http://arxiv.org/abs/2504.01939v1

Mild traumatic brain injuries (mTBI) are a highly prevalent condition with
heterogeneous outcomes between individuals. A key factor governing brain tissue
deformation and the risk of mTBI is the rotational kinematics of the head.
Instrumented mouthguards are a widely accepted method for measuring rotational
head motions, owing to their robust sensor-skull coupling. However, wearing
mouthguards is not feasible in all situations, especially for long-term data
collection. Therefore, alternative wearable devices are needed. In this study,
we present an improved design and data processing scheme for an instrumented
headband. Our instrumented headband utilizes an array of inertial measurement
units (IMUs) and a new data-processing scheme based on continuous wavelet
transforms to address sources of error in the IMU measurements. The headband
performance was evaluated in the laboratory on an anthropomorphic test device,
which was impacted with a soccer ball to replicate soccer heading. When
comparing the measured peak rotational velocities (PRV) and peak rotational
**acceleration**s (PRA) between the reference sensors and the headband for impacts
to the front of the head, the correlation coefficients (r) were 0.80 and 0.63,
and the normalized root mean square error (NRMSE) values were 0.20 and 0.28,
respectively. However, when considering all impact locations, r dropped to 0.42
and 0.34 and NRMSE increased to 0.5 and 0.41 for PRV and PRA, respectively.
This new instrumented headband improves upon previous headband designs in
reconstructing the rotational head kinematics resulting from frontal soccer
ball impacts, providing a potential alternative to instrumented mouthguards.


## Review, Refine, Repeat Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection

>Authors: Souradip Chakraborty, Mohammadreza Pourreza, Ruoxi Sun, Yiwen Song, Nino Scherrer, Jindong Gu, Furong Huang, Amrit Singh Bedi, Ahmad Beirami, Hamid Palangi, Tomas Pfister

>2025-04-02

> http://arxiv.org/abs/2504.01931v1

While AI agents have shown remarkable performance at various tasks, they
still struggle with complex multi-modal applications, structured generation and
strategic planning. Improvements via standard fine-tuning is often impractical,
as solving agentic tasks usually relies on black box API access without control
over model parameters. Inference-time methods such as Best-of-N (BON) sampling
offer a simple yet effective alternative to improve performance. However, BON
lacks iterative feedback integration mechanism. Hence, we propose Iterative
Agent Decoding (IAD) which combines iterative refinement with dynamic candidate
evaluation and selection guided by a verifier. IAD differs in how feedback is
designed and integrated, specifically optimized to extract maximal signal from
reward scores. We conduct a detailed comparison of baselines across key metrics
on Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms
baselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and
without LLM judges) and 8--10% gains on Webshop across multiple metrics. To
better understand the source of IAD's gains, we perform controlled experiments
to disentangle the effect of adaptive feedback from stochastic sampling, and
find that IAD's improvements are primarily driven by verifier-guided
refinement, not merely sampling diversity. We also show that both IAD and BON
exhibit inference-time scaling with increased compute when guided by an optimal
verifier. Our analysis highlights the critical role of verifier quality in
effective inference-time optimization and examines the impact of noisy and
**sparse** rewards on scaling behavior. Together, these findings offer key insights
into the trade-offs and principles of effective inference-time optimization.


## Architect Your Landscape Approach (AYLA) for Optimizations in Deep Learning

>Authors: Ben Keslaki

>2025-04-02

> http://arxiv.org/abs/2504.01875v1

Stochastic Gradient Descent (SGD) and its variants, such as ADAM, are
foundational to deep learning optimization, adjusting model parameters using
fixed or adaptive learning rates based on loss function gradients. However,
these methods often face challenges in balancing adaptability and efficiency in
non-convex, high-dimensional settings. This paper introduces AYLA, a novel
optimization technique that enhances training dynamics through loss function
transformations. By applying a tunable power-law transformation, AYLA preserves
critical points while scaling loss values to amplify gradient sensitivity,
accelerating convergence. We further propose a dynamic (effective) learning
rate that adapts to the transformed loss, improving optimization efficiency.
Empirical tests on finding minimum of a synthetic non-convex polynomial, a
non-convex curve-fitting dataset, and digit classification (MNIST) demonstrate
that AYLA surpasses SGD and ADAM in convergence speed and stability. This
approach redefines the loss landscape for better optimization outcomes,
offering a promising advancement for deep neural networks and can be applied to
any optimization method and potentially improve the performance of it.


## Quattro Transformer-Accelerated Iterative Linear Quadratic Regulator Framework for Fast Trajectory Optimization

>Authors: Yue Wang, Haoyu Wang, Zhaoxing Li

>2025-04-02

> http://arxiv.org/abs/2504.01806v2

Real-time optimal control remains a fundamental challenge in robotics,
especially for nonlinear systems with stringent performance requirements. As
one of the representative trajectory optimization algorithms, the iterative
Linear Quadratic Regulator (iLQR) faces limitations due to their inherently
sequential computational nature, which restricts the efficiency and
applicability of real-time control for robotic systems. While existing parallel
implementations aim to overcome the above limitations, they typically demand
additional computational iterations and high-performance hardware, leading to
only modest practical improvements. In this paper, we introduce Quattro, a
transformer-accelerated iLQR framework employing an algorithm-hardware
co-design strategy to predict intermediate feedback and feedforward matrices.
It facilitates effective parallel computations on resource-constrained devices
without sacrificing accuracy. Experiments on cart-pole and quadrotor systems
show an algorithm-level **acceleration** of up to 5.3$\times$ and 27$\times$ per
iteration, respectively. When integrated into a Model Predictive Control (MPC)
framework, Quattro achieves overall speedups of 2.8$\times$ for the cart-pole
and 17.8$\times$ for the quadrotor compared to the one that applies traditional
iLQR. Transformer inference is deployed on FPGA to maximize performance,
achieving further up to 20.8$\times$ speedup over prevalent embedded CPUs with
over 11$\times$ power reduction than GPU and low hardware resource overhead.


## UniViTAR Unified Vision Transformer with Native Resolution

>Authors: Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma

>2025-04-02

> http://arxiv.org/abs/2504.01792v1

Conventional Vision Transformer simplifies visual modeling by standardizing
input resolutions, often disregarding the variability of natural visual data
and compromising spatial-contextual fidelity. While preliminary explorations
have superficially investigated native resolution modeling, existing approaches
still lack systematic analysis from a visual representation perspective. To
bridge this gap, we introduce UniViTAR, a family of homogeneous vision
foundation models tailored for unified visual modality and native resolution
scenario in the era of multimodal. Our framework first conducts architectural
upgrades to the vanilla paradigm by integrating multiple advanced components.
Building upon these improvements, a progressive training paradigm is
introduced, which strategically combines two core mechanisms: (1) resolution
curriculum learning, transitioning from fixed-resolution pretraining to native
resolution tuning, thereby leveraging ViT's inherent adaptability to
variable-length sequences, and (2) visual modality adaptation via inter-batch
image-video switching, which balances computational efficiency with enhanced
temporal reasoning. In parallel, a hybrid training framework further synergizes
sigmoid-based contrastive loss with feature distillation from a frozen teacher
model, thereby accelerating early-stage convergence. Finally, trained
exclusively on public datasets, externsive experiments across multiple model
scales from 0.3B to 1B demonstrate its effectiveness.


## Token Pruning in Audio Transformers Optimizing Performance and Decoding Patch Importance

>Authors: Taehan Lee, Hyukjun Lee

>2025-04-02

> http://arxiv.org/abs/2504.01690v1

Vision Transformers (ViTs) have achieved state-of-the-art performance across
various computer vision tasks, but their high computational cost remains a
challenge. Token **pruning** has been proposed to reduce this cost by selectively
removing less important tokens. While effective in vision tasks by discarding
non-object regions, applying this technique to audio tasks presents unique
challenges, as distinguishing relevant from irrelevant regions in
time-frequency representations is less straightforward. In this study, for the
first time, we applied token **pruning** to ViT-based audio classification models
using Mel-spectrograms and analyzed the trade-offs between model performance
and computational cost: TopK token **pruning** can reduce MAC operations of
AudioMAE and AST by 30-40%, with less than a 1% drop in classification
accuracy. Our analysis reveals that while high-intensity tokens contribute
significantly to model accuracy, low-intensity tokens remain important. In
particular, they play a more critical role in general audio classification
tasks than in speech-specific tasks.


## Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams the Case of Luxembourgish

>Authors: Cedric Lothritz, Jordi Cabot

>2025-04-02

> http://arxiv.org/abs/2504.01667v2

Large Language Models (LLMs) have become an increasingly important tool in
research and society at large. While LLMs are regularly used all over the world
by experts and lay-people alike, they are predominantly developed with
English-speaking users in mind, performing well in English and other
wide-spread languages while less-resourced languages such as Luxembourgish are
seen as a lower priority. This lack of attention is also reflected in the
**sparsity** of available evaluation tools and datasets. In this study, we
investigate the viability of language proficiency exams as such evaluation
tools for the Luxembourgish language. We find that large models such as
ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller
models show weak performances. We also find that the performances in such
language exams can be used to predict performances in other NLP tasks.


## Generative Retrieval and Alignment Model A New Paradigm for E-commerce Retrieval

>Authors: Ming Pang, Chunyuan Yuan, Xiaoyu He, Zheng Fang, Donghao Xie, Fanyi Qu, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao

>2025-04-02

> http://arxiv.org/abs/2504.01403v1

Traditional **sparse** and dense retrieval methods struggle to leverage general
world knowledge and often fail to capture the nuanced features of queries and
products. With the advent of large language models (LLMs), industrial search
systems have started to employ LLMs to generate identifiers for product
retrieval. Commonly used identifiers include (1) static/semantic IDs and (2)
product term sets. The first approach requires creating a product ID system
from scratch, missing out on the world knowledge embedded within LLMs. While
the second approach leverages this general knowledge, the significant
difference in word distribution between queries and products means that
product-based identifiers often do not align well with user search queries,
leading to missed product recalls. Furthermore, when queries contain numerous
attributes, these algorithms generate a large number of identifiers, making it
difficult to assess their quality, which results in low overall recall
efficiency.
  To address these challenges, this paper introduces a novel e-commerce
retrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM
employs joint training on text information from both queries and products to
generate shared text identifier codes, effectively bridging the gap between
queries and products. This approach not only enhances the connection between
queries and products but also improves inference efficiency. The model uses a
co-alignment strategy to generate codes optimized for maximizing retrieval
efficiency. Additionally, it introduces a query-product scoring mechanism to
compare product values across different codes, further boosting retrieval
efficiency. Extensive offline and online A/B testing demonstrates that GRAM
significantly outperforms traditional models and the latest generative
retrieval models, confirming its effectiveness and practicality.


## Attention Mamba Time Series Modeling with Adaptive Pooling Acceleration and Receptive Field Enhancements

>Authors: Sijie Xiong, Shuqing Liu, Cheng Tang, Fumiya Okubo, Haoling Xiong, Atsushi Shimada

>2025-04-02

> http://arxiv.org/abs/2504.02013v1

"This work has been submitted to the lEEE for possible publication. Copyright
may be transferred without noticeafter which this version may no longer be
accessible." Time series modeling serves as the cornerstone of real-world
applications, such as weather forecasting and transportation management.
Recently, Mamba has become a promising model that combines near-linear
computational complexity with high prediction accuracy in time series modeling,
while facing challenges such as insufficient modeling of nonlinear dependencies
in attention and restricted receptive fields caused by convolutions. To
overcome these limitations, this paper introduces an innovative framework,
Attention Mamba, featuring a novel Adaptive Pooling block that accelerates
attention computation and incorporates global information, effectively
overcoming the constraints of limited receptive fields. Furthermore, Attention
Mamba integrates a bidirectional Mamba block, efficiently capturing long-short
features and transforming inputs into the Value representations for attention
mechanisms. Extensive experiments conducted on diverse datasets underscore the
effectiveness of Attention Mamba in extracting nonlinear dependencies and
enhancing receptive fields, establishing superior performance among leading
counterparts. Our codes will be available on GitHub.


## When Reasoning Meets Compression Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks

>Authors: Nan Zhang, Yusen Zhang, Prasenjit Mitra, Rui Zhang

>2025-04-02

> http://arxiv.org/abs/2504.02010v1

Recent open-source large reasoning models (LRMs) exhibit strong performance
on complex reasoning tasks, but their large parameter count makes them
prohibitively expensive for individuals. The compression of large language
models (LLMs) offers an effective solution to reduce cost of computational
resources. However, systematic studies on the performance of compressed LLMs in
complex reasoning tasks, especially for LRMs, are lacking. Most works on
**quantization** and **pruning** focus on preserving language modeling performance,
while existing distillation works do not comprehensively benchmark student
models based on reasoning difficulty or compression impact on knowledge and
reasoning. In this paper, we benchmark compressed DeepSeek-R1 models on four
different reasoning datasets (AIME 2024, FOLIO, Temporal Sequences of BIG-Bench
Hard, and MuSiQue), ranging from mathematical to multihop reasoning, using
**quantization**, distillation, and **pruning** methods. We benchmark 2.51-, 1.73-, and
1.58-bit R1 models that adopt dynamic **quantization**. We also benchmark distilled
R1 models that are based on LLaMA or Qwen and run SparseGPT on them to obtain
various **sparsity** levels. Studying the performance and behavior of compressed
LRMs, we report their performance scores and test-time compute (number of
tokens spent on each question). Notably, using MuSiQue, we find that parameter
count has a much greater impact on LRMs' knowledge memorization than on their
reasoning capability, which can inform the choice of compression techniques.
Through our empirical analysis of test-time compute, we find that shorter model
outputs generally achieve better performance than longer ones across several
benchmarks for both R1 and its compressed variants, highlighting the need for
more concise reasoning chains.


## Global Rice Multi-Class Segmentation Dataset (RiceSEG) A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms

>Authors: Junchi Zhou, Haozhou Wang, Yoichiro Kato, Tejasri Nampally, P. Rajalakshmi, M. Balram, Keisuke Katsura, Hao Lu, Yue Mu, Wanneng Yang, Yangmingrui Gao, Feng Xiao, Hongtao Chen, Yuhao Chen, Wenjuan Li, Jingwen Wang, Fenghua Yu, Jian Zhou, Wensheng Wang, Xiaochun Hu, Yuanzhu Yang, Yanfeng Ding, Wei Guo, Shouyang Liu

>2025-04-02

> http://arxiv.org/abs/2504.02880v1

Developing computer vision-based rice phenotyping techniques is crucial for
precision field management and accelerating breeding, thereby continuously
advancing rice production. Among phenotyping tasks, distinguishing image
components is a key prerequisite for characterizing plant growth and
development at the organ scale, enabling deeper insights into eco-physiological
processes. However, due to the fine structure of rice organs and complex
illumination within the canopy, this task remains highly challenging,
underscoring the need for a high-quality training dataset. Such datasets are
scarce, both due to a lack of large, representative collections of rice field
images and the time-intensive nature of annotation. To address this gap, we
established the first comprehensive multi-class rice semantic segmentation
dataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based
images from five major rice-growing countries (China, Japan, India, the
Philippines, and Tanzania), encompassing over 6,000 genotypes across all growth
stages. From these original images, 3,078 representative samples were selected
and annotated with six classes (background, green vegetation, senescent
vegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably,
the sub-dataset from China spans all major genotypes and rice-growing
environments from the northeast to the south. Both state-of-the-art
convolutional neural networks and transformer-based semantic segmentation
models were used as baselines. While these models perform reasonably well in
segmenting background and green vegetation, they face difficulties during the
reproductive stage, when canopy structures are more complex and multiple
classes are involved. These findings highlight the importance of our dataset
for developing specialized segmentation models for rice and other crops.


## ThinkPrune Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning

>Authors: Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang

>2025-04-02

> http://arxiv.org/abs/2504.01296v1

We present ThinkPrune, a simple yet effective method for **pruning** the thinking
length for long-thinking LLMs, which has been found to often produce
inefficient and redundant thinking processes. Existing preliminary explorations
of reducing thinking length primarily focus on forcing the thinking process to
early exit, rather than adapting the LLM to optimize and consolidate the
thinking process, and therefore the length-performance tradeoff observed so far
is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that
continuously trains the long-thinking LLMs via reinforcement learning (RL) with
an added token limit, beyond which any unfinished thoughts and answers will be
discarded, resulting in a zero reward. To further preserve model performance,
we introduce an iterative length **pruning** approach, where multiple rounds of RL
are conducted, each with an increasingly more stringent token limit. We
observed that ThinkPrune results in a remarkable performance-length tradeoff --
on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B
can be reduced by half with only 2% drop in performance. We also observed that
after **pruning**, the LLMs can bypass unnecessary steps while keeping the core
reasoning process complete. Code is available at
https://github.com/UCSB-NLP-Chang/ThinkPrune.


## PolygoNet Leveraging Simplified Polygonal Representation for Effective Image Classification

>Authors: Salim Khazem, Jeremy Fix, Cédric Pradalier

>2025-04-01

> http://arxiv.org/abs/2504.01214v1

Deep learning models have achieved significant success in various image
related tasks. However, they often encounter challenges related to
computational complexity and overfitting. In this paper, we propose an
efficient approach that leverages polygonal representations of images using
dominant points or contour coordinates. By transforming input images into these
compact forms, our method significantly reduces computational requirements,
accelerates training, and conserves resources making it suitable for real time
and resource constrained applications. These representations inherently capture
essential image features while filtering noise, providing a natural
regularization effect that mitigates overfitting. The resulting lightweight
models achieve performance comparable to state of the art methods using full
resolution images while enabling deployment on edge devices. Extensive
experiments on benchmark datasets validate the effectiveness of our approach in
reducing complexity, improving generalization, and facilitating edge computing
applications. This work demonstrates the potential of polygonal representations
in advancing efficient and scalable deep learning solutions for real world
scenarios. The code for the experiments of the paper is provided in
https://github.com/salimkhazem/PolygoNet.


## Efficient n-body simulations using physics informed graph neural networks

>Authors: Víctor Ramos-Osuna, Alberto Díaz-Álvarez, Raúl Lara-Cabrera

>2025-04-01

> http://arxiv.org/abs/2504.01169v1

This paper presents a novel approach for accelerating n-body simulations by
integrating a physics-informed graph neural networks (GNN) with traditional
numerical methods. Our method implements a leapfrog-based simulation engine to
generate datasets from diverse astrophysical scenarios which are then
transformed into graph representations. A custom-designed GNN is trained to
predict particle **acceleration**s with high precision. Experiments, conducted on
60 training and 6 testing simulations spanning from 3 to 500 bodies over 1000
time steps, demonstrate that the proposed model achieves extremely low
prediction errors-loss values while maintaining robust long-term stability,
with accumulated errors in position, velocity, and **acceleration** remaining
insignificant. Furthermore, our method yields a modest speedup of approximately
17% over conventional simulation techniques. These results indicate that the
integration of deep learning with traditional physical simulation methods
offers a promising pathway to significantly enhance computational efficiency
without compromising accuracy.


## MaLAware Automating the Comprehension of Malicious Software Behaviours using Large Language Models (LLMs)

>Authors: Bikash Saha, Nanda Rani, Sandeep Kumar Shukla

>2025-04-01

> http://arxiv.org/abs/2504.01145v1

Current malware (malicious software) analysis tools focus on detection and
family classification but fail to provide clear and actionable narrative
insights into the malignant activity of the malware. Therefore, there is a need
for a tool that translates raw malware data into human-readable descriptions.
Developing such a tool accelerates incident response, reduces malware analysts'
cognitive load, and enables individuals having limited technical expertise to
understand malicious software behaviour. With this objective, we present
MaLAware, which automatically summarizes the full spectrum of malicious
activity of malware executables. MaLAware processes Cuckoo Sandbox-generated
reports using large language models (LLMs) to correlate malignant activities
and generate concise summaries explaining malware behaviour. We evaluate the
tool's performance on five open-source LLMs. The evaluation uses the
human-written malware behaviour description dataset as ground truth. The
model's performance is measured using 11 extensive performance metrics, which
boosts the confidence of MaLAware's effectiveness. The current version of the
tool, i.e., MaLAware, supports Qwen2.5-7B, Llama2-7B, Llama3.1-8B, Mistral-7B,
and Falcon-7B, along with the **quantization** feature for resource-constrained
environments. MaLAware lays a foundation for future research in malware
behavior explanation, and its extensive evaluation demonstrates LLMs' ability
to narrate malware behavior in an actionable and comprehensive manner.


## Near-energy-free Photonic Fourier Transformation for Convolution Operation Acceler

>Authors: Hangbo Yang, Nicola Peserico, Shurui Li, Xiaoxuan Ma, Russell L. T. Schwartz, Mostafa Hosseini, Aydin Babakhani, Chee Wei Wong, Puneet Gupta, Volker J. Sorger

>2025-04-01

> http://arxiv.org/abs/2504.01117v1

Convolutional operations are computationally intensive in artificial
intelligence services, and their overhead in electronic hardware limits machine
learning scaling. Here, we introduce a photonic joint transform correlator
(pJTC) using a near-energy-free on-chip Fourier transformation to accelerate
convolution operations. The pJTC reduces computational complexity for both
convolution and cross-correlation from O(N4) to O(N2), where N2 is the input
data size. Demonstrating functional Fourier transforms and convolution, this
pJTC achieves 98.0% accuracy on an exemplary MNIST inference task. Furthermore,
a wavelength-multiplexed pJTC architecture shows potential for high throughput
and energy efficiency, reaching 305 TOPS/W and 40.2 TOPS/mm2, based on
currently available foundry processes. An efficient, compact, and low-latency
convolution accelerator promises to advance next-generation AI capabilities
across edge demands, high-performance computing, and cloud services.


## MergeVQ A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization

>Authors: Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei

>2025-04-01

> http://arxiv.org/abs/2504.00999v1

Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great
success in both self-supervised pre-training and image generation. However,
most existing methods struggle to address the trade-off in shared latent space
for generation quality vs. representation learning and efficiency. To push the
limits of this paradigm, we propose MergeVQ, which incorporates token merging
techniques into VQ-based generative models to bridge the gap between image
generation and visual representation learning in a unified architecture. During
pre-training, MergeVQ decouples top-k semantics from latent space with the
token merge module after self-attention blocks in the encoder for subsequent
Look-up Free Quantization (LFQ) and global alignment and recovers their
fine-grained details through cross-attention in the decoder for reconstruction.
As for the second-stage generation, we introduce MergeAR, which performs **KV**
Cache compression for efficient raster-order prediction. Extensive experiments
on ImageNet verify that MergeVQ as an AR generative model achieves competitive
performance in both visual representation learning and image generation tasks
while maintaining favorable token efficiency and inference speed. The code and
model will be available at https://apexgen-x.github.io/MergeVQ.


## Accelerating drug discovery with Artificial a whole-lab orchestration and scheduling system for self-driving labs

>Authors: Yao Fehlis, Paul Mandel, Charles Crain, Betty Liu, David Fuller

>2025-04-01

> http://arxiv.org/abs/2504.00986v1

Self-driving labs are transforming drug discovery by enabling automated,
AI-guided experimentation, but they face challenges in orchestrating complex
workflows, integrating diverse instruments and AI models, and managing data
efficiently. Artificial addresses these issues with a comprehensive
orchestration and scheduling system that unifies lab operations, automates
workflows, and integrates AI-driven decision-making. By incorporating AI/ML
models like NVIDIA BioNeMo - which facilitates molecular interaction prediction
and biomolecular analysis - Artificial enhances drug discovery and accelerates
data-driven research. Through real-time coordination of instruments, robots,
and personnel, the platform streamlines experiments, enhances reproducibility,
and advances drug discovery.


## DropGaussian Structural Regularization for Sparse-view Gaussian Splatting

>Authors: Hyunwoo Park, Gun Ryu, Wonjun Kim

>2025-04-01

> http://arxiv.org/abs/2504.00773v1

Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in
the field of novel view synthesis due to its fast performance while yielding
the excellent image quality. However, 3DGS in **sparse**-view settings (e.g.,
three-view inputs) often faces with the problem of overfitting to training
views, which significantly drops the visual quality of novel view images. Many
existing approaches have tackled this issue by using strong priors, such as 2D
generative contextual information and external depth signals. In contrast, this
paper introduces a prior-free method, so-called DropGaussian, with simple
changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians
during the training process in a similar way of dropout, which allows
non-excluded Gaussians to have larger gradients while improving their
visibility. This makes the remaining Gaussians to contribute more to the
optimization process for rendering with **sparse** input views. Such simple
operation effectively alleviates the overfitting problem and enhances the
quality of novel view synthesis. By simply applying DropGaussian to the
original 3DGS framework, we can achieve the competitive performance with
existing prior-based 3DGS methods in **sparse**-view settings of benchmark datasets
without any additional complexity. The code and model are publicly available
at: https://github.com/DCVL-3D/DropGaussian release.


## Accelerated Inorganic Materials Design with Generative AI Agents

>Authors: Izumi Takahara, Teruyasu Mizoguchi, Bang Liu

>2025-04-01

> http://arxiv.org/abs/2504.00741v1

Designing inorganic crystalline materials with tailored properties is
critical to technological innovation, yet current generative computational
methods often struggle to efficiently explore desired targets with sufficient
interpretability. Here, we present MatAgent, a generative approach for
inorganic materials discovery that harnesses the powerful reasoning
capabilities of large language models (LLMs). By combining a diffusion-based
generative model for crystal structure estimation with a predictive model for
property evaluation, MatAgent uses iterative, feedback-driven guidance to steer
material exploration precisely toward user-defined targets. Integrated with
external cognitive tools-including short-term memory, long-term memory, the
periodic table, and a comprehensive materials knowledge base-MatAgent emulates
human expert reasoning to vastly expand the accessible compositional space. Our
results demonstrate that MatAgent robustly directs exploration toward desired
properties while consistently achieving high compositional validity,
uniqueness, and material novelty. This framework thus provides a highly
interpretable, practical, and versatile AI-driven solution to accelerate the
discovery and design of next-generation inorganic materials.


## ToReMi Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection

>Authors: Xiaoxuan Zhu, Zhouhong Gu, Suhang Zheng, Tao Wang, Tianyu Li, Hongwei Feng, Yanghua Xiao

>2025-04-01

> http://arxiv.org/abs/2504.00695v1

Pre-training large language models (LLMs) necessitates enormous diverse
textual corpora, making effective data selection a key challenge for balancing
computational resources and model performance. Current methodologies primarily
emphasize data quality metrics and mixing proportions, yet they fail to
adequately capture the underlying semantic connections between training samples
and quality disparities within individual domains. We introduce ToReMi
(Topic-based Reweighting for Model improvement), a novel two-stage framework
that dynamically adjusts training sample weights according to their topical
associations and observed learning patterns. Our comprehensive experiments
reveal that ToReMi variants consistently achieve superior performance over
conventional pre-training approaches, demonstrating accelerated perplexity
reduction across multiple domains and enhanced capabilities on downstream
evaluation tasks. Code is available at https://github.com/zxx000728/ToReMi.


## Coca-Splat Collaborative Optimization for Camera Parameters and 3D Gaussians

>Authors: Jiamin Wu, Hongyang Li, Xiaoke Jiang, Yuan Yao, Lei Zhang

>2025-04-01

> http://arxiv.org/abs/2504.00639v1

In this work, we introduce Coca-Splat, a novel approach to addressing the
challenges of **sparse** view pose-free scene reconstruction and novel view
synthesis (NVS) by jointly optimizing camera parameters with 3D Gaussians.
Inspired by deformable DEtection TRansformer, we design separate queries for 3D
Gaussians and camera parameters and update them layer by layer through
deformable Transformer layers, enabling joint optimization in a single network.
This design demonstrates better performance because to accurately render views
that closely approximate ground-truth images relies on precise estimation of
both 3D Gaussians and camera parameters. In such a design, the centers of 3D
Gaussians are projected onto each view by camera parameters to get projected
points, which are regarded as 2D reference points in deformable
cross-attention. With camera-aware multi-view deformable cross-attention
(CaMDFA), 3D Gaussians and camera parameters are intrinsically connected by
sharing the 2D reference points. Additionally, 2D reference point determined
rays (RayRef) defined from camera centers to the reference points assist in
modeling relationship between 3D Gaussians and camera parameters through
RQ-decomposition on an overdetermined system of equations derived from the
rays, enhancing the relationship between 3D Gaussians and camera parameters.
Extensive evaluation shows that our approach outperforms previous methods, both
pose-required and pose-free, on RealEstate10K and ACID within the same
pose-free setting.


## Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features

>Authors: Jewon Lee, Ki-Ung Song, Seungmin Yang, Donguk Lim, Jaeyeon Kim, Wooksu Shin, Bo-Kyeong Kim, Yong Jae Lee, Tae-Ho Kim

>2025-04-01

> http://arxiv.org/abs/2504.00557v1

Visual token reduction lowers inference costs caused by extensive image
features in large vision-language models (LVLMs). Unlike relevant studies that
prune tokens in self-attention-only LVLMs, our work uniquely addresses
cross-attention-based models, which achieve superior performance. We identify
that the key-value (**KV**) cache size for image tokens in cross-attention layers
significantly exceeds that of text tokens in self-attention layers, posing a
major compute bottleneck. To mitigate this issue, we exploit the **sparse** nature
in cross-attention maps to selectively prune redundant visual features. Our
Trimmed Llama effectively reduces **KV** cache demands without requiring additional
training. By benefiting from 50%-reduced visual features, our model can reduce
inference latency and memory usage while achieving benchmark parity.


## Preconditioned Additive Gaussian Processes with Fourier Acceleration

>Authors: Theresa Wagner, Tianshi Xu, Franziska Nestler, Yuanzhe Xi, Martin Stoll

>2025-04-01

> http://arxiv.org/abs/2504.00480v1

Gaussian processes (GPs) are crucial in machine learning for quantifying
uncertainty in predictions. However, their associated covariance matrices,
defined by kernel functions, are typically dense and large-scale, posing
significant computational challenges. This paper introduces a matrix-free
method that utilizes the Non-equispaced Fast Fourier Transform (NFFT) to
achieve nearly linear complexity in the multiplication of kernel matrices and
their derivatives with vectors for a predetermined accuracy level. To address
high-dimensional problems, we propose an additive kernel approach. Each
sub-kernel in this approach captures lower-order feature interactions, allowing
for the efficient application of the NFFT method and potentially increasing
accuracy across various real-world datasets. Additionally, we implement a
preconditioning strategy that accelerates hyperparameter tuning, further
improving the efficiency and effectiveness of GPs.


## Spatiotemporal Airy rings wavepackets

>Authors: Xiaolin Su, Andy Chong, Qian Cao, Qiwen Zhan

>2025-04-01

> http://arxiv.org/abs/2504.00439v1

Airy waves, known for their non-diffracting and self-accelerating properties,
have been extensively studied in spatial and temporal domains, but their
spatiotemporal (ST) counterparts remain largely unexplored. We report the first
experimental realization of a spatiotemporal Airy rings wavepacket, which
exhibits an Airy function distribution in the radial dimension of the ST
domain. The wavepacket demonstrates abrupt autofocusing under the combined
effects of diffraction and dispersion, achieving a 110 um spatial and 320 fs
temporal focus with a sharp intensity contrast along the propagation direction
- ideal for nonlinear microscopy and multiphoton 3D printing. Notably, the
wavepacket retains its autofocusing capability even after spatial obstruction,
showcasing robust self-healing. Furthermore, by embedding a vortex phase, we
create an ST-Airy vortex wavepacket that confines transverse orbital angular
momentum (t-OAM) within a compact ST volume, enabling new avenues for studying
light-matter interactions with t-OAM. Our findings advance the fundamental
understanding of ST Airy waves and highlight their potential for transformative
applications in ultrafast optics, structured light, and precision laser
processing.


## HawkeyeEfficient Reasoning with Model Collaboration

>Authors: Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, Haonan Li, Qirong Ho

>2025-04-01

> http://arxiv.org/abs/2504.00424v1

Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in
enhancing the reasoning abilities of large language models (LLMs). However, its
efficiency remains a challenge due to the generation of excessive intermediate
reasoning tokens, which introduce semantic redundancy and overly detailed
reasoning steps. Moreover, computational expense and latency are significant
concerns, as the cost scales with the number of output tokens, including those
intermediate steps. In this work, we observe that most CoT tokens are
unnecessary, and retaining only a small portion of them is sufficient for
producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel
post-training and inference framework where a large model produces concise CoT
instructions to guide a smaller model in response generation. HAWKEYE
quantifies redundancy in CoT reasoning and distills high-density information
via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able
to expand responses while reducing token usage and computational cost
significantly. Our evaluation shows that HAWKEYE can achieve comparable
response quality using only 35% of the full CoTs, while improving clarity,
coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can
accelerate end-to-end reasoning by up to 3.4x on complex math tasks while
reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the
models will be available soon.


## Spatiotemporal Attention Learning Framework for Event-Driven Object Recognition

>Authors: Tiantian Xie, Pengpai Wang, Rosa H. M. Chan

>2025-04-01

> http://arxiv.org/abs/2504.00370v1

Event-based vision sensors, inspired by biological neural systems,
asynchronously capture local pixel-level intensity changes as a **sparse** event
stream containing position, polarity, and timestamp information. These
neuromorphic sensors offer significant advantages in dynamic range, latency,
and power efficiency. Their working principle inherently addresses traditional
camera limitations such as motion blur and redundant background information,
making them particularly suitable for dynamic vision tasks. While recent works
have proposed increasingly complex event-based architectures, the computational
overhead and parameter complexity of these approaches limit their practical
deployment. This paper presents a novel spatiotemporal learning framework for
event-based object recognition, utilizing a VGG network enhanced with
Convolutional Block Attention Module (CBAM). Our approach achieves comparable
performance to state-of-the-art ResNet-based methods while reducing parameter
count by 2.3% compared to the original VGG model. Specifically, it outperforms
ResNet-based methods like MVF-Net, achieving the highest Top-1 accuracy of
76.4% (pretrained) and 71.3% (not pretrained) on CIFAR10-DVS, and 72.4% (not
pretrained) on N-Caltech101. These results highlight the robustness of our
method when pretrained weights are not used, making it suitable for scenarios
where transfer learning is unavailable. Moreover, our approach reduces reliance
on data augmentation. Experimental results on standard event-based datasets
demonstrate the framework's efficiency and effectiveness for real-world
applications.


## VNJPTranslate A comprehensive pipeline for Vietnamese-Japanese translation

>Authors: Hoang Hai Phan, Nguyen Duc Minh Vu, Nam Dang Phuong

>2025-04-01

> http://arxiv.org/abs/2504.00339v1

Neural Machine Translation (NMT) driven by Transformer architectures has
advanced significantly, yet faces challenges with low-resource language pairs
like Vietnamese-Japanese (Vi-Ja). Issues include **sparse** parallel data and
handling linguistic/cultural nuances. Recent progress in Large Language Models
(LLMs) with strong reasoning, often refined via Reinforcement Learning (RL),
enables high-quality synthetic data generation. We introduce VNJPTranslate, a
pipeline designed to systematically address the Vi-Ja translation task. It
features a targeted data augmentation strategy using advanced LLMs with
Chain-of-Thought prompting for challenging segments identified via corpus
analysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth
with QLoRA) on a capable, low-parameter autoregressive model (specifically, a
fine-tuned version of the 1.8B parameter Sailor model, which is based on the
Qwen architecture) to create a practical and high-performing translation
system. This integrated approach aims to improve Vi-Ja translation quality
significantly over existing baselines.


## Co-design Optimization of Moving Parts for Compliance and Collision Avoidance

>Authors: Amir M. Mirzendehdel, Morad Behandish

>2025-03-31

> http://arxiv.org/abs/2504.00292v1

Design requirements for moving parts in mechanical assemblies are typically
specified in terms of interactions with other parts. Some are purely kinematic
(e.g., pairwise collision avoidance) while others depend on physics and
material properties (e.g., deformation under loads). Kinematic design methods
and physics-based shape/topology optimization (SO/TO) deal separately with
these requirements. They rarely talk to each other as the former uses set
algebra and group theory while the latter requires discretizing and solving
differential equations. Hence, optimizing a moving part based on physics
typically relies on either neglecting or **pruning** kinematic constraints in
advance, e.g., by restricting the design domain to a collision-free space using
an unsweep operation. In this paper, we show that TO can be used to co-design
two or more parts in relative motion to simultaneously satisfy physics-based
criteria and collision avoidance. We restrict our attention to maximizing
linear-elastic stiffness while penalizing collision measures aggregated in
time. We couple the TO loops for two parts in relative motion so that the
evolution of each part's shape is accounted for when penalizing collision for
the other part. The collision measures are computed by a correlation functional
that can be discretized by left- and right-multiplying the shape design
variables by a pre-computed matrix that depends solely on the motion. This
decoupling is key to making the computations scalable for TO iterations. We
demonstrate the effectiveness of the approach with 2D and 3D examples.


## PIM-LLM A High-Throughput Hybrid PIM Architecture for 1-bit LLMs

>Authors: Jinendra Malekar, Peyton Chandarana, Md Hasibul Amin, Mohammed E. Elbtity, Ramtin Zand

>2025-03-31

> http://arxiv.org/abs/2504.01994v1

In this paper, we propose PIM-LLM, a hybrid architecture developed to
accelerate 1-bit large language models (LLMs). PIM-LLM leverages analog
processing-in-memory (PIM) architectures and digital systolic arrays to
accelerate low-precision matrix multiplication (MatMul) operations in
projection layers and high-precision MatMul operations in attention heads of
1-bit LLMs, respectively. Our design achieves up to roughly 80x improvement in
tokens per second and a 70% increase in tokens per joule compared to
conventional hardware accelerators. Additionally, PIM-LLM outperforms previous
PIM-based LLM accelerators, setting a new benchmark with at least 2x and 5x
improvement in GOPS and GOPS/W, respectively.


## ERUPT Efficient Rendering with Unposed Patch Transformer

>Authors: Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor

>2025-03-31

> http://arxiv.org/abs/2503.24374v1

This work addresses the problem of novel view synthesis in diverse scenes
from small collections of RGB images. We propose ERUPT (Efficient Rendering
with Unposed Patch Transformer) a state-of-the-art scene reconstruction model
capable of efficient scene rendering using unposed imagery. We introduce
patch-based querying, in contrast to existing pixel-based queries, to reduce
the compute required to render a target view. This makes our model highly
efficient both during training and at inference, capable of rendering at 600
fps on commercial hardware. Notably, our model is designed to use a learned
latent camera pose which allows for training using unposed targets in datasets
with **sparse** or inaccurate ground truth camera pose. We show that our approach
can generalize on large real-world data and introduce a new benchmark dataset
(MSVS-1M) for latent view synthesis using street-view imagery collected from
Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense
imagery and precise metadata, ERUPT can render novel views of arbitrary scenes
with as few as five unposed input images. ERUPT achieves better rendered image
quality than current state-of-the-art methods for unposed image synthesis
tasks, reduces labeled data requirements by ~95\% and decreases computational
requirements by an order of magnitude, providing efficient novel view synthesis
for diverse real-world scenes.


## SQuat Subspace-orthogonal KV Cache Quantization

>Authors: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

>2025-03-31

> http://arxiv.org/abs/2503.24358v1

The key-value (**KV**) cache accelerates LLMs decoding by storing **KV** tensors from
previously generated tokens. It reduces redundant computation at the cost of
increased memory usage. To mitigate this overhead, existing approaches compress
**KV** tensors into lower-bit representations; however, **quantization** errors can
accumulate as more tokens are generated, potentially resulting in undesired
outputs. In this paper, we introduce SQuat (Subspace-orthogonal **KV** cache
**quantization**). It first constructs a subspace spanned by query tensors to
capture the most critical task-related information. During key tensor
**quantization**, it enforces that the difference between the (de)**quantize**d and
original keys remains orthogonal to this subspace, minimizing the impact of
**quantization** errors on the attention mechanism's outputs. SQuat requires no
model fine-tuning, no additional calibration dataset for offline learning, and
is grounded in a theoretical framework we develop. Through numerical
experiments, we show that our method reduces peak memory by 2.17 to 2.82,
improves throughput by 2.45 to 3.60, and achieves more favorable benchmark
scores than existing **KV** cache **quantization** algorithms.


## Pyrometheus Symbolic abstractions for XPU and automatically differentiated computation of combustion kinetics and thermodynamics

>Authors: Esteban Cisneros-Garibay, Henry Le Berre, Spencer H. Bryngelson, Jonathan B. Freund

>2025-03-31

> http://arxiv.org/abs/2503.24286v1

The cost of combustion simulations is often dominated by the evaluation of
net production rates of chemical species and mixture thermodynamics
(thermochemistry). Execution on computing accelerators (XPUs) like graphic
processing units (GPUs) can greatly reduce this cost. However, established
thermochemistry software is not readily portable to such devices or sacrifices
valuable analytical forms that enable differentiation for sensitivity analysis
and implicit time integration. Symbolic abstractions are developed with
corresponding transformations that enable computation on accelerators and
automatic differentiation by avoiding premature specification of detail. The
software package Pyrometheus is introduced as an implementation of these
abstractions and their transformations for combustion thermochemistry. The
formulation facilitates code generation from the symbolic representation of a
specific thermochemical mechanism in multiple target languages, including
Python, C++, and Fortran. Computational concerns are separated: the generated
code processes array-valued expressions but does not specify their semantics.
These semantics are provided by compatible array libraries, such as NumPy,
Pytato, and Google JAX. Thus, the generated code retains a symbolic
representation of the thermochemistry, which translates to computation on
accelerators and CPUs and automatic differentiation. The design and operation
of these symbolic abstractions and their companion tool, Pyrometheus, are
discussed throughout. Roofline demonstrations show that the computation of
chemical source terms within MFC, a Fortran-based flow solver we link to
Pyrometheus, is performant.


## Style Quantization for Data-Efficient GAN Training

>Authors: Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv

>2025-03-31

> http://arxiv.org/abs/2503.24282v1

Under limited data setting, GANs often struggle to navigate and effectively
exploit the input latent space. Consequently, images generated from adjacent
variables in a **sparse** input latent space may exhibit significant discrepancies
in realism, leading to suboptimal consistency regularization (CR) outcomes. To
address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by
introducing a style space **quantization** scheme. This method transforms the
**sparse**, continuous input latent space into a compact, structured discrete proxy
space, allowing each element to correspond to a specific real data point,
thereby improving CR performance. Instead of direct **quantization**, we first map
the input latent variables into a less entangled ``style'' space and apply
**quantization** using a learnable codebook. This enables each **quantize**d code to
control distinct factors of variation. Additionally, we optimize the optimal
transport distance to align the codebook codes with features extracted from the
training data by a foundation model, embedding external knowledge into the
codebook and establishing a semantically rich vocabulary that properly
describes the training dataset. Extensive experiments demonstrate significant
improvements in both discriminator robustness and generation quality with our
method.


## Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality

>Authors: Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier

>2025-03-31

> http://arxiv.org/abs/2503.24277v1

Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic
interpretability, but leading SAE approaches with top-$k$ style activation
functions lack theoretical grounding for selecting the hyperparameter $k$. SAEs
are based on the linear representation hypothesis (LRH), which assumes that the
representations of large language models (LLMs) are linearly encoded, and the
superposition hypothesis (SH), which states that there can be more features in
the model than its dimensionality. We show that, based on the formal
definitions of the LRH and SH, the magnitude of **sparse** feature vectors (the
latent representations learned by SAEs of the dense embeddings of LLMs) can be
approximated using their corresponding dense vector with a closed-form error
bound. To visualize this, we propose the ZF plot, which reveals a previously
unknown relationship between LLM hidden embeddings and SAE feature vectors,
allowing us to make the first empirical measurement of the extent to which
feature vectors of pre-trained SAEs are over- or under-activated for a given
input. Correspondingly, we introduce Approximate Feature Activation (AFA),
which approximates the magnitude of the ground-truth **sparse** feature vector, and
propose a new evaluation metric derived from AFA to assess the alignment
between inputs and activations. We also leverage AFA to introduce a novel SAE
architecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with
theoretical justifications; and (b) obviate the need to tune SAE **sparsity**
hyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve
reconstruction loss comparable to that of state-of-the-art top-k SAEs, without
requiring the hyperparameter $k$ to be tuned. Our code is available at:
https://github.com/SewoongLee/top-afa-sae.


## Text2Tracks Prompt-based Music Recommendation via Generative Retrieval

>Authors: Enrico Palumbo, Gustavo Penha, Andreas Damianou, José Luis Redondo García, Timothy Christopher Heath, Alice Wang, Hugues Bouchard, Mounia Lalmas

>2025-03-31

> http://arxiv.org/abs/2503.24193v2

In recent years, Large Language Models (LLMs) have enabled users to provide
highly specific music recommendation requests using natural language prompts
(e.g. "Can you recommend some old classics for slow dancing?"). In this setup,
the recommended tracks are predicted by the LLM in an autoregressive way, i.e.
the LLM generates the track titles one token at a time. While intuitive, this
approach has several limitation. First, it is based on a general purpose
tokenization that is optimized for words rather than for track titles. Second,
it necessitates an additional entity resolution layer that matches the track
title to the actual track identifier. Third, the number of decoding steps
scales linearly with the length of the track title, slowing down inference. In
this paper, we propose to address the task of prompt-based music recommendation
as a generative retrieval task. Within this setting, we introduce novel,
effective, and efficient representations of track identifiers that
significantly outperform commonly used strategies. We introduce Text2Tracks, a
generative retrieval model that learns a mapping from a user's music
recommendation prompt to the relevant track IDs directly. Through an offline
evaluation on a dataset of playlists with language inputs, we find that (1) the
strategy to create IDs for music tracks is the most important factor for the
effectiveness of Text2Tracks and semantic IDs significantly outperform commonly
used strategies that rely on song titles as identifiers (2) provided with the
right choice of track identifiers, Text2Tracks outperforms **sparse** and dense
retrieval solutions trained to retrieve tracks from language prompts.


## ReaLM Reliable and Efficient Large Language Model Inference with Statistical Algorithm-Based Fault Tolerance

>Authors: Tong Xie, Jiawang Zhao, Zishen Wan, Zuodong Zhang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

>2025-03-31

> http://arxiv.org/abs/2503.24053v1

The demand for efficient large language model (LLM) inference has propelled
the development of dedicated accelerators. As accelerators are vulnerable to
hardware faults due to aging, variation, etc, existing accelerator designs
often reserve a large voltage margin or leverage algorithm-based fault
tolerance (ABFT) techniques to ensure LLM inference correctness. However,
previous methods often overlook the inherent fault tolerance of LLMs, leading
to high computation and energy overhead. To enable reliable yet efficient LLM
inference, in this paper, we propose a novel algorithm/circuit co-design
framework, dubbed ReaLM. For the first time, we systematically characterize the
fault tolerance of LLMs by performing a large-scale error injection study of
representative LLMs and natural language understanding tasks. Then, we propose
a statistical ABFT algorithm that fully leverages the error robustness to
minimize error recovery as much as possible. We also customize the error
detection circuits to enable a low-cost online collection of error statistics.
Extensive experiments show that with only 1.42% circuit area and 1.79% power
overhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29.
Compared to existing methods, ReaLM consistently reduces recovery costs across
different operating voltages and improves energy efficiency by up to 35.83%
without compromising LLM performance. Our error injection code is available at
https://github.com/2000012835xt/ReaLM-DAC.


## Towards Scientific Intelligence A Survey of LLM-based Scientific Agents

>Authors: Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang

>2025-03-31

> http://arxiv.org/abs/2503.24047v1

As scientific research becomes increasingly complex, innovative tools are
needed to manage vast data, facilitate interdisciplinary collaboration, and
accelerate discovery. Large language models (LLMs) are now evolving into
LLM-based scientific agents that automate critical tasks, ranging from
hypothesis generation and experiment design to data analysis and simulation.
Unlike general-purpose LLMs, these specialized agents integrate domain-specific
knowledge, advanced tool sets, and robust validation mechanisms, enabling them
to handle complex data types, ensure reproducibility, and drive scientific
breakthroughs. This survey provides a focused review of the architectures,
design, benchmarks, applications, and ethical considerations surrounding
LLM-based scientific agents. We highlight why they differ from general agents
and the ways in which they advance research across various scientific fields.
By examining their development and challenges, this survey offers a
comprehensive roadmap for researchers and practitioners to harness these agents
for more efficient, reliable, and ethically sound scientific discovery.


## CITRAS Covariate-Informed Transformer for Time Series Forecasting

>Authors: Yosuke Yamaguchi, Issei Suemitsu, Wenpeng Wei

>2025-03-31

> http://arxiv.org/abs/2503.24007v1

Covariates play an indispensable role in practical time series forecasting,
offering rich context from the past and sometimes extending into the future.
However, their availability varies depending on the scenario, and situations
often involve multiple target variables simultaneously. Moreover, the
cross-variate dependencies between them are multi-granular, with some
covariates having a short-term impact on target variables and others showing
long-term correlations. This heterogeneity and the intricate dependencies
arising in covariate-informed forecasting present significant challenges to
existing deep models. To address these issues, we propose CITRAS, a patch-based
Transformer that flexibly leverages multiple targets and covariates covering
both the past and the future forecasting horizon. While preserving the strong
autoregressive capabilities of the canonical Transformer, CITRAS introduces two
novel mechanisms in patch-wise cross-variate attention: Key-Value (**KV**) Shift
and Attention Score Smoothing. **KV** Shift seamlessly incorporates future known
covariates into the forecasting of target variables based on their concurrent
dependencies. Additionally, Attention Score Smoothing transforms locally
accurate patch-wise cross-variate dependencies into global variate-level
dependencies by smoothing the past series of attention scores. Experimentally,
CITRAS achieves state-of-the-art performance in both covariate-informed and
multivariate forecasting, demonstrating its versatile ability to leverage
cross-variate dependency for improved forecasting accuracy.


## Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving

>Authors: Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen

>2025-03-31

> http://arxiv.org/abs/2503.24000v1

Key-Value cache (\texttt{**KV**} \texttt{cache}) compression has emerged as a
promising technique to optimize Large Language Model (LLM) serving. It
primarily decreases the memory consumption of \texttt{**KV**} \texttt{cache} to
reduce the computation cost. Despite the development of many compression
algorithms, their applications in production environments are still not
prevalent. In this paper, we revisit mainstream \texttt{**KV**} \texttt{cache}
compression solutions from a practical perspective. Our contributions are
three-fold. First, we comprehensively review existing algorithmic designs and
benchmark studies for \texttt{**KV**} \texttt{cache} compression and identify
missing pieces in their performance measurement, which could hinder their
adoption in practice. Second, we empirically evaluate representative
\texttt{**KV**} \texttt{cache} compression methods to uncover two key issues that
affect the computational efficiency: (1) while compressing \texttt{**KV**}
\texttt{cache} can reduce memory consumption, current implementations (e.g.,
FlashAttention, PagedAttention) do not optimize for production-level LLM
serving, resulting in suboptimal throughput performance; (2) compressing
\texttt{**KV**} \texttt{cache} may lead to longer outputs, resulting in increased
end-to-end latency. We further investigate the accuracy performance of
individual samples rather than the overall performance, revealing the intrinsic
limitations in \texttt{**KV**} \texttt{cache} compression when handling specific
LLM tasks. Third, we provide tools to shed light on future \texttt{**KV**}
\texttt{cache} compression studies and facilitate their practical deployment in
production. They are open-sourced in
\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.


## DenseFormer Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model

>Authors: Ming Yuan, Sichao Wang, Chuang Zhang, Lei He, Qing Xu, Jianqiang Wang

>2025-03-31

> http://arxiv.org/abs/2503.23993v1

The depth completion task is a critical problem in autonomous driving,
involving the generation of dense depth maps from **sparse** depth maps and RGB
images. Most existing methods employ a spatial propagation network to
iteratively refine the depth map after obtaining an initial dense depth. In
this paper, we propose DenseFormer, a novel method that integrates the
diffusion model into the depth completion task. By incorporating the denoising
mechanism of the diffusion model, DenseFormer generates the dense depth map by
progressively refining an initial random depth distribution through multiple
iterations. We propose a feature extraction module that leverages a feature
pyramid structure, along with multi-layer deformable attention, to effectively
extract and integrate features from **sparse** depth maps and RGB images, which
serve as the guiding condition for the diffusion process. Additionally, this
paper presents a depth refinement module that applies multi-step iterative
refinement across various ranges to the dense depth results generated by the
diffusion process. The module utilizes image features enriched with multi-scale
information and **sparse** depth input to further enhance the accuracy of the
predicted depth map. Extensive experiments on the KITTI outdoor scene dataset
demonstrate that DenseFormer outperforms classical depth completion methods.


## AirCache Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference

>Authors: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

>2025-03-31

> http://arxiv.org/abs/2503.23956v1

Recent advancements in Large Visual Language Models (LVLMs) have gained
significant attention due to their remarkable reasoning capabilities and
proficiency in generalization. However, processing a large number of visual
tokens and generating long-context outputs impose substantial computational
overhead, leading to excessive demands for key-value (**KV**) cache. To address
this critical bottleneck, we propose AirCache, a novel **KV** cache compression
method aimed at accelerating LVLMs inference. This work systematically
investigates the correlations between visual and textual tokens within the
attention mechanisms of LVLMs. Our empirical analysis reveals considerable
redundancy in cached visual tokens, wherein strategically eliminating these
tokens preserves model performance while significantly accelerating context
generation. Inspired by these findings, we introduce an elite observation
window for assessing the importance of visual components in the **KV** cache,
focusing on stable inter-modal relevancy modeling with enhanced
multi-perspective consistency. Additionally, we develop an adaptive layer-wise
budget allocation strategy that capitalizes on the strength and skewness of
token importance distribution, showcasing superior efficiency compared to
uniform allocation. Comprehensive evaluations across multiple LVLMs and
benchmarks demonstrate that our method achieves comparable performance to the
full cache while retaining only 10% of visual **KV** cache, thereby reducing
decoding latency by 29% to 66% across various batch size and prompt length of
inputs. Notably, as cache retention rates decrease, our method exhibits
increasing performance advantages over existing approaches.


## Choco-Q Commute Hamiltonian-based QAOA for Constrained Binary Optimization

>Authors: Debin Xiang, Qifan Jiang, Liqiang Lu, Siwei Tan, Jianwei Yin

>2025-03-31

> http://arxiv.org/abs/2503.23941v1

Constrained binary optimization aims to find an optimal assignment to
minimize or maximize the objective meanwhile satisfying the constraints, which
is a representative NP problem in various domains, including transportation,
scheduling, and economy. Quantum approximate optimization algorithms (QAOA)
provide a promising methodology for solving this problem by exploiting the
parallelism of quantum entanglement. However, existing QAOA approaches based on
penalty-term or Hamiltonian simulation fail to thoroughly encode the
constraints, leading to extremely low success rate and long searching latency.
  This paper proposes Choco-Q, a formal and universal framework for constrained
binary optimization problems, which comprehensively covers all constraints and
exhibits high deployability for current quantum devices. The main innovation of
Choco-Q is to embed the commute Hamiltonian as the driver Hamiltonian,
resulting in a much more general encoding formulation that can deal with
arbitrary linear constraints. Leveraging the arithmetic features of commute
Hamiltonian, we propose three optimization techniques to squeeze the overall
circuit complexity, including Hamiltonian serialization, equivalent
decomposition, and variable elimination. The serialization mechanism transforms
the original Hamiltonian into smaller ones. Our decomposition methods only take
linear time complexity, achieving end-to-end **acceleration**. Experiments
demonstrate that Choco-Q shows more than 235$\times$ algorithmic improvement in
successfully finding the optimal solution, and achieves 4.69$\times$ end-to-end
**acceleration**, compared to prior QAOA designs.


## Model Hemorrhage and the Robustness Limits of Large Language Models

>Authors: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao

>2025-03-31

> http://arxiv.org/abs/2503.23924v1

Large language models (LLMs) demonstrate strong performance across natural
language processing tasks, yet undergo significant performance degradation when
modified for deployment through **quantization**, **pruning**, or decoding strategy
adjustments. We define this phenomenon as model hemorrhage - performance
decline caused by parameter alterations and architectural changes. Through
systematic analysis of various LLM frameworks, we identify key vulnerability
patterns: layer expansion frequently disrupts attention mechanisms, compression
techniques induce information loss cascades, and decoding adjustments amplify
prediction divergences. Our investigation reveals transformer architectures
exhibit inherent robustness thresholds that determine hemorrhage severity
across modification types. We propose three mitigation strategies:
gradient-aware **pruning** preserves critical weight pathways, dynamic **quantization**
scaling maintains activation integrity, and decoding calibration aligns
generation trajectories with original model distributions. This work
establishes foundational metrics for evaluating model stability during
adaptation, providing practical guidelines for maintaining performance while
enabling efficient LLM deployment. Our findings advance understanding of neural
network resilience under architectural transformations, particularly for
large-scale language models.


## An End-to-End Comprehensive Gear Fault Diagnosis Method Based on Multi-Scale Feature-Level Fusion Strategy

>Authors: Bowei Qiao, Hongwei Wang

>2025-03-31

> http://arxiv.org/abs/2503.23887v1

To satisfy the requirements of the end-to-end fault diagnosis of gears, an
integrated intelligent method of fault diagnosis for gears using **acceleration**
signals was proposed, which was based on Gabor-based Adaptive Short-Time
Fourier Transform (Gabor-ASTFT) and Dual-Tree Complex Wavelet Transform(DTCWT)
algorithms, Dilated Residual structure and feature fusion layer, is proposed in
this paper. Initially, the raw one-dimensional **acceleration** signals collected
from the gearbox base using vibration sensors undergo pre-segmentation
processing. The Gabor-ASTFT and DTCWT are then applied to convert the original
one-dimensional time-domain signals into two-dimensional time-frequency
representations, facilitating the preliminary extraction of fault features and
obtaining weak feature maps.Subsequently, a dual-channel structure is
established using deconvolution and dilated convolution to perform upsampling
and downsampling on the feature maps, adjusting their sizes accordingly. A
feature fusion layer is then constructed to integrate the dual-channel
features, enabling multi-scale analysis of the extracted fault
features.Finally, a convolutional neural network (CNN) model incorporating a
residual structure is developed to conduct deep feature extraction from the
fused feature maps. The extracted features are subsequently fed into a Global
Average Pooling(GAP) and a classification function for fault classification.
Conducting comparative experiments on different datasets, the proposed method
is demonstrated to effectively meet the requirements of end-to-end fault
diagnosis for gears.


## MVDRAM Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration

>Authors: Tatsuya Kubo, Daichi Tokuda, Tomoya Nagatani, Masayuki Usui, Lei Qu, Ting Cao, Shinya Takamaeda-Yamazaki

>2025-03-31

> http://arxiv.org/abs/2503.23817v1

General matrix-vector multiplication (GeMV) remains a critical latency
bottleneck in large language model (LLM) inference, even with **quantize**d **low-bit**
models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has
the potential to repurpose on-device DRAM as a GeMV engine, offering additional
high-throughput processing capabilities to widespread consumer devices without
DRAM modifications. However, applying PUD to GeMV operations in the LLM
inference pipeline incurs significant overheads $\textit{before}$ and
$\textit{after}$ in-DRAM computation, diminishing the benefits of its
high-throughput processing capabilities.
  This paper presents MVDRAM, the first practical system to accelerate GeMV
operations for **low-bit** LLM inference using unmodified DRAM. By leveraging the
data sharing patterns and mathematical linearity in GeMV operations, MVDRAM
orchestrates the processor and DRAM to eliminate the costs associated with
pre-arranging inputs and bit-transposition of outputs required in conventional
PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows
that MVDRAM achieves comparable or even better inference speed than the
processor-based implementation for GeMV operations in **low-bit** (under 4-bit)
LLM. In particular, MVDRAM achieves up to 7.29$\times$ speedup and 30.5$\times$
energy efficiency for **low-bit** GeMV operations. For end-to-end LLM inference,
MVDRAM achieves 2.18$\times$ and 1.31$\times$ throughput improvements, along
with 3.04$\times$ and 2.35$\times$ energy efficiency, for 2-bit and 4-bit
**quantize**d **low-bit** models, respectively. MVDRAM has the potential to redefine
the AI hardware landscape by demonstrating the feasibility of standard DRAM as
an LLM accelerator.


## Adaptive Layer-skipping in Pre-trained LLMs

>Authors: Xuan Luo, Weizhi Wang, Xifeng Yan

>2025-03-31

> http://arxiv.org/abs/2503.23798v1

Various layer-skipping methods have been proposed to accelerate token
generation in large language models (LLMs). However, they have overlooked a
fundamental question: How do computational demands vary across the generation
of different tokens? In this work, we introduce FlexiDepth, a method that
dynamically adjusts the number of Transformer layers used in text generation.
By incorporating a plug-in router and adapter, FlexiDepth enables adaptive
layer-skipping in LLMs without modifying their original parameters. Introducing
FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,
and meanwhile maintains the full 100\% benchmark performance. Experimental
results with FlexiDepth demonstrate that computational demands in LLMs
significantly vary based on token type. Specifically, generating repetitive
tokens or fixed phrases requires fewer layers, whereas producing tokens
involving computation or high uncertainty requires more layers. Interestingly,
this adaptive allocation pattern aligns with human intuition. To advance
research in this area, we open sourced FlexiDepth and a dataset documenting
FlexiDepth's layer allocation patterns for future exploration.


## Performing Path Integral Molecular Dynamics Using Artificial Intelligence Enhanced Molecular Simulation Framework

>Authors: Cheng Fan, Maodong Li, Sihao Yuan, Zhaoxin Xie, Dechin Chen, Yi Isaac Yang, Yi Qin Gao

>2025-03-31

> http://arxiv.org/abs/2503.23728v1

This study employed an artificial intelligence-enhanced molecular simulation
framework to enable efficient Path Integral Molecular Dynamics (PIMD)
simulations. Owing to its modular architecture and high-throughput
capabilities, the framework effectively mitigates the computational complexity
and resource-intensive limitations associated with conventional PIMD
approaches. By integrating machine learning force fields (MLFFs) into the
framework, we rigorously tested its performance through two representative
cases: a small-molecule reaction system (double proton transfer in formic acid
dimer) and a bulk-phase transition system (water-ice phase transformation).
Computational results demonstrate that the proposed framework achieves
accelerated PIMD simulations while preserving quantum mechanical accuracy.
These findings show that nuclear quantum effects can be captured for complex
molecular systems, using relatively low computational cost.


## A Conceptual Framework for Human-AI Collaborative Genome Annotation

>Authors: Xiaomei Li, Alex Whan, Meredith McNeil, David Starns, Jessica Irons, Samuel C. Andrew, Rad Suchecki

>2025-03-31

> http://arxiv.org/abs/2503.23691v1

Genome annotation is essential for understanding the functional elements
within genomes. While automated methods are indispensable for processing
large-scale genomic data, they often face challenges in accurately predicting
gene structures and functions. Consequently, manual curation by domain experts
remains crucial for validating and refining these predictions. These combined
outcomes from automated tools and manual curation highlight the importance of
integrating human expertise with AI capabilities to improve both the accuracy
and efficiency of genome annotation. However, the manual curation process is
inherently labor-intensive and time-consuming, making it difficult to scale for
large datasets. To address these challenges, we propose a conceptual framework,
Human-AI Collaborative Genome Annotation (HAICoGA), which leverages the
synergistic partnership between humans and artificial intelligence to enhance
human capabilities and accelerate the genome annotation process. Additionally,
we explore the potential of integrating Large Language Models (LLMs) into this
framework to support and augment specific tasks. Finally, we discuss emerging
challenges and outline open research questions to guide further exploration in
this area.


## Uni-Render A Unified Accelerator for Real-Time Rendering Across Diverse Neural Renderers

>Authors: Chaojian Li, Sixu Li, Linrui Jiang, Jingqun Zhang, Yingyan Celine Lin

>2025-03-31

> http://arxiv.org/abs/2503.23644v1

Recent advancements in neural rendering technologies and their supporting
devices have paved the way for immersive 3D experiences, significantly
transforming human interaction with intelligent devices across diverse
applications. However, achieving the desired real-time rendering speeds for
immersive interactions is still hindered by (1) the lack of a universal
algorithmic solution for different application scenarios and (2) the dedication
of existing devices or accelerators to merely specific rendering pipelines. To
overcome this challenge, we have developed a unified neural rendering
accelerator that caters to a wide array of typical neural rendering pipelines,
enabling real-time and on-device rendering across different applications while
maintaining both efficiency and compatibility. Our accelerator design is based
on the insight that, although neural rendering pipelines vary and their
algorithm designs are continually evolving, they typically share common
operators, predominantly executing similar workloads. Building on this insight,
we propose a reconfigurable hardware architecture that can dynamically adjust
dataflow to align with specific rendering metric requirements for diverse
applications, effectively supporting both typical and the latest hybrid
rendering pipelines. Benchmarking experiments and ablation studies on both
synthetic and real-world scenes demonstrate the effectiveness of the proposed
accelerator. The proposed unified accelerator stands out as the first solution
capable of achieving real-time neural rendering across varied representative
pipelines on edge devices, potentially paving the way for the next generation
of neural graphics applications.


## Stochastic analysis of impulsive thrust uncertainties in the CR3BP

>Authors: Sharad Sharan, Amit Jain, Roshan T. Eapen, Puneet Singla

>2025-03-30

> http://arxiv.org/abs/2503.23628v1

This paper employs an alternate dynamical model of the circular restricted
three body problem to quantify uncertainties associated with spacecraft
thrusting maneuvers. A non-product quadrature scheme known as Conjugate
Unscented Transform (CUT) is employed to determine the higher order system
sensitivities through a computationally efficient data driven approach.
Moreover, the CUT scheme, in conjunction with a **sparse** approximation method, is
used to find an analytical representation of the time evolution of the state
probability density function (pdf).


## Using Artificial Neural Networks to Optimize Acceleration Due to Gravity g Measurement in a Compound Pendulum Experiment

>Authors: Sudakhina Prusty, Raja Das, Saralasrita Mohanty

>2025-03-30

> http://arxiv.org/abs/2503.23506v2

In this study, we explore a novel approach of implementing the Artificial
Neural Network (ANN) model to validate a traditional experiment performed in
the undergraduate physics laboratory, e.g. measurement of **acceleration** due to
gravity g using compound pendulum. The input layer for the ANN model comprises
of different parameters of the compound pendulum experiment such as its
effective length, time period of oscillation and initial angular displacement.
The model is first trained by using 70 percent of the experimental data. Then
the trained model is validated and tested on the rest 30 percent of the
experimental data which are treated as unseen data to predict the value of g.
The ANN-predicted values are compared with the experimental value of g to
assess precision. The average value of g was determined to be 1009.029797cm/s^2
with a random error of $\pm$6.817633 cm/s^2 through traditional experiment.
However, the ANN-predicted value of g was 1009.029858 with a mean absolute
error of 0.000592 cm/s^2. The authors propose that ANN-based methodologies
could bridge the gap between theoretical understanding and practical
application by introducing students to cutting-edge computational techniques.
Such integration allows for a deeper engagement with the subject matter,
encouraging critical thinking and problem-solving skills in experimental
physics. This innovative approach underscores the transformative role of
machine learning in modern physics education. The model's performance
demonstrates high accuracy and robustness in predicting g, outperforming
traditional empirical approaches. Our results indicate that ANN-based
optimization can significantly improve the precision of gravitational
measurements, offering a reliable and efficient tool for applications that
require high-accuracy determination of gravitational **acceleration**.


## Reinforcement Learning-based Token Pruning in Vision Transformers A Markov Game Approach

>Authors: Chenglong Lu, Shen Liang, Xuewei Wang, Wei Wang

>2025-03-30

> http://arxiv.org/abs/2503.23459v1

Vision Transformers (ViTs) have computational costs scaling quadratically
with the number of tokens, calling for effective token **pruning** policies. Most
existing policies are handcrafted, lacking adaptivity to varying inputs.
Moreover, they fail to consider the sequential nature of token **pruning** across
multiple layers. In this work, for the first time (as far as we know), we
exploit Reinforcement Learning (RL) to data-adaptively learn a **pruning** policy.
Formulating token **pruning** as a sequential decision-making problem, we model it
as a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO)
where each agent makes an individualized **pruning** decision for a single token.
We also develop reward functions that enable simultaneous collaboration and
competition of these agents to balance efficiency and accuracy. On the
well-known ImageNet-1k dataset, our method improves the inference speed by up
to 44% while incurring only a negligible accuracy drop of 0.4%. The source code
is available at https://github.com/daashuai/rl4evit.


## Efficient Token Compression for Vision Transformer with Spatial Information Preserved

>Authors: Junzhu Mao, Yang Shen, Jinyang Guo, Yazhou Yao, Xiansheng Hua

>2025-03-30

> http://arxiv.org/abs/2503.23455v1

Token compression is essential for reducing the computational and memory
requirements of transformer models, enabling their deployment in
resource-constrained environments. In this work, we propose an efficient and
hardware-compatible token compression method called Prune and Merge. Our
approach integrates token **pruning** and merging operations within transformer
models to achieve layer-wise token compression. By introducing trainable merge
and reconstruct matrices and utilizing shortcut connections, we efficiently
merge tokens while preserving important information and enabling the
restoration of pruned tokens. Additionally, we introduce a novel
gradient-weighted attention scoring mechanism that computes token importance
scores during the training phase, eliminating the need for separate
computations during inference and enhancing compression efficiency. We also
leverage gradient information to capture the global impact of tokens and
automatically identify optimal compression structures. Extensive experiments on
the ImageNet-1k and ADE20K datasets validate the effectiveness of our approach,
achieving significant speed-ups with minimal accuracy degradation compared to
state-of-the-art methods. For instance, on DeiT-Small, we achieve a
1.64$\times$ speed-up with only a 0.2\% drop in accuracy on ImageNet-1k.
Moreover, by compressing segmenter models and comparing with existing methods,
we demonstrate the superior performance of our approach in terms of efficiency
and effectiveness. Code and models have been made available at
https://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge.


## Filtering with Time-frequency Analysis An Adaptive and Lightweight Model for Sequential Recommender Systems Based on Discrete Wavelet Transform

>Authors: Sheng Lu, Mingxi Ge, Jiuyi Zhang, Wanli Zhu, Guanjin Li, Fangming Gu

>2025-03-30

> http://arxiv.org/abs/2503.23436v1

Sequential Recommender Systems (SRS) aim to model sequential behaviors of
users to capture their interests which usually evolve over time.
Transformer-based SRS have achieved distinguished successes recently. However,
studies reveal self-attention mechanism in Transformer-based models is
essentially a low-pass filter and ignores high frequency information
potentially including meaningful user interest patterns. This motivates us to
seek better filtering technologies for SRS, and finally we find Discrete
Wavelet Transform (DWT), a famous time-frequency analysis technique from
digital signal processing field, can effectively process both low-frequency and
high-frequency information. We design an adaptive time-frequency filter with
DWT technique, which decomposes user interests into multiple signals with
different frequency and time, and can automatically learn weights of these
signals. Furthermore, we develop DWTRec, a model for sequential recommendation
all based on the adaptive time-frequency filter. Thanks to fast DWT technique,
DWTRec has a lower time complexity and space complexity theoretically, and is
Proficient in modeling long sequences. Experiments show that our model
outperforms state-of-the-art baseline models in datasets with different
domains, **sparsity** levels and average sequence lengths. Especially, our model
shows great performance increase in contrast with previous models when the
sequence grows longer, which demonstrates another advantage of our model.


## Representations of knot groups in $\textrm{AGL}_{1}(\mathbb{C})$ and Alexander invariants

>Authors: Ángel González-Prieto, Javier Martínez, Vicente Muñoz

>2025-03-30

> http://arxiv.org/abs/2503.23364v1

This paper reinterprets Alexander-type invariants of knots via representation
varieties of knot groups into the group $\textrm{AGL}_1(\mathbb{C})$ of affine
transformations of the complex line. In particular, we prove that the
coordinate ring of the $\textrm{AGL}_{1}(\mathbb{C})$-representation variety is
isomorphic to the symmetric algebra of the Alexander module. This yields a
natural interpretation of the Alexander polynomial as the singular locus of a
coherent sheaf over $\mathbb{C}^*$, whose fibres correspond to quandle
representation varieties of the knot quandle. As a by-product, we construct
Topological Quantum Field Theories that provide effective computational methods
and recover the Burau representations of braids. This theory offers a new
geometric perspective on classical Alexander invariants and their functorial
**quantization**.


## Solve sparse PCA problem by employing Hamiltonian system and leapfrog method

>Authors: Loc Hoang Tran

>2025-03-30

> http://arxiv.org/abs/2503.23335v1

Principal Component Analysis (PCA) is a widely utilized technique for
dimensionality reduction; however, its inherent lack of
interpretability-stemming from dense linear combinations of all feature-limits
its applicability in many domains. In this paper, we propose a novel **sparse** PCA
algorithm that imposes **sparsity** through a smooth L1 penalty and leverages a
Hamiltonian formulation solved via geometric integration techniques.
Specifically, we implement two distinct numerical methods-one based on the
Proximal Gradient (ISTA) approach and another employing a leapfrog
(fourth-order Runge-Kutta) scheme-to minimize the energy function that balances
variance maximization with **sparsity** enforcement. To extract a subset of **sparse**
principal components, we further incorporate a deflation technique and
subsequently transform the original high-dimensional face data into a
lower-dimensional feature space. Experimental evaluations on a face recognition
dataset-using both k-nearest neighbor and kernel ridge regression
classifiers-demonstrate that the proposed **sparse** PCA methods consistently
achieve higher classification accuracy than conventional PCA. Future research
will extend this framework to integrate **sparse** PCA with modern deep learning
architectures for multimodal recognition tasks.


## HiPART Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation

>Authors: Hongwei Zheng, Han Li, Wenrui Dai, Ziyang Zheng, Chenglin Li, Junni Zou, Hongkai Xiong

>2025-03-30

> http://arxiv.org/abs/2503.23331v1

Existing 2D-to-3D human pose estimation (HPE) methods struggle with the
occlusion issue by enriching information like temporal and visual cues in the
lifting stage. In this paper, we argue that these methods ignore the limitation
of the **sparse** skeleton 2D input representation, which fundamentally restricts
the 2D-to-3D lifting and worsens the occlusion issue. To address these, we
propose a novel two-stage generative densification method, named Hierarchical
Pose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense
poses from the original **sparse** 2D pose. Specifically, we first develop a
multi-scale skeleton tokenization module to **quantize** the highly dense 2D pose
into hierarchical tokens and propose a Skeleton-aware Alignment to strengthen
token connections. We then develop a Hierarchical AutoRegressive Modeling
scheme for hierarchical 2D pose generation. With generated hierarchical poses
as inputs for 2D-to-3D lifting, the proposed method shows strong robustness in
occluded scenarios and achieves state-of-the-art performance on the
single-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame
methods while reducing parameter and computational complexity and can also
complement them to further enhance performance and robustness.


## AI Agents in Engineering Design A Multi-Agent Framework for Aesthetic and Aerodynamic Car Design

>Authors: Mohamed Elrefaie, Janet Qian, Raina Wu, Qian Chen, Angela Dai, Faez Ahmed

>2025-03-30

> http://arxiv.org/abs/2503.23315v1

We introduce the concept of "Design Agents" for engineering applications,
particularly focusing on the automotive design process, while emphasizing that
our approach can be readily extended to other engineering and design domains.
Our framework integrates AI-driven design agents into the traditional
engineering workflow, demonstrating how these specialized computational agents
interact seamlessly with engineers and designers to augment creativity, enhance
efficiency, and significantly accelerate the overall design cycle. By
automating and streamlining tasks traditionally performed manually, such as
conceptual sketching, styling enhancements, 3D shape retrieval and generative
modeling, computational fluid dynamics (CFD) meshing, and aerodynamic
simulations, our approach reduces certain aspects of the conventional workflow
from weeks and days down to minutes. These agents leverage state-of-the-art
vision-language models (VLMs), large language models (LLMs), and geometric deep
learning techniques, providing rapid iteration and comprehensive design
exploration capabilities. We ground our methodology in industry-standard
benchmarks, encompassing a wide variety of conventional automotive designs, and
utilize high-fidelity aerodynamic simulations to ensure practical and
applicable outcomes. Furthermore, we present design agents that can swiftly and
accurately predict simulation outcomes, empowering engineers and designers to
engage in more informed design optimization and exploration. This research
underscores the transformative potential of integrating advanced generative AI
techniques into complex engineering tasks, paving the way for broader adoption
and innovation across multiple engineering disciplines.


## Cocktail Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference

>Authors: Wei Tao, Bin Zhang, Xiaoyang Qu, Jiguang Wan, Jianzong Wang

>2025-03-30

> http://arxiv.org/abs/2503.23294v1

Recently, large language models (LLMs) have been able to handle longer and
longer contexts. However, a context that is too long may cause intolerant
inference latency and GPU memory usage. Existing methods propose
mixed-precision **quantization** to the key-value (**KV**) cache in LLMs based on token
granularity, which is time-consuming in the search process and hardware
inefficient during computation. This paper introduces a novel approach called
Cocktail, which employs chunk-adaptive mixed-precision **quantization** to optimize
the **KV** cache. Cocktail consists of two modules: chunk-level **quantization** search
and chunk-level **KV** cache computation. Chunk-level **quantization** search
determines the optimal bitwidth configuration of the **KV** cache chunks quickly
based on the similarity scores between the corresponding context chunks and the
query, maintaining the model accuracy. Furthermore, chunk-level **KV** cache
computation reorders the **KV** cache chunks before **quantization**, avoiding the
hardware inefficiency caused by mixed-precision **quantization** in inference
computation. Extensive experiments demonstrate that Cocktail outperforms
state-of-the-art **KV** cache **quantization** methods on various models and datasets.


## SketchVideo Sketch-based Video Generation and Editing

>Authors: Feng-Lin Liu, Hongbo Fu, Xintao Wang, Weicai Ye, Pengfei Wan, Di Zhang, Lin Gao

>2025-03-30

> http://arxiv.org/abs/2503.23284v1

Video generation and editing conditioned on text prompts or images have
undergone significant advancements. However, challenges remain in accurately
controlling global layout and geometry details solely by texts, and supporting
motion control and local modification through images. In this paper, we aim to
achieve sketch-based spatial and motion control for video generation and
support fine-grained editing of real or synthetic videos. Based on the DiT
video generation model, we propose a memory-efficient control structure with
sketch control blocks that predict residual features of skipped DiT blocks.
Sketches are drawn on one or two keyframes (at arbitrary time points) for easy
interaction. To propagate such temporally **sparse** sketch conditions across all
frames, we propose an inter-frame attention mechanism to analyze the
relationship between the keyframes and each video frame. For sketch-based video
editing, we design an additional video insertion module that maintains
consistency between the newly edited content and the original video's spatial
feature and dynamic motion. During inference, we use latent fusion for the
accurate preservation of unedited regions. Extensive experiments demonstrate
that our SketchVideo achieves superior performance in controllable video
generation and editing.


## Vacuum polarization current in presence of intense Sauter field

>Authors: Deepak Sah, Manoranjan P. Singh

>2025-03-29

> http://arxiv.org/abs/2503.23232v1

The quantum vacuum becomes unstable under an external field, leading to
spontaneous particle-antiparticle pair creation. In canonical **quantization**, the
time-dependent particle number, defined via Bogoliubov transformations lacks
physical meaning until the external field vanishes. To address this, we explore
dynamical quantities that remain well-defined at both asymptotic and
intermediate times, focusing on the vacuum polarization current. Investigating
this observable provides insights into the system's intermediate-time behavior.
We consider pair creation in a spatially homogeneous, time-dependent, intense
Sauter field. Specifically, we analyze the real and imaginary parts of the
correlation function, linking them to vacuum polarization effects. The vacuum
polarization current in an intense laser pulse is computed numerically,
revealing that it correlates with the real part of the correlation function.
Initially, the current changes sign and gradually decreases, but unlike the
particle number, it does not reach a constant asymptotic value. Instead, for
large times, it exhibits nearly undamped oscillations, a distinctive feature of
scalar particles, oscillating strongly around zero. Additionally, we explore
the uniqueness of the vacuum polarization current in the adiabatic basis,
comparing different reference mode function choices. Notably, we find that the
current remains independent of the basis choice.


## TRA Better Length Generalisation with Threshold Relative Attention

>Authors: Mattia Opper, Roland Fernandez, Paul Smolensky, Jianfeng Gao

>2025-03-29

> http://arxiv.org/abs/2503.23174v2

Transformers struggle with length generalisation, displaying poor performance
even on basic tasks. We test whether these limitations can be explained through
two key failures of the self-attention mechanism. The first is the inability to
fully remove irrelevant information. The second is tied to position, even if
the dot product between a key and query is highly negative (i.e. an irrelevant
key) learned positional biases may unintentionally up-weight such information -
dangerous when distances become out of distribution. Put together, these two
failure cases lead to compounding generalisation difficulties. We test whether
they can be mitigated through the combination of a) selective **sparsity** -
completely removing irrelevant keys from the attention softmax and b)
contextualised relative distance - distance is only considered as between the
query and the keys that matter. We show how refactoring the attention mechanism
with these two mitigations in place can substantially improve generalisation
capabilities of decoder only transformers.


## Reasoning-SQL Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL

>Authors: Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan "O. Arik

>2025-03-29

> http://arxiv.org/abs/2503.23157v2

Text-to-SQL is a challenging task involving multiple reasoning-intensive
subtasks, including natural language understanding, database schema
comprehension, and precise SQL query formulation. Existing approaches often
rely on handcrafted reasoning paths with inductive biases that can limit their
overall effectiveness. Motivated by the recent success of reasoning-enhanced
models such as DeepSeek R1 and OpenAI o1, which effectively leverage
reward-driven self-exploration to enhance reasoning capabilities and
generalization, we propose a novel set of partial rewards tailored specifically
for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,
n-gram similarity, and syntax check, explicitly designed to address the reward
**sparsity** issue prevalent in reinforcement learning (RL). Leveraging group
relative policy optimization (GRPO), our approach explicitly encourages large
language models (LLMs) to develop intrinsic reasoning skills necessary for
accurate SQL query generation. With models of different sizes, we demonstrate
that RL-only training with our proposed rewards consistently achieves higher
accuracy and superior generalization compared to supervised fine-tuning (SFT).
Remarkably, our RL-trained 14B-parameter model significantly outperforms larger
proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD
benchmark. These highlight the efficacy of our proposed RL-training framework
with partial rewards for enhancing both accuracy and reasoning capabilities in
Text-to-SQL tasks.


## SupertonicTTS Towards Highly Scalable and Efficient Text-to-Speech System

>Authors: Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee

>2025-03-29

> http://arxiv.org/abs/2503.23108v1

We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for
improved scalability and efficiency in speech synthesis. SupertonicTTS is
comprised of three components: a speech autoencoder for continuous latent
representation, a text-to-latent module leveraging flow-matching for
text-to-latent mapping, and an utterance-level duration predictor. To enable a
lightweight architecture, we employ a low-dimensional latent space, temporal
compression of latents, and ConvNeXt blocks. We further simplify the TTS
pipeline by operating directly on raw character-level text and employing
cross-attention for text-speech alignment, thus eliminating the need for
grapheme-to-phoneme (G2P) modules and external aligners. In addition, we
introduce context-sharing batch expansion that accelerates loss convergence and
stabilizes text-speech alignment. Experimental results demonstrate that
SupertonicTTS achieves competitive performance while significantly reducing
architectural complexity and computational overhead compared to contemporary
TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are
available at: https://supertonictts.github.io/.


## RL2Grid Benchmarking Reinforcement Learning in Power Grid Operations

>Authors: Enrico Marchesini, Benjamin Donnot, Constance Crozier, Ian Dytham, Christian Merz, Lars Schewe, Nico Westerbeck, Cathy Wu, Antoine Marot, Priya L. Donti

>2025-03-29

> http://arxiv.org/abs/2503.23101v1

Reinforcement learning (RL) can transform power grid operations by providing
adaptive and scalable controllers essential for grid decarbonization. However,
existing methods struggle with the complex dynamics, aleatoric uncertainty,
long-horizon goals, and hard physical constraints that occur in real-world
systems. This paper presents RL2Grid, a benchmark designed in collaboration
with power system operators to accelerate progress in grid control and foster
RL maturity. Built on a power simulation framework developed by RTE France,
RL2Grid standardizes tasks, state and action spaces, and reward structures
within a unified interface for a systematic evaluation and comparison of RL
approaches. Moreover, we integrate real control heuristics and safety
constraints informed by the operators' expertise to ensure RL2Grid aligns with
grid operation requirements. We benchmark popular RL baselines on the grid
control tasks represented within RL2Grid, establishing reference performance
metrics. Our results and discussion highlight the challenges that power grids
pose for RL methods, emphasizing the need for novel algorithms capable of
handling real-world physical systems.


## A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support

>Authors: Yichun Feng, Jiawei Wang, Ruikun He, Lu Zhou, Yixue Li

>2025-03-29

> http://arxiv.org/abs/2503.23029v1

Knowledge graphs and large language models (LLMs) are key tools for
biomedical knowledge integration and reasoning, facilitating structured
organization of scientific articles and discovery of complex semantic
relationships. However, current methods face challenges: knowledge graph
construction is limited by complex terminology, data heterogeneity, and rapid
knowledge evolution, while LLMs show limitations in retrieval and reasoning,
making it difficult to uncover cross-document associations and reasoning
pathways. To address these issues, we propose a pipeline that uses LLMs to
construct a biomedical knowledge graph (BioStrataKG) from large-scale articles
and builds a cross-document question-answering dataset (BioCDQA) to evaluate
latent knowledge retrieval and multi-hop reasoning. We then introduce
Integrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance
retrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall
through Integrated Reasoning-based Retrieval and refines knowledge via
Progressive Reasoning-based Generation, using self-reflection to achieve deep
thinking and precise contextual understanding. Experiments show that IP-RAR
improves document retrieval F1 score by 20\% and answer generation accuracy by
25\% over existing methods. This framework helps doctors efficiently integrate
treatment evidence for personalized medication plans and enables researchers to
analyze advancements and research gaps, accelerating scientific discovery and
decision-making.


## Federated Semantic Learning for Privacy-preserving Cross-domain Recommendation

>Authors: Ziang Lu, Lei Guo, Xu Yu, Zhiyong Cheng, Xiaohui Han, Lei Zhu

>2025-03-29

> http://arxiv.org/abs/2503.23026v1

In the evolving landscape of recommender systems, the challenge of
effectively conducting privacy-preserving Cross-Domain Recommendation (CDR),
especially under strict non-overlapping constraints, has emerged as a key
focus. Despite extensive research has made significant progress, several
limitations still exist: 1) Previous semantic-based methods fail to deeply
exploit rich textual information, since they **quantize** the text into codes,
losing its original rich semantics. 2) The current solution solely relies on
the text-modality, while the synergistic effects with the ID-modality are
ignored. 3) Existing studies do not consider the impact of irrelevant semantic
features, leading to inaccurate semantic representation. To address these
challenges, we introduce federated semantic learning and devise FFMSR as our
solution. For Limitation 1, we locally learn items'semantic encodings from
their original texts by a multi-layer semantic encoder, and then cluster them
on the server to facilitate the transfer of semantic knowledge between domains.
To tackle Limitation 2, we integrate both ID and Text modalities on the
clients, and utilize them to learn different aspects of items. To handle
Limitation 3, a Fast Fourier Transform (FFT)-based filter and a gating
mechanism are developed to alleviate the impact of irrelevant semantic
information in the local model. We conduct extensive experiments on two
real-world datasets, and the results demonstrate the superiority of our FFMSR
method over other SOTA methods. Our source codes are publicly available at:
https://github.com/Sapphire-star/FFMSR.


## DAT Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation

>Authors: Hsin-Ling Hsu, Jengnan Tzeng

>2025-03-29

> http://arxiv.org/abs/2503.23013v1

Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG) systems
enhance information retrieval by combining dense and **sparse** (e.g., BM25-based)
retrieval methods. However, existing approaches struggle with adaptability, as
fixed weighting schemes fail to adjust to different queries. To address this,
we propose DAT (Dynamic Alpha Tuning), a novel hybrid retrieval framework that
dynamically balances dense retrieval and BM25 for each query. DAT leverages a
large language model (LLM) to evaluate the effectiveness of the top-1 results
from both retrieval methods, assigning an effectiveness score to each. It then
calibrates the optimal weighting factor through effectiveness score
normalization, ensuring a more adaptive and query-aware weighting between the
two approaches. Empirical results show that DAT consistently significantly
outperforms fixed-weighting hybrid retrieval methods across various evaluation
metrics. Even on smaller models, DAT delivers strong performance, highlighting
its efficiency and adaptability.


## Multimodal machine learning with large language embedding model for polymer property prediction

>Authors: Tianren Zhang, Dai-Bei Yang

>2025-03-29

> http://arxiv.org/abs/2503.22962v1

Contemporary large language models (LLMs), such as GPT-4 and Llama, have
harnessed extensive computational power and diverse text corpora to achieve
remarkable proficiency in interpreting and generating domain-specific content,
including materials science. To leverage the domain knowledge embedded within
these models, we propose a simple yet effective multimodal architecture,
PolyLLMem, which integrates text embeddings generated by Llama 3 with molecular
structure embeddings derived from Uni-Mol, for polymer properties prediction
tasks. In our model, Low-rank adaptation (LoRA) layers were also incorporated
during the property prediction tasks to refine the embeddings based on our
limited polymer dataset, thereby enhancing their chemical relevance for polymer
SMILES representation. This balanced fusion of fine-tuned textual and
structural information enables PolyLLMem to accurately predict a variety of
polymer properties despite the scarcity of training data. Its performance is
comparable to, and in some cases exceeds, that of graph-based models, as well
as transformer-based models that typically require pretraining on millions of
polymer samples. These findings demonstrate that LLM, such as Llama, can
effectively capture chemical information encoded in polymer PSMILES, and
underscore the efficacy of multimodal fusion of LLM embeddings and molecular
structure embeddings in overcoming data scarcity and accelerating the discovery
of advanced polymeric materials.


## Adaptive Interactive Navigation of Quadruped Robots using Large Language Models

>Authors: Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu

>2025-03-29

> http://arxiv.org/abs/2503.22942v1

Robotic navigation in complex environments remains a critical research
challenge. Traditional navigation methods focus on optimal trajectory
generation within free space, struggling in environments lacking viable paths
to the goal, such as disaster zones or cluttered warehouses. To address this
gap, we propose an adaptive interactive navigation approach that proactively
interacts with environments to create feasible paths to reach originally
unavailable goals. Specifically, we present a primitive tree for task planning
with large language models (LLMs), facilitating effective reasoning to
determine interaction objects and sequences. To ensure robust subtask
execution, we adopt reinforcement learning to pre-train a comprehensive skill
library containing versatile locomotion and interaction behaviors for motion
planning. Furthermore, we introduce an adaptive replanning method featuring two
LLM-based modules: an advisor serving as a flexible replanning trigger and an
arborist for autonomous plan adjustment. Integrated with the tree structure,
the replanning mechanism allows for convenient node addition and **pruning**,
enabling rapid plan modification in unknown environments. Comprehensive
simulations and experiments have demonstrated our method's effectiveness and
adaptivity in diverse scenarios. The supplementary video is available at page:
https://youtu.be/W5ttPnSap2g.


## SSM-RDU A Reconfigurable Dataflow Unit for Long-Sequence State-Space Models

>Authors: Sho Ko

>2025-03-29

> http://arxiv.org/abs/2503.22937v1

Long-sequence state-space models (SSMs) such as Hyena and Mamba replace the
quadratic complexity of self-attention with more efficient FFT and scan
operations. However, modern accelerators like GPUs are poorly suited to these
non-GEMM workloads due to rigid execution models and specialization for dense
matrix operations. This paper proposes architectural extensions to a baseline
Reconfigurable Dataflow Unit (RDU) that efficiently support FFT-based and
scan-based SSMs. By introducing lightweight interconnect enhancements within
compute tiles, the extended RDU enables spatial mapping of FFT and scan
dataflows with less than 1% area and power overhead. The resulting architecture
achieves a 5.95X speedup over the GPU and a 1.95X speedup over the baseline RDU
for Hyena, and a 2.12X and 1.75X speedup over the GPU and baseline RDU,
respectively, for Mamba.


## Token-Driven GammaTune Adaptive Calibration for Enhanced Speculative Decoding

>Authors: Aayush Gautam, Susav Shrestha, Narasimha Reddy

>2025-03-28

> http://arxiv.org/abs/2504.00030v2

Speculative decoding accelerates large language model (LLM) inference by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, selecting an optimal speculation length is
critical for maximizing speedup while minimizing wasted computation. We
introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive
algorithms that dynamically adjust speculation length based on token acceptance
rates using a heuristic-based switching mechanism. Evaluated on SpecBench
across multiple tasks and model pairs, our method outperforms other
heuristic-based approaches and fixed-length speculative decoding, achieving an
average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%)
with \textit{GammaTune+}, while reducing performance variance. This makes
\textit{GammaTune} a robust and efficient solution for real-world deployment.


## Quamba2 A Robust and Scalable Post-training Quantization Framework for Selective State Space Models

>Authors: Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu

>2025-03-28

> http://arxiv.org/abs/2503.22879v2

State Space Models (SSMs) are emerging as a compelling alternative to
Transformers because of their consistent memory usage and high performance.
Despite this, scaling up SSMs on cloud services or limited-resource devices is
challenging due to their storage requirements and computational power. To
overcome this, quantizing SSMs with low bit-width data formats can reduce model
size and benefit from hardware **acceleration**. As SSMs are prone to
**quantization**-induced errors, recent efforts have focused on optimizing a
particular model or bit-width for efficiency without sacrificing performance.
However, distinct bit-width configurations are essential for different
scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for
enhancing generation speed in short prompt applications for a single user. To
this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both
Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment
on various platforms. Based on the channel order preserving and activation
persistence of SSMs, we propose an offline approach to **quantize** inputs of a
linear recurrence in 8-bit by sorting and clustering for input $x$, combined
with a per-state-group **quantization** for input-dependent parameters $B$ and $C$.
To ensure compute-invariance in the SSM output, we rearrange weights offline
according to the clustering sequence. The experiments show that Quamba2-8B
outperforms several state-of-the-art SSM **quantization** methods and delivers
1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages,
respectively, while offering 4$\times$ memory reduction with only a $1.6\%$
average accuracy drop. The evaluation on MMLU shows the generalizability and
robustness of our framework. The code and **quantize**d models will be released at:
https://github.com/enyac-group/Quamba.


## A Pilot Study on Tunable Precision Emulation via Automatic BLAS Offloading

>Authors: Hang Liu, Junjie Li, Yinzhi Wang

>2025-03-28

> http://arxiv.org/abs/2503.22875v2

This study explores the use of automatic BLAS offloading and INT8-based
emulation for accelerating traditional HPC workloads on modern GPU
architectures. Through the use of **low-bit**width integer units and cache-coherent
Unified Memory Architecture, we emulate double-precision matrix multiplications
in the MuST application without code changes. We find that accuracy depends on
both arithmetic precision and the properties of the operator, which can be
dealt with through tunable precision emulation. Unlike traditional
mixed-precision approaches, this method preserves original algorithms while
optimizing hardware utilization. We showcases the potential of improving
accuracy and performance at the same time. This work highlights the potential
of AI-driven hardware to transform HPC, advocating for adaptive precision
strategies in future scientific computing.


## MCRB for Parameter Estimation from One-Bit Quantized and Oversampled Measurements

>Authors: Nadav E. Rosenthal, Joseph Tabrikian

>2025-03-28

> http://arxiv.org/abs/2503.22860v1

One-bit **quantization** has garnered significant attention in recent years for
various signal processing and communication applications. Estimating model
parameters from one bit **quantize**d data can be challenging, particularly when
the **quantization** process is explicitly accounted for in the estimator. In many
cases, the estimator disregards **quantization** effects, leading to model
misspecification. Consequently, estimation errors arise from both **quantization**
and misspecification. Traditional performance bounds, such as the Cramer-Rao
bound (CRB), fail to capture the impact of misspecification on estimation
performance. To address this limitation, we derive the misspecified CRB (MCRB)
for parameter estimation in a **quantize**d data model consisting of a signal
component in additive Gaussian noise. We apply this bound to
direction-of-arrival estimation using **quantize**d measurements from a sensor
array and to frequency estimation with oversampled **quantize**d data. The
simulations show that the MCRB is asymptotically achieved by the
mean-squared-error of the misspecified maximum-likelihood estimator. Our
results demonstrate that, unlike in finely **quantize**d scenarios, oversampling
can significantly enhance the estimation performance in the presence of
misspecified one-bit **quantize**d measurements.


## DiTFastAttnV2 Head-wise Attention Compression for Multi-Modality Diffusion Transformers

>Authors: Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen Yibo Fan, Shengen Yan, Guohao Dai, Yu Wang

>2025-03-28

> http://arxiv.org/abs/2503.22796v1

Text-to-image generation models, especially Multimodal Diffusion Transformers
(MMDiT), have shown remarkable progress in generating high-quality images.
However, these models often face significant computational bottlenecks,
particularly in attention mechanisms, which hinder their scalability and
efficiency. In this paper, we introduce DiTFastAttnV2, a post-training
compression method designed to accelerate attention in MMDiT. Through an
in-depth analysis of MMDiT's attention patterns, we identify key differences
from prior DiT-based methods and propose head-wise arrow attention and caching
mechanisms to dynamically adjust attention heads, effectively bridging this
gap. We also design an Efficient Fused Kernel for further **acceleration**. By
leveraging local metric methods and optimization techniques, our approach
significantly reduces the search time for optimal compression schemes to just
minutes while maintaining generation quality. Furthermore, with the customized
kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x
end-to-end speedup on 2K image generation without compromising visual fidelity.


## Unicorn Text-Only Data Synthesis for Vision Language Model Training

>Authors: Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, Donglin Wang

>2025-03-28

> http://arxiv.org/abs/2503.22655v1

Training vision-language models (VLMs) typically requires large-scale,
high-quality image-text pairs, but collecting or synthesizing such data is
costly. In contrast, text data is abundant and inexpensive, prompting the
question: can high-quality multimodal training data be synthesized purely from
text? To tackle this, we propose a cross-integrated three-stage multimodal data
synthesis framework, which generates two datasets: Unicorn-1.2M and
Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we
construct 1.2M semantically diverse high-quality captions by expanding **sparse**
caption seeds using large language models (LLMs). In Stage 2:
Instruction-Tuning Data Generation, we further process 471K captions into
multi-turn instruction-tuning tasks to support complex reasoning. Finally, in
Stage 3: Modality Representation Transfer, these textual captions
representations are transformed into visual representations, resulting in
diverse synthetic image representations. This three-stage process enables us to
construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for
instruction-tuning, without relying on real images. By eliminating the
dependency on real images while maintaining data quality and diversity, our
framework offers a cost-effective and scalable solution for VLMs training. Code
is available at https://github.com/Yu-xm/Unicorn.git.


## Learnable cut flow

>Authors: Jing Li, Hao Sun

>2025-03-28

> http://arxiv.org/abs/2503.22498v1

Neural networks have emerged as a powerful paradigm for tasks in high energy
physics, yet their opaque training process renders them as a black box. In
contrast, the traditional cut flow method offers simplicity and
interpretability but demands human effort to identify optimal boundaries. To
merge the strengths of both approaches, we propose the Learnable Cut Flow
(LCF), a neural network that transforms the traditional cut selection into a
fully differentiable, data-driven process. LCF implements two cut
strategies-parallel, where observable distributions are treated independently,
and sequential, where prior cuts shape subsequent ones-to flexibly determine
optimal boundaries. Building on this, we introduce the Learnable Importance, a
metric that quantifies feature importance and adjusts their contributions to
the loss accordingly, offering model-driven insights unlike ad-hoc metrics. To
ensure differentiability, a modified loss function replaces hard cuts with mask
operations, preserving data shape throughout the training process. LCF is
tested on six varied mock datasets and a realistic diboson vs. QCD dataset.
Results demonstrate that LCF (1) accurately learns cut boundaries across
typical feature distributions in both parallel and sequential strategies, (2)
assigns higher importance to discriminative features with minimal overlap, (3)
handles redundant or correlated features robustly, and (4) performs effectively
in real-world scenarios. In diboson dataset, LCF initially underperforms
boosted decision trees and multiplayer perceptrons when using all observables.
However, **pruning** less critical features-guided by learned importance-boosts its
performance to match or exceed these baselines. LCF bridges the gap between
traditional cut flow method and modern black-box neural networks, delivering
actionable insights into the training process and feature importance.


## Movable Antenna Enhanced Downlink Multi-User Integrated Sensing and Communication System

>Authors: Yanze Han, Min Li, Xingyu Zhao, Ming-Min Zhao, Min-Jian Zhao

>2025-03-28

> http://arxiv.org/abs/2503.22486v1

This work investigates the potential of exploiting movable antennas (MAs) to
enhance the performance of a multi-user downlink integrated sensing and
communication (ISAC) system. Specifically, we formulate an optimization problem
to maximize the transmit beampattern gain for sensing while simultaneously
meeting each user's communication requirement by jointly optimizing antenna
positions and beamforming design. The problem formulated is highly non-convex
and involves multivariate-coupled constraints. To address these challenges, we
introduce a series of auxiliary random variables and transform the original
problem into an augmented Lagrangian problem. A double-loop algorithm based on
a penalty dual decomposition framework is then developed to solve the problem.
Numerical results validate the effectiveness of the proposed design,
demonstrating its superiority over MA designs based on successive convex
approximation optimization and other baseline approaches in ISAC systems. The
results also highlight the advantages of MAs in achieving better sensing
performance and improved beam control, especially for **sparse** arrays with large
apertures.


## Missing Components in ΛCDM from DESI Y1 BAO Measurements Insights from Redshift Remapping

>Authors: E. Fernández-García, R. Wojtak, F. Prada, J. L. Cervantes-Cota, O. Alves, G. Valogiannis, J. Aguilar, S. Ahlen, S. BenZvi, D. Bianchi, D. Brooks, T. Claybaugh, A. de la Macorra, P. Doel, E. Gaztañaga, S. Gontcho A Gontcho, G. Gutierrez, K. Honscheid, M. Ishak, S. Juneau, D. Kirkby, T. Kisner, M. Landriau, L. Le Guillou, M. E. Levi, T. S. Li, M. Manera, A. Meisner, R. Miquel, J. Moustakas, A. Muñoz-Gutiérrez, S. Nadathur, W. J. Percival, I. Pérez-Ràfols, G. Rossi, E. Sanchez, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, B. A. Weaver, H. Zou

>2025-03-28

> http://arxiv.org/abs/2503.22469v1

We explore transformations of the Friedman-Lema\^itre-Robertson-Walker (FLRW)
metric and cosmological parameters that align with observational data, aiming
to gain insights into potential extensions of standard cosmological models.
  We modify the FLRW metric by introducing a scaling factor, $e^{2\Theta(a)}$
(the cosmological scaling function, CSF), which alters the standard
relationship between cosmological redshift and the cosmic scale factor without
affecting angular measurements or Cosmic Microwave Background (CMB)
anisotropies. Using data from DESI Year 1, Pantheon+ supernovae, and the Planck
CMB temperature power spectrum, we constrain both the CSF and cosmological
parameters through a Markov Chain Monte Carlo approach.
  Our results indicate that the CSF model fits observational data with a lower
Hubble constant (although it is compatible with the value given by Planck 2018
within 1$\sigma$) and is predominantly dark-matter-dominated. Additionally, the
CSF model produces temperature and lensing power spectra similar to those
predicted by the standard model, though with lower values in the CSF model at
large scales. We have also checked that when fitting a CSF model without dark
energy to the data, we obtain a more negative conformal function. This suggests
that the CSF model may offer hints about missing elements and opens up a new
avenue for exploring physical interpretations of cosmic **acceleration**.


## STADE Standard Deviation as a Pruning Metric

>Authors: Diego Coello de Portugal Mecke, Haya Alyoussef, Ilia Koloiarov, Maximilian Stubbemann, Lars Schmidt-Thieme

>2025-03-28

> http://arxiv.org/abs/2503.22451v1

Recently, Large Language Models (LLMs) have become very widespread and are
used to solve a wide variety of tasks. To successfully handle these tasks, LLMs
require longer training times and larger model sizes. This makes LLMs ideal
candidates for **pruning** methods that reduce computational demands while
maintaining performance. Previous methods require a retraining phase after
**pruning** to maintain the original model's performance. However, state-of-the-art
**pruning** methods, such as Wanda, prune the model without retraining, making the
**pruning** process faster and more efficient. Building upon Wanda's work, this
study provides a theoretical explanation of why the method is effective and
leverages these insights to enhance the **pruning** process. Specifically, a
theoretical analysis of the **pruning** problem reveals a common scenario in
Machine Learning where Wanda is the optimal **pruning** method. Furthermore, this
analysis is extended to cases where Wanda is no longer optimal, leading to the
development of a new method, STADE, based on the standard deviation of the
input. From a theoretical standpoint, STADE demonstrates better generality
across different scenarios. Finally, extensive experiments on Llama and Open
Pre-trained Transformers (OPT) models validate these theoretical findings,
showing that depending on the training conditions, Wanda's optimal performance
varies as predicted by the theoretical framework. These insights contribute to
a more robust understanding of **pruning** strategies and their practical
implications. Code is available at: https://github.com/Coello-dev/STADE/


## CoSIL Software Issue Localization via LLM-Driven Code Repository Graph Searching

>Authors: Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, Zhongxin Liu

>2025-03-28

> http://arxiv.org/abs/2503.22424v1

Large language models (LLMs) have significantly advanced autonomous software
engineering, leading to a growing number of software engineering agents that
assist developers in automatic program repair. Issue localization forms the
basis for accurate patch generation. However, because of limitations caused by
the context window length of LLMs, existing issue localization methods face
challenges in balancing concise yet effective contexts and adequately
comprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,
simple yet powerful function level issue localization method without training
or indexing. CoSIL reduces the search space through module call graphs,
iteratively searches the function call graph to obtain relevant contexts, and
uses context **pruning** to control the search direction and manage contexts
effectively. Importantly, the call graph is dynamically constructed by the LLM
during search, eliminating the need for pre-parsing. Experiment results
demonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent
and 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using
Qwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When
CoSIL is applied to guide the patch generation stage, the resolved rate further
improves by 9.3 to 31.5 percent.


## GAITGen Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain

>Authors: Vida Adeli, Soroush Mehraban, Majid Mirmehdi, Alan Whone, Benjamin Filtjens, Amirhossein Dadashzadeh, Alfonso Fasano, Andrea Iaboni Babak Taati

>2025-03-28

> http://arxiv.org/abs/2503.22397v1

Gait analysis is crucial for the diagnosis and monitoring of movement
disorders like Parkinson's Disease. While computer vision models have shown
potential for objectively evaluating parkinsonian gait, their effectiveness is
limited by scarce clinical datasets and the challenge of collecting large and
well-labelled data, impacting model accuracy and risk of bias. To address these
gaps, we propose GAITGen, a novel framework that generates realistic gait
sequences conditioned on specified pathology severity levels. GAITGen employs a
Conditional Residual Vector Quantized Variational Autoencoder to learn
disentangled representations of motion dynamics and pathology-specific factors,
coupled with Mask and Residual Transformers for conditioned sequence
generation. GAITGen generates realistic, diverse gait sequences across severity
levels, enriching datasets and enabling large-scale model training in
parkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset
demonstrate that GAITGen outperforms adapted state-of-the-art models in both
reconstruction fidelity and generation quality, accurately capturing critical
pathology-specific gait features. A clinical user study confirms the realism
and clinical relevance of our generated sequences. Moreover, incorporating
GAITGen-generated data into downstream tasks improves parkinsonian gait
severity estimation, highlighting its potential for advancing clinical gait
analysis.


## MASCOTS Model-Agnostic Symbolic COunterfactual explanations for Time Series

>Authors: Dawid Płudowski, Francesco Spinnato, Piotr Wilczyński, Krzysztof Kotowski, Evridiki Vasileia Ntagiou, Riccardo Guidotti, Przemysław Biecek

>2025-03-28

> http://arxiv.org/abs/2503.22389v1

Counterfactual explanations provide an intuitive way to understand model
decisions by identifying minimal changes required to alter an outcome. However,
applying counterfactual methods to time series models remains challenging due
to temporal dependencies, high dimensionality, and the lack of an intuitive
human-interpretable representation. We introduce MASCOTS, a method that
leverages the Bag-of-Receptive-Fields representation alongside symbolic
transformations inspired by Symbolic Aggregate Approximation. By operating in a
symbolic feature space, it enhances interpretability while preserving fidelity
to the original data and model. Unlike existing approaches that either depend
on model structure or autoencoder-based sampling, MASCOTS directly generates
meaningful and diverse counterfactual observations in a model-agnostic manner,
operating on both univariate and multivariate data. We evaluate MASCOTS on
univariate and multivariate benchmark datasets, demonstrating comparable
validity, proximity, and plausibility to state-of-the-art methods, while
significantly improving interpretability and **sparsity**. Its symbolic nature
allows for explanations that can be expressed visually, in natural language, or
through semantic representations, making counterfactual reasoning more
accessible and actionable.


## A Refined Analysis of Massive Activations in LLMs

>Authors: Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian Güra

>2025-03-28

> http://arxiv.org/abs/2503.22329v1

Motivated in part by their relevance for low-precision training and
**quantization**, massive activations in large language models (LLMs) have recently
emerged as a topic of interest. However, existing analyses are limited in
scope, and generalizability across architectures is unclear. This paper helps
address some of these gaps by conducting an analysis of massive activations
across a broad range of LLMs, including both GLU-based and non-GLU-based
architectures. Our findings challenge several prior assumptions, most
importantly: (1) not all massive activations are detrimental, i.e. suppressing
them does not lead to an explosion of perplexity or a collapse in downstream
task performance; (2) proposed mitigation strategies such as Attention **KV** bias
are model-specific and ineffective in certain cases. We consequently
investigate novel hybrid mitigation strategies; in particular pairing Target
Variance Rescaling (TVR) with Attention **KV** bias or Dynamic Tanh (DyT)
successfully balances the mitigation of massive activations with preserved
downstream model performance in the scenarios we investigated. Our code is
available at: https://github.com/bluorion-com/refine_massive_activations.


## Make Some Noise Towards LLM audio reasoning and generation using sound tokens

>Authors: Shivam Mehta, Nebojsa Jojic, Hannes Gamper

>2025-03-28

> http://arxiv.org/abs/2503.22275v1

Integrating audio comprehension and generation into large language models
(LLMs) remains challenging due to the continuous nature of audio and the
resulting high sampling rates. Here, we introduce a novel approach that
combines Variational Quantization with Conditional Flow Matching to convert
audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless
integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM
using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true
multimodal capabilities, i.e., audio comprehension and generation. Our
tokenizer outperforms a traditional VQ-VAE across various datasets with diverse
acoustic events. Despite the substantial loss of fine-grained details through
audio tokenization, our multimodal LLM trained with discrete tokens achieves
competitive results in audio comprehension with state-of-the-art methods,
though audio generation is poor. Our results highlight the need for larger,
more diverse datasets and improved evaluation metrics to advance multimodal LLM
performance.


## FLAM Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation

>Authors: Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao

>2025-03-28

> http://arxiv.org/abs/2503.22249v1

Humanoid robots have attracted significant attention in recent years.
Reinforcement Learning (RL) is one of the main ways to control the whole body
of humanoid robots. RL enables agents to complete tasks by learning from
environment interactions, guided by task rewards. However, existing RL methods
rarely explicitly consider the impact of body stability on humanoid locomotion
and manipulation. Achieving high performance in whole-body control remains a
challenge for RL methods that rely solely on task rewards. In this paper, we
propose a Foundation model-based method for humanoid Locomotion And
Manipulation (FLAM for short). FLAM integrates a stabilizing reward function
with a basic policy. The stabilizing reward function is designed to encourage
the robot to learn stable postures, thereby accelerating the learning process
and facilitating task completion. Specifically, the robot pose is first mapped
to the 3D virtual human model. Then, the human pose is stabilized and
reconstructed through a human motion reconstruction model. Finally, the pose
before and after reconstruction is used to compute the stabilizing reward. By
combining this stabilizing reward with the task reward, FLAM effectively guides
policy learning. Experimental results on a humanoid robot benchmark demonstrate
that FLAM outperforms state-of-the-art RL methods, highlighting its
effectiveness in improving stability and overall performance.


## EdgeInfinite A Memory-Efficient Infinite-Context Transformer for Edge Devices

>Authors: Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, Xiaoxin Chen

>2025-03-28

> http://arxiv.org/abs/2503.22196v1

Transformer-based large language models (LLMs) encounter challenges in
processing long sequences on edge devices due to the quadratic complexity of
attention mechanisms and growing memory demands from Key-Value (**KV**) cache.
Existing **KV** cache optimizations struggle with irreversible token eviction in
long-output tasks, while alternative sequence modeling architectures prove
costly to adopt within established Transformer infrastructure. We present
EdgeInfinite, a memory-efficient solution for infinite contexts that integrates
compressed memory into Transformer-based LLMs through a trainable memory-gating
module. This approach maintains full compatibility with standard Transformer
architectures, requiring fine-tuning only a small part of parameters, and
enables selective activation of the memory-gating module for long and short
context task routing. The experimental result shows that EdgeInfinite achieves
comparable performance to baseline Transformer-based LLM on long context
benchmarks while optimizing memory consumption and time to first token.


## Enhanced Charging in Multi-Battery Systems by Nonreciprocity

>Authors: Hua-Wei Zhao, Yong Xie, Xinyao Huang, Guo-Feng Zhang

>2025-03-28

> http://arxiv.org/abs/2503.22187v1

Quantum batteries (QBs), harnessing quantum systems to transfer and store
energy, have garnered substantial attention recently, enabling potentials in
enhanced charging capacity, increased charging power, and device
miniaturization. However, constrained by the weak interaction between the
quantum nodes, the implementations of QB networks exhibit limited charging
performance. In this work, we propose an efficient approach to improving
charging in multi-battery systems by capitalizing on nonreciprocity. By
constructing non-Hermitian Aharonov-Bohm triangles to establish unidirectional
energy transfer in both cascaded and parallel configurations, we can achieve a
significant enhancement of the stored energy in QBs especially in the weak
interaction regime. Remarkably, the nonreciprocal cascaded setups display an
exponentially increasing gain in the battery energy as the charging distance
lengthens compared to the reciprocal counterparts. Furthermore, we demonstrate
that nonreciprocity can also lead to the same enhancement in the charging power
of QBs, accelerating the charging processes. Our findings provide a practical
pathway for enhancing the charging performance of QBs and exhibit the
potentials for constructing efficient QB networks.


## PharmAgents Building a Virtual Pharma with Large Language Model Agents

>Authors: Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan

>2025-03-28

> http://arxiv.org/abs/2503.22164v2

The discovery of novel small molecule drugs remains a critical scientific
challenge with far-reaching implications for treating diseases and advancing
human health. Traditional drug development--especially for small molecule
therapeutics--is a highly complex, resource-intensive, and time-consuming
process that requires multidisciplinary collaboration. Recent breakthroughs in
artificial intelligence (AI), particularly the rise of large language models
(LLMs), present a transformative opportunity to streamline and accelerate this
process. In this paper, we introduce PharmAgents, a virtual pharmaceutical
ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates
the full drug discovery workflow--from target discovery to preclinical
evaluation--by integrating explainable, LLM-driven agents equipped with
specialized machine learning models and computational tools. Through structured
knowledge exchange and automated optimization, PharmAgents identifies potential
therapeutic targets, discovers promising lead compounds, enhances binding
affinity and key molecular properties, and performs in silico analyses of
toxicity and synthetic feasibility. Additionally, the system supports
interpretability, agent interaction, and self-evolvement, enabling it to refine
future drug designs based on prior experience. By showcasing the potential of
LLM-powered multi-agent systems in drug discovery, this work establishes a new
paradigm for autonomous, explainable, and scalable pharmaceutical research,
with future extensions toward comprehensive drug lifecycle management.


## Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data

>Authors: Ziang Yan, Xingyu Zhao, Hanqing Ma, Wei Chen, Jianpeng Qi, Yanwei Yu, Junyu Dong

>2025-03-28

> http://arxiv.org/abs/2504.01979v1

With the rise of social media and Location-Based Social Networks (LBSN),
check-in data across platforms has become crucial for User Identity Linkage
(UIL). These data not only reveal users' spatio-temporal information but also
provide insights into their behavior patterns and interests. However,
cross-platform identity linkage faces challenges like poor data quality, high
**sparsity**, and noise interference, which hinder existing methods from extracting
cross-platform user information. To address these issues, we propose a
Correlation-Attention Masked Transformer for User Identity Linkage Network
(MT-Link), a transformer-based framework to enhance model performance by
learning spatio-temporal co-occurrence patterns of cross-platform users. Our
model effectively captures spatio-temporal co-occurrence in cross-platform user
check-in sequences. It employs a correlation attention mechanism to detect the
spatio-temporal co-occurrence between user check-in sequences. Guided by
attention weight maps, the model focuses on co-occurrence points while
filtering out noise, ultimately improving classification performance.
Experimental results show that our model significantly outperforms
state-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in
terms of Macro-F1 and Area Under Curve (AUC).


## Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition

>Authors: Akshay Rangamani

>2025-03-28

> http://arxiv.org/abs/2503.22059v1

Modular addition tasks serve as a useful test bed for observing empirical
phenomena in deep learning, including the phenomenon of \emph{grokking}. Prior
work has shown that one-layer transformer architectures learn Fourier
Multiplication circuits to solve modular addition tasks. In this paper, we show
that Recurrent Neural Networks (RNNs) trained on modular addition tasks also
use a Fourier Multiplication strategy. We identify low rank structures in the
model weights, and attribute model components to specific Fourier frequencies,
resulting in a **sparse** representation in the Fourier space. We also show
empirically that the RNN is robust to removing individual frequencies, while
the performance degrades drastically as more frequencies are ablated from the
model.

