# 2024-10-17

# Table of Contents
* [Long-LRM Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats](#Long-LRM-Long-sequence-Large-Reconstruction-Model-for-Wide-coverage-Gaussian-Splats)
* [Tracking Universal Features Through Fine-Tuning and Model Merging](#Tracking-Universal-Features-Through-Fine-Tuning-and-Model-Merging)
* [ERVQ Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs](#ERVQ-Enhanced-Residual-Vector-Quantization-with-Intra-and-Inter-Codebook-Optimization-for-Neural-Audio-Codecs)
* [Irregularity-Informed Time Series Analysis Adaptive Modelling of Spatial and Temporal Dynamics](#Irregularity-Informed-Time-Series-Analysis-Adaptive-Modelling-of-Spatial-and-Temporal-Dynamics)
* [EG-HumanNeRF Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View](#EG-HumanNeRF-Efficient-Generalizable-Human-NeRF-Utilizing-Human-Prior-for-Sparse-View)
* [Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay](#Enhancing-LLM-Agents-for-Code-Generation-with-Possibility-and-Pass-rate-Prioritized-Experience-Replay)
* [Order-Aware Interactive Segmentation](#Order-Aware-Interactive-Segmentation)
* [DAQ Density-Aware Post-Training Weight-Only Quantization For LLMs](#DAQ-Density-Aware-Post-Training-Weight-Only-Quantization-For-LLMs)
* [COMET Towards Partical W4A4KV4 LLMs Serving](#COMET-Towards-Partical-W4A4KV4-LLMs-Serving)
* [Scaling laws for post-training quantized large language models](#Scaling-laws-for-post-training-quantized-large-language-models)
* [MoE-Pruner Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router](#MoE-Pruner-Pruning-Mixture-of-Experts-Large-Language-Model-using-the-Hints-from-Its-Router)
* [DISP-LLM Dimension-Independent Structural Pruning for Large Language Models](#DISP-LLM-Dimension-Independent-Structural-Pruning-for-Large-Language-Models)
* [A Complete Decomposition of KL Error using Refined Information and Mode Interaction Selection](#A-Complete-Decomposition-of-KL-Error-using-Refined-Information-and-Mode-Interaction-Selection)
* [GaVaMoE Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation](#GaVaMoE-Gaussian-Variational-Gated-Mixture-of-Experts-for-Explainable-Recommendation)
* [Layer-wise Importance Matters Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models](#Layer-wise-Importance-Matters-Less-Memory-for-Better-Performance-in-Parameter-efficient-Fine-tuning-of-Large-Language-Models)
* [ED-ViT Splitting Vision Transformer for Distributed Inference on Edge Devices](#ED-ViT-Splitting-Vision-Transformer-for-Distributed-Inference-on-Edge-Devices)
* [It Takes Two to Tango Directly Optimizing for Constrained Synthesizability in Generative Molecular Design](#It-Takes-Two-to-Tango-Directly-Optimizing-for-Constrained-Synthesizability-in-Generative-Molecular-Design)
* [QSpec Speculative Decoding with Complementary Quantization Schemes](#QSpec-Speculative-Decoding-with-Complementary-Quantization-Schemes)
* [Subspace Optimization for Large Language Models with Convergence Guarantees](#Subspace-Optimization-for-Large-Language-Models-with-Convergence-Guarantees)
* [Beyond Linear Approximations A Novel Pruning Approach for Attention Matrix](#Beyond-Linear-Approximations-A-Novel-Pruning-Approach-for-Attention-Matrix)
* [Error Diffusion Post Training Quantization with Block-Scaled Number Formats for Neural Networks](#Error-Diffusion-Post-Training-Quantization-with-Block-Scaled-Number-Formats-for-Neural-Networks)
* [Strings and membranes from $\mathcal{A}$-theory five brane](#Strings-and-membranes-from-$\mathcal{A}$-theory-five-brane)


## Long-LRM Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats

>Authors: Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, Zexiang Xu

>2024-10-16

> http://arxiv.org/abs/2410.12781v1

We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is
capable of reconstructing a large scene from a long sequence of input images.
Specifically, our model can process 32 source images at 960x540 resolution
within only 1.3 seconds on a single A100 80G GPU. Our architecture features a
mixture of the recent Mamba2 blocks and the classical transformer blocks which
allowed many more tokens to be processed than prior work, enhanced by efficient
token merging and Gaussian pruning steps that balance between quality and
efficiency. Unlike previous feed-forward models that are limited to processing
1~4 input images and can only reconstruct a small portion of a large scene,
Long-LRM reconstructs the entire scene in a single feed-forward step. On
large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method
achieves performance comparable to optimization-based approaches while being
two orders of magnitude more efficient. Project page:
https://arthurhero.github.io/projects/llrm


## Tracking Universal Features Through Fine-Tuning and Model Merging

>Authors: Niels Horn, Desmond Elliott

>2024-10-16

> http://arxiv.org/abs/2410.12391v1

We study how features emerge, disappear, and persist across models fine-tuned
on different domains of text. More specifically, we start from a base one-layer
Transformer language model that is trained on a combination of the BabyLM
corpus, and a collection of Python code from The Stack. This base model is
adapted to two new domains of text: TinyStories, and the Lua programming
language, respectively; and then these two models are merged using these two
models using spherical linear interpolation. Our exploration aims to provide
deeper insights into the stability and transformation of features across
typical transfer-learning scenarios using small-scale models and sparse
auto-encoders.


## ERVQ Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs

>Authors: Rui-Chen Zheng, Hui-Peng Du, Xiao-Hang Jiang, Yang Ai, Zhen-Hua Ling

>2024-10-16

> http://arxiv.org/abs/2410.12359v1

Current neural audio codecs typically use residual vector quantization (RVQ)
to discretize speech signals. However, they often experience codebook collapse,
which reduces the effective codebook size and leads to suboptimal performance.
To address this problem, we introduce ERVQ, Enhanced Residual Vector
Quantization, a novel enhancement strategy for the RVQ framework in neural
audio codecs. ERVQ mitigates codebook collapse and boosts codec performance
through both intra- and inter-codebook optimization. Intra-codebook
optimization incorporates an online clustering strategy and a code balancing
loss to ensure balanced and efficient codebook utilization. Inter-codebook
optimization improves the diversity of quantized features by minimizing the
similarity between successive quantizations. Our experiments show that ERVQ
significantly enhances audio codec performance across different models,
sampling rates, and bitrates, achieving superior quality and generalization
capabilities. It also achieves 100% codebook utilization on one of the most
advanced neural audio codecs. Further experiments indicate that audio codecs
improved by the ERVQ strategy can improve unified speech-and-text large
language models (LLMs). Specifically, there is a notable improvement in the
naturalness of generated speech in downstream zero-shot text-to-speech tasks.
Audio samples are available here.


## Irregularity-Informed Time Series Analysis Adaptive Modelling of Spatial and Temporal Dynamics

>Authors: Liangwei Nathan Zheng, Zhengyang Li, Chang George Dong, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen

>2024-10-16

> http://arxiv.org/abs/2410.12257v1

Irregular Time Series Data (IRTS) has shown increasing prevalence in
real-world applications. We observed that IRTS can be divided into two
specialized types: Natural Irregular Time Series (NIRTS) and Accidental
Irregular Time Series (AIRTS). Various existing methods either ignore the
impacts of irregular patterns or statically learn the irregular dynamics of
NIRTS and AIRTS data and suffer from limited data availability due to the
sparsity of IRTS. We proposed a novel transformer-based framework for general
irregular time series data that treats IRTS from four views: Locality, Time,
Spatio and Irregularity to motivate the data usage to the highest potential.
Moreover, we design a sophisticated irregularity-gate mechanism to adaptively
select task-relevant information from irregularity, which improves the
generalization ability to various IRTS data. We implement extensive experiments
to demonstrate the resistance of our work to three highly missing ratio
datasets (88.4\%, 94.9\%, 60\% missing value) and investigate the significance
of the irregularity information for both NIRTS and AIRTS by additional ablation
study. We release our implementation in
https://github.com/IcurasLW/MTSFormer-Irregular_Time_Series.git


## EG-HumanNeRF Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View

>Authors: Zhaorong Wang, Yoshihiro Kanamori, Yuki Endo

>2024-10-16

> http://arxiv.org/abs/2410.12242v1

Generalizable neural radiance field (NeRF) enables neural-based digital human
rendering without per-scene retraining. When combined with human prior
knowledge, high-quality human rendering can be achieved even with sparse input
views. However, the inference of these methods is still slow, as a large number
of neural network queries on each ray are required to ensure the rendering
quality. Moreover, occluded regions often suffer from artifacts, especially
when the input views are sparse. To address these issues, we propose a
generalizable human NeRF framework that achieves high-quality and real-time
rendering with sparse input views by extensively leveraging human prior
knowledge. We accelerate the rendering with a two-stage sampling reduction
strategy: first constructing boundary meshes around the human geometry to
reduce the number of ray samples for sampling guidance regression, and then
volume rendering using fewer guided samples. To improve rendering quality,
especially in occluded regions, we propose an occlusion-aware attention
mechanism to extract occlusion information from the human priors, followed by
an image space refinement network to improve rendering quality. Furthermore,
for volume rendering, we adopt a signed ray distance function (SRDF)
formulation, which allows us to propose an SRDF loss at every sample position
to improve the rendering quality further. Our experiments demonstrate that our
method outperforms the state-of-the-art methods in rendering quality and has a
competitive rendering speed compared with speed-prioritized novel view
synthesis methods.


## Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay

>Authors: Yuyang Chen, Kaiyan Zhao, Yiming Wang, Ming Yang, Jian Zhang, Xiaoguang Niu

>2024-10-16

> http://arxiv.org/abs/2410.12236v1

Nowadays transformer-based Large Language Models (LLM) for code generation
tasks usually apply sampling and filtering pipelines. Due to the sparse reward
problem in code generation tasks caused by one-token incorrectness,
transformer-based models will sample redundant programs till they find a
correct one, leading to low efficiency. To overcome the challenge, we
incorporate Experience Replay (ER) in the fine-tuning phase, where codes and
programs produced are stored and will be replayed to give the LLM agent a
chance to learn from past experiences. Based on the spirit of ER, we introduce
a novel approach called BTP pipeline which consists of three phases: beam
search sampling, testing phase, and prioritized experience replay phase. The
approach makes use of failed programs collected by code models and replays
programs with high Possibility and Pass-rate Prioritized value (P2Value) from
the replay buffer to improve efficiency. P2Value comprehensively considers the
possibility of transformers' output and pass rate and can make use of the
redundant resources caused by the problem that most programs collected by LLMs
fail to pass any tests. We empirically apply our approach in several LLMs,
demonstrating that it enhances their performance in code generation tasks and
surpasses existing baselines.


## Order-Aware Interactive Segmentation

>Authors: Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, Ziyan Wu

>2024-10-16

> http://arxiv.org/abs/2410.12214v1

Interactive segmentation aims to accurately segment target objects with
minimal user interactions. However, current methods often fail to accurately
separate target objects from the background, due to a limited understanding of
order, the relative depth between objects in a scene. To address this issue, we
propose OIS: order-aware interactive segmentation, where we explicitly encode
the relative depth between objects into order maps. We introduce a novel
order-aware attention, where the order maps seamlessly guide the user
interactions (in the form of clicks) to attend to the image features. We
further present an object-aware attention module to incorporate a strong
object-level understanding to better differentiate objects with similar order.
Our approach allows both dense and sparse integration of user clicks, enhancing
both accuracy and efficiency as compared to prior works. Experimental results
demonstrate that OIS achieves state-of-the-art performance, improving mIoU
after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset
as compared to the previous state-of-the-art SegNext, while also doubling
inference speed compared to current leading methods. The project page is
https://ukaukaaaa.github.io/projects/OIS/index.html


## DAQ Density-Aware Post-Training Weight-Only Quantization For LLMs

>Authors: Yingsong Luo, Ling Chen

>2024-10-16

> http://arxiv.org/abs/2410.12187v1

Large language models (LLMs) excel in various tasks but face deployment
challenges due to hardware constraints. We propose density-aware post-training
weight-only quantization (DAQ), which has two stages: 1) density-centric
alignment, which identifies the center of high-density weights and centers the
dynamic range on this point to align high-density weight regions with
floating-point high-precision regions; 2) learnable dynamic range adjustment,
which adjusts the dynamic range by optimizing quantization parameters (i.e.,
scale and zero-point) based on the impact of weights on the model output.
Experiments on LLaMA and LLaMA-2 show that DAQ consistently outperforms the
best baseline method, reducing perplexity loss by an average of 22.8% on LLaMA
and 19.6% on LLaMA-2. Our code is available at
https://anonymous.4open.science/r/DAQ-E747.


## COMET Towards Partical W4A4KV4 LLMs Serving

>Authors: Lian Liu, Haimeng Ren, Long Cheng, Zhaohui Xu, Yudong Pan, Mengdi Wang, Xiaowei Li, Yinhe Han, Ying Wang

>2024-10-16

> http://arxiv.org/abs/2410.12168v1

Quantization is a widely-used compression technology to reduce the overhead
of serving large language models (LLMs) on terminal devices and in cloud data
centers. However, prevalent quantization methods, such as 8-bit
weight-activation or 4-bit weight-only quantization, achieve limited
performance improvements due to poor support for low-precision (e.g., 4-bit)
activation. This work, for the first time, realizes practical W4A4KV4 serving
for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the
memory bottleneck caused by the KV cache. Specifically, we propose a novel
fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most
activations into 4-bit with negligible accuracy loss. To support
mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly
optimized W4Ax kernel. Our approach introduces a novel mixed-precision data
layout to facilitate access and fast dequantization for activation and weight
tensors, utilizing the GPU's software pipeline to hide the overhead of data
loading and conversion. Additionally, we propose fine-grained streaming
multiprocessor (SM) scheduling to achieve load balance across different SMs. We
integrate the optimized W4Ax kernel into our inference framework, COMET, and
provide efficient management to support popular LLMs such as LLaMA-3-70B.
Extensive evaluations demonstrate that, when running LLaMA family models on a
single A100-80G-SMX4, COMET achieves a kernel-level speedup of
\textbf{$2.88\times$} over cuBLAS and a \textbf{$2.02 \times$} throughput
improvement compared to TensorRT-LLM from an end-to-end framework perspective.


## Scaling laws for post-training quantized large language models

>Authors: Zifei Xu, Alexander Lan, Wanzin Yazar, Tristan Webb, Sayeh Sharify, Xin Wang

>2024-10-15

> http://arxiv.org/abs/2410.12119v1

Generalization abilities of well-trained large language models (LLMs) are
known to scale predictably as a function of model size. In contrast to the
existence of practical scaling laws governing pre-training, the quality of LLMs
after post-training compression remains highly unpredictable, often requiring
case-by-case validation in practice. In this work, we attempted to close this
gap for post-training weight quantization of LLMs by conducting a systematic
empirical study on multiple LLM families quantized to numerous low-precision
tensor data types using popular weight quantization techniques. We identified
key scaling factors pertaining to characteristics of the local loss landscape,
based on which the performance of quantized LLMs can be reasonably well
predicted by a statistical model.


## MoE-Pruner Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router

>Authors: Yanyue Xie, Zhi Zhang, Ding Zhou, Cong Xie, Ziang Song, Xin Liu, Yanzhi Wang, Xue Lin, An Xu

>2024-10-15

> http://arxiv.org/abs/2410.12013v1

Mixture-of-Experts (MoE) architectures face challenges such as high memory
consumption and redundancy in experts. Pruning MoE can reduce network weights
while maintaining model performance. Motivated by the recent observation of
emergent large magnitude features in Large Language Models (LLM) and MoE
routing policy, we propose MoE-Pruner, a method that prunes weights with the
smallest magnitudes multiplied by the corresponding input activations and
router weights, on each output neuron. Our pruning method is one-shot,
requiring no retraining or weight updates. We evaluate our method on
Mixtral-8x7B and Mixtral-8x22B across multiple language benchmarks.
Experimental results show that our pruning method significantly outperforms
state-of-the-art LLM pruning methods. Furthermore, our pruned MoE models can
benefit from a pretrained teacher model through expert-wise knowledge
distillation, improving performance post-pruning. Experimental results
demonstrate that the Mixtral-8x7B model with 50% sparsity maintains 99% of the
performance of the original model after the expert-wise knowledge distillation.


## DISP-LLM Dimension-Independent Structural Pruning for Large Language Models

>Authors: Shangqian Gao, Chi-Heng Lin, Ting Hua, Tang Zheng, Yilin Shen, Hongxia Jin, Yen-Chang Hsu

>2024-10-15

> http://arxiv.org/abs/2410.11988v1

Large Language Models (LLMs) have achieved remarkable success in various
natural language processing tasks, including language modeling, understanding,
and generation. However, the increased memory and computational costs
associated with these models pose significant challenges for deployment on
resource-limited devices. Structural pruning has emerged as a promising
solution to reduce the costs of LLMs without requiring post-processing steps.
Prior structural pruning methods either follow the dependence of structures at
the cost of limiting flexibility, or introduce non-trivial additional
parameters by incorporating different projection matrices. In this work, we
propose a novel approach that relaxes the constraint imposed by regular
structural pruning methods and eliminates the structural dependence along the
embedding dimension. Our dimension-independent structural pruning method offers
several benefits. Firstly, our method enables different blocks to utilize
different subsets of the feature maps. Secondly, by removing structural
dependence, we facilitate each block to possess varying widths along its input
and output dimensions, thereby significantly enhancing the flexibility of
structural pruning. We evaluate our method on various LLMs, including OPT,
LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our
approach outperforms other state-of-the-art methods, showing for the first time
that structural pruning can achieve an accuracy similar to semi-structural
pruning.


## A Complete Decomposition of KL Error using Refined Information and Mode Interaction Selection

>Authors: James Enouen, Mahito Sugiyama

>2024-10-15

> http://arxiv.org/abs/2410.11964v1

The log-linear model has received a significant amount of theoretical
attention in previous decades and remains the fundamental tool used for
learning probability distributions over discrete variables. Despite its large
popularity in statistical mechanics and high-dimensional statistics, the vast
majority of such energy-based modeling approaches only focus on the
two-variable relationships, such as Boltzmann machines and Markov graphical
models. Although these approaches have easier-to-solve structure learning
problems and easier-to-optimize parametric distributions, they often ignore the
rich structure which exists in the higher-order interactions between different
variables. Using more recent tools from the field of information geometry, we
revisit the classical formulation of the log-linear model with a focus on
higher-order mode interactions, going beyond the 1-body modes of independent
distributions and the 2-body modes of Boltzmann distributions. This perspective
allows us to define a complete decomposition of the KL error. This then
motivates the formulation of a sparse selection problem over the set of
possible mode interactions. In the same way as sparse graph selection allows
for better generalization, we find that our learned distributions are able to
more efficiently use the finite amount of data which is available in practice.
On both synthetic and real-world datasets, we demonstrate our algorithm's
effectiveness in maximizing the log-likelihood for the generative task and also
the ease of adaptability to the discriminative task of classification.


## GaVaMoE Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation

>Authors: Fei Tang, Yongliang Shen, Hang Zhang, Zeqi Tan, Wenqi Zhang, Guiyang Hou, Kaitao Song, Weiming Lu, Yueting Zhuang

>2024-10-15

> http://arxiv.org/abs/2410.11841v1

Large language model-based explainable recommendation (LLM-based ER) systems
show promise in generating human-like explanations for recommendations.
However, they face challenges in modeling user-item collaborative preferences,
personalizing explanations, and handling sparse user-item interactions. To
address these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated
Mixture of Experts framework for explainable recommendation. GaVaMoE introduces
two key components: (1) a rating reconstruction module that employs Variational
Autoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex
user-item collaborative preferences, serving as a pre-trained multi-gating
mechanism; and (2) a set of fine-grained expert models coupled with the
multi-gating mechanism for generating highly personalized explanations. The VAE
component models latent factors in user-item interactions, while the GMM
clusters users with similar behaviors. Each cluster corresponds to a gate in
the multi-gating mechanism, routing user-item pairs to appropriate expert
models. This architecture enables GaVaMoE to generate tailored explanations for
specific user types and preferences, mitigating data sparsity by leveraging
user similarities. Extensive experiments on three real-world datasets
demonstrate that GaVaMoE significantly outperforms existing methods in
explanation quality, personalization, and consistency. Notably, GaVaMoE
exhibits robust performance in scenarios with sparse user-item interactions,
maintaining high-quality explanations even for users with limited historical
data.


## Layer-wise Importance Matters Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models

>Authors: Kai Yao, Penlei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu

>2024-10-15

> http://arxiv.org/abs/2410.11772v1

Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
popularity for adapting pre-trained Large Language Models (LLMs) to downstream
tasks, primarily due to their potential to significantly reduce memory and
computational overheads. However, a common limitation in most PEFT approaches
is their application of a uniform architectural design across all layers. This
uniformity involves identical trainable modules and ignores the varying
importance of each layer, leading to sub-optimal fine-tuning results. To
overcome the above limitation and obtain better performance, we develop a novel
approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent
sparsity and select the most important subset of full layers with effective
layer-wise importance scoring. The proposed IST is a versatile and
plug-and-play technique compatible with various PEFT methods that operate on a
per-layer basis. By leveraging the estimated importance scores, IST dynamically
updates these selected layers in PEFT modules, leading to reduced memory
demands. We further provide theoretical proof of convergence and empirical
evidence of superior performance to demonstrate the advantages of IST over
uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,
and downstream tasks substantiate the effectiveness of our proposed method,
showcasing IST's capacity to enhance existing layer-based PEFT methods. Our
code is available at https://github.com/Kaiseem/IST.


## ED-ViT Splitting Vision Transformer for Distributed Inference on Edge Devices

>Authors: Xiang Liu, Yijun Song, Xia Li, Yifei Sun, Huiying Lan, Zemin Liu, Linshan Jiang, Jialin Li

>2024-10-15

> http://arxiv.org/abs/2410.11650v1

Deep learning models are increasingly deployed on resource-constrained edge
devices for real-time data analytics. In recent years, Vision Transformer
models and their variants have demonstrated outstanding performance across
various computer vision tasks. However, their high computational demands and
inference latency pose significant challenges for model deployment on
resource-constraint edge devices. To address this issue, we propose a novel
Vision Transformer splitting framework, ED-ViT, designed to execute complex
models across multiple edge devices efficiently. Specifically, we partition
Vision Transformer models into several sub-models, where each sub-model is
tailored to handle a specific subset of data classes. To further minimize
computation overhead and inference latency, we introduce a class-wise pruning
technique that reduces the size of each sub-model. We conduct extensive
experiments on five datasets with three model structures, demonstrating that
our approach significantly reduces inference latency on edge devices and
achieves a model size reduction of up to 28.9 times and 34.1 times,
respectively, while maintaining test accuracy comparable to the original Vision
Transformer. Additionally, we compare ED-ViT with two state-of-the-art methods
that deploy CNN and SNN models on edge devices, evaluating accuracy, inference
time, and overall model size. Our comprehensive evaluation underscores the
effectiveness of the proposed ED-ViT framework.


## It Takes Two to Tango Directly Optimizing for Constrained Synthesizability in Generative Molecular Design

>Authors: Jeff Guo, Philippe Schwaller

>2024-10-15

> http://arxiv.org/abs/2410.11527v1

Constrained synthesizability is an unaddressed challenge in generative
molecular design. In particular, designing molecules satisfying multi-parameter
optimization objectives, while simultaneously being synthesizable and enforcing
the presence of specific commercial building blocks in the synthesis. This is
practically important for molecule re-purposing, sustainability, and
efficiency. In this work, we propose a novel reward function called TANimoto
Group Overlap (TANGO), which uses chemistry principles to transform a sparse
reward function into a dense and learnable reward function -- crucial for
reinforcement learning. TANGO can augment general-purpose molecular generative
models to directly optimize for constrained synthesizability while
simultaneously optimizing for other properties relevant to drug discovery using
reinforcement learning. Our framework is general and addresses
starting-material, intermediate, and divergent synthesis constraints. Contrary
to most existing works in the field, we show that incentivizing a
general-purpose (without any inductive biases) model is a productive approach
to navigating challenging optimization scenarios. We demonstrate this by
showing that the trained models explicitly learn a desirable distribution. Our
framework is the first generative approach to tackle constrained
synthesizability.


## QSpec Speculative Decoding with Complementary Quantization Schemes

>Authors: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

>2024-10-15

> http://arxiv.org/abs/2410.11305v1

Quantization has been substantially adopted to accelerate inference and
reduce memory consumption of large language models (LLMs). While
activation-weight joint quantization speeds up the inference process through
low-precision kernels, we demonstrate that it suffers severe performance
degradation on multi-step reasoning tasks, rendering it ineffective. We propose
a novel quantization paradigm called QSPEC, which seamlessly integrates two
complementary quantization schemes for speculative decoding. Leveraging nearly
cost-free execution switching, QSPEC drafts tokens with low-precision, fast
activation-weight quantization, and verifies them with high-precision
weight-only quantization, effectively combining the strengths of both
quantization schemes. Compared to high-precision quantization methods, QSPEC
empirically boosts token generation throughput by up to 1.80x without any
quality compromise, distinguishing it from other low-precision quantization
approaches. This enhancement is also consistent across various serving tasks,
model sizes, quantization methods, and batch sizes. Unlike existing speculative
decoding techniques, our approach reuses weights and the KV cache, avoiding
additional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage
without requiring any training. We believe that QSPEC demonstrates unique
strengths for future deployment of high-fidelity quantization schemes,
particularly in memory-constrained scenarios (e.g., edge devices).


## Subspace Optimization for Large Language Models with Convergence Guarantees

>Authors: Yutong He, Pengrui Li, Yipeng Hu, Chuyan Chen, Kun Yuan

>2024-10-15

> http://arxiv.org/abs/2410.11289v1

Subspace optimization algorithms, with GaLore (Zhao et al., 2024) as a
representative method, have gained popularity for pre-training or fine-tuning
large language models (LLMs) due to their memory efficiency. However, their
convergence guarantees remain unclear, particularly in stochastic settings. In
this paper, we unexpectedly discover that GaLore does not always converge to
the optimal solution and substantiate this finding with an explicit
counterexample. We then investigate the conditions under which GaLore can
achieve convergence, demonstrating that it does so either in deterministic
scenarios or when using a sufficiently large mini-batch size. More
significantly, we introduce GoLore (Gradient random Low-rank projection), a
novel variant of GaLore that provably converges in stochastic settings, even
with standard batch sizes. Our convergence analysis can be readily extended to
other sparse subspace optimization algorithms. Finally, we conduct numerical
experiments to validate our theoretical results and empirically explore the
proposed mechanisms. Codes are available at https://github.com/pkumelon/Golore.


## Beyond Linear Approximations A Novel Pruning Approach for Attention Matrix

>Authors: Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou

>2024-10-15

> http://arxiv.org/abs/2410.11261v1

Large Language Models (LLMs) have shown immense potential in enhancing
various aspects of our daily lives, from conversational AI to search and AI
assistants. However, their growing capabilities come at the cost of extremely
large model sizes, making deployment on edge devices challenging due to memory
and computational constraints. This paper introduces a novel approach to LLM
weight pruning that directly optimizes for approximating the attention matrix,
a core component of transformer architectures. Unlike existing methods that
focus on linear approximations, our approach accounts for the non-linear nature
of the Softmax attention mechanism. We provide theoretical guarantees for the
convergence of our Gradient Descent-based optimization method to a near-optimal
pruning mask solution. Our preliminary empirical results demonstrate the
effectiveness of this approach in maintaining model performance while
significantly reducing computational costs. This work establishes a new
theoretical foundation for pruning algorithm design in LLMs, potentially paving
the way for more efficient LLM inference on resource-constrained devices.


## Error Diffusion Post Training Quantization with Block-Scaled Number Formats for Neural Networks

>Authors: Alireza Khodamoradi, Kristof Denolf, Eric Dellinger

>2024-10-15

> http://arxiv.org/abs/2410.11203v1

Quantization reduces the model's hardware costs, such as data movement,
storage, and operations like multiply and addition. It also affects the model's
behavior by degrading the output quality. Therefore, there is a need for
methods that preserve the model's behavior when quantizing model parameters.
More exotic numerical encodings, such as block-scaled number formats, have
shown advantages for utilizing a fixed bit budget to encode model parameters.
This paper presents error diffusion (ED), a hyperparameter-free method for
post-training quantization with support for block-scaled data formats. Our
approach does not rely on backpropagation or Hessian information. We describe
how to improve the quantization process by viewing the neural model as a
composite function and diffusing the quantization error in every layer. In
addition, we introduce TensorCast, an open-source library based on PyTorch to
emulate a variety of number formats, including the block-scaled ones, to aid
the research in neural model quantization. We demonstrate the efficacy of our
algorithm through rigorous testing on various architectures, including vision
and large language models (LLMs), where it consistently delivers competitive
results. Our experiments confirm that block-scaled data formats provide a
robust choice for post-training quantization and could be used effectively to
enhance the practical deployment of advanced neural networks.


## Strings and membranes from $\mathcal{A}$-theory five brane

>Authors: Machiko Hatsuda, Ondřej Hulík, William D. Linch, Warren D. Siegel, Di Wang, Yu-Ping Wang

>2024-10-15

> http://arxiv.org/abs/2410.11197v1

The $\mathcal{A}$-theory takes U-duality symmetry as a guiding principle,
with the SL(5) U-duality symmetry being described as the world-volume theory of
a 5-brane. Furthermore, by unifying the 6-dimensional world-volume Lorentz
symmetry with the SL(5) spacetime symmetry, it extends to SL(6) U-duality
symmetry. The SL(5) spacetime vielbein fields and the 5-brane world-volume
vielbein fields are mixed under the SL(6) U-duality transformation. We
demonstrate that consistent sectionings of the SL(6) $\mathcal{A}$5-brane
world-volume Lagrangian yield Lagrangians of the $\mathcal{T}$-string with
O(D,D) T-duality symmetry, the conventional string, the ${\cal M}$5-brane with
GL(4) duality symmetry, and the non-perturbative M2-brane in supergravity
theory. The GL(4) covariant Lagrangian of the ${\cal M}$5-brane derived in this
manner is a new, perturbatively quantizable theory.

