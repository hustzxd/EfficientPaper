# 2025-05-30

# Table of Contents
* [Fast-dLLM Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](#Fast-dLLM-Training-free-Acceleration-of-Diffusion-LLM-by-Enabling-KV-Cache-and-Parallel-Decoding)
* [Attention-based Neural Network Emulators for Multi-Probe Data Vectors Part III Modeling The Next Generation Surveys](#Attention-based-Neural-Network-Emulators-for-Multi-Probe-Data-Vectors-Part-III-Modeling-The-Next-Generation-Surveys)
* [Fusion Steering Prompt-Specific Activation Control](#Fusion-Steering-Prompt-Specific-Activation-Control)
* [SHTOcc Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](#SHTOcc-Effective-3D-Occupancy-Prediction-with-Sparse-Head-and-Tail-Voxels)
* [RC-AutoCalib An End-to-End Radar-Camera Automatic Calibration Network](#RC-AutoCalib-An-End-to-End-Radar-Camera-Automatic-Calibration-Network)
* [UP-SLAM Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments](#UP-SLAM-Adaptively-Structured-Gaussian-SLAM-with-Uncertainty-Prediction-in-Dynamic-Environments)
* [A topological invariant in the context of the loop representation of the massive Kalb-Ramond-Klein-Gordon model](#A-topological-invariant-in-the-context-of-the-loop-representation-of-the-massive-Kalb-Ramond-Klein-Gordon-model)
* [Enjoying Information Dividend Gaze Track-based Medical Weakly Supervised Segmentation](#Enjoying-Information-Dividend-Gaze-Track-based-Medical-Weakly-Supervised-Segmentation)
* [Refining Datapath for Microscaling ViTs](#Refining-Datapath-for-Microscaling-ViTs)
* [Q-VDiT Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers](#Q-VDiT-Towards-Accurate-Quantization-and-Distillation-of-Video-Generation-Diffusion-Transformers)
* [InComeS Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](#InComeS-Integrating-Compression-and-Selection-Mechanisms-into-LLMs-for-Efficient-Model-Editing)
* [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling)
* [VIRAL Vision-grounded Integration for Reward design And Learning](#VIRAL-Vision-grounded-Integration-for-Reward-design-And-Learning)
* [iDSE Navigating Design Space Exploration in High-Level Synthesis Using LLMs](#iDSE-Navigating-Design-Space-Exploration-in-High-Level-Synthesis-Using-LLMs)
* [The Resurrection of the ReLU](#The-Resurrection-of-the-ReLU)
* [Balanced Token Pruning Accelerating Vision Language Models Beyond Local Optimization](#Balanced-Token-Pruning-Accelerating-Vision-Language-Models-Beyond-Local-Optimization)
* [Evaporation-induced freezing dynamics of droplets levitated in acoustic field](#Evaporation-induced-freezing-dynamics-of-droplets-levitated-in-acoustic-field)
* [ACE Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](#ACE-Exploring-Activation-Cosine-Similarity-and-Variance-for-Accurate-and-Calibration-Efficient-LLM-Pruning)
* [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](#Compressing-Sine-Activated-Low-Rank-Adapters-through-Post-Training-Quantization)
* [Almost Linear Convergence under Minimal Score Assumptions Quantized Transition Diffusion](#Almost-Linear-Convergence-under-Minimal-Score-Assumptions-Quantized-Transition-Diffusion)
* [EFIM Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](#EFIM-Efficient-Serving-of-LLMs-for-Infilling-Tasks-with-Improved-KV-Cache-Reuse)
* [ALTER All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation](#ALTER-All-in-One-Layer-Pruning-and-Temporal-Expert-Routing-for-Efficient-Diffusion-Generation)
* [Simulating the Unseen Crash Prediction Must Learn from What Did Not Happen](#Simulating-the-Unseen-Crash-Prediction-Must-Learn-from-What-Did-Not-Happen)
* [Preconditioning transformations of adjoint systems for evolution equations](#Preconditioning-transformations-of-adjoint-systems-for-evolution-equations)
* [Make Planning Research Rigorous Again!](#Make-Planning-Research-Rigorous-Again!)
* [Rethinking the Outlier Distribution in Large Language Models An In-depth Study](#Rethinking-the-Outlier-Distribution-in-Large-Language-Models-An-In-depth-Study)
* [Hardware-Efficient Attention for Fast Decoding](#Hardware-Efficient-Attention-for-Fast-Decoding)
* [Improving LLM-based Global Optimization with Search Space Partitioning](#Improving-LLM-based-Global-Optimization-with-Search-Space-Partitioning)
* [Towards Interpretability Without Sacrifice Faithful Dense Layer Decomposition with Mixture of Decoders](#Towards-Interpretability-Without-Sacrifice-Faithful-Dense-Layer-Decomposition-with-Mixture-of-Decoders)
* [HoliTom Holistic Token Merging for Fast Video Large Language Models](#HoliTom-Holistic-Token-Merging-for-Fast-Video-Large-Language-Models)
* [Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation](#Spectral-Compression-Transformer-with-Line-Pose-Graph-for-Monocular-3D-Human-Pose-Estimation)
* [JavaSith A Client-Side Framework for Analyzing Potentially Malicious Extensions in Browsers, VS Code, and NPM Packages](#JavaSith-A-Client-Side-Framework-for-Analyzing-Potentially-Malicious-Extensions-in-Browsers,-VS-Code,-and-NPM-Packages)
* [BindEnergyCraft Casting Protein Structure Predictors as Energy-Based Models for Binder Design](#BindEnergyCraft-Casting-Protein-Structure-Predictors-as-Energy-Based-Models-for-Binder-Design)
* [CROP Contextual Region-Oriented Visual Token Pruning](#CROP-Contextual-Region-Oriented-Visual-Token-Pruning)
* [Unveiling Instruction-Specific Neurons & Experts An Analytical Framework for LLM's Instruction-Following Capabilities](#Unveiling-Instruction-Specific-Neurons-&-Experts-An-Analytical-Framework-for-LLM's-Instruction-Following-Capabilities)
* [M-Wanda Improving One-Shot Pruning for Multilingual LLMs](#M-Wanda-Improving-One-Shot-Pruning-for-Multilingual-LLMs)
* [FastFace Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention](#FastFace-Tuning-Identity-Preservation-in-Distilled-Diffusion-via-Guidance-and-Attention)
* [SageAttention2++ A More Efficient Implementation of SageAttention2](#SageAttention2++-A-More-Efficient-Implementation-of-SageAttention2)
* [Decoding Breast Cancer in X-ray Mammograms A Multi-Parameter Approach Using Fractals, Multifractals, and Structural Disorder Analysis](#Decoding-Breast-Cancer-in-X-ray-Mammograms-A-Multi-Parameter-Approach-Using-Fractals,-Multifractals,-and-Structural-Disorder-Analysis)
* [Efficient Large Language Model Inference with Neural Block Linearization](#Efficient-Large-Language-Model-Inference-with-Neural-Block-Linearization)
* [RainFusion Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](#RainFusion-Adaptive-Video-Generation-Acceleration-via-Multi-Dimensional-Visual-Redundancy)
* [LLaMEA-BO A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms](#LLaMEA-BO-A-Large-Language-Model-Evolutionary-Algorithm-for-Automatically-Generating-Bayesian-Optimization-Algorithms)
* [Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning](#Object-Centric-Action-Enhanced-Representations-for-Robot-Visuo-Motor-Policy-Learning)
* [Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization](#Efficient-and-Microphone-Fault-Tolerant-3D-Sound-Source-Localization)
* [RepoMaster Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving](#RepoMaster-Autonomous-Exploration-and-Understanding-of-GitHub-Repositories-for-Complex-Task-Solving)
* [FireQ Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](#FireQ-Fast-INT4-FP8-Kernel-and-RoPE-aware-Quantization-for-LLM-Inference-Acceleration)
* [SpecExtend A Drop-in Enhancement for Speculative Decoding of Long Sequences](#SpecExtend-A-Drop-in-Enhancement-for-Speculative-Decoding-of-Long-Sequences)
* [Cold-Start Recommendation with Knowledge-Guided Retrieval-Augmented Generation](#Cold-Start-Recommendation-with-Knowledge-Guided-Retrieval-Augmented-Generation)
* [MetaSlot Break Through the Fixed Number of Slots in Object-Centric Learning](#MetaSlot-Break-Through-the-Fixed-Number-of-Slots-in-Object-Centric-Learning)
* [MoPFormer Motion-Primitive Transformer for Wearable-Sensor Activity Recognition](#MoPFormer-Motion-Primitive-Transformer-for-Wearable-Sensor-Activity-Recognition)
* [LeDiFlow Learned Distribution-guided Flow Matching to Accelerate Image Generation](#LeDiFlow-Learned-Distribution-guided-Flow-Matching-to-Accelerate-Image-Generation)
* [Sparsified State-Space Models are Efficient Highway Networks](#Sparsified-State-Space-Models-are-Efficient-Highway-Networks)
* [Open-Det An Efficient Learning Framework for Open-Ended Detection](#Open-Det-An-Efficient-Learning-Framework-for-Open-Ended-Detection)
* [Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction](#Plug-and-Play-Co-Occurring-Face-Attention-for-Robust-Audio-Visual-Speaker-Extraction)
* [Knowledge Distillation Approach for SOS Fusion Staging Towards Fully Automated Skeletal Maturity Assessment](#Knowledge-Distillation-Approach-for-SOS-Fusion-Staging-Towards-Fully-Automated-Skeletal-Maturity-Assessment)
* [Electrolyzers-HSI Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](#Electrolyzers-HSI-Close-Range-Multi-Scene-Hyperspectral-Imaging-Benchmark-Dataset)
* [HAMburger Accelerating LLM Inference via Token Smashing](#HAMburger-Accelerating-LLM-Inference-via-Token-Smashing)
* [SCAR Shapley Credit Assignment for More Efficient RLHF](#SCAR-Shapley-Credit-Assignment-for-More-Efficient-RLHF)
* [Does quantization affect models' performance on long-context tasks?](#Does-quantization-affect-models'-performance-on-long-context-tasks?)
* [syftr Pareto-Optimal Generative AI](#syftr-Pareto-Optimal-Generative-AI)
* [Position Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](#Position-Mechanistic-Interpretability-Should-Prioritize-Feature-Consistency-in-SAEs)
* [Efficient Optimization Accelerator Framework for Multistate Ising Problems](#Efficient-Optimization-Accelerator-Framework-for-Multistate-Ising-Problems)
* [From What to How Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](#From-What-to-How-Attributing-CLIP's-Latent-Components-Reveals-Unexpected-Semantic-Reliance)
* [FLAME-MoE A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](#FLAME-MoE-A-Transparent-End-to-End-Research-Platform-for-Mixture-of-Experts-Language-Models)
* [Gradient Flow Matching for Learning Update Dynamics in Neural Network Training](#Gradient-Flow-Matching-for-Learning-Update-Dynamics-in-Neural-Network-Training)
* [PathBench A comprehensive comparison benchmark for pathology foundation models towards precision oncology](#PathBench-A-comprehensive-comparison-benchmark-for-pathology-foundation-models-towards-precision-oncology)
* [Research on feature fusion and multimodal patent text based on graph attention network](#Research-on-feature-fusion-and-multimodal-patent-text-based-on-graph-attention-network)
* [Pangu Light Weight Re-Initialization for Pruning and Accelerating LLMs](#Pangu-Light-Weight-Re-Initialization-for-Pruning-and-Accelerating-LLMs)
* [AdaTP Attention-Debiased Token Pruning for Video Large Language Models](#AdaTP-Attention-Debiased-Token-Pruning-for-Video-Large-Language-Models)
* [Grokking ExPLAIND Unifying Model, Data, and Training Attribution to Study Model Behavior](#Grokking-ExPLAIND-Unifying-Model,-Data,-and-Training-Attribution-to-Study-Model-Behavior)
* [TabPFN One Model to Rule Them All?](#TabPFN-One-Model-to-Rule-Them-All?)
* [MiniLongBench The Low-cost Long Context Understanding Benchmark for Large Language Models](#MiniLongBench-The-Low-cost-Long-Context-Understanding-Benchmark-for-Large-Language-Models)
* [Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling](#Accelerating-Flow-Matching-Based-Text-to-Speech-via-Empirically-Pruned-Step-Sampling)
* [CA3D Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge](#CA3D-Convolutional-Attentional-3D-Nets-for-Efficient-Video-Activity-Recognition-on-the-Edge)
* [ScienceBoard Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](#ScienceBoard-Evaluating-Multimodal-Autonomous-Agents-in-Realistic-Scientific-Workflows)
* [Sparse2DGS Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud](#Sparse2DGS-Sparse-View-Surface-Reconstruction-using-2D-Gaussian-Splatting-with-Dense-Point-Cloud)
* [GoLF-NRT Integrating Global Context and Local Geometry for Few-Shot View Synthesis](#GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis)
* [Divide and Conquer Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning](#Divide-and-Conquer-Grounding-LLMs-as-Efficient-Decision-Making-Agents-via-Offline-Hierarchical-Reinforcement-Learning)
* [Large Language Models in Code Co-generation for Safe Autonomous Vehicles](#Large-Language-Models-in-Code-Co-generation-for-Safe-Autonomous-Vehicles)
* [MoESD Unveil Speculative Decoding's Potential for Accelerating Sparse MoE](#MoESD-Unveil-Speculative-Decoding's-Potential-for-Accelerating-Sparse-MoE)
* [Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression](#Memory-Efficient-Visual-Autoregressive-Modeling-with-Scale-Aware-KV-Cache-Compression)
* [TailorKV A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](#TailorKV-A-Hybrid-Framework-for-Long-Context-Inference-via-Tailored-KV-Cache-Optimization)
* [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](#Accelerating-Prefilling-for-Long-Context-LLMs-via-Sparse-Pattern-Sharing)
* [FastCache Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](#FastCache-Fast-Caching-for-Diffusion-Transformer-Through-Learnable-Linear-Approximation)
* [FlowCut Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](#FlowCut-Rethinking-Redundancy-via-Information-Flow-for-Efficient-Vision-Language-Models)
* [Small Language Models Architectures, Techniques, Evaluation, Problems and Future Adaptation](#Small-Language-Models-Architectures,-Techniques,-Evaluation,-Problems-and-Future-Adaptation)
* [Multimodal Machine Translation with Visual Scene Graph Pruning](#Multimodal-Machine-Translation-with-Visual-Scene-Graph-Pruning)
* [Win Fast or Lose Slow Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs](#Win-Fast-or-Lose-Slow-Balancing-Speed-and-Accuracy-in-Latency-Sensitive-Decisions-of-LLMs)
* [FlowSE Efficient and High-Quality Speech Enhancement via Flow Matching](#FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching)
* [The Birth of Knowledge Emergent Features across Time, Space, and Scale in Large Language Models](#The-Birth-of-Knowledge-Emergent-Features-across-Time,-Space,-and-Scale-in-Large-Language-Models)
* [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression](#Can-Compressed-LLMs-Truly-Act?-An-Empirical-Evaluation-of-Agentic-Capabilities-in-LLM-Compression)
* [Frictional Agent Alignment Framework Slow Down and Don't Break Things](#Frictional-Agent-Alignment-Framework-Slow-Down-and-Don't-Break-Things)
* [WINA Weight Informed Neuron Activation for Accelerating Large Language Model Inference](#WINA-Weight-Informed-Neuron-Activation-for-Accelerating-Large-Language-Model-Inference)
* [Fusion Intelligence for Digital Twinning AI Data Centers A Synergistic GenAI-PhyAI Approach](#Fusion-Intelligence-for-Digital-Twinning-AI-Data-Centers-A-Synergistic-GenAI-PhyAI-Approach)
* [Foundations of Top-$k$ Decoding For Language Models](#Foundations-of-Top-$k$-Decoding-For-Language-Models)
* [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](#Optimized-Text-Embedding-Models-and-Benchmarks-for-Amharic-Passage-Retrieval)
* [DECA A Near-Core LLM Decompression Accelerator Supporting Out-of-Order Invocation](#DECA-A-Near-Core-LLM-Decompression-Accelerator-Supporting-Out-of-Order-Invocation)
* [Communication-Efficient Multi-Device Inference Acceleration for Transformer Models](#Communication-Efficient-Multi-Device-Inference-Acceleration-for-Transformer-Models)
* [PolyPose Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms](#PolyPose-Localizing-Deformable-Anatomy-in-3D-from-Sparse-2D-X-ray-Images-using-Polyrigid-Transforms)
* [York time in JT gravity](#York-time-in-JT-gravity)
* [DREAM Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](#DREAM-Drafting-with-Refined-Target-Features-and-Entropy-Adaptive-Cross-Attention-Fusion-for-Multimodal-Speculative-Decoding)
* [LIMOPro Reasoning Refinement for Efficient and Effective Test-time Scaling](#LIMOPro-Reasoning-Refinement-for-Efficient-and-Effective-Test-time-Scaling)
* [DLF Enhancing Explicit-Implicit Interaction via Dynamic Low-Order-Aware Fusion for CTR Prediction](#DLF-Enhancing-Explicit-Implicit-Interaction-via-Dynamic-Low-Order-Aware-Fusion-for-CTR-Prediction)
* [Sparse-to-Dense A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](#Sparse-to-Dense-A-Free-Lunch-for-Lossless-Acceleration-of-Video-Understanding-in-LLMs)
* [SRDiffusion Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](#SRDiffusion-Accelerate-Video-Diffusion-Inference-via-Sketching-Rendering-Cooperation)
* [ADGSyn Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction](#ADGSyn-Dual-Stream-Learning-for-Efficient-Anticancer-Drug-Synergy-Prediction)
* [FP4 All the Way Fully Quantized Training of LLMs](#FP4-All-the-Way-Fully-Quantized-Training-of-LLMs)
* [Bayesian sparse modeling for interpretable prediction of hydroxide ion conductivity in anion-conductive polymer membranes](#Bayesian-sparse-modeling-for-interpretable-prediction-of-hydroxide-ion-conductivity-in-anion-conductive-polymer-membranes)
* [Tokenizing Electron Cloud in Protein-Ligand Interaction Learning](#Tokenizing-Electron-Cloud-in-Protein-Ligand-Interaction-Learning)
* [FastMamba A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization](#FastMamba-A-High-Speed-and-Efficient-Mamba-Accelerator-on-FPGA-with-Accurate-Quantization)
* [System-1.5 Reasoning Traversal in Language and Latent Spaces with Dynamic Shortcuts](#System-1.5-Reasoning-Traversal-in-Language-and-Latent-Spaces-with-Dynamic-Shortcuts)
* [How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation](#How-Do-Images-Align-and-Complement-LiDAR?-Towards-a-Harmonized-Multi-modal-3D-Panoptic-Segmentation)
* [Efficient SRAM-PIM Co-design by Joint Exploration of Value-Level and Bit-Level Sparsity](#Efficient-SRAM-PIM-Co-design-by-Joint-Exploration-of-Value-Level-and-Bit-Level-Sparsity)
* [Graph-Based Operator Learning from Limited Data on Irregular Domains](#Graph-Based-Operator-Learning-from-Limited-Data-on-Irregular-Domains)
* [Partition Generative Modeling Masked Modeling Without Masks](#Partition-Generative-Modeling-Masked-Modeling-Without-Masks)
* [Sparse VideoGen2 Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation](#Sparse-VideoGen2-Accelerate-Video-Generation-with-Sparse-Attention-via-Semantic-Aware-Permutation)
* [FlatAttention Dataflow and Fabric Collectives Co-Optimization for Efficient Multi-Head Attention on Tile-Based Many-PE Accelerators](#FlatAttention-Dataflow-and-Fabric-Collectives-Co-Optimization-for-Efficient-Multi-Head-Attention-on-Tile-Based-Many-PE-Accelerators)
* [High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction](#High-order-Equivariant-Flow-Matching-for-Density-Functional-Theory-Hamiltonian-Prediction)
* [VORTA Efficient Video Diffusion via Routing Sparse Attention](#VORTA-Efficient-Video-Diffusion-via-Routing-Sparse-Attention)
* [ALPS Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](#ALPS-Attention-Localization-and-Pruning-Strategy-for-Efficient-Alignment-of-Large-Language-Models)
* [ToDRE Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models](#ToDRE-Visual-Token-Pruning-via-Diversity-and-Task-Awareness-for-Efficient-Large-Vision-Language-Models)
* [LoTA-QAF Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning](#LoTA-QAF-Lossless-Ternary-Adaptation-for-Quantization-Aware-Fine-Tuning)
* [AI-Researcher Autonomous Scientific Innovation](#AI-Researcher-Autonomous-Scientific-Innovation)
* [DVD-Quant Data-free Video Diffusion Transformers Quantization](#DVD-Quant-Data-free-Video-Diffusion-Transformers-Quantization)
* [Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees](#Adaptive-Prediction-Powered-AutoEval-with-Reliability-and-Efficiency-Guarantees)
* [Lookahead Q-Cache Achieving More Consistent KV Cache Eviction via Pseudo Query](#Lookahead-Q-Cache-Achieving-More-Consistent-KV-Cache-Eviction-via-Pseudo-Query)
* [Think Before You Accept Semantic Reflective Verification for Faster Speculative Decoding](#Think-Before-You-Accept-Semantic-Reflective-Verification-for-Faster-Speculative-Decoding)
* [PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](#PM-KVQ-Progressive-Mixed-precision-KV-Cache-Quantization-for-Long-CoT-LLMs)
* [Safety Alignment via Constrained Knowledge Unlearning](#Safety-Alignment-via-Constrained-Knowledge-Unlearning)
* [Autocomp LLM-Driven Code Optimization for Tensor Accelerators](#Autocomp-LLM-Driven-Code-Optimization-for-Tensor-Accelerators)
* [Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition](#Scalable-Gaussian-Processes-with-Low-Rank-Deep-Kernel-Decomposition)
* [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](#Is-Attention-Required-for-Transformer-Inference?-Explore-Function-preserving-Attention-Replacement)
* [A Survey of LLM $\times$ DATA](#A-Survey-of-LLM-$\times$-DATA)
* [DB-KSVD Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](#DB-KSVD-Scalable-Alternating-Optimization-for-Disentangling-High-Dimensional-Embedding-Spaces)
* [Task Specific Pruning with LLM-Sieve How Many Parameters Does Your Task Really Need?](#Task-Specific-Pruning-with-LLM-Sieve-How-Many-Parameters-Does-Your-Task-Really-Need?)
* [PerMedCQA Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](#PerMedCQA-Benchmarking-Large-Language-Models-on-Medical-Consumer-Question-Answering-in-Persian-Language)
* [PLUMAGE Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training](#PLUMAGE-Probabilistic-Low-rank-Unbiased-Min-Variance-Gradient-Estimator-for-Efficient-Large-Model-Training)
* [A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception](#A-Dataset-and-Benchmarks-for-Deep-Learning-Based-Optical-Microrobot-Pose-and-Depth-Perception)
* [Thinking Fast and Right Balancing Accuracy and Reasoning Length with Adaptive Rewards](#Thinking-Fast-and-Right-Balancing-Accuracy-and-Reasoning-Length-with-Adaptive-Rewards)
* [Einstein-Gauss-Bonnet-Myrzakulov Gravity from $R + F(T, G)$ Numerical Insights and Torsion-Gauss-Bonnet Dynamics in Weitzenböck Spacetime](#Einstein-Gauss-Bonnet-Myrzakulov-Gravity-from-$R-+-F(T,-G)$-Numerical-Insights-and-Torsion-Gauss-Bonnet-Dynamics-in-Weitzenböck-Spacetime)
* [Guided by Gut Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](#Guided-by-Gut-Efficient-Test-Time-Scaling-with-Reinforced-Intrinsic-Confidence)
* [Accelerating Learned Image Compression Through Modeling Neural Training Dynamics](#Accelerating-Learned-Image-Compression-Through-Modeling-Neural-Training-Dynamics)
* [QwenLong-CPRS Towards $\infty$-LLMs with Dynamic Context Optimization](#QwenLong-CPRS-Towards-$\infty$-LLMs-with-Dynamic-Context-Optimization)
* [Reward Model Generalization for Compute-Aware Test-Time Reasoning](#Reward-Model-Generalization-for-Compute-Aware-Test-Time-Reasoning)
* [LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision](#LookWhere?-Efficient-Visual-Recognition-by-Learning-Where-to-Look-and-What-to-See-from-Self-Supervision)
* [Towards Analyzing and Understanding the Limitations of VAPO A Theoretical Perspective](#Towards-Analyzing-and-Understanding-the-Limitations-of-VAPO-A-Theoretical-Perspective)
* [ADLGen Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling](#ADLGen-Synthesizing-Symbolic,-Event-Triggered-Sensor-Sequences-for-Human-Activity-Modeling)
* [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](#Are-Large-Language-Models-Reliable-AI-Scientists?-Assessing-Reverse-Engineering-of-Black-Box-Systems)
* [NeuroTrails Training with Dynamic Sparse Heads as the Key to Effective Ensembling](#NeuroTrails-Training-with-Dynamic-Sparse-Heads-as-the-Key-to-Effective-Ensembling)
* [Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization](#Hyperspectral-Anomaly-Detection-Fused-Unified-Nonconvex-Tensor-Ring-Factors-Regularization)
* [The emergence of sparse attention impact of data distribution and benefits of repetition](#The-emergence-of-sparse-attention-impact-of-data-distribution-and-benefits-of-repetition)
* [Stochastic Weight Sharing for Bayesian Neural Networks](#Stochastic-Weight-Sharing-for-Bayesian-Neural-Networks)
* [ELDeR Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](#ELDeR-Getting-Efficient-LLMs-through-Data-Driven-Regularized-Layer-wise-Pruning)
* [NSNQuant A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache](#NSNQuant-A-Double-Normalization-Approach-for-Calibration-Free-Low-Bit-Vector-Quantization-of-KV-Cache)
* [RECIPE-TKG From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion](#RECIPE-TKG-From-Sparse-History-to-Structured-Reasoning-for-LLM-based-Temporal-Knowledge-Graph-Completion)
* [Titanus Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration](#Titanus-Enabling-KV-Cache-Pruning-and-Quantization-On-the-Fly-for-LLM-Acceleration)
* [Inference-Time Decomposition of Activations (ITDA) A Scalable Approach to Interpreting Large Language Models](#Inference-Time-Decomposition-of-Activations-(ITDA)-A-Scalable-Approach-to-Interpreting-Large-Language-Models)
* [Structured Linear CDEs Maximally Expressive and Parallel-in-Time Sequence Models](#Structured-Linear-CDEs-Maximally-Expressive-and-Parallel-in-Time-Sequence-Models)
* [Slot-MLLM Object-Centric Visual Tokenization for Multimodal LLM](#Slot-MLLM-Object-Centric-Visual-Tokenization-for-Multimodal-LLM)
* [FlashForge Ultra-Efficient Prefix-Aware Attention for LLM Decoding](#FlashForge-Ultra-Efficient-Prefix-Aware-Attention-for-LLM-Decoding)
* [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](#Dual-Attention-Residual-U-Net-for-Accurate-Brain-Ultrasound-Segmentation-in-IVH-Detection)
* [Rethinking Agent Design From Top-Down Workflows to Bottom-Up Skill Evolution](#Rethinking-Agent-Design-From-Top-Down-Workflows-to-Bottom-Up-Skill-Evolution)
* [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](#Automating-Versatile-Time-Series-Analysis-with-Tiny-Transformers-on-Embedded-FPGAs)
* [PreMoe Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval](#PreMoe-Lightening-MoEs-on-Constrained-Memory-by-Expert-Pruning-and-Retrieval)
* [Causal Spatio-Temporal Prediction An Effective and Efficient Multi-Modal Approach](#Causal-Spatio-Temporal-Prediction-An-Effective-and-Efficient-Multi-Modal-Approach)
* [Navigate the Unknown Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](#Navigate-the-Unknown-Enhancing-LLM-Reasoning-with-Intrinsic-Motivation-Guided-Exploration)
* [NeUQI Near-Optimal Uniform Quantization Parameter Initialization](#NeUQI-Near-Optimal-Uniform-Quantization-Parameter-Initialization)
* [H2Towards Efficient Large-Scale LLM Training on Hyper-Heterogeneous Cluster over 1,000 Chips](#H2Towards-Efficient-Large-Scale-LLM-Training-on-Hyper-Heterogeneous-Cluster-over-1,000-Chips)
* [MEGADance Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation](#MEGADance-Mixture-of-Experts-Architecture-for-Genre-Aware-3D-Dance-Generation)
* [L-MTP Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](#L-MTP-Leap-Multi-Token-Prediction-Beyond-Adjacent-Context-for-Large-Language-Models)
* [The GMRES method for solving the large indefinite least squares problem via an accelerated preconditioner](#The-GMRES-method-for-solving-the-large-indefinite-least-squares-problem-via-an-accelerated-preconditioner)
* [The Discovery Engine A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes](#The-Discovery-Engine-A-Framework-for-AI-Driven-Synthesis-and-Navigation-of-Scientific-Knowledge-Landscapes)
* [ProxySPEX Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](#ProxySPEX-Inference-Efficient-Interpretability-via-Sparse-Feature-Interactions-in-LLMs)
* [HiLAB A Hybrid Inverse-Design Framework](#HiLAB-A-Hybrid-Inverse-Design-Framework)
* [Hydra Structured Cross-Source Enhanced Large Language Model Reasoning](#Hydra-Structured-Cross-Source-Enhanced-Large-Language-Model-Reasoning)
* [Discovering Forbidden Topics in Language Models](#Discovering-Forbidden-Topics-in-Language-Models)
* [UniTTS An end-to-end TTS system without decoupling of acoustic and semantic information](#UniTTS-An-end-to-end-TTS-system-without-decoupling-of-acoustic-and-semantic-information)
* [DASH Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](#DASH-Input-Aware-Dynamic-Layer-Skipping-for-Efficient-LLM-Inference-with-Markov-Decision-Policies)
* [Direct3D-S2 Gigascale 3D Generation Made Easy with Spatial Sparse Attention](#Direct3D-S2-Gigascale-3D-Generation-Made-Easy-with-Spatial-Sparse-Attention)
* [CIM-NET A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](#CIM-NET-A-Video-Denoising-Deep-Neural-Network-Model-Optimized-for-Computing-in-Memory-Architectures)


## Fast-dLLM Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding

>Authors: Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie

>2025-05-28

> http://arxiv.org/abs/2505.22618v1

Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (**KV**) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate **KV** Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.


## Attention-based Neural Network Emulators for Multi-Probe Data Vectors Part III Modeling The Next Generation Surveys

>Authors: Yijie Zhu, Evan Saraivanov, Joshua A. Kable, Artemis Sofia Giannakopoulou, Amritpal Nijjar, Vivian Miranda, Marco Bonici, Tim Eifler, Elisabeth Krause

>2025-05-28

> http://arxiv.org/abs/2505.22574v1

Machine learning can accelerate cosmological inferences that involve many
sequential evaluations of computationally expensive data vectors. Previous
works in this series have examined how machine learning architectures impact
emulator accuracy and training time for optical shear and galaxy clustering
2-point function. In this final manuscript, we explore neural network
performance when emulating Cosmic Microwave Background temperature and
polarization power spectra. We maximize the volume of applicability in the
parameter space of our emulators within the standard $\Lambda$-cold-dark-matter
model while ensuring that errors are below cosmic variance. Relative to
standard multi-layer perceptron architectures, we find the
dot-product-attention mechanism reduces the number of outliers among testing
cosmologies, defined as the fraction of testing points with $\Delta \chi^2 >
0.2$ relative to \textsc{CAMB} outputs, for a wide range of training set sizes.
Such precision enables attention-based emulators to be directly applied to real
data without requiring any additional correction via importance sampling.
Combined with pre-processing techniques and optimized activation and loss
functions, attention-based models can meet the precision criteria set by
current and future CMB and lensing experiments. For each of Planck, Simons
Observatory, CMB S4, and CMB HD, we find the fraction of outlier points to be
less than $10\%$ with around $2\times10^5$ to $4\times10^5$ training data
vectors. We further explore the applications of these methods to supernova
distance, weak lensing, and galaxy clustering, as well as alternative
architectures and pre-processing techniques.


## Fusion Steering Prompt-Specific Activation Control

>Authors: Waldemar Chang, Alhassan Yasin

>2025-05-28

> http://arxiv.org/abs/2505.22572v1

We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
**quantization**, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to **sparse**
representations, such as Neuronpedia or **sparse** crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.


## SHTOcc Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels

>Authors: Qiucheng Yu, Yuan Xie, Xin Tan

>2025-05-28

> http://arxiv.org/abs/2505.22461v2

3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses **sparse** head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc


## RC-AutoCalib An End-to-End Radar-Camera Automatic Calibration Network

>Authors: Van-Tin Luu, Yon-Lin Cai, Vu-Hoang Tran, Wei-Chen Chiu, Yi-Ting Chen, Ching-Chun Huang

>2025-05-28

> http://arxiv.org/abs/2505.22427v1

This paper presents a groundbreaking approach - the first online automatic
geometric calibration method for radar and camera systems. Given the
significant data **sparsity** and measurement uncertainty in radar height data,
achieving automatic calibration during system operation has long been a
challenge. To address the **sparsity** issue, we propose a Dual-Perspective
representation that gathers features from both frontal and bird's-eye views.
The frontal view contains rich but sensitive height information, whereas the
bird's-eye view provides robust features against height uncertainty. We thereby
propose a novel Selective Fusion Mechanism to identify and fuse reliable
features from both perspectives, reducing the effect of height uncertainty.
Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism
to explicitly find location correspondences through cross-modal matching.
During the training phase, we also design a Noise-Resistant Matcher to provide
better supervision and enhance the robustness of the matching mechanism against
**sparsity** and height uncertainty. Our experimental results, tested on the
nuScenes dataset, demonstrate that our method significantly outperforms
previous radar-camera auto-calibration methods, as well as existing
state-of-the-art LiDAR-camera calibration techniques, establishing a new
benchmark for future research. The code is available at
https://github.com/nycu-acm/RC-AutoCalib.


## UP-SLAM Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments

>Authors: Wancai Zheng, Linlin Ou, Jiajie He, Libo Zhou, Xinyi Yu, Yan Wei

>2025-05-28

> http://arxiv.org/abs/2505.22335v1

Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and **pruning** without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/


## A topological invariant in the context of the loop representation of the massive Kalb-Ramond-Klein-Gordon model

>Authors: E. Iñiguez, M. Freire, L. Leal, E. Contreras

>2025-05-28

> http://arxiv.org/abs/2505.22292v1

We employ the Dirac procedure to **quantize** the self-dual massive
Kalb-Ramond-Klein-Gordon model in $2+1$ dimensional spacetimes. The canonical
fields are expressed in terms of $2$-surfaces and signed points, ensuring the
automatic realization of the quantum algebra. As the duality rotation
preserving the action can be implemented infinitesimally, we derive the
conserved quantity that generates the transformation. Given that such a
generator is a two dimensional topological quantity, its representation in
terms of geometrical operators yields a two dimensional invariant (reminiscent
of a projection of Gauss's law in electrodynamics), which encodes the same
information of the well-known winding number.


## Enjoying Information Dividend Gaze Track-based Medical Weakly Supervised Segmentation

>Authors: Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia

>2025-05-28

> http://arxiv.org/abs/2505.22230v1

Weakly supervised semantic segmentation (WSSS) in medical imaging struggles
with effectively using **sparse** annotations. One promising direction for WSSS
leverages gaze annotations, captured via eye trackers that record regions of
interest during diagnostic procedures. However, existing gaze-based methods,
such as GazeMedSeg, do not fully exploit the rich information embedded in gaze
data. In this paper, we propose GradTrack, a framework that utilizes
physicians' gaze track, including fixation points, durations, and temporal
order, to enhance WSSS performance. GradTrack comprises two key components:
Gaze Track Map Generation and Track Attention, which collaboratively enable
progressive feature refinement through multi-level gaze supervision during the
decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets
demonstrate that GradTrack consistently outperforms existing gaze-based
methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively.
Moreover, GradTrack significantly narrows the performance gap with fully
supervised models such as nnUNet.


## Refining Datapath for Microscaling ViTs

>Authors: Can Xiao, Jianyi Cheng, Aaron Zhao

>2025-05-28

> http://arxiv.org/abs/2505.22194v1

Vision Transformers (ViTs) leverage the transformer architecture to
effectively capture global context, demonstrating strong performance in
computer vision tasks. A major challenge in ViT hardware **acceleration** is that
the model family contains complex arithmetic operations that are sensitive to
model accuracy, such as the Softmax and LayerNorm operations, which cannot be
mapped onto efficient hardware with low precision. Existing methods only
exploit parallelism in the matrix multiplication operations of the model on
hardware and keep these complex operations on the CPU. This results in
suboptimal performance due to the communication overhead between the CPU and
accelerator. Can new data formats solve this problem?
  In this work, we present the first ViT accelerator that maps all operations
of the ViT models onto FPGAs. We exploit a new arithmetic format named
Microscaling Integer (MXInt) for datapath designs and evaluate how different
design choices can be made to trade off accuracy, hardware performance, and
hardware utilization. Our contributions are twofold. First, we **quantize** ViTs
using the MXInt format, achieving both high area efficiency and accuracy.
Second, we propose MXInt-specific hardware optimization that map these complex
arithmetic operations into custom hardware. Within 1\% accuracy loss, our
method achieves at least 93$\times$ speedup compared to Float16 and at least
1.9$\times$ speedup compared to related work.


## Q-VDiT Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers

>Authors: Weilun Feng, Chuanguang Yang, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno

>2025-05-28

> http://arxiv.org/abs/2505.22167v1

Diffusion transformers (DiT) have demonstrated exceptional performance in
video generation. However, their large number of parameters and high
computational complexity limit their deployment on edge devices. Quantization
can reduce storage requirements and accelerate inference by lowering the
bit-width of model parameters. Yet, existing **quantization** methods for image
generation models do not generalize well to video generation tasks. We identify
two primary challenges: the loss of information during **quantization** and the
misalignment between optimization objectives and the unique requirements of
video generation. To address these challenges, we present Q-VDiT, a
**quantization** framework specifically designed for video DiT models. From the
**quantization** perspective, we propose the Token-aware Quantization Estimator
(TQE), which compensates for **quantization** errors in both the token and feature
dimensions. From the optimization perspective, we introduce Temporal
Maintenance Distillation (TMD), which preserves the spatiotemporal correlations
between frames and enables the optimization of each frame with respect to the
overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,
setting a new benchmark and outperforming current state-of-the-art **quantization**
methods by 1.9$\times$. Code will be available at
https://github.com/cantbebetter2/Q-VDiT.


## InComeS Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing

>Authors: Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam

>2025-05-28

> http://arxiv.org/abs/2505.22156v1

Although existing model editing methods perform well in recalling exact edit
facts, they often struggle in complex scenarios that require deeper semantic
understanding rather than mere knowledge regurgitation. Leveraging the strong
contextual reasoning abilities of large language models (LLMs), in-context
learning (ICL) becomes a promising editing method by comprehending edit
information through context encoding. However, this method is constrained by
the limited context window of LLMs, leading to degraded performance and
efficiency as the number of edits increases. To overcome this limitation, we
propose InComeS, a flexible framework that enhances LLMs' ability to process
editing contexts through explicit compression and selection mechanisms.
Specifically, InComeS compresses each editing context into the key-value (**KV**)
cache of a special gist token, enabling efficient handling of multiple edits
without being restricted by the model's context window. Furthermore,
specialized cross-attention modules are added to dynamically select the most
relevant information from the gist pools, enabling adaptive and effective
utilization of edit information. We conduct experiments on diverse model
editing benchmarks with various editing formats, and the results demonstrate
the effectiveness and efficiency of our method.


## Curse of High Dimensionality Issue in Transformer for Long-context Modeling

>Authors: Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan

>2025-05-28

> http://arxiv.org/abs/2505.22107v1

Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{**sparse**}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention **sparsity**, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.


## VIRAL Vision-grounded Integration for Reward design And Learning

>Authors: Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun

>2025-05-28

> http://arxiv.org/abs/2505.22092v1

The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.


## iDSE Navigating Design Space Exploration in High-Level Synthesis Using LLMs

>Authors: Runkai Li, Jia Xiong, Xi Wang

>2025-05-28

> http://arxiv.org/abs/2505.22086v1

High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.


## The Resurrection of the ReLU

>Authors: Coşku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse, Thomas Martinetz, Sebastian Otte

>2025-05-28

> http://arxiv.org/abs/2505.22074v1

Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent **sparsity**, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing **sparse**r activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.


## Balanced Token Pruning Accelerating Vision Language Models Beyond Local Optimization

>Authors: Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen

>2025-05-28

> http://arxiv.org/abs/2505.22038v1

Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token **pruning**, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of **pruning** on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
**pruning** decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for **pruning** vision tokens. Specifically, our
method utilizes a small calibration set to divide the **pruning** process into
multiple stages. In the early stages, our method emphasizes the impact of
**pruning** on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.


## Evaporation-induced freezing dynamics of droplets levitated in acoustic field

>Authors: Misaki Mitsuno, Xiao Ma, Koji Hasegawa

>2025-05-28

> http://arxiv.org/abs/2505.22001v1

This paper presents the evaporation-induced freezing dynamics of pure
cyclohexane droplets levitated via acoustic levitation. Acoustic levitation has
attracted considerable attention across various fields owing to its potential
to create lab-in-a-drop systems. While droplet evaporation is a fundamental
physicochemical process in such a platform, the freezing of droplets induced by
evaporation has been **sparse**ly explored experimentally. For pure cyclohexane,
the rapid evaporation of levitated droplets initiated freezing at the droplet
surface. To better understand this evaporation-induced freezing process, the
evaporation behavior of the levitated cyclohexane droplets was visualized and
quantified using a high-speed camera and an infrared camera. According to the
obtained experimental data, the evaporative heat transfer characteristics of
the droplets were identified with theoretical models. Using the derived heat
transfer coefficient, a mathematical prediction method for the onset of
freezing was proposed and validated with the experimental data. These
experimental findings offer valuable insights into the phase transition process
and its potential physicochemical applications in a containerless environment.


## ACE Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning

>Authors: Zhendong Mi, Zhenglun Kong, Geng Yuan, Shaoyi Huang

>2025-05-28

> http://arxiv.org/abs/2505.21987v1

With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM **pruning** aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal **pruning**
performance or low time efficiency during the **pruning** process. In this work, we
propose an efficient and effective **pruning** method that simultaneously achieves
high **pruning** performance and fast **pruning** speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided **pruning** metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided **pruning** metric, which helps preserve semantic
distinctions in output activations after **pruning**, enabling effective **pruning**
with shorter input sequences. These two components can be readily combined to
enhance LLM **pruning** in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in **pruning** time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.


## Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization

>Authors: Cameron Gordon, Yiping Ji, Hemanth Saratchandran, Paul Albert, Simon Lucey

>2025-05-28

> http://arxiv.org/abs/2505.21895v1

Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to **quantize**d LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a **quantize**d adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under **quantization**. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
**quantization**, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.


## Almost Linear Convergence under Minimal Score Assumptions Quantized Transition Diffusion

>Authors: Xunpeng Huang, Yingyu Lin, Nikki Lijing Kuang, Hanze Dong, Difan Zou, Yian Ma, Tong Zhang

>2025-05-28

> http://arxiv.org/abs/2505.21892v1

Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data **quantization**
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.


## EFIM Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse

>Authors: Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang

>2025-05-28

> http://arxiv.org/abs/2505.21889v2

Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (**KV**)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the **KV** cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the **KV** cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of **KV** cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability. EFIM's source code is publicly
available at https://github.com/gty111/EFIM.


## ALTER All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation

>Authors: Xiaomeng Yang, Lei Lu, Qihui Fan, Changdi Yang, Juyi Lin, Yanzhi Wang, Xuan Zhang, Shangqian Gao

>2025-05-27

> http://arxiv.org/abs/2505.21817v1

Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images. However, their iterative denoising process results in
significant computational overhead during inference, limiting their practical
deployment in resource-constrained environments. Existing **acceleration** methods
often adopt uniform strategies that fail to capture the temporal variations
during diffusion generation, while the commonly adopted sequential
**pruning**-then-fine-tuning strategy suffers from sub-optimality due to the
misalignment between **pruning** decisions made on pretrained weights and the
model's final parameters. To address these limitations, we introduce ALTER:
All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that
transforms diffusion models into a mixture of efficient temporal experts. ALTER
achieves a single-stage optimization that unifies layer **pruning**, expert
routing, and model fine-tuning by employing a trainable hypernetwork, which
dynamically generates layer **pruning** decisions and manages timestep routing to
specialized, pruned expert sub-networks throughout the ongoing fine-tuning of
the UNet. This unified co-optimization strategy enables significant efficiency
gains while preserving high generative quality. Specifically, ALTER achieves
same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model
while utilizing only 25.9% of its total MACs with just 20 inference steps and
delivering a 3.64x speedup through 35% **sparsity**.


## Simulating the Unseen Crash Prediction Must Learn from What Did Not Happen

>Authors: Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou

>2025-05-27

> http://arxiv.org/abs/2505.21743v1

Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on **sparse**, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
**sparse** crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.


## Preconditioning transformations of adjoint systems for evolution equations

>Authors: Brian K. Tran, Ben S. Southworth, Hannah F. Blumhoefer, Samuel Olivier

>2025-05-27

> http://arxiv.org/abs/2505.21705v1

Achieving robust control and optimization in high-fidelity physics
simulations is extremely challenging, especially for evolutionary systems whose
solutions span vast scales across space, time, and physical variables. In
conjunction with gradient-based methods, adjoint systems are widely used in the
optimization of systems subject to differential equation constraints. In
optimization, gradient-based methods are often transformed using suitable
preconditioners to accelerate the convergence of the optimization algorithm.
Inspired by preconditioned gradient descent methods, we introduce a framework
for the preconditioning of adjoint systems associated to evolution equations,
which allows one to reshape the dynamics of the adjoint system. We develop two
classes of adjoint preconditioning transformations: those that transform both
the state dynamics and the adjoint equation and those that transform only the
adjoint equation while leaving the state dynamics invariant. Both classes of
transformations have the flexibility to include generally nonlinear
state-dependent transformations. Using techniques from symplectic geometry and
Hamiltonian mechanics, we further show that these preconditioned adjoint
systems preserve the property that the adjoint system backpropagates the
derivative of an objective function. We then apply this framework to the
setting of coupled evolution equations, where we develop a notion of scale
preconditioning of the adjoint equations when the state dynamics exhibit large
scale-separation. We demonstrate the proposed scale preconditioning on an
inverse problem for the radiation diffusion equations. Naive gradient descent
is unstable for any practical gradient descent step size, whereas our proposed
scale-preconditioned adjoint descent converges in 10-15 gradient-based
optimization iterations, with highly accurate reproduction of the wavefront at
the final time.


## Make Planning Research Rigorous Again!

>Authors: Michael Katz, Harsha Kokel, Christian Muise, Shirin Sohrabi, Sarath Sreedharan

>2025-05-27

> http://arxiv.org/abs/2505.21674v1

In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.


## Rethinking the Outlier Distribution in Large Language Models An In-depth Study

>Authors: Rahul Raman, Khushi Sharma, Sai Qian Zhang

>2025-05-27

> http://arxiv.org/abs/2505.21670v1

Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
**quantization** and compression. Outliers often cause considerable **quantization**
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the **quantization** process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous **quantization** algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.


## Hardware-Efficient Attention for Fast Decoding

>Authors: Ted Zadouri, Hubert Strauss, Tri Dao

>2025-05-27

> http://arxiv.org/abs/2505.21487v1

LLM decoding is bottlenecked for large batches and long contexts by loading
the key-value (**KV**) cache from high-bandwidth memory, which inflates per-token
latency, while the sequential nature of decoding limits parallelism. We analyze
the interplay among arithmetic intensity, parallelization, and model quality
and question whether current architectures fully exploit modern hardware. This
work redesigns attention to perform more computation per byte loaded from
memory to maximize hardware efficiency without trading off parallel
scalability. We first propose Grouped-Tied Attention (GTA), a simple variant
that combines and reuses key and value states, reducing memory transfers
without compromising model quality. We then introduce Grouped Latent Attention
(GLA), a parallel-friendly latent attention paired with low-level optimizations
for fast decoding while maintaining high model quality. Experiments show that
GTA matches Grouped-Query Attention (GQA) quality while using roughly half the
**KV** cache and that GLA matches Multi-head Latent Attention (MLA) and is easier
to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for
example, in a speculative decoding setting when the query length exceeds one.
Furthermore, by fetching a smaller **KV** cache per device, GLA reduces end-to-end
latency and increases throughput in online serving benchmarks by up to
2$\times$.


## Improving LLM-based Global Optimization with Search Space Partitioning

>Authors: Andrej Schwanke, Lyubomir Ivanov, David Salinas, Fabio Ferreira, Aaron Klein, Frank Hutter, Arber Zela

>2025-05-27

> http://arxiv.org/abs/2505.21372v1

Large Language Models (LLMs) have recently emerged as effective surrogate
models and candidate generators within global optimization frameworks for
expensive blackbox functions. Despite promising results, LLM-based methods
often struggle in high-dimensional search spaces or when lacking
domain-specific priors, leading to **sparse** or uninformative suggestions. To
overcome these limitations, we propose HOLLM, a novel global optimization
algorithm that enhances LLM-driven sampling by partitioning the search space
into promising subregions. Each subregion acts as a ``meta-arm'' selected via a
bandit-inspired scoring mechanism that effectively balances exploration and
exploitation. Within each selected subregion, an LLM then proposes high-quality
candidate points, without any explicit domain knowledge. Empirical evaluation
on standard optimization benchmarks shows that HOLLM consistently matches or
surpasses leading Bayesian optimization and trust-region methods, while
substantially outperforming global LLM-based sampling strategies.


## Towards Interpretability Without Sacrifice Faithful Dense Layer Decomposition with Mixture of Decoders

>Authors: James Oldfield, Shawn Im, Yixuan Li, Mihalis A. Nicolaou, Ioannis Patras, Grigorios G Chrysos

>2025-05-27

> http://arxiv.org/abs/2505.21364v1

Multilayer perceptrons (MLPs) are an integral part of large language models,
yet their dense representations render them difficult to understand, edit, and
steer. Recent methods learn interpretable approximations via neuron-level
**sparsity**, yet fail to faithfully reconstruct the original
mapping--significantly increasing model's next-token cross-entropy loss. In
this paper, we advocate for moving to layer-level **sparsity** to overcome the
accuracy trade-off in **sparse** layer approximation. Under this paradigm, we
introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear
Units, expanding pre-trained dense layers into tens of thousands of specialized
sublayers. Through a flexible form of tensor factorization, each **sparse**ly
activating MxD sublayer implements a linear transformation with full-rank
weights--preserving the original decoders' expressive capacity even under heavy
**sparsity**. Experimentally, we show that MxDs significantly outperform
state-of-the-art methods (e.g., Transcoders) on the **sparsity**-accuracy frontier
in language models with up to 3B parameters. Further evaluations on **sparse**
probing and feature steering demonstrate that MxDs learn similarly specialized
features of natural language--opening up a promising new avenue for designing
interpretable yet faithful decompositions. Our code is included at:
https://github.com/james-oldfield/MxD/.


## HoliTom Holistic Token Merging for Fast Video Large Language Models

>Authors: Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang

>2025-05-27

> http://arxiv.org/abs/2505.21334v2

Video large language models (video LLMs) excel at video comprehension but
face significant computational inefficiency due to redundant video tokens.
Existing token **pruning** methods offer solutions. However, approaches operating
within the LLM (inner-LLM **pruning**), such as FastV, incur intrinsic
computational overhead in shallow layers. In contrast, methods performing token
**pruning** before the LLM (outer-LLM **pruning**) primarily address spatial redundancy
within individual frames or limited temporal windows, neglecting the crucial
global temporal dynamics and correlations across longer video sequences. This
leads to sub-optimal spatio-temporal reduction and does not leverage video
compressibility fully. Crucially, the synergistic potential and mutual
influence of combining these strategies remain unexplored. To further reduce
redundancy, we introduce HoliTom, a novel training-free holistic token merging
framework. HoliTom employs outer-LLM **pruning** through global redundancy-aware
temporal segmentation, followed by spatial-temporal merging to reduce visual
tokens by over 90%, significantly alleviating the LLM's computational burden.
Complementing this, we introduce a robust inner-LLM token similarity-based
merging approach, designed for superior performance and compatibility with
outer-LLM **pruning**. Evaluations demonstrate our method's promising
efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational
costs to 6.9% of FLOPs while maintaining 99.1% of the original performance.
Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a
1.32x **acceleration** in decoding throughput, highlighting the practical benefits
of our integrated **pruning** approach for efficient video LLMs inference.


## Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation

>Authors: Zenghao Zheng, Lianping Yang, Hegui Zhu, Mingrui Ye

>2025-05-27

> http://arxiv.org/abs/2505.21309v1

Transformer-based 3D human pose estimation methods suffer from high
computational costs due to the quadratic complexity of self-attention with
respect to sequence length. Additionally, pose sequences often contain
significant redundancy between frames. However, recent methods typically fail
to improve model capacity while effectively eliminating sequence redundancy. In
this work, we introduce the Spectral Compression Transformer (SCT) to reduce
sequence length and accelerate computation. The SCT encoder treats hidden
features between blocks as Temporal Feature Signals (TFS) and applies the
Discrete Cosine Transform, a Fourier transform-based technique, to determine
the spectral components to be retained. By filtering out certain high-frequency
noise components, SCT compresses the sequence length and reduces redundancy. To
further enrich the input sequence with prior structural information, we propose
the Line Pose Graph (LPG) based on line graph theory. The LPG generates
skeletal position information that complements the input 2D joint positions,
thereby improving the model's performance. Finally, we design a dual-stream
network architecture to effectively model spatial joint relationships and the
compressed motion trajectory within the pose sequence. Extensive experiments on
two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our
model achieves state-of-the-art performance with improved computational
efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE
of 37.7mm while maintaining a low computational cost. Furthermore, we perform
ablation studies on each module to assess its effectiveness. The code and
models will be released.


## JavaSith A Client-Side Framework for Analyzing Potentially Malicious Extensions in Browsers, VS Code, and NPM Packages

>Authors: Avihay Cohen

>2025-05-27

> http://arxiv.org/abs/2505.21263v1

Modern software supply chains face an increasing threat from malicious code
hidden in trusted components such as browser extensions, IDE extensions, and
open-source packages. This paper introduces JavaSith, a novel client-side
framework for analyzing potentially malicious extensions in web browsers,
Visual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a
runtime sandbox that emulates browser/Node.js extension APIs (with a ``time
machine'' to accelerate time-based triggers) with static analysis and a local
large language model (LLM) to assess risk from code and metadata. We present
the design and architecture of JavaSith, including techniques for intercepting
extension behavior over simulated time and extracting suspicious patterns.
Through case studies on real-world attacks (such as a supply-chain compromise
of a Chrome extension and malicious VSCode extensions installing cryptominers),
we demonstrate how JavaSith can catch stealthy malicious behaviors that evade
traditional detection. We evaluate the framework's effectiveness and discuss
its limitations and future enhancements. JavaSith's client-side approach
empowers end-users/organizations to vet extensions and packages before
trustingly integrating them into their environments.


## BindEnergyCraft Casting Protein Structure Predictors as Energy-Based Models for Binder Design

>Authors: Divya Nori, Anisha Parsan, Caroline Uhler, Wengong Jin

>2025-05-27

> http://arxiv.org/abs/2505.21241v1

Protein binder design has been transformed by hallucination-based methods
that optimize structure prediction confidence metrics, such as the interface
predicted TM-score (ipTM), via backpropagation. However, these metrics do not
reflect the statistical likelihood of a binder-target complex under the learned
distribution and yield **sparse** gradients for optimization. In this work, we
propose a method to extract such likelihoods from structure predictors by
reinterpreting their confidence outputs as an energy-based model (EBM). By
leveraging the Joint Energy-based Modeling (JEM) framework, we introduce
pTMEnergy, a statistical energy function derived from predicted inter-residue
error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a
design pipeline that maintains the same optimization framework as BindCraft but
replaces ipTM with our energy-based objective. BECraft outperforms BindCraft,
RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in
silico binder success rates while reducing structural clashes. Furthermore,
pTMEnergy establishes a new state-of-the-art in structure-based virtual
screening tasks for miniprotein and RNA aptamer binders.


## CROP Contextual Region-Oriented Visual Token Pruning

>Authors: Jiawei Guo, Feifei Zhai, Pu Jian, Qianrun Wei, Yu Zhou

>2025-05-27

> http://arxiv.org/abs/2505.21233v1

Current VLM-based VQA methods often process entire images, leading to
excessive visual tokens that include redundant information irrelevant to the
posed question. This abundance of unnecessary image details creates numerous
visual tokens, drastically increasing memory and computational requirements in
VLMs. To address this, we propose Contextual Region-Oriented Visual Token
Pruning (CROP), a novel framework to compress visual tokens through a two-step
process: Localization and Pruning. Specifically, CROP first employs an
efficient model to identify the contextual region relevant to the input query.
Subsequently, two distinct strategies are introduced for **pruning**: (1) Pre-LLM
Compression (PLC), which adaptively compresses different image regions with
varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that
prunes tokens within early LLM layers guided by the identified contextual
region. Extensive experiments on a wide range of VQA tasks demonstrate that
CROP significantly outperforms existing visual token **pruning** methods and
achieves state-of-the-art performance. Our code and datasets will be made
available.


## Unveiling Instruction-Specific Neurons & Experts An Analytical Framework for LLM's Instruction-Following Capabilities

>Authors: Junyan Zhang, Yubo Gao, Yibo Yan, Jungang Li, Zhaorui Hou, Sicheng Tao, Shuliang Liu, Song Dai, Yonghua Hei, Junzhuo Li, Xuming Hu

>2025-05-27

> http://arxiv.org/abs/2505.21191v1

The finetuning of Large Language Models (LLMs) has significantly advanced
their instruction-following capabilities, yet the underlying computational
mechanisms driving these improvements remain poorly understood. This study
systematically examines how fine-tuning reconfigures LLM computations by
isolating and analyzing instruction-specific **sparse** components, i.e., neurons
in dense models and both neurons and experts in Mixture-of-Experts (MoE)
architectures. In particular, we introduce HexaInst, a carefully curated and
balanced instructional dataset spanning six distinct categories, and propose
SPARCOM, a novel analytical framework comprising three key contributions: (1) a
method for identifying these **sparse** components, (2) an evaluation of their
functional generality and uniqueness, and (3) a systematic comparison of their
alterations. Through experiments, we demonstrate functional generality,
uniqueness, and the critical role of these components in instruction execution.
By elucidating the relationship between fine-tuning-induced adaptations and
**sparse** computational substrates, this work provides deeper insights into how
LLMs internalize instruction-following behavior for the trustworthy LLM
community.


## M-Wanda Improving One-Shot Pruning for Multilingual LLMs

>Authors: Rochelle Choenni, Ivan Titov

>2025-05-27

> http://arxiv.org/abs/2505.21171v1

Multilingual LLM performance is often critically dependent on model size.
With an eye on efficiency, this has led to a surge in interest in one-shot
**pruning** methods that retain the benefits of large-scale pretraining while
shrinking the model size. However, as **pruning** tends to come with performance
loss, it is important to understand the trade-offs between multilinguality and
sparsification. In this work, we study multilingual performance under different
**sparsity** constraints and show that moderate ratios already substantially harm
performance. To help bridge this gap, we propose M-Wanda, a **pruning** method that
models cross-lingual variation by incorporating language-aware activation
statistics into its **pruning** criterion and dynamically adjusts layerwise
**sparsity** based on cross-lingual importance. We show that M-Wanda consistently
improves performance at minimal additional costs. We are the first to
explicitly optimize **pruning** to retain multilingual performance, and hope to
inspire future advances in multilingual **pruning**.


## FastFace Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention

>Authors: Sergey Karpukhin, Vadim Titov, Andrey Kuznetsov, Aibek Alanov

>2025-05-27

> http://arxiv.org/abs/2505.21144v2

In latest years plethora of identity-preserving adapters for a personalized
generation with diffusion models have been released. Their main disadvantage is
that they are dominantly trained jointly with base diffusion models, which
suffer from slow multi-step inference. This work aims to tackle the challenge
of training-free adaptation of pretrained ID-adapters to diffusion models
accelerated via distillation - through careful re-design of classifier-free
guidance for few-step stylistic generation and attention manipulation
mechanisms in decoupled blocks to improve identity similarity and fidelity, we
propose universal FastFace framework. Additionally, we develop a disentangled
public evaluation protocol for id-preserving adapters.


## SageAttention2++ A More Efficient Implementation of SageAttention2

>Authors: Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen

>2025-05-27

> http://arxiv.org/abs/2505.21136v2

The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
**quantization** to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.


## Decoding Breast Cancer in X-ray Mammograms A Multi-Parameter Approach Using Fractals, Multifractals, and Structural Disorder Analysis

>Authors: Santanu Maity, Mousa Alrubayan, Prabhakar Pradhan

>2025-05-27

> http://arxiv.org/abs/2505.21080v1

We explored the fractal and multifractal characteristics of breast mammogram
micrographs to identify quantitative biomarkers associated with breast cancer
progression. In addition to conventional fractal and multifractal analyses, we
employed a recently developed fractal-functional distribution method, which
transforms fractal measures into Gaussian distributions for more robust
statistical interpretation. Given the **sparsity** of mammogram intensity data, we
also analyzed how variations in intensity thresholds, used for binary
transformations of the fractal dimension, follow unique trajectories that may
serve as novel indicators of disease progression. Our findings demonstrate that
fractal, multifractal, and fractal-functional parameters effectively
differentiate between benign and cancerous tissue. Furthermore, the
threshold-dependent behavior of intensity-based fractal measures presents
distinct patterns in cancer cases. To complement these analyses, we applied the
Inverse Participation Ratio (IPR) light localization technique to quantify
structural disorder at the microscopic level. This multi-parametric approach,
integrating spatial complexity and structural disorder metrics, offers a
promising framework for enhancing the sensitivity and specificity of breast
cancer detection.


## Efficient Large Language Model Inference with Neural Block Linearization

>Authors: Mete Erdogan, Francesco Tonin, Volkan Cevher

>2025-05-27

> http://arxiv.org/abs/2505.21077v1

The high inference demands of transformer-based Large Language Models (LLMs)
pose substantial challenges in their deployment. To this end, we introduce
Neural Block Linearization (NBL), a novel framework for accelerating
transformer model inference by replacing self-attention layers with linear
approximations derived from Linear Minimum Mean Squared Error estimators. NBL
leverages Canonical Correlation Analysis to compute a theoretical upper bound
on the approximation error. Then, we use this bound as a criterion for
substitution, selecting the LLM layers with the lowest linearization error. NBL
can be efficiently applied to pre-trained LLMs without the need for
fine-tuning. In experiments, NBL achieves notable computational speed-ups while
preserving competitive accuracy on multiple reasoning benchmarks. For instance,
applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B
increases the inference speed by 32% with less than 1% accuracy trade-off,
making it a flexible and promising solution to improve the inference efficiency
of LLMs.


## RainFusion Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy

>Authors: Aiyue Chen, Bin Dong, Jingru Li, Jing Lin, Yiwu Yao, Gongyi Wang

>2025-05-27

> http://arxiv.org/abs/2505.21036v1

Video generation using diffusion models is highly computationally intensive,
with 3D attention in Diffusion Transformer (DiT) models accounting for over
80\% of the total computational resources. In this work, we introduce {\bf
RainFusion}, a novel training-free **sparse** attention method that exploits
inherent **sparsity** nature in visual data to accelerate attention computation
while preserving video quality. Specifically, we identify three unique **sparse**
patterns in video generation attention calculations--Spatial Pattern, Temporal
Pattern and Textural Pattern. The **sparse** pattern for each attention head is
determined online with negligible overhead (\textasciitilde\,0.2\%) with our
proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed
{\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated
into state-of-the-art 3D-attention video generation models without additional
training or calibration. We evaluate our method on leading open-sourced models
including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its
broad applicability and effectiveness. Experimental results show that
RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation
while maintaining video quality, with only a minimal impact on VBench scores
(-0.2\%).


## LLaMEA-BO A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms

>Authors: Wenhu Li, Niki van Stein, Thomas Bäck, Elena Raponi

>2025-05-27

> http://arxiv.org/abs/2505.21034v1

Bayesian optimization (BO) is a powerful class of algorithms for optimizing
expensive black-box functions, but designing effective BO algorithms remains a
manual, expertise-driven task. Recent advancements in Large Language Models
(LLMs) have opened new avenues for automating scientific discovery, including
the automatic design of optimization algorithms. While prior work has used LLMs
within optimization loops or to generate non-BO algorithms, we tackle a new
challenge: Using LLMs to automatically generate full BO algorithm code. Our
framework uses an evolution strategy to guide an LLM in generating Python code
that preserves the key components of BO algorithms: An initial design, a
surrogate model, and an acquisition function. The LLM is prompted to produce
multiple candidate algorithms, which are evaluated on the established Black-Box
Optimization Benchmarking (BBOB) test suite from the COmparing Continuous
Optimizers (COCO) platform. Based on their performance, top candidates are
selected, combined, and mutated via controlled prompt variations, enabling
iterative refinement. Despite no additional fine-tuning, the LLM-generated
algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB
functions in dimension 5 and generalize well to higher dimensions, and
different tasks (from the Bayesmark framework). This work demonstrates that
LLMs can serve as algorithmic co-designers, offering a new paradigm for
automating BO development and accelerating the discovery of novel algorithmic
combinations. The source code is provided at
https://github.com/Ewendawi/LLaMEA-BO.


## Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning

>Authors: Nikos Giannakakis, Argyris Manetas, Panagiotis P. Filntisis, Petros Maragos, George Retsinas

>2025-05-27

> http://arxiv.org/abs/2505.20962v1

Learning visual representations from observing actions to benefit robot
visuo-motor policy generation is a promising direction that closely resembles
human cognitive function and perception. Motivated by this, and further
inspired by psychological theories suggesting that humans process scenes in an
object-based fashion, we propose an object-centric encoder that performs
semantic segmentation and visual representation generation in a coupled manner,
unlike other works, which treat these as separate processes. To achieve this,
we leverage the Slot Attention mechanism and use the SOLV model, pretrained in
large out-of-domain datasets, to bootstrap fine-tuning on human action video
data. Through simulated robotic tasks, we demonstrate that visual
representations can enhance reinforcement and imitation learning training,
highlighting the effectiveness of our integrated approach for semantic
segmentation and encoding. Furthermore, we show that exploiting models
pretrained on out-of-domain datasets can benefit this process, and that
fine-tuning on datasets depicting human actions -- although still out-of-domain
-- , can significantly improve performance due to close alignment with robotic
tasks. These findings show the capability to reduce reliance on annotated or
robot-specific action datasets and the potential to build on existing visual
encoders to accelerate training and improve generalizability.


## Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization

>Authors: Yiyuan Yang, Shitong Xu, Niki Trigoni, Andrew Markham

>2025-05-27

> http://arxiv.org/abs/2505.20961v1

Sound source localization (SSL) is a critical technology for determining the
position of sound sources in complex environments. However, existing methods
face challenges such as high computational costs and precise calibration
requirements, limiting their deployment in dynamic or resource-constrained
environments. This paper introduces a novel 3D SSL framework, which uses **sparse**
cross-attention, pretraining, and adaptive signal coherence metrics, to achieve
accurate and computationally efficient localization with fewer input
microphones. The framework is also fault-tolerant to unreliable or even unknown
microphone position inputs, ensuring its applicability in real-world scenarios.
Preliminary experiments demonstrate its scalability for multi-source
localization without requiring additional hardware. This work advances SSL by
balancing the model's performance and efficiency and improving its robustness
for real-world scenarios.


## RepoMaster Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving

>Authors: Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, Pin Lyu

>2025-05-27

> http://arxiv.org/abs/2505.21577v1

The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.


## FireQ Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration

>Authors: Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi, Kihyo Moon, Minsung Jang, Hyojung Lee

>2025-05-27

> http://arxiv.org/abs/2505.20839v2

As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
**quantization** (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ **quantize**s linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from **quantization**, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
**quantization** scaling factor of INT4 **quantization**, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
**quantization** challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.


## SpecExtend A Drop-in Enhancement for Speculative Decoding of Long Sequences

>Authors: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

>2025-05-27

> http://arxiv.org/abs/2505.20776v1

Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), but its performance degrades on long inputs
due to increased attention cost and reduced draft accuracy. We introduce
SpecExtend, a drop-in enhancement that improves the performance of speculative
decoding on long sequences without any additional training. SpecExtend
integrates efficient attention mechanisms such as FlashAttention and Hybrid
Tree Attention into both the draft and target models, reducing latency across
all stages. To improve draft accuracy and speed, we propose Cross-model
Retrieval, a novel **KV** cache update strategy that uses the target model's
attention scores to dynamically select relevant context for the draft model.
Extensive evaluations on three long-context understanding datasets show that
SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x
for inputs up to 16K tokens, providing an effective solution for speculative
decoding of long sequences. The code is available at
https://github.com/jycha98/SpecExtend .


## Cold-Start Recommendation with Knowledge-Guided Retrieval-Augmented Generation

>Authors: Wooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei Han, Yu Wang, Junhyun Lee, Philip S. Yu

>2025-05-27

> http://arxiv.org/abs/2505.20773v1

Cold-start items remain a persistent challenge in recommender systems due to
their lack of historical user interactions, which collaborative models rely on.
While recent zero-shot methods leverage large language models (LLMs) to address
this, they often struggle with **sparse** metadata and hallucinated or incomplete
knowledge. We propose ColdRAG, a retrieval-augmented generation approach that
builds a domain-specific knowledge graph dynamically to enhance LLM-based
recommendation in cold-start scenarios, without requiring task-specific
fine-tuning. ColdRAG begins by converting structured item attributes into rich
natural-language profiles, from which it extracts entities and relationships to
construct a unified knowledge graph capturing item semantics. Given a user's
interaction history, it scores edges in the graph using an LLM, retrieves
candidate items with supporting evidence, and prompts the LLM to rank them. By
enabling multi-hop reasoning over this graph, ColdRAG grounds recommendations
in verifiable evidence, reducing hallucinations and strengthening semantic
connections. Experiments on three public benchmarks demonstrate that ColdRAG
surpasses existing zero-shot baselines in both Recall and NDCG. This framework
offers a practical solution to cold-start recommendation by combining
knowledge-graph reasoning with retrieval-augmented LLM generation.


## MetaSlot Break Through the Fixed Number of Slots in Object-Centric Learning

>Authors: Hongjia Liu, Rongzhen Zhao, Haohan Chen, Joni Pajarinen

>2025-05-27

> http://arxiv.org/abs/2505.20772v1

Learning object-level, structured representations is widely regarded as a key
to better generalization in vision and underpins the design of next-generation
Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)
methods adopt Slot Attention or its variants to iteratively aggregate objects'
super-pixels into a fixed set of query feature vectors, termed slots. However,
their reliance on a static slot count leads to an object being represented as
multiple parts when the number of objects varies. We introduce MetaSlot, a
plug-and-play Slot Attention variant that adapts to variable object counts.
MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset
by vector-quantizing the resulting slot representations; (ii) removes duplicate
slots from the traditionally aggregated slots by quantizing them with the
codebook; and (iii) injects progressively weaker noise into the Slot Attention
iterations to accelerate and stabilize the aggregation. MetaSlot is a general
Slot Attention variant that can be seamlessly integrated into existing OCL
architectures. Across multiple public datasets and tasks--including object
discovery and recognition--models equipped with MetaSlot achieve significant
performance gains and markedly interpretable slot representations, compared
with existing Slot Attention variants.


## MoPFormer Motion-Primitive Transformer for Wearable-Sensor Activity Recognition

>Authors: Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang

>2025-05-27

> http://arxiv.org/abs/2505.20744v1

Human Activity Recognition (HAR) with wearable sensors is challenged by
limited interpretability, which significantly impacts cross-dataset
generalization. To address this challenge, we propose Motion-Primitive
Transformer (MoPFormer), a novel self-supervised framework that enhances
interpretability by tokenizing inertial measurement unit signals into
semantically meaningful motion primitives and leverages a Transformer
architecture to learn rich temporal representations. MoPFormer comprises
two-stages. first stage is to partition multi-channel sensor streams into short
segments and quantizing them into discrete "motion primitive" codewords, while
the second stage enriches those tokenized sequences through a context-aware
embedding module and then processes them with a Transformer encoder. The
proposed MoPFormer can be pre-trained using a masked motion-modeling objective
that reconstructs missing primitives, enabling it to develop robust
representations across diverse sensor configurations. Experiments on six HAR
benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art
methods but also successfully generalizes across multiple datasets. Most
importantly, the learned motion primitives significantly enhance both
interpretability and cross-dataset performance by capturing fundamental
movement patterns that remain consistent across similar activities regardless
of dataset origin.


## LeDiFlow Learned Distribution-guided Flow Matching to Accelerate Image Generation

>Authors: Pascal Zwick, Nils Friederich, Maximilian Beichter, Lennart Hilbert, Ralf Mikut, Oliver Bringmann

>2025-05-27

> http://arxiv.org/abs/2505.20723v1

Enhancing the efficiency of high-quality image generation using Diffusion
Models (DMs) is a significant challenge due to the iterative nature of the
process. Flow Matching (FM) is emerging as a powerful generative modeling
paradigm based on a simulation-free training objective instead of a score-based
one used in DMs. Typical FM approaches rely on a Gaussian distribution prior,
which induces curved, conditional probability paths between the prior and
target data distribution. These curved paths pose a challenge for the Ordinary
Differential Equation (ODE) solver, requiring a large number of inference calls
to the flow prediction network. To address this issue, we present Learned
Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for
training FM-based image generation models using a better-suited prior
distribution learned via a regression-based auxiliary model. By initializing
the ODE solver with a prior closer to the target data distribution, LeDiFlow
enables the learning of more computationally tractable probability paths. These
paths directly translate to fewer solver steps needed for high-quality image
generation at inference time. Our method utilizes a State-Of-The-Art (SOTA)
transformer architecture combined with latent space sampling and can be trained
on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably
outperforms the respective FM baselines. For instance, when operating directly
on pixels, our model accelerates inference by up to 3.75x compared to the
corresponding pixel-space baseline. Simultaneously, our latent FM model
enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy
(CMMD) metric against its respective baseline.


## Sparsified State-Space Models are Efficient Highway Networks

>Authors: Woomin Song, Jihoon Tack, Sangwoo Mo, Seunghyuk Oh, Jinwoo Shin

>2025-05-27

> http://arxiv.org/abs/2505.20698v1

State-space models (SSMs) offer a promising architecture for sequence
modeling, providing an alternative to Transformers by replacing expensive
self-attention with linear recurrences. In this paper, we propose a simple yet
effective trick to enhance SSMs within given computational budgets by
sparsifying them. Our intuition is that tokens in SSMs are highly redundant due
to gradual recurrent updates, and dense recurrence operations block the
delivery of past information. In particular, we observe that upper layers of
SSMs tend to be more redundant as they encode global information, while lower
layers encode local information. Motivated by this, we introduce Simba, a
hierarchical sparsification method for SSMs based on token **pruning**. Simba
sparsifies upper layers more than lower layers, encouraging the upper layers to
behave like highways. To achieve this, we propose a novel token **pruning**
criterion for SSMs, measuring the global impact of tokens on the final output
by accumulating local recurrences. We demonstrate that Simba outperforms the
baseline model, Mamba, with the same FLOPS in various natural language tasks.
Moreover, we illustrate the effect of highways, showing that Simba not only
enhances efficiency but also improves the information flow across long
sequences. Code is available at https://github.com/woominsong/Simba.


## Open-Det An Efficient Learning Framework for Open-Ended Detection

>Authors: Guiping Cao, Tao Wang, Wenjian Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang

>2025-05-27

> http://arxiv.org/abs/2505.20639v1

Open-Ended object Detection (OED) is a novel and challenging task that
detects objects and generates their category names in a free-form manner,
without requiring additional vocabularies during inference. However, the
existing OED models, such as GenerateU, require large-scale datasets for
training, suffer from slow convergence, and exhibit limited performance. To
address these issues, we present a novel and efficient Open-Det framework,
consisting of four collaborative parts. Specifically, Open-Det accelerates
model training in both the bounding box and object name generation process by
reconstructing the Object Detector and the Object Name Generator. To bridge the
semantic gap between Vision and Language modalities, we propose a
Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms,
incorporating with the Prompts Distiller to transfer knowledge from the VLM
into VL-prompts, enabling accurate object name generation for the LLM. In
addition, we design a Masked Alignment Loss to eliminate contradictory
supervision and introduce a Joint Loss to enhance classification, resulting in
more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of
the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs.
149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher
performance (+1.0% in APr). The source codes are available at:
https://github.com/Med-Process/Open-Det.


## Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction

>Authors: Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, Bin Ma

>2025-05-27

> http://arxiv.org/abs/2505.20635v1

Audio-visual speaker extraction isolates a target speaker's speech from a
mixture speech signal conditioned on a visual cue, typically using the target
speaker's face recording. However, in real-world scenarios, other co-occurring
faces are often present on-screen, providing valuable speaker activity cues in
the scene. In this work, we introduce a plug-and-play inter-speaker attention
module to process these flexible numbers of co-occurring faces, allowing for
more accurate speaker extraction in complex multi-person environments. We
integrate our module into two prominent models: the AV-DPRNN and the
state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,
including the highly overlapped VoxCeleb2 and **sparse**ly overlapped MISP,
demonstrate that our approach consistently outperforms baselines. Furthermore,
cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and
generalizability of our method.


## Knowledge Distillation Approach for SOS Fusion Staging Towards Fully Automated Skeletal Maturity Assessment

>Authors: Omid Halimi Milani, Amanda Nikho, Marouane Tliba, Lauren Mills, Ahmet Enis Cetin, Mohammed H Elnagar

>2025-05-27

> http://arxiv.org/abs/2505.21561v1

We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.


## Electrolyzers-HSI Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset

>Authors: Elias Arbash, Ahmed Jamal Afifi, Ymane Belahsen, Margret Fuchs, Pedram Ghamisi, Paul Scheunders, Richard Gloaguen

>2025-05-26

> http://arxiv.org/abs/2505.20507v1

The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.


## HAMburger Accelerating LLM Inference via Token Smashing

>Authors: Jingyu Liu, Ce Zhang

>2025-05-26

> http://arxiv.org/abs/2505.20438v1

The growing demand for efficient Large Language Model (LLM) inference
requires a holistic optimization on algorithms, systems, and hardware. However,
very few works have fundamentally changed the generation pattern: each token
needs one forward pass and one **KV** cache. This can be sub-optimal because we
found that LLMs are extremely capable of self-identifying the exact dose of
information that a single **KV** cache can store, and many tokens can be generated
confidently without global context. Based on this insight, we introduce
HAMburger, a Hierarchically Auto-regressive Model that redefines resource
allocation in LLMs by moving beyond uniform computation and storage per token
during inference. Stacking a compositional embedder and a micro-step decoder in
between a base LLM, HAMburger smashes multiple tokens into a single **KV** and
generates several tokens per step. Additionally, HAMburger functions as a
speculative decoding framework where it can blindly trust self-drafted tokens.
As a result, HAMburger shifts the growth of **KV** cache and forward FLOPs from
linear to sub-linear with respect to output length, and adjusts its inference
speed based on query perplexity and output structure. Extensive evaluations
show that HAMburger reduces the **KV** cache computation by up to 2$\times$ and
achieves up to 2$\times$ TPS, while maintaining quality in both short- and
long-context tasks. Our method explores an extremely challenging inference
regime that requires both computation- and memory-efficiency with a
hardware-agnostic design.


## SCAR Shapley Credit Assignment for More Efficient RLHF

>Authors: Meng Cao, Shuyuan Zhang, Xiao-Wen Chang, Doina Precup

>2025-05-26

> http://arxiv.org/abs/2505.20417v1

Reinforcement Learning from Human Feedback (RLHF) is a widely used technique
for aligning Large Language Models (LLMs) with human preferences, yet it often
suffers from **sparse** reward signals, making effective credit assignment
challenging. In typical setups, the reward model provides a single scalar score
for an entire generated sequence, offering little insight into which token or
span-level decisions were responsible for the outcome. To address this, we
propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages
Shapley values in cooperative game theory. SCAR distributes the total
sequence-level reward among constituent tokens or text spans based on their
principled marginal contributions. This creates dense reward signals,
crucially, without necessitating the training of auxiliary critique models or
recourse to fine-grained human annotations at intermediate generation stages.
Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for
fair credit attribution. Theoretically, we demonstrate that SCAR preserves the
original optimal policy, and empirically, across diverse tasks including
sentiment control, text summarization, and instruction tuning, we show that
SCAR converges significantly faster and achieves higher final reward scores
compared to standard RLHF and attention-based dense reward baselines. Our
findings suggest that SCAR provides a more effective and theoretically sound
method for credit assignment in RLHF, leading to more efficient alignment of
LLMs.


## Does quantization affect models' performance on long-context tasks?

>Authors: Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit Iyyer

>2025-05-26

> http://arxiv.org/abs/2505.20276v2

Large language models (LLMs) now support context windows exceeding 128K
tokens, but this comes with significant memory requirements and high inference
latency. Quantization can mitigate these costs, but may degrade performance. In
this work, we present the first systematic evaluation of **quantize**d LLMs on
tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation
spans 9.7K test examples, five **quantization** methods (FP8, GPTQ-int8, AWQ-int4,
GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,
and 72B). We find that, on average, 8-bit **quantization** preserves accuracy
(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for
tasks involving long context inputs (drops of up to 59%). This degradation
tends to worsen when the input is in a language other than English. Crucially,
the effects of **quantization** depend heavily on the **quantization** method, model,
and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,
Llama-3.1 70B experiences a 32% performance drop on the same task. These
findings highlight the importance of a careful, task-specific evaluation before
deploying **quantize**d LLMs, particularly in long-context scenarios and with
languages other than English.


## syftr Pareto-Optimal Generative AI

>Authors: Alexander Conway, Debadeepta Dey, Stefan Hackmann, Matthew Hausknecht, Michael Schmidt, Mark Steadman, Nick Volynets

>2025-05-26

> http://arxiv.org/abs/2505.20266v1

Retrieval-Augmented Generation (RAG) pipelines are central to applying large
language models (LLMs) to proprietary or dynamic data. However, building
effective RAG flows is complex, requiring careful selection among vector
databases, embedding models, text splitters, retrievers, and synthesizing LLMs.
The challenge deepens with the rise of agentic paradigms. Modules like
verifiers, rewriters, and rerankers-each with intricate hyperparameter
dependencies have to be carefully tuned. Balancing tradeoffs between latency,
accuracy, and cost becomes increasingly difficult in performance-sensitive
applications.
  We introduce syftr, a framework that performs efficient multi-objective
search over a broad space of agentic and non-agentic RAG configurations. Using
Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly
optimize task accuracy and cost. A novel early-stopping mechanism further
improves efficiency by **pruning** clearly suboptimal candidates. Across multiple
RAG benchmarks, syftr finds flows which are on average approximately 9 times
cheaper while preserving most of the accuracy of the most accurate flows on the
Pareto-frontier. Furthermore, syftr's ability to design and optimize allows
integrating new modules, making it even easier and faster to realize
high-performing generative AI pipelines.


## Position Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs

>Authors: Xiangchen Song, Aashiq Muhamed, Yujia Zheng, Lingjing Kong, Zeyu Tang, Mona T. Diab, Virginia Smith, Kun Zhang

>2025-05-26

> http://arxiv.org/abs/2505.20254v1

Sparse Autoencoders (SAEs) are a prominent tool in mechanistic
interpretability (MI) for decomposing neural network activations into
interpretable features. However, the aspiration to identify a canonical set of
features is challenged by the observed inconsistency of learned SAE features
across different training runs, undermining the reliability and efficiency of
MI research. This position paper argues that mechanistic interpretability
should prioritize feature consistency in SAEs -- the reliable convergence to
equivalent feature sets across independent runs. We propose using the Pairwise
Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to
operationalize consistency and demonstrate that high levels are achievable
(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.
Our contributions include detailing the benefits of prioritizing consistency;
providing theoretical grounding and synthetic validation using a model
organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;
and extending these findings to real-world LLM data, where high feature
consistency strongly correlates with the semantic similarity of learned feature
explanations. We call for a community-wide shift towards systematically
measuring feature consistency to foster robust cumulative progress in MI.


## Efficient Optimization Accelerator Framework for Multistate Ising Problems

>Authors: Chirag Garg, Sayeef Salahuddin

>2025-05-26

> http://arxiv.org/abs/2505.20250v1

Ising Machines are a prominent class of hardware architectures that aim to
solve NP-hard combinatorial optimization problems. These machines consist of a
network of interacting binary spins/neurons that evolve to represent the
optimum ground state energy solution. Generally, combinatorial problems are
transformed into quadratic unconstrained binary optimization (QUBO) form to
harness the computational efficiency of these Ising machines. However, this
transformation, especially for multi-state problems, often leads to a more
complex exploration landscape than the original problem, thus severely
impacting the solution quality. To address this challenge, we model the spin
interactions as a generalized boolean logic function to significantly reduce
the exploration space. We benchmark the graph coloring problem from the class
of multi-state NP-hard optimization using probabilistic Ising solvers to
illustrate the effectiveness of our framework. The proposed methodology
achieves similar accuracy compared to state-of-the-art heuristics and machine
learning algorithms, and demonstrates significant improvement over the existing
Ising methods. Additionally, we demonstrate that combining parallel tempering
with our existing framework further reduces the coloring error by up to 50%
compared to the conventionally used Gibbs sampling algorithm. We also design a
1024-neuron all-to-all connected probabilistic Ising accelerator that shows up
to 10000x performance **acceleration** compared to heuristics while reducing the
number of required physical neurons by 1.5-4x compared to conventional Ising
machines. Indeed, this accelerator solution demonstrates improvement across all
metrics over the current methods, i.e., energy, performance, area, and solution
quality. Thus, this work expands the potential of existing Ising hardware to
solve a broad class of these multistate optimization problems.


## From What to How Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance

>Authors: Maximilian Dreyer, Lorenz Hufe, Jim Berend, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

>2025-05-26

> http://arxiv.org/abs/2505.20229v1

Transformer-based CLIP models are widely used for text-image probing and
feature extraction, making it relevant to understand the internal mechanisms
behind their predictions. While recent works show that Sparse Autoencoders
(SAEs) yield interpretable latent components, they focus on what these encode
and miss how they drive predictions. We introduce a scalable framework that
reveals what latent components activate for, how they align with expected
semantics, and how important they are to predictions. To achieve this, we adapt
attribution patching for instance-wise component attributions in CLIP and
highlight key faithfulness limitations of the widely used Logit Lens technique.
By combining attributions with semantic alignment scores, we can automatically
uncover reliance on components that encode semantically unexpected or spurious
concepts. Applied across multiple CLIP variants, our method uncovers hundreds
of surprising components linked to polysemous words, compound nouns, visual
typography and dataset artifacts. While text embeddings remain prone to
semantic ambiguity, they are more robust to spurious correlations compared to
linear classifiers trained on image embeddings. A case study on skin lesion
detection highlights how such classifiers can amplify hidden shortcuts,
underscoring the need for holistic, mechanistic interpretability. We provide
code at https://github.com/maxdreyer/attributing-clip.


## FLAME-MoE A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models

>Authors: Hao Kang, Zichun Yu, Chenyan Xiong

>2025-05-26

> http://arxiv.org/abs/2505.20225v1

Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4
increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong
efficiency-performance trade-offs by activating only a fraction of the model
per token. Yet academic researchers still lack a fully open, end-to-end MoE
platform for investigating scaling, routing, and expert behavior. We release
FLAME-MoE, a completely open-source research suite composed of seven
decoder-only models, ranging from 38M to 1.7B active parameters, whose
architecture--64 experts with top-8 gating and 2 shared experts--closely
reflects modern production LLMs. All training data pipelines, scripts, logs,
and checkpoints are publicly available to enable reproducible experimentation.
Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4
points over dense baselines trained with identical FLOPs. Leveraging full
training trace transparency, we present initial analyses showing that (i)
experts increasingly specialize on distinct token subsets, (ii) co-activation
matrices remain **sparse**, reflecting diverse expert usage, and (iii) routing
behavior stabilizes early in training. All code, training logs, and model
checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.


## Gradient Flow Matching for Learning Update Dynamics in Neural Network Training

>Authors: Xiao Shou, Yanna Ding, Jianxi Gao

>2025-05-26

> http://arxiv.org/abs/2505.20221v1

Training deep neural networks remains computationally intensive due to the
itera2 tive nature of gradient-based optimization. We propose Gradient Flow
Matching (GFM), a continuous-time modeling framework that treats neural network
training as a dynamical system governed by learned optimizer-aware vector
fields. By leveraging conditional flow matching, GFM captures the underlying
update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth
extrapolation of weight trajectories toward convergence. Unlike black-box
sequence models, GFM incorporates structural knowledge of gradient-based
updates into the learning objective, facilitating accurate forecasting of final
weights from partial training sequences. Empirically, GFM achieves forecasting
accuracy that is competitive with Transformer-based models and significantly
outperforms LSTM and other classical baselines. Furthermore, GFM generalizes
across neural architectures and initializations, providing a unified framework
for studying optimization dynamics and accelerating convergence prediction.


## PathBench A comprehensive comparison benchmark for pathology foundation models towards precision oncology

>Authors: Jiabo Ma, Yingxue Xu, Fengtao Zhou, Yihui Wang, Cheng Jin, Zhengrui Guo, Jianfeng Wu, On Ki Tang, Huajun Zhou, Xi Wang, Luyang Luo, Zhengyu Zhang, Du Cai, Zizhao Gao, Wei Wang, Yueping Liu, Jiankun He, Jing Cui, Zhenhui Li, Jing Zhang, Feng Gao, Xiuming Zhang, Li Liang, Ronald Cheong Kin Chan, Zhe Wang, Hao Chen

>2025-05-26

> http://arxiv.org/abs/2505.20202v1

The emergence of pathology foundation models has revolutionized computational
histopathology, enabling highly accurate, generalized whole-slide image
analysis for improved cancer diagnosis, and prognosis assessment. While these
models show remarkable potential across cancer diagnostics and prognostics,
their clinical translation faces critical challenges including variability in
optimal model across cancer types, potential data leakage in evaluation, and
lack of standardized benchmarks. Without rigorous, unbiased evaluation, even
the most advanced PFMs risk remaining confined to research settings, delaying
their life-saving applications. Existing benchmarking efforts remain limited by
narrow cancer-type focus, potential pretraining data overlaps, or incomplete
task coverage. We present PathBench, the first comprehensive benchmark
addressing these gaps through: multi-center in-hourse datasets spanning common
cancers with rigorous leakage prevention, evaluation across the full clinical
spectrum from diagnosis to prognosis, and an automated leaderboard system for
continuous model assessment. Our framework incorporates large-scale data,
enabling objective comparison of PFMs while reflecting real-world clinical
complexity. All evaluation data comes from private medical providers, with
strict exclusion of any pretraining usage to avoid data leakage risks. We have
collected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing
over 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs
shows that Virchow2 and H-Optimus-1 are the most effective models overall. This
work provides researchers with a robust platform for model development and
offers clinicians actionable insights into PFM performance across diverse
clinical scenarios, ultimately accelerating the translation of these
transformative technologies into routine pathology practice.


## Research on feature fusion and multimodal patent text based on graph attention network

>Authors: Zhenzhen Song, Ziwei Liu, Hongji Li

>2025-05-26

> http://arxiv.org/abs/2505.20188v1

Aiming at the problems of cross-modal feature fusion, low efficiency of long
text modeling and lack of hierarchical semantic coherence in patent text
semantic mining, this study proposes HGM-Net, a deep learning framework that
integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention
Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a
dynamic mask, contrast and cross-structural similarity constraints on the word,
sentence and paragraph hierarchies through HCL. Contrast and cross-structural
similarity constraints are constructed at the word and paragraph levels by HCL
to strengthen the local semantic and global thematic consistency of patent
text; M-GAT models patent classification codes, citation relations and text
semantics as heterogeneous graph structures, and achieves dynamic fusion of
multi-source features by cross-modal gated attention; MSA adopts a hierarchical
**sparsity** strategy to optimize the computational efficiency of long text
modeling at word, phrase, sentence and paragraph granularity. Experiments show
that the framework demonstrates significant advantages over existing deep
learning methods in tasks such as patent classification and similarity
matching, and provides a solution with both theoretical innovation and
practical value for solving the problems of patent examination efficiency
improvement and technology relevance mining.


## Pangu Light Weight Re-Initialization for Pruning and Accelerating LLMs

>Authors: Hanting Chen, Jiarui Qin, Jialong Guo, Tao Yuan, Yichun Yin, Huiling Zhen, Yasheng Wang, Jinpeng Li, Xiaojun Meng, Meng Zhang, Rongju Ruan, Zheyuan Bai, Yehui Tang, Can Chen, Xinghao Chen, Fisher Yu, Ruiming Tang, Yunhe Wang

>2025-05-26

> http://arxiv.org/abs/2505.20155v1

Large Language Models (LLMs) deliver state-of-the-art capabilities across
numerous tasks, but their immense size and inference costs pose significant
computational challenges for practical deployment. While structured **pruning**
offers a promising avenue for model compression, existing methods often
struggle with the detrimental effects of aggressive, simultaneous width and
depth reductions, leading to substantial performance degradation. This paper
argues that a critical, often overlooked, aspect in making such aggressive
joint **pruning** viable is the strategic re-initialization and adjustment of
remaining weights to improve the model post-**pruning** training accuracies. We
introduce Pangu Light, a framework for LLM **acceleration** centered around
structured **pruning** coupled with novel weight re-initialization techniques
designed to address this ``missing piece''. Our framework systematically
targets multiple axes, including model width, depth, attention heads, and
RMSNorm, with its effectiveness rooted in novel re-initialization methods like
Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)
that mitigate performance drops by providing the network a better training
starting point. Further enhancing efficiency, Pangu Light incorporates
specialized optimizations such as absorbing Post-RMSNorm computations and
tailors its strategies to Ascend NPU characteristics. The Pangu Light models
consistently exhibit a superior accuracy-efficiency trade-off, outperforming
prominent baseline **pruning** methods like Nemotron and established LLMs like
Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average
score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and
2225 tokens/s.


## AdaTP Attention-Debiased Token Pruning for Video Large Language Models

>Authors: Fengyuan Sun, Leqi Shen, Hui Chen, Sicheng Zhao, Jungong Han, Guiguang Ding

>2025-05-26

> http://arxiv.org/abs/2505.20100v1

Video Large Language Models (Video LLMs) have achieved remarkable results in
video understanding tasks. However, they often suffer from heavy computational
overhead due to the large number of visual tokens generated from multiple video
frames. Existing visual token compression methods often rely on attention
scores from language models as guidance. However, these scores exhibit inherent
biases: global bias reflects a tendency to focus on the two ends of the visual
token sequence, while local bias leads to an over-concentration on the same
spatial positions across different frames. To address the issue of attention
bias, we propose $\textbf{A}$ttention-$\textbf{D}$ebi$\textbf{a}$sed
$\textbf{T}$oken $\textbf{P}$runing for Video Large Language Models
($\textbf{AdaTP}$), a novel token **pruning** pipeline for Video LLMs. AdaTP
integrates two dedicated debiasing modules into the pipeline, targeting global
attention bias and local attention bias, respectively. Without the need for
additional training, our method significantly reduces the computational
overhead of Video LLMs while retaining the performance of vanilla models.
Extensive evaluation shows that AdaTP achieves state-of-the-art performance in
various commonly used video understanding benchmarks. In particular, on
LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using
only up to $27.3\%$ FLOPs compared to the vanilla model. Our code will be
released soon.


## Grokking ExPLAIND Unifying Model, Data, and Training Attribution to Study Model Behavior

>Authors: Florian Eichin, Yupei Du, Philipp Mondorf, Barbara Plank, Michael A. Hedderich

>2025-05-26

> http://arxiv.org/abs/2505.20076v1

Post-hoc interpretability methods typically attribute a model's behavior to
its components, data, or training trajectory in isolation. This leads to
explanations that lack a unified view and may miss key interactions. While
combining existing methods or applying them at different training stages offers
broader insights, these approaches usually lack theoretical support. In this
work, we present ExPLAIND, a unified framework that integrates all three
perspectives. First, we generalize recent work on gradient path kernels, which
reformulate models trained by gradient descent as a kernel machine, to more
realistic training settings. Empirically, we find that both a CNN and a
Transformer model are replicated accurately by this reformulation. Second, we
derive novel parameter- and step-wise influence scores from the kernel feature
maps. We show their effectiveness in parameter **pruning** that is comparable to
existing methods, reinforcing their value for model component attribution.
Finally, jointly interpreting model components and data over the training
process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.
Among other things, our findings support previously proposed stages of
Grokking, while refining the final phase as one of alignment of input
embeddings and final layers around a representation pipeline learned after the
memorization phase. Overall, ExPLAIND provides a theoretically grounded,
unified framework to interpret model behavior and training dynamics.


## TabPFN One Model to Rule Them All?

>Authors: Qiong Zhang, Yan Shuo Tan, Qinglong Tian, Pengfei Li

>2025-05-26

> http://arxiv.org/abs/2505.20003v1

Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a
transformer-based deep learning model for regression and classification on
tabular data, which they claim "outperforms all previous methods on datasets
with up to 10,000 samples by a wide margin, using substantially less training
time." Furthermore, they have called TabPFN a "foundation model" for tabular
data, as it can support "data generation, density estimation, learning reusable
embeddings and fine-tuning". If these statements are well-supported, TabPFN may
have the potential to supersede existing modeling approaches on a wide range of
statistical tasks, mirroring a similar revolution in other areas of artificial
intelligence that began with the advent of large language models. In this
paper, we provide a tailored explanation of how TabPFN works for a statistics
audience, by emphasizing its interpretation as approximate Bayesian inference.
We also provide more evidence of TabPFN's "foundation model" capabilities: We
show that an out-of-the-box application of TabPFN vastly outperforms
specialized state-of-the-art methods for semi-supervised parameter estimation,
prediction under covariate shift, and heterogeneous treatment effect
estimation. We further show that TabPFN can outperform LASSO at **sparse**
regression and can break a robustness-efficiency trade-off in classification.
All experiments can be reproduced using the code provided at
https://github.com/qinglong-tian/tabpfn_study
(https://github.com/qinglong-tian/tabpfn_study).


## MiniLongBench The Low-cost Long Context Understanding Benchmark for Large Language Models

>Authors: Zhongzhan Huang, Guoming Ling, Shanshan Zhong, Hefeng Wu, Liang Lin

>2025-05-26

> http://arxiv.org/abs/2505.19959v1

Long Context Understanding (LCU) is a critical area for exploration in
current large language models (LLMs). However, due to the inherently lengthy
nature of long-text data, existing LCU benchmarks for LLMs often result in
prohibitively high evaluation costs, like testing time and inference expenses.
Through extensive experimentation, we discover that existing LCU benchmarks
exhibit significant redundancy, which means the inefficiency in evaluation. In
this paper, we propose a concise data compression method tailored for long-text
data with **sparse** information characteristics. By **pruning** the well-known LCU
benchmark LongBench, we create MiniLongBench. This benchmark includes only 237
test samples across six major task categories and 21 distinct tasks. Through
empirical analysis of over 60 LLMs, MiniLongBench achieves an average
evaluation cost reduced to only 4.5% of the original while maintaining an
average rank correlation coefficient of 0.97 with LongBench results. Therefore,
our MiniLongBench, as a low-cost benchmark, holds great potential to
substantially drive future research into the LCU capabilities of LLMs. See
https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.


## Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling

>Authors: Qixi Zheng, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiaofei Wang, Kai Yu, Xie Chen

>2025-05-26

> http://arxiv.org/abs/2505.19931v1

Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS,
and F5-TTS, have attracted significant attention in recent years. These models
require multiple sampling steps to reconstruct speech from noise, making
inference speed a key challenge. Reducing the number of sampling steps can
greatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a
training-free approach to accelerate the inference of flow-matching-based TTS
models. By inspecting the sampling trajectory of F5-TTS, we identify redundant
steps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform
time-step sampling strategy that effectively reduces the number of sampling
steps. Our approach achieves a 7-step generation with an inference RTF of 0.030
on an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS
while maintaining comparable performance. Furthermore, EPSS performs well on E2
TTS models, demonstrating its strong generalization ability.


## CA3D Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge

>Authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato

>2025-05-26

> http://arxiv.org/abs/2505.19928v1

In this paper, we introduce a deep learning solution for video activity
recognition that leverages an innovative combination of convolutional layers
with a linear-complexity attention mechanism. Moreover, we introduce a novel
**quantization** mechanism to further improve the efficiency of our model during
both training and inference. Our model maintains a reduced computational cost,
while preserving robust learning and generalization capabilities. Our approach
addresses the issues related to the high computing requirements of current
models, with the goal of achieving competitive accuracy on consumer and edge
devices, enabling smart home and smart healthcare applications where efficiency
and privacy issues are of concern. We experimentally validate our model on
different established and publicly available video activity recognition
benchmarks, improving accuracy over alternative models at a competitive
computing cost.


## ScienceBoard Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows

>Authors: Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, Zhiyong Wu

>2025-05-26

> http://arxiv.org/abs/2505.19897v1

Large Language Models (LLMs) have extended their impact beyond Natural
Language Processing, substantially fostering the development of
interdisciplinary research. Recently, various LLM-based agents have been
developed to assist scientific discovery progress across multiple aspects and
domains. Among these, computer-using agents, capable of interacting with
operating systems as humans do, are paving the way to automated scientific
problem-solving and addressing routines in researchers' workflows. Recognizing
the transformative potential of these agents, we introduce ScienceBoard, which
encompasses two complementary contributions: (i) a realistic, multi-domain
environment featuring dynamic and visually rich scientific workflows with
integrated professional software, where agents can autonomously interact via
different interfaces to accelerate complex research tasks and experiments; and
(ii) a challenging benchmark of 169 high-quality, rigorously validated
real-world tasks curated by humans, spanning scientific-discovery workflows in
domains such as biochemistry, astronomy, and geoinformatics. Extensive
evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude
3.7, UI-TARS) show that, despite some promising results, they still fall short
of reliably assisting scientists in complex workflows, achieving only a 15%
overall success rate. In-depth analysis further provides valuable insights for
addressing current agent limitations and more effective design principles,
paving the way to build more capable agents for scientific discovery. Our code,
environment, and benchmark are at
https://qiushisun.github.io/ScienceBoard-Home/.


## Sparse2DGS Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud

>Authors: Natsuki Takama, Shintaro Ito, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki

>2025-05-26

> http://arxiv.org/abs/2505.19854v2

Gaussian Splatting (GS) has gained attention as a fast and effective method
for novel view synthesis. It has also been applied to 3D reconstruction using
multi-view images and can achieve fast and accurate 3D reconstruction. However,
GS assumes that the input contains a large number of multi-view images, and
therefore, the reconstruction accuracy significantly decreases when only a
limited number of input images are available. One of the main reasons is the
insufficient number of 3D points in the **sparse** point cloud obtained through
Structure from Motion (SfM), which results in a poor initialization for
optimizing the Gaussian primitives. We propose a new 3D reconstruction method,
called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three
images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along
with COLMAP MVS to generate highly accurate and dense 3D point clouds, which
are then used to initialize 2D Gaussians. Through experiments on the DTU
dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of
objects using just three images. The project page is available at
https://gsisaoki.github.io/SPARSE2DGS/


## GoLF-NRT Integrating Global Context and Local Geometry for Few-Shot View Synthesis

>Authors: You Wang, Li Fang, Hao Zhu, Fei Hu, Long Ye, Zhan Ma

>2025-05-26

> http://arxiv.org/abs/2505.19813v1

Neural Radiance Fields (NeRF) have transformed novel view synthesis by
modeling scene-specific volumetric representations directly from images. While
generalizable NeRF models can generate novel views across unknown scenes by
learning latent ray representations, their performance heavily depends on a
large number of multi-view observations. However, with limited input views,
these methods experience significant degradation in rendering quality. To
address this limitation, we propose GoLF-NRT: a Global and Local feature
Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable
neural rendering from few input views by leveraging a 3D transformer with
efficient **sparse** attention to capture global scene context. In parallel, it
integrates local geometric features extracted along the epipolar line, enabling
high-quality scene reconstruction from as few as 1 to 3 input views.
Furthermore, we introduce an adaptive sampling strategy based on attention
weights and kernel regression, improving the accuracy of transformer-based
neural rendering. Extensive experiments on public datasets show that GoLF-NRT
achieves state-of-the-art performance across varying numbers of input views,
highlighting the effectiveness and superiority of our approach. Code is
available at https://github.com/KLMAV-CUC/GoLF-NRT.


## Divide and Conquer Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning

>Authors: Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng

>2025-05-26

> http://arxiv.org/abs/2505.19761v1

While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in **sparse**-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.


## Large Language Models in Code Co-generation for Safe Autonomous Vehicles

>Authors: Ali Nouri, Beatriz Cabrero-Daniel, Zhennan Fei, Krishna Ronanki, Håkan Sivencrona, Christian Berger

>2025-05-26

> http://arxiv.org/abs/2505.19658v1

Software engineers in various industrial domains are already using Large
Language Models (LLMs) to accelerate the process of implementing parts of
software systems. When considering its potential use for ADAS or AD systems in
the automotive context, there is a need to systematically assess this new
setup: LLMs entail a well-documented set of risks for safety-related systems'
development due to their stochastic nature. To reduce the effort for code
reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to
conduct sanity-checks on the generated code. We compare the performance of six
state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,
Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we
qualitatively analyse the most frequent faults generated by these LLMs,
creating a failure-mode catalogue to support human reviewers. Finally, the
limitations and capabilities of LLMs in code generation, and the use of the
proposed pipeline in the existing process, are discussed.


## MoESD Unveil Speculative Decoding's Potential for Accelerating Sparse MoE

>Authors: Zongle Huang, Lei Zhu, Zongyuan Zhan, Ting Hu, Weikai Mao, Xianzhi Yu, Yongpan Liu, Tianyu Zhang

>2025-05-26

> http://arxiv.org/abs/2505.19645v1

Large Language Models (LLMs) have achieved remarkable success across many
applications, with Mixture of Experts (MoE) models demonstrating great
potential. Compared to traditional dense models, MoEs achieve better
performance with less computation. Speculative decoding (SD) is a widely used
technique to accelerate LLM inference without accuracy loss, but it has been
considered efficient only for dense models. In this work, we first demonstrate
that, under medium batch sizes, MoE surprisingly benefits more from SD than
dense models. Furthermore, as MoE becomes **sparse**r -- the prevailing trend in
MoE designs -- the batch size range where SD **acceleration** is expected to be
effective becomes broader. To quantitatively understand tradeoffs involved in
SD, we develop a reliable modeling based on theoretical analyses. While current
SD research primarily focuses on improving acceptance rates of algorithms,
changes in workload and model architecture can still lead to degraded SD
**acceleration** even with high acceptance rates. To address this limitation, we
introduce a new metric 'target efficiency' that characterizes these effects,
thus helping researchers identify system bottlenecks and understand SD
**acceleration** more comprehensively. For scenarios like private serving, this
work unveils a new perspective to speed up MoE inference, where existing
solutions struggle. Experiments on different GPUs show up to 2.29x speedup for
Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.


## Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression

>Authors: Kunjun Li, Zigeng Chen, Cheng-Yen Yang, Jenq-Neng Hwang

>2025-05-26

> http://arxiv.org/abs/2505.19602v1

Visual Autoregressive (VAR) modeling has garnered significant attention for
its innovative next-scale prediction approach, which yields substantial
improvements in efficiency, scalability, and zero-shot generalization.
Nevertheless, the coarse-to-fine methodology inherent in VAR results in
exponential growth of the **KV** cache during inference, causing considerable
memory consumption and computational redundancy. To address these bottlenecks,
we introduce Scale**KV**, a novel **KV** cache compression framework tailored for VAR
architectures. Scale**KV** leverages two critical observations: varying cache
demands across transformer layers and distinct attention patterns at different
scales. Based on these insights, Scale**KV** categorizes transformer layers into
two functional groups: drafters and refiners. Drafters exhibit dispersed
attention across multiple scales, thereby requiring greater cache capacity.
Conversely, refiners focus attention on the current token map to process local
details, consequently necessitating substantially reduced cache capacity.
Scale**KV** optimizes the multi-scale inference pipeline by identifying
scale-specific drafters and refiners, facilitating differentiated cache
management tailored to each scale. Evaluation on the state-of-the-art
text-to-image VAR model family, Infinity, demonstrates that our approach
effectively reduces the required **KV** cache memory to 10% while preserving
pixel-level fidelity.


## TailorKV A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization

>Authors: Dingyu Yao, Bowen Shen, Zheng Lin, Wei Liu, Jian Luan, Bin Wang, Weiping Wang

>2025-05-26

> http://arxiv.org/abs/2505.19586v2

The Key-Value (**KV**) cache in generative large language models (LLMs)
introduces substantial memory overhead. Existing works mitigate this burden by
offloading or compressing the **KV** cache. However, loading the entire cache
incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU
communication, while aggressive compression causes notable performance
degradation. We identify that certain layers in the LLM need to maintain global
information and are unsuitable for selective loading. In contrast, other layers
primarily focus on a few tokens with dominant activations that potentially
incur substantial **quantization** error. This observation leads to a key insight
that loading dominant tokens and quantizing all tokens can complement each
other. Building on this insight, we propose a hybrid compression method,
Tailor**KV**, which seamlessly integrates **quantization** and offloading. Tailor**KV**
develops an inference framework along with a hardware-friendly implementation
that leverages these complementary characteristics. Extensive long-context
evaluations exhibit that Tailor**KV** achieves nearly lossless performance under
aggressive compression settings, outperforming the state-of-the-art.
Particularly, the Llama-3.1-8B with 128k context can be served within a single
RTX 3090 GPU, reaching 82 ms per token during decoding.


## Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing

>Authors: Dan Peng, Zhihui Fu, Zewen Ye, Zhuoran Song, Jun Wang

>2025-05-26

> http://arxiv.org/abs/2505.19578v1

Sparse attention methods exploit the inherent **sparsity** in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing **sparse** attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate **sparse** attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.


## FastCache Fast Caching for Diffusion Transformer Through Learnable Linear Approximation

>Authors: Dong Liu, Jiayi Zhang, Yifan Li, Yanxuan Yu, Ben Lengerich, Ying Nian Wu

>2025-05-26

> http://arxiv.org/abs/2505.20353v1

Diffusion Transformers (DiT) are powerful generative models but remain
computationally intensive due to their iterative structure and deep transformer
stacks. To alleviate this inefficiency, we propose FastCache, a
hidden-state-level caching and compression framework that accelerates DiT
inference by exploiting redundancy within the model's internal representations.
FastCache introduces a dual strategy: (1) a spatial-aware token selection
mechanism that adaptively filters redundant tokens based on hidden state
saliency, and (2) a transformer-level cache that reuses latent activations
across timesteps when changes are statistically insignificant. These modules
work jointly to reduce unnecessary computation while preserving generation
fidelity through learnable linear approximation. Theoretical analysis shows
that FastCache maintains bounded approximation error under a
hypothesis-testing-based decision rule. Empirical evaluations across multiple
DiT variants demonstrate substantial reductions in latency and memory usage,
with best generation output quality compared to other cache methods, as
measured by FID and t-FID. Code implementation of FastCache is available on
GitHub at https://github.com/NoakLiu/FastCache-xDiT.


## FlowCut Rethinking Redundancy via Information Flow for Efficient Vision-Language Models

>Authors: Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li

>2025-05-26

> http://arxiv.org/abs/2505.19536v1

Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
**pruning** methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware **pruning** framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut


## Small Language Models Architectures, Techniques, Evaluation, Problems and Future Adaptation

>Authors: Tanjil Hasan Sakib, Md. Tanzib Hosain, Md. Kishor Morol

>2025-05-26

> http://arxiv.org/abs/2505.19529v2

Small Language Models (SLMs) have gained substantial attention due to their
ability to execute diverse language tasks successfully while using fewer
computer resources. These models are particularly ideal for deployment in
limited environments, such as mobile devices, on-device processing, and edge
systems. In this study, we present a complete assessment of SLMs, focussing on
their design frameworks, training approaches, and techniques for lowering model
size and complexity. We offer a novel classification system to organize the
optimization approaches applied for SLMs, encompassing strategies like **pruning**,
**quantization**, and model compression. Furthermore, we assemble SLM's studies of
evaluation suite with some existing datasets, establishing a rigorous platform
for measuring SLM capabilities. Alongside this, we discuss the important
difficulties that remain unresolved in this sector, including trade-offs
between efficiency and performance, and we suggest directions for future study.
We anticipate this study to serve as a beneficial guide for researchers and
practitioners who aim to construct compact, efficient, and high-performing
language models.


## Multimodal Machine Translation with Visual Scene Graph Pruning

>Authors: Chenyu Lu, Shiliang Sun, Jing Zhao, Nan Zhang, Tengfei Song, Hao Yang

>2025-05-26

> http://arxiv.org/abs/2505.19507v1

Multimodal machine translation (MMT) seeks to address the challenges posed by
linguistic polysemy and ambiguity in translation tasks by incorporating visual
information. A key bottleneck in current MMT research is the effective
utilization of visual data. Previous approaches have focused on extracting
global or region-level image features and using attention or gating mechanisms
for multimodal information fusion. However, these methods have not adequately
tackled the issue of visual information redundancy in MMT, nor have they
proposed effective solutions. In this paper, we introduce a novel
approach--multimodal machine translation with visual Scene Graph Pruning (PSG),
which leverages language scene graph information to guide the **pruning** of
redundant nodes in visual scene graphs, thereby reducing noise in downstream
translation tasks. Through extensive comparative experiments with
state-of-the-art methods and ablation studies, we demonstrate the effectiveness
of the PSG model. Our results also highlight the promising potential of visual
information **pruning** in advancing the field of MMT.


## Win Fast or Lose Slow Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs

>Authors: Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman

>2025-05-26

> http://arxiv.org/abs/2505.19481v1

Large language models (LLMs) have shown remarkable performance across diverse
reasoning and generation tasks, and are increasingly deployed as agents in
dynamic environments such as code generation and recommendation systems.
However, many real-world applications, such as high-frequency trading and
real-time competitive gaming, require decisions under strict latency
constraints, where faster responses directly translate into higher rewards.
Despite the importance of this latency quality trade off, it remains
underexplored in the context of LLM based agents. In this work, we present the
first systematic study of this trade off in real time decision making tasks. To
support our investigation, we introduce two new benchmarks: HFTBench, a high
frequency trading simulation, and StreetFighter, a competitive gaming platform.
Our analysis reveals that optimal latency quality balance varies by task, and
that sacrificing quality for lower latency can significantly enhance downstream
performance. To address this, we propose FPX, an adaptive framework that
dynamically selects model size and **quantization** level based on real time
demands. Our method achieves the best performance on both benchmarks, improving
win rate by up to 80% in Street Fighter and boosting daily yield by up to
26.52% in trading, underscoring the need for latency aware evaluation and
deployment strategies for LLM based agents. These results demonstrate the
critical importance of latency aware evaluation and deployment strategies for
real world LLM based agents. Our benchmarks are available at Latency Sensitive
Benchmarks.


## FlowSE Efficient and High-Quality Speech Enhancement via Flow Matching

>Authors: Ziqian Wang, Zikai Liu, Xinfa Zhu, Yike Zhu, Mingshuai Liu, Jun Chen, Longshuai Xiao, Chao Weng, Lei Xie

>2025-05-26

> http://arxiv.org/abs/2505.19476v2

Generative models have excelled in audio tasks using approaches such as
language models, diffusion, and flow matching. However, existing generative
approaches for speech enhancement (SE) face notable challenges: language
model-based methods suffer from **quantization** loss, leading to compromised
speaker similarity and intelligibility, while diffusion models require complex
training and high inference latency. To address these challenges, we propose
FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous
transformation between noisy and clean speech distributions in a single pass,
significantly reducing inference latency while maintaining high-quality
reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and
optional character sequences, optimizing a conditional flow matching loss with
ground-truth mel spectrograms as supervision. It implicitly learns speech's
temporal-spectral structure and text-speech alignment. During inference, FlowSE
can operate with or without textual information, achieving impressive results
in both scenarios, with further improvements when transcripts are available.
Extensive experiments demonstrate that FlowSE significantly outperforms
state-of-the-art generative methods, establishing a new paradigm for
generative-based SE and demonstrating the potential of flow matching to advance
the field. Our code, pre-trained checkpoints, and audio samples are available.


## The Birth of Knowledge Emergent Features across Time, Space, and Scale in Large Language Models

>Authors: Shashata Sawmya, Micah Adler, Nir Shavit

>2025-05-26

> http://arxiv.org/abs/2505.19440v1

This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using **sparse** autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.


## Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression

>Authors: Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li

>2025-05-26

> http://arxiv.org/abs/2505.19433v1

Post-training compression reduces the computational and memory costs of large
language models (LLMs), enabling resource-efficient deployment. However,
existing compression benchmarks only focus on language modeling (e.g.,
perplexity) and natural language understanding tasks (e.g., GLUE accuracy),
ignoring the agentic capabilities - workflow, tool use/function call,
long-context understanding and real-world application. We introduce the Agent
Compression Benchmark (ACBench), the first comprehensive benchmark for
evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)
12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,
Needle-in-Haystack for long-context retrieval), (2) **quantization** (GPTQ, AWQ)
and **pruning** (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),
standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).
Our experiments reveal compression tradeoffs: 4-bit **quantization** preserves
workflow generation and tool use (1%-3% drop) but degrades real-world
application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation
and Energy to systematize analysis. ACBench provides actionable insights for
optimizing LLM compression in agentic scenarios. The code can be found in
https://github.com/pprp/ACBench.


## Frictional Agent Alignment Framework Slow Down and Don't Break Things

>Authors: Abhijnan Nath, Carine Graff, Andrei Bachinin, Nikhil Krishnaswamy

>2025-05-26

> http://arxiv.org/abs/2505.19428v1

AI support of collaborative interactions entails mediating potential
misalignment between interlocutor beliefs. Common preference alignment methods
like DPO excel in static settings, but struggle in dynamic collaborative tasks
where the explicit signals of interlocutor beliefs are **sparse** and skewed. We
propose the Frictional Agent Alignment Framework (FAAF), to generate precise,
context-aware "friction" that prompts for deliberation and re-examination of
existing evidence. FAAF's two-player objective decouples from data skew: a
frictive-state policy identifies belief misalignments, while an intervention
policy crafts collaborator-preferred responses. We derive an analytical
solution to this objective, enabling training a single policy via a simple
supervised loss. Experiments on three benchmarks show FAAF outperforms
competitors in producing concise, interpretable friction and in OOD
generalization. By aligning LLMs to act as adaptive "thought partners" -- not
passive responders -- FAAF advances scalable, dynamic human-AI collaboration.
Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.


## WINA Weight Informed Neuron Activation for Accelerating Large Language Model Inference

>Authors: Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen

>2025-05-26

> http://arxiv.org/abs/2505.19427v1

The growing computational demands of large language models (LLMs) make
efficient inference and activation strategies increasingly critical. While
recent approaches, such as Mixture-of-Experts (MoE), leverage selective
activation but require specialized training, training-free **sparse** activation
methods offer broader applicability and superior resource efficiency through
their plug-and-play design. However, many existing methods rely solely on
hidden state magnitudes to determine activation, resulting in high
approximation errors and suboptimal inference accuracy. To address these
limitations, we propose WINA (Weight Informed Neuron Activation), a novel,
simple, and training-free **sparse** activation framework that jointly considers
hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices.
We show that this leads to a sparsification strategy that obtains optimal
approximation error bounds with theoretical guarantees tighter than existing
techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,
TEAL) by up to $2.94\%$ in average performance at the same **sparsity** levels,
across a diverse set of LLM architectures and datasets. These results position
WINA as a new performance frontier for training-free **sparse** activation in LLM
inference, advancing training-free **sparse** activation methods and setting a
robust baseline for efficient inference. The source code is available at
https://github.com/microsoft/wina.


## Fusion Intelligence for Digital Twinning AI Data Centers A Synergistic GenAI-PhyAI Approach

>Authors: Ruihang Wang, Minghao Li, Zhiwei Cao, Jimin Jia, Kyle Guan, Yonggang Wen

>2025-05-26

> http://arxiv.org/abs/2505.19409v1

The explosion in artificial intelligence (AI) applications is pushing the
development of AI-dedicated data centers (AIDCs), creating management
challenges that traditional methods and standalone AI solutions struggle to
address. While digital twins are beneficial for AI-based design validation and
operational optimization, current AI methods for their creation face
limitations. Specifically, physical AI (PhyAI) aims to capture the underlying
physical laws, which demands extensive, case-specific customization, and
generative AI (GenAI) can produce inaccurate or hallucinated results. We
propose Fusion Intelligence, a novel framework synergizing GenAI's automation
with PhyAI's domain grounding. In this dual-agent collaboration, GenAI
interprets natural language prompts to generate tokenized AIDC digital twins.
Subsequently, PhyAI optimizes these generated twins by enforcing physical
constraints and assimilating real-time data. Case studies demonstrate the
advantages of our framework in automating the creation and validation of AIDC
digital twins. These twins deliver predictive analytics to support power usage
effectiveness (PUE) optimization in the design stage. With operational data
collected, the digital twin accuracy is further improved compared with pure
physics-based models developed by human experts. Fusion Intelligence offers a
promising pathway to accelerate digital transformation. It enables more
reliable and efficient AI-driven digital transformation for a broad range of
mission-critical infrastructures.


## Foundations of Top-$k$ Decoding For Language Models

>Authors: Georgy Noarov, Soham Mallick, Tao Wang, Sunay Joshi, Yan Sun, Yangxinyu Xie, Mengxin Yu, Edgar Dobriban

>2025-05-25

> http://arxiv.org/abs/2505.19371v1

Top-$k$ decoding is a widely used method for sampling from LLMs: at each
token, only the largest $k$ next-token-probabilities are kept, and the next
token is sampled after re-normalizing them to sum to unity. Top-$k$ and other
sampling methods are motivated by the intuition that true next-token
distributions are **sparse**, and the noisy LLM probabilities need to be truncated.
However, to our knowledge, a precise theoretical motivation for the use of
top-$k$ decoding is missing. In this work, we develop a theoretical framework
that both explains and generalizes top-$k$ decoding. We view decoding at a
fixed token as the recovery of a **sparse** probability distribution. We consider
\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence
(for both the \emph{primal} and \emph{dual} cases) with a **sparsity**-inducing
$\ell_0$ regularization. Despite the combinatorial nature of the objective, we
show how to optimize it efficiently for a large class of divergences. We show
that the optimal decoding strategies are greedy, and further that the loss
function is discretely convex in $k$, so that binary search provably and
efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a
special case for the KL divergence, and identify new decoding strategies that
have distinct behaviors (e.g., non-linearly up-weighting larger probabilities
after re-normalization).


## Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval

>Authors: Kidist Amde Mekonnen, Yosef Worku Alemneh, Maarten de Rijke

>2025-05-25

> http://arxiv.org/abs/2505.19356v1

Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both **sparse** and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.


## DECA A Near-Core LLM Decompression Accelerator Supporting Out-of-Order Invocation

>Authors: Gerasimos Gerogiannis, Stijn Eyerman, Evangelos Georganas, Wim Heirman, Josep Torrellas

>2025-05-25

> http://arxiv.org/abs/2505.19349v1

To alleviate the memory bandwidth bottleneck in Large Language Model (LLM)
inference workloads, weight matrices are stored in memory in **quantize**d and
sparsified formats. Hence, before tiles of these matrices can be processed by
in-core generalized matrix multiplication (GeMM) hardware engines, they need to
be de**quantize**d and de-sparsified. This is currently performed in software with
vector operations. Unfortunately, this approach delivers only modest
performance. Moreover, it is hard to understand how to improve the system, as
the overall GeMM performance depends on the interaction between memory
resources, vector units, and hardware matrix engines.
  To improve the performance of LLM inference in advanced platforms equipped
with in-core GeMM engines and HBM, this paper makes three main contributions.
First, it develops an analytical performance model with a 3D visual
representation that provides insights into how memory resources, vector units,
and hardware matrix engines interact to deliver compressed GeMM performance.
Second, it proposes DECA, a new near-core ML-model decompression accelerator.
DECA offloads tile de-sparsification and de**quantization** from the CPU, producing
ready-to-use tiles for in-core GeMM engines. Third, it introduces a new ISA
extension that enables out-of-order invocation of the near-core accelerator.
With this extension, accelerator and core computations can interleave and
overlap with high-performance. Our evaluation shows that, in a simulated
56-core Xeon 4 server with HBM, DECA accelerates the execution of compressed
GeMMs by up to 4x over the use of optimized Intel software kernels. Further,
DECA reduces the next-token generation time of Llama2-70B and OPT-66B by
1.6x-2.6x.


## Communication-Efficient Multi-Device Inference Acceleration for Transformer Models

>Authors: Xiao Liu, Lijun Zhang, Deepak Ganesan, Hui Guan

>2025-05-25

> http://arxiv.org/abs/2505.19342v1

Transformer models power many AI applications but suffer from high inference
latency, limiting their use in real-time settings. Multi-device inference can
reduce latency by parallelizing computation. Yet, existing methods require high
inter-device bandwidth, making them impractical for bandwidth-constrained
environments. We propose ASTRA, a communication-efficient framework that
accelerates Transformer inference through a novel integration of sequence
parallelism and a Mixed-Precision Attention mechanism designed to minimize
inter-device communication. ASTRA compresses non-local token embeddings via
vector **quantization** and preserves task accuracy through two optimizations,
Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT
and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X
speedups over single-device inference and up to 15.25X speedups over
state-of-the-art multi-device inferences, while operating under bandwidths as
low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.


## PolyPose Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms

>Authors: Vivek Gopalakrishnan, Neel Dey, Polina Golland

>2025-05-25

> http://arxiv.org/abs/2505.19256v2

Determining the 3D pose of a patient from a limited set of 2D X-ray images is
a critical task in interventional settings. While preoperative volumetric
imaging (e.g., CT and MRI) provides precise 3D localization and visualization
of anatomical targets, these modalities cannot be acquired during procedures,
where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance
into intraoperative procedures, we present PolyPose, a simple and robust method
for deformable 2D/3D registration. PolyPose parameterizes complex 3D
deformation fields as a composition of rigid transforms, leveraging the
biological constraint that individual bones do not bend in typical motion.
Unlike existing methods that either assume no inter-joint movement or fail
outright in this under-determined setting, our polyrigid formulation enforces
anatomically plausible priors that respect the piecewise rigid nature of human
movement. This approach eliminates the need for expensive deformation
regularizers that require patient- and procedure-specific hyperparameter
optimization. Across extensive experiments on diverse datasets from orthopedic
surgery and radiotherapy, we show that this strong inductive bias enables
PolyPose to successfully align the patient's preoperative volume to as few as
two X-ray images, thereby providing crucial 3D guidance in challenging
**sparse**-view and limited-angle settings where current registration methods fail.


## York time in JT gravity

>Authors: Onkar Parrikar, Sunil Kumar Sake

>2025-05-25

> http://arxiv.org/abs/2505.19231v1

The notion of time in general relativity must arise from an internal clock,
i.e., a degree of freedom in the gravitational theory internal to the system
that can serve the role of a physical clock. One such internal notion of time
is the York time, corresponding to constant extrinsic curvature slicing of
spacetime. We study the Hartle-Hawking wavefunction of asymptotically $AdS_2$
JT gravity as a function of York time. Using both canonical **quantization** and
the JT gravity path integral, we explicitly calculate this wavefunction and
show that it satisfies a Schrodinger equation with respect to York time. We
find the corresponding York Hamiltonian, which turns out to be manifestly
Hermitian. Our analysis cleanly avoids operator ordering ambiguities. The
dependence of the wavefunction on York time should be thought of as emerging
from a unitary transformation of the gravitational length basis states, and not
from a physical time evolution of the state in the dual boundary theory.


## DREAM Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding

>Authors: Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang

>2025-05-25

> http://arxiv.org/abs/2505.19201v2

Speculative decoding (SD) has emerged as a powerful method for accelerating
autoregressive generation in large language models (LLMs), yet its integration
into vision-language models (VLMs) remains underexplored. We introduce DREAM, a
novel speculative decoding framework tailored for VLMs that combines three key
innovations: (1) a cross-attention-based mechanism to inject intermediate
features from the target model into the draft model for improved alignment, (2)
adaptive intermediate feature selection based on attention entropy to guide
efficient draft model training, and (3) visual token compression to reduce
draft model latency. DREAM enables efficient, accurate, and parallel multimodal
decoding with significant throughput improvement. Experiments across a diverse
set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,
demonstrate up to 3.6x speedup over conventional decoding and significantly
outperform prior SD baselines in both inference throughput and speculative
draft acceptance length across a broad range of multimodal benchmarks. The code
is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git


## LIMOPro Reasoning Refinement for Efficient and Effective Test-time Scaling

>Authors: Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu

>2025-05-25

> http://arxiv.org/abs/2505.19187v1

Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.


## DLF Enhancing Explicit-Implicit Interaction via Dynamic Low-Order-Aware Fusion for CTR Prediction

>Authors: Kefan Wang, Hao Wang, Wei Guo, Yong Liu, Jianghao Lin, Defu Lian, Enhong Chen

>2025-05-25

> http://arxiv.org/abs/2505.19182v1

Click-through rate (CTR) prediction is a critical task in online advertising
and recommender systems, relying on effective modeling of feature interactions.
Explicit interactions capture predefined relationships, such as inner products,
but often suffer from data **sparsity**, while implicit interactions excel at
learning complex patterns through non-linear transformations but lack inductive
biases for efficient low-order modeling. Existing two-stream architectures
integrate these paradigms but face challenges such as limited information
sharing, gradient imbalance, and difficulty preserving low-order signals in
**sparse** CTR data. We propose a novel framework, Dynamic Low-Order-Aware Fusion
(DLF), which addresses these limitations through two key components: a
Residual-Aware Low-Order Interaction Network (RLI) and a Network-Aware
Attention Fusion Module (NAF). RLI explicitly preserves low-order signals while
mitigating redundancy from residual connections, and NAF dynamically integrates
explicit and implicit representations at each layer, enhancing information
sharing and alleviating gradient imbalance. Together, these innovations balance
low-order and high-order interactions, improving model expressiveness.
Extensive experiments on public datasets demonstrate that DLF achieves
state-of-the-art performance in CTR prediction, addressing key limitations of
existing models. The implementation is publicly available at
https://github.com/USTC-StarTeam/DLF.


## Sparse-to-Dense A Free Lunch for Lossless Acceleration of Video Understanding in LLMs

>Authors: Xuan Zhang, Cunxiao Du, Sicheng Yu, Jiawei Wu, Fengzhuo Zhang, Wei Gao, Qian Liu

>2025-05-25

> http://arxiv.org/abs/2505.19155v1

Due to the auto-regressive nature of current video large language models
(Video-LLMs), the inference latency increases as the input sequence length
grows, posing challenges for the efficient processing of video sequences that
are usually very long. We observe that during decoding, the attention scores of
most tokens in Video-LLMs tend to be **sparse** and concentrated, with only certain
tokens requiring comprehensive full attention. Based on this insight, we
introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two
distinct modules: one leveraging **sparse** top-K attention and the other employing
dense full attention. These modules collaborate to accelerate Video-LLMs
without loss. The fast (**sparse**) model speculatively decodes multiple tokens,
while the slow (dense) model verifies them in parallel. StD is a tuning-free,
plug-and-play solution that achieves up to a 1.94$\times$ walltime speedup in
video processing. It maintains model performance while enabling a seamless
transition from a standard Video-LLM to a **sparse** Video-LLM with minimal code
modifications.


## SRDiffusion Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation

>Authors: Shenggan Cheng, Yuanxin Wei, Lansong Diao, Yong Liu, Bujiao Chen, Lianghua Huang, Yu Liu, Wenyuan Yu, Jiangsu Du, Wei Lin, Yang You

>2025-05-25

> http://arxiv.org/abs/2505.19151v1

Leveraging the diffusion transformer (DiT) architecture, models like Sora,
CogVideoX and Wan have achieved remarkable progress in text-to-video,
image-to-video, and video editing tasks. Despite these advances,
diffusion-based video generation remains computationally intensive, especially
for high-resolution, long-duration videos. Prior work accelerates its inference
by skipping computation, usually at the cost of severe quality degradation. In
this paper, we propose SRDiffusion, a novel framework that leverages
collaboration between large and small models to reduce inference cost. The
large model handles high-noise steps to ensure semantic and motion fidelity
(Sketching), while the smaller model refines visual details in low-noise steps
(Rendering). Experimental results demonstrate that our method outperforms
existing approaches, over 3$\times$ speedup for Wan with nearly no quality loss
for VBench, and 2$\times$ speedup for CogVideoX. Our method is introduced as a
new direction orthogonal to existing **acceleration** strategies, offering a
practical solution for scalable video generation.


## ADGSyn Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction

>Authors: Yuxuan Nie, Yutong Song, Hong Peng

>2025-05-25

> http://arxiv.org/abs/2505.19144v1

Drug combinations play a critical role in cancer therapy by significantly
enhancing treatment efficacy and overcoming drug resistance. However, the
combinatorial space of possible drug pairs grows exponentially, making
experimental screening highly impractical. Therefore, developing efficient
computational methods to predict promising drug combinations and guide
experimental validation is of paramount importance. In this work, we propose
ADGSyn, an innovative method for predicting drug synergy. The key components of
our approach include: (1) shared projection matrices combined with attention
mechanisms to enable cross-drug feature alignment; (2) automatic mixed
precision (AMP)-optimized graph operations that reduce memory consumption by
40\% while accelerating training speed threefold; and (3) residual pathways
stabilized by LayerNorm to ensure stable gradient propagation during training.
Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,
ADGSyn demonstrates superior performance over eight baseline methods. Moreover,
the framework supports full-batch processing of up to 256 molecular graphs on a
single GPU, setting a new standard for efficiency in drug synergy prediction
within the field of computational oncology.


## FP4 All the Way Fully Quantized Training of LLMs

>Authors: Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry

>2025-05-25

> http://arxiv.org/abs/2505.19115v1

We demonstrate, for the first time, fully **quantize**d training (FQT) of large
language models (LLMs) using predominantly 4-bit floating-point (FP4) precision
for weights, activations, and gradients on datasets up to 200 billion tokens.
We extensively investigate key design choices for FP4, including block sizes,
scaling formats, and rounding methods. Our analysis shows that the NVFP4
format, where each block of 16 FP4 values (E2M1) shares a scale represented in
E4M3, provides optimal results. We use stochastic rounding for backward and
update passes and round-to-nearest for the forward pass to enhance stability.
Additionally, we identify a theoretical and empirical threshold for effective
**quantize**d training: when the gradient norm falls below approximately $\sqrt{3}$
times the **quantization** noise, **quantize**d training becomes less effective.
Leveraging these insights, we successfully train a 7-billion-parameter model on
256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves
downstream task performance comparable to a standard BF16 baseline, confirming
that FP4 training is a practical and highly efficient approach for large-scale
LLM training. A reference implementation is supplied in
https://github.com/Anonymous1252022/fp4-all-the-way .


## Bayesian sparse modeling for interpretable prediction of hydroxide ion conductivity in anion-conductive polymer membranes

>Authors: Ryo Murakami, Kenji Miyatake, Ahmed Mohamed Ahmed Mahmoud, Hideki Yoshikawa, Kenji Nagata

>2025-05-25

> http://arxiv.org/abs/2505.19044v1

Anion-conductive polymer membranes have attracted considerable attention as
solid electrolytes for alkaline fuel cells and electrolysis cells. Their
hydroxide ion conductivity varies depending on factors such as the type and
distribution of quaternary ammonium groups, as well as the structure and
connectivity of hydrophilic and hydrophobic domains. In particular, the size
and connectivity of hydrophilic domains significantly influence the mobility of
hydroxide ions; however, this relationship has remained largely qualitative. In
this study, we calculated the number of key constituent elements in the
hydrophilic and hydrophobic units based on the copolymer composition, and
investigated their relationship with hydroxide ion conductivity by using
Bayesian **sparse** modeling. As a result, we successfully identified
composition-derived features that are critical for accurately predicting
hydroxide ion conductivity.


## Tokenizing Electron Cloud in Protein-Ligand Interaction Learning

>Authors: Haitao Lin, Odin Zhang, Jia Xu, Yunfan Liu, Zheng Cheng, Lirong Wu, Yufei Huang, Zhifeng Gao, Stan Z. Li

>2025-05-25

> http://arxiv.org/abs/2505.19014v1

The affinity and specificity of protein-molecule binding directly impact
functional outcomes, uncovering the mechanisms underlying biological regulation
and signal transduction. Most deep-learning-based prediction approaches focus
on structures of atoms or fragments. However, quantum chemical properties, such
as electronic structures, are the key to unveiling interaction patterns but
remain largely underexplored. To bridge this gap, we propose ECBind, a method
for tokenizing electron cloud signals into **quantize**d embeddings, enabling their
integration into downstream tasks such as binding affinity prediction. By
incorporating electron densities, ECBind helps uncover binding modes that
cannot be fully represented by atom-level models. Specifically, to remove the
redundancy inherent in electron cloud signals, a structure-aware transformer
and hierarchical codebooks encode 3D binding sites enriched with electron
structures into tokens. These tokenized codes are then used for specific tasks
with labels. To extend its applicability to a wider range of scenarios, we
utilize knowledge distillation to develop an electron-cloud-agnostic prediction
model. Experimentally, ECBind demonstrates state-of-the-art performance across
multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure
Pearson and Spearman correlation coefficients, respectively.


## FastMamba A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization

>Authors: Aotao Wang, Haikuo Shao, Shaobo Ma, Zhongfeng Wang

>2025-05-25

> http://arxiv.org/abs/2505.18975v2

State Space Models (SSMs), like recent Mamba2, have achieved remarkable
performance and received extensive attention. However, deploying Mamba2 on
resource-constrained edge devices encounters many problems: severe outliers
within the linear layer challenging the **quantization**, diverse and irregular
element-wise tensor operations, and hardware-unfriendly nonlinear functions in
the SSM block. To address these issues, this paper presents FastMamba, a
dedicated accelerator on FPGA with hardware-algorithm co-design to promote the
deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit
**quantization** for linear layers through Hadamard transformation to eliminate
outliers. Moreover, a hardware-friendly and fine-grained power-of-two
**quantization** framework is presented for the SSM block and convolution layer,
and a first-order linear approximation is developed to optimize the nonlinear
functions. Based on the accurate algorithm **quantization**, we propose an
accelerator that integrates parallel vector processing units, pipelined
execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which
enhances computational efficiency and reduces hardware complexity. Finally, we
evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on
Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel
Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode
experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency
than RTX 3090 GPU.


## System-1.5 Reasoning Traversal in Language and Latent Spaces with Dynamic Shortcuts

>Authors: Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu

>2025-05-25

> http://arxiv.org/abs/2505.18962v2

Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move
beyond fast System-1 responses and engage in deliberative System-2 reasoning.
However, this comes at the cost of significant inefficiency due to verbose
intermediate output. Recent latent-space reasoning methods improve efficiency
by operating on hidden states without decoding into language, yet they treat
all steps uniformly, failing to distinguish critical deductions from auxiliary
steps and resulting in suboptimal use of computational resources. In this
paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that
dynamically allocates computation across reasoning steps through shortcut paths
in latent space. Specifically, System-1.5 Reasoning introduces two types of
dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the
vertical depth by early exiting non-critical tokens through lightweight adapter
branches, while allowing critical tokens to continue through deeper Transformer
layers. The step shortcut (SS) reuses hidden states across the decoding steps
to skip trivial steps and reason horizontally in latent space. Training
System-1.5 Reasoning involves a two-stage self-distillation process: first
distilling natural language CoT into latent-space continuous thought, and then
distilling full-path System-2 latent reasoning into adaptive shortcut paths
(System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior
performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves
reasoning performance comparable to traditional CoT fine-tuning methods while
accelerating inference by over 20x and reducing token generation by 92.31% on
average.


## How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation

>Authors: Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao

>2025-05-25

> http://arxiv.org/abs/2505.18956v1

LiDAR-based 3D panoptic segmentation often struggles with the inherent
**sparsity** of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.


## Efficient SRAM-PIM Co-design by Joint Exploration of Value-Level and Bit-Level Sparsity

>Authors: Cenlin Duan, Jianlei Yang, Yikun Wang, Yiou Wang, Yingjie Qi, Xiaolin He, Bonan Yan, Xueyan Wang, Xiaotao Jia, Weisheng Zhao

>2025-05-25

> http://arxiv.org/abs/2505.18954v1

Processing-in-memory (PIM) is a transformative architectural paradigm
designed to overcome the Von Neumann bottleneck. Among PIM architectures,
digital SRAM-PIM emerges as a promising solution, offering significant
advantages by directly integrating digital logic within the SRAM array.
However, rigid crossbar architecture and full array activation pose challenges
in efficiently utilizing traditional value-level **sparsity**. Moreover, neural
network models exhibit a high proportion of zero bits within non-zero values,
which remain underutilized due to architectural constraints. To overcome these
limitations, we present Dyadic Block PIM (DB-PIM), a groundbreaking
algorithm-architecture co-design framework to harness both value-level and
bit-level **sparsity**. At the algorithm level, our hybrid-grained **pruning**
technique, combined with a novel **sparsity** pattern, enables effective **sparsity**
management. Architecturally, DB-PIM incorporates a **sparse** network and
customized digital SRAM-PIM macros, including input pre-processing unit (IPU),
dyadic block multiply units (DBMUs), and Canonical Signed Digit (CSD)-based
adder trees. It circumvents structured zero values in weights and bypasses
unstructured zero bits within non-zero weights and block-wise all-zero bit
columns in input features. As a result, the DB-PIM framework skips a majority
of unnecessary computations, thereby driving significant gains in computational
efficiency. Results demonstrate that our DB-PIM framework achieves up to 8.01x
speedup and 85.28% energy savings, significantly boosting computational
efficiency in digital SRAM-PIM systems.


## Graph-Based Operator Learning from Limited Data on Irregular Domains

>Authors: Yile Li, Shandian Zhe

>2025-05-25

> http://arxiv.org/abs/2505.18923v1

Operator learning seeks to approximate mappings from input functions to
output solutions, particularly in the context of partial differential equations
(PDEs). While recent advances such as DeepONet and Fourier Neural Operator
(FNO) have demonstrated strong performance, they often rely on regular grid
discretizations, limiting their applicability to complex or irregular domains.
In this work, we propose a Graph-based Operator Learning with Attention (GOLA)
framework that addresses this limitation by constructing graphs from
irregularly sampled spatial points and leveraging attention-enhanced Graph
Neural Netwoks (GNNs) to model spatial dependencies with global information. To
improve the expressive capacity, we introduce a Fourier-based encoder that
projects input functions into a frequency space using learnable complex
coefficients, allowing for flexible embeddings even with **sparse** or nonuniform
samples. We evaluated our approach across a range of 2D PDEs, including Darcy
Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling
densities. Our method consistently outperforms baselines, particularly in
data-scarce regimes, demonstrating strong generalization and efficiency on
irregular domains.


## Partition Generative Modeling Masked Modeling Without Masks

>Authors: Justin Deschenaux, Lan Tran, Caglar Gulcehre

>2025-05-24

> http://arxiv.org/abs/2505.18883v1

We introduce ``Partition Generative Models'' (PGMs), a novel approach to
masked generative modeling (MGMs), particularly effective for masked diffusion
language modeling (MDLMs). PGM divides tokens into two distinct groups and
employs **sparse** attention patterns to prevent cross-group information exchange.
Hence, the model is trained to predict tokens in one group based solely on
information from the other group. This partitioning strategy eliminates the
need for MASK tokens entirely. While traditional MGMs inefficiently process
MASK tokens during generation, PGMs achieve greater computational efficiency by
operating exclusively on unmasked tokens. Our experiments on OpenWebText with a
context length of 1024 tokens demonstrate that PGMs deliver at least 5x
improvements in both latency and throughput compared to MDLM when using the
same number of sampling steps, while generating samples with better generative
perplexity than MDLM. Finally, we show that PGMs can be distilled with
Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in
order to achieve further inference gains.


## Sparse VideoGen2 Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation

>Authors: Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, Jianfei Chen, Song Han, Kurt Keutzer, Ion Stoica

>2025-05-24

> http://arxiv.org/abs/2505.18875v1

Diffusion Transformers (DiTs) are essential for video generation but suffer
from significant latency due to the quadratic complexity of attention. By
computing only critical tokens, **sparse** attention reduces computational costs
and offers a promising **acceleration** approach. However, we identify that
existing methods fail to approach optimal generation quality under the same
computation budget for two reasons: (1) Inaccurate critical token
identification: current methods cluster tokens based on position rather than
semantics, leading to imprecise aggregated representations. (2) Excessive
computation waste: critical tokens are scattered among non-critical ones,
leading to wasted computation on GPUs, which are optimized for processing
contiguous tokens. In this paper, we propose SVG2, a training-free framework
that maximizes identification accuracy and minimizes computation waste,
achieving a Pareto frontier trade-off between generation quality and
efficiency. The core of SVG2 is semantic-aware permutation, which clusters and
reorders tokens based on semantic similarity using k-means. This approach
ensures both a precise cluster representation, improving identification
accuracy, and a densified layout of critical tokens, enabling efficient
computation without padding. Additionally, SVG2 integrates top-p dynamic budget
control and customized kernel implementations, achieving up to 2.30x and 1.89x
speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan
2.1, respectively.


## FlatAttention Dataflow and Fabric Collectives Co-Optimization for Efficient Multi-Head Attention on Tile-Based Many-PE Accelerators

>Authors: Chi Zhang, Luca Colagrande, Renzo Andri, Thomas Benz, Gamze Islamoglu, Alessandro Nadalini, Francesco Conti, Yawei Li, Luca Benini

>2025-05-24

> http://arxiv.org/abs/2505.18824v1

Multi-Head Attention (MHA) is a critical computational kernel in
transformer-based AI models. Emerging scalable tile-based accelerator
architectures integrate increasing numbers of tightly-packed processing
elements (PEs) with tensor units. MHA dataflow mapping is crucial for achieving
high utilization of the available units. We propose FlatAttention, a new
dataflow for MHA on tile-based many-PE accelerators, minimizing costly main
memory (HBM) accesses by leveraging collective primitives integrated into the
on-chip network fabric. FlatAttention achieves up to 89.3% utilization, and
4.1x performance speedup over FlashAttention-3 dataflow on tile-based
accelerators whilst reducing HBM traffic by 16x. Through algorithm-architecture
co-exploration, we identify an optimal configuration for a large scaled-out
tile-based accelerator featuring a 32x32 tile mesh with 1024 TFLOPS @ FP16 peak
performance, comparable to the state-of-the-art Nvidia H100 GPU. FlatAttention
in this configuration achieves up to 1.3x higher utilization over
FlashAttention-3 on the H100 GPU. Meanwhile, this tile-based accelerator
configuration requires 40% less HBM bandwidth compared to the H100, enabling a
1.8x reduction in die size, estimated on the same technology node.


## High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction

>Authors: Seongsu Kim, Nayoung Kim, Dongwoo Kim, Sungsoo Ahn

>2025-05-24

> http://arxiv.org/abs/2505.18817v1

Density functional theory (DFT) is a fundamental method for simulating
quantum chemical properties, but it remains expensive due to the iterative
self-consistent field (SCF) process required to solve the Kohn-Sham equations.
Recently, deep learning methods are gaining attention as a way to bypass this
step by directly predicting the Hamiltonian. However, they rely on
deterministic regression and do not consider the highly structured nature of
Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow
matching framework that generates Hamiltonian matrices conditioned on molecular
geometry. Flow matching models continuous-time trajectories between simple
priors and complex targets, learning the structured distributions over
Hamiltonians instead of direct regression. To further incorporate symmetry, we
use a neural architecture that predicts SE(3)-equivariant vector fields,
improving accuracy and generalization across diverse geometries. To further
enhance physical fidelity, we additionally introduce a fine-tuning scheme to
align predicted orbital energies with the target. QHFlow achieves
state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%
on QH9. Moreover, we further show that QHFlow accelerates the DFT process
without trading off the solution quality when initializing SCF iterations with
the predicted Hamiltonian, significantly reducing the number of iterations and
runtime.


## VORTA Efficient Video Diffusion via Routing Sparse Attention

>Authors: Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Shunyu Liu, Dacheng Tao

>2025-05-24

> http://arxiv.org/abs/2505.18809v1

Video Diffusion Transformers (VDiTs) have achieved remarkable progress in
high-quality video generation, but remain computationally expensive due to the
quadratic complexity of attention over high-dimensional video sequences. Recent
attention **acceleration** methods leverage the **sparsity** of attention patterns to
improve efficiency; however, they often overlook inefficiencies of redundant
long-range interactions. To address this problem, we propose \textbf{VORTA}, an
**acceleration** framework with two novel components: 1) a **sparse** attention
mechanism that efficiently captures long-range dependencies, and 2) a routing
strategy that adaptively replaces full 3D attention with specialized **sparse**
attention variants throughout the sampling process. It achieves a $1.76\times$
end-to-end speedup without quality loss on VBench. Furthermore, VORTA can
seamlessly integrate with various other **acceleration** methods, such as caching
and step distillation, reaching up to $14.41\times$ speedup with negligible
performance degradation. VORTA demonstrates its efficiency and enhances the
practicality of VDiTs in real-world settings.


## ALPS Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models

>Authors: Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao

>2025-05-24

> http://arxiv.org/abs/2505.18799v2

Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant training adjustment costs. Prior research has explored
various avenues to enhance alignment efficiency, primarily through minimal-data
training or data-driven activations to identify key attention heads. However,
these approaches inherently introduce data dependency, which hinders
generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment. The code is available at
https://github.com/VoiceBeer/ALPS.


## ToDRE Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models

>Authors: Duo Li, Zuhao Yang, Shijian Lu

>2025-05-24

> http://arxiv.org/abs/2505.18757v1

The representation of visual inputs of large vision-language models (LVLMs)
usually involves substantially more tokens than that of textual inputs, leading
to significant computational overhead. Several recent studies strive to
mitigate this issue by either conducting token compression to prune redundant
visual tokens or guiding them to bypass certain computational stages. While
most existing work exploits token importance as the redundancy indicator, our
study reveals that two largely neglected factors, namely, the diversity of
retained visual tokens and their task relevance, often offer more robust
criteria in token **pruning**. To this end, we design ToDRE, a two-stage and
training-free token compression framework that achieves superior performance by
**pruning** Tokens based on token Diversity and token-task RElevance. Instead of
**pruning** redundant tokens, ToDRE introduces a greedy k-center algorithm to
select and retain a small subset of diverse visual tokens after the vision
encoder. Additionally, ToDRE addresses the "information migration" by further
eliminating task-irrelevant visual tokens within the decoder of large language
model (LLM). Extensive experiments show that ToDRE effectively reduces 90% of
visual tokens after vision encoder and adaptively prunes all visual tokens
within certain LLM's decoder layers, leading to a 2.6x speed-up in total
inference time while maintaining 95.1% of model performance and excellent
compatibility with efficient attention operators.


## LoTA-QAF Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning

>Authors: Junyu Chen, Junzhuo Li, Zhen Peng, Wenjie Wang, Yuxiang Ren, Long Shi, Xuming Hu

>2025-05-24

> http://arxiv.org/abs/2505.18724v1

Quantization and fine-tuning are crucial for deploying large language models
(LLMs) on resource-constrained edge devices. However, fine-tuning **quantize**d
models presents significant challenges, primarily stemming from: First, the
mismatch in data types between the low-precision **quantize**d weights (e.g.,
4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch
limits the computational efficiency advantage offered by **quantize**d weights
during inference. Second, potential accuracy degradation when merging these
high-precision adaptation weights into the low-precision **quantize**d weights, as
the adaptation weights often necessitate approximation or truncation. Third, as
far as we know, no existing methods support the lossless merging of adaptation
while adjusting all **quantize**d weights. To address these challenges, we
introduce lossless ternary adaptation for **quantization**-aware fine-tuning
(LoTA-QAF). This is a novel fine-tuning method specifically designed for
**quantize**d LLMs, enabling the lossless merging of ternary adaptation weights
into **quantize**d weights and the adjustment of all **quantize**d weights. LoTA-QAF
operates through a combination of: i) A custom-designed ternary adaptation (TA)
that aligns ternary weights with the **quantization** grid and uses these ternary
weights to adjust **quantize**d weights. ii) A TA-based mechanism that enables the
lossless merging of adaptation weights. iii) Ternary signed gradient descent
(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and
Qwen-2.5 model families and validate its effectiveness on several downstream
tasks. On the MMLU benchmark, our method effectively recovers performance for
**quantize**d models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific
fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still
outperforms other methods.


## AI-Researcher Autonomous Scientific Innovation

>Authors: Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang

>2025-05-24

> http://arxiv.org/abs/2505.18705v1

The powerful reasoning capabilities of Large Language Models (LLMs) in
mathematics and coding, combined with their ability to automate complex tasks
through agentic frameworks, present unprecedented opportunities for
accelerating scientific innovation. In this paper, we introduce AI-Researcher,
a fully autonomous research system that transforms how AI-driven scientific
discovery is conducted and evaluated. Our framework seamlessly orchestrates the
complete research pipeline--from literature review and hypothesis generation to
algorithm implementation and publication-ready manuscript preparation--with
minimal human intervention. To rigorously assess autonomous research
capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising
state-of-the-art papers across diverse AI research domains, featuring both
guided innovation and open-ended exploration tasks. Through extensive
experiments, we demonstrate that AI-Researcher achieves remarkable
implementation success rates and produces research papers that approach
human-level quality. This work establishes new foundations for autonomous
scientific innovation that can complement human researchers by systematically
exploring solution spaces beyond cognitive limitations.


## DVD-Quant Data-free Video Diffusion Transformers Quantization

>Authors: Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang

>2025-05-24

> http://arxiv.org/abs/2505.18663v1

Diffusion Transformers (DiTs) have emerged as the state-of-the-art
architecture for video generation, yet their computational and memory demands
hinder practical deployment. While post-training **quantization** (PTQ) presents a
promising approach to accelerate Video DiT models, existing methods suffer from
two critical limitations: (1) dependence on lengthy, computation-heavy
calibration procedures, and (2) considerable performance deterioration after
**quantization**. To address these challenges, we propose DVD-Quant, a novel
Data-free **quantization** framework for Video DiTs. Our approach integrates three
key innovations: (1) Progressive Bounded Quantization (PBQ) and (2)
Auto-scaling Rotated Quantization (ARQ) for calibration data-free **quantization**
error reduction, as well as (3) $\delta$-Guided Bit Switching ($\delta$-GBS)
for adaptive bit-width allocation. Extensive experiments across multiple video
generation benchmarks demonstrate that DVD-Quant achieves an approximately
2$\times$ speedup over full-precision baselines on HunyuanVideo while
maintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ
for Video DiTs without compromising video quality. Code and models will be
available at https://github.com/lhxcs/DVD-Quant.


## Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees

>Authors: Sangwoo Park, Matteo Zecchin, Osvaldo Simeone

>2025-05-24

> http://arxiv.org/abs/2505.18659v1

Selecting artificial intelligence (AI) models, such as large language models
(LLMs), from multiple candidates requires accurate performance estimation. This
is ideally achieved through empirical evaluations involving abundant real-world
data. However, such evaluations are costly and impractical at scale. To address
this challenge, autoevaluation methods leverage synthetic data produced by
automated evaluators, such as LLMs-as-judges, reducing variance but potentially
introducing bias. Recent approaches have employed semi-supervised
prediction-powered inference (\texttt{PPI}) to correct for the bias of
autoevaluators. However, the use of autoevaluators may lead in practice to a
degradation in sample efficiency compared to conventional methods using only
real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel
framework that provides finite-sample reliability guarantees on the model
evaluation, while also ensuring an enhanced (or at least no worse) sample
efficiency compared to conventional methods. The key innovation of
\texttt{R-AutoEval+} is an adaptive construction of the model evaluation
variable, which dynamically tunes its reliance on synthetic data, reverting to
conventional methods when the autoevaluator is insufficiently accurate.
Experiments on the use of LLMs-as-judges for the optimization of **quantization**
settings for the weights of an LLM, and for prompt design in LLMs confirm the
reliability and efficiency of \texttt{R-AutoEval+}.


## Lookahead Q-Cache Achieving More Consistent KV Cache Eviction via Pseudo Query

>Authors: Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che

>2025-05-24

> http://arxiv.org/abs/2505.20334v1

Large language models (LLMs) rely on key-value cache (**KV** cache) to accelerate
decoding by reducing redundant computations. However, the **KV** cache memory usage
grows substantially with longer text sequences, posing challenges for efficient
deployment. Existing **KV** cache eviction methods prune tokens using
prefilling-stage attention scores, causing inconsistency with actual inference
queries, especially under tight memory budgets. In this paper, we propose
Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost
pseudo lookahead queries to better approximate the true decoding-stage queries.
By using these lookahead queries as the observation window for importance
estimation, LAQ achieves more consistent and accurate **KV** cache eviction aligned
with real inference scenarios. Experimental results on LongBench and
Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods
across various budget levels, achieving a 1 $\sim$ 4 point improvement on
LongBench under limited cache budget. Moreover, LAQ is complementary to
existing approaches and can be flexibly combined to yield further improvements.


## Think Before You Accept Semantic Reflective Verification for Faster Speculative Decoding

>Authors: Yixuan Wang, Yijun Liu, Shiyu ji, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che

>2025-05-24

> http://arxiv.org/abs/2505.18629v1

Large language models (LLMs) suffer from high inference latency due to the
auto-regressive decoding process. Speculative decoding accelerates inference by
generating multiple draft tokens using a lightweight model and verifying them
in parallel. However, existing verification methods rely heavily on
distributional consistency while overlooking semantic correctness, thereby
limiting the potential speedup of speculative decoding. While some methods
employ additional models for relaxed verification of draft tokens, they often
fail to generalize effectively to more diverse or open-domain settings. In this
work, we propose Reflective Verification, a training-free and semantics-aware
approach that achieves a better trade-off between correctness and efficiency.
Specifically, we leverage the inherent reflective capacity of LLMs to
semantically assess the correctness of draft tokens in parallel during
verification. Using prompt-based probing, we obtain both the original and
reflective distributions of draft tokens in a single forward pass. The fusion
of these distributions enables semantic-level verification of draft tokens that
incorporates both consistency and correctness. Experiments across multiple
domain benchmarks and model scales demonstrate that our method significantly
increases the acceptance length of draft tokens without compromising model
performance. Furthermore, we find that the proposed Reflective Verification is
orthogonal to existing statistical verification methods, and their combination
yields additional 5$\sim$15\% improvements in decoding speed.


## PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs

>Authors: Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang

>2025-05-24

> http://arxiv.org/abs/2505.18610v1

Recently, significant progress has been made in developing reasoning-capable
Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.
However, this long-CoT reasoning process imposes substantial memory overhead
due to the large Key-Value (**KV**) Cache memory overhead. Post-training **KV** Cache
**quantization** has emerged as a promising compression technique and has been
extensively studied in short-context scenarios. However, directly applying
existing methods to long-CoT LLMs causes significant performance degradation
due to the following two reasons: (1) Large cumulative error: Existing methods
fail to adequately leverage available memory, and they directly **quantize** the **KV**
Cache during each decoding step, leading to large cumulative **quantization**
error. (2) Short-context calibration: Due to Rotary Positional Embedding
(RoPE), the use of short-context data during calibration fails to account for
the distribution of less frequent channels in the Key Cache, resulting in
performance loss. We propose Progressive Mixed-Precision **KV** Cache Quantization
(PM-**KV**Q) for long-CoT LLMs to address the above issues in two folds: (1) To
reduce cumulative error, we design a progressive **quantization** strategy to
gradually lower the bit-width of **KV** Cache in each block. Then, we propose
block-wise memory allocation to assign a higher bit-width to more sensitive
transformer blocks. (2) To increase the calibration length without additional
overhead, we propose a new calibration strategy with positional interpolation
that leverages short calibration data with positional interpolation to
approximate the data distribution of long-context data. Extensive experiments
on 7B-70B long-CoT LLMs show that PM-**KV**Q improves reasoning benchmark
performance by up to 8% over SOTA baselines under the same memory budget. Our
code is available at https://github.com/thu-nics/PM-**KV**Q.


## Safety Alignment via Constrained Knowledge Unlearning

>Authors: Zesheng Shi, Yucheng Zhou, Jing Li

>2025-05-24

> http://arxiv.org/abs/2505.18588v1

Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.


## Autocomp LLM-Driven Code Optimization for Tensor Accelerators

>Authors: Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao

>2025-05-24

> http://arxiv.org/abs/2505.18574v1

Hardware accelerators, especially those designed for tensor processing, have
become ubiquitous in today's computing landscape. However, even with
significant efforts in building compilers, programming these tensor
accelerators remains challenging, leaving much of their potential
underutilized. Recently, large language models (LLMs), trained on large amounts
of code, have shown significant promise in code generation and optimization
tasks, but generating low-resource languages like specialized tensor
accelerator code still poses a significant challenge. We tackle this challenge
with Autocomp, an approach that empowers accelerator programmers to leverage
domain knowledge and hardware feedback to optimize code via an automated
LLM-driven search. We accomplish this by: 1) formulating each optimization pass
as a structured two-phase prompt, divided into planning and code generation
phases, 2) inserting domain knowledge during planning via a concise and
adaptable optimization menu, and 3) integrating correctness and performance
metrics from hardware as feedback at each search iteration. Across three
categories of representative workloads and two different accelerators, we
demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x
(convolution) faster than the vendor-provided library, and outperforms
expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x
(fine-grained linear algebra). Additionally, we demonstrate that optimization
schedules generated from Autocomp can be reused across similar tensor
operations, improving speedups by up to 24% under a fixed sample budget.


## Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition

>Authors: Yunqin Zhu, Henry Shaowu Yuchi, Yao Xie

>2025-05-24

> http://arxiv.org/abs/2505.18526v1

Kernels are key to encoding prior beliefs and data structures in Gaussian
process (GP) models. The design of expressive and scalable kernels has garnered
significant research attention. Deep kernel learning enhances kernel
flexibility by feeding inputs through a neural network before applying a
standard parametric form. However, this approach remains limited by the choice
of base kernels, inherits high inference costs, and often demands **sparse**
approximations. Drawing on Mercer's theorem, we introduce a fully data-driven,
scalable deep kernel representation where a neural network directly represents
a low-rank kernel through a small set of basis functions. This construction
enables highly efficient exact GP inference in linear time and memory without
invoking inducing points. It also supports scalable mini-batch training based
on a principled variational inference framework. We further propose a simple
variance correction procedure to guard against overconfidence in uncertainty
estimates. Experiments on synthetic and real-world data demonstrate the
advantages of our deep kernel GP in terms of predictive accuracy, uncertainty
quantification, and computational efficiency.


## Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement

>Authors: Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang

>2025-05-24

> http://arxiv.org/abs/2505.21535v2

While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural **pruning** framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.


## A Survey of LLM $\times$ DATA

>Authors: Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu

>2025-05-24

> http://arxiv.org/abs/2505.18458v2

The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, **KV**-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.


## DB-KSVD Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces

>Authors: Romeo Valentin, Sydney M. Katz, Vincent Vanhoucke, Mykel J. Kochenderfer

>2025-05-24

> http://arxiv.org/abs/2505.18441v1

Dictionary learning has recently emerged as a promising approach for
mechanistic interpretability of large transformer models. Disentangling
high-dimensional transformer embeddings, however, requires algorithms that
scale to high-dimensional data with large sample sizes. Recent work has
explored **sparse** autoencoders (SAEs) for this problem. However, SAEs use a
simple linear encoder to solve the **sparse** encoding subproblem, which is known
to be NP-hard. It is therefore interesting to understand whether this structure
is sufficient to find good solutions to the dictionary learning problem or if a
more sophisticated algorithm could find better solutions. In this work, we
propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm
that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich
theoretical foundations of KSVD but scales to datasets with millions of samples
and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by
disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics
from the SAEBench benchmark, where we achieve competitive results when compared
to established approaches based on SAEs. By matching SAE performance with an
entirely different optimization approach, our results suggest that (i) SAEs do
find strong solutions to the dictionary learning problem and (ii) that
traditional optimization approaches can be scaled to the required problem
sizes, offering a promising avenue for further research. We provide an
implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.


## Task Specific Pruning with LLM-Sieve How Many Parameters Does Your Task Really Need?

>Authors: Waleed Reda, Abhinav Jangda, Krishna Chintalapudi

>2025-05-23

> http://arxiv.org/abs/2505.18350v1

As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific **pruning** of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform **pruning** or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated **pruning** levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and **quantization**,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.


## PerMedCQA Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language

>Authors: Naghmeh Jamali, Milad Mohammadi, Danial Baledi, Zahra Rezvani, Hesham Faili

>2025-05-23

> http://arxiv.org/abs/2505.18331v1

Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain **sparse**. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA


## PLUMAGE Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training

>Authors: Matan Haroush, Daniel Soudry

>2025-05-23

> http://arxiv.org/abs/2505.18313v1

Accelerator memory and networking constraints have emerged as dominant
bottlenecks when training large language models LLMs with billions of
parameters. Existing low rank gradient estimators such as GaLoRE and FLORA
compress gradients and optimizer tensors by projecting weight gradients onto a
rank r subspace, enabling LLM training on consumer hardware. Yet, these methods
are either biased or subject to high estimator variance. Moreover, the
optimizer state based on the first and second moments estimates expressed in
the previous subspace becomes misaligned whenever the projection is updated,
leading to instabilities during training. We propose PLUMAGE: Probabilistic Low
rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in
replacement for existing low rank gradient estimators. It does not introduce
new hyperparameters beyond the chosen rank r and the update interval. In
addition, we resolve optimizer state misalignment issues to prevent spurious
weight updates and enhance training stability. We empirically demonstrate that
PLUMAGE shrinks the full rank optimization's gap over the pre training
evaluation loss by 33% on average across models and the average training loss
across the GLUE benchmark by 28% within a similar computational and memory
footprint as GaloRE.


## A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception

>Authors: Lan Wei, Dandan Zhang

>2025-05-23

> http://arxiv.org/abs/2505.18303v1

Optical microrobots, manipulated via optical tweezers (OT), have broad
applications in biomedicine. However, reliable pose and depth perception remain
fundamental challenges due to the transparent or low-contrast nature of the
microrobots, as well as the noisy and dynamic conditions of the microscale
environments in which they operate. An open dataset is crucial for enabling
reproducible research, facilitating benchmarking, and accelerating the
development of perception models tailored to microscale challenges.
Standardised evaluation enables consistent comparison across algorithms,
ensuring objective benchmarking and facilitating reproducible research. Here,
we introduce the OpTical MicroRobot dataset (OTMR), the first publicly
available dataset designed to support microrobot perception under the optical
microscope. OTMR contains 232,881 images spanning 18 microrobot types and 176
distinct poses. We benchmarked the performance of eight deep learning models,
including architectures derived via neural architecture search (NAS), on two
key tasks: pose classification and depth regression. Results indicated that
Vision Transformer (ViT) achieve the highest accuracy in pose classification,
while depth regression benefits from deeper architectures. Additionally,
increasing the size of the training dataset leads to substantial improvements
across both tasks, highlighting OTMR's potential as a foundational resource for
robust and generalisable microrobot perception in complex microscale
environments.


## Thinking Fast and Right Balancing Accuracy and Reasoning Length with Adaptive Rewards

>Authors: Jinyan Su, Claire Cardie

>2025-05-23

> http://arxiv.org/abs/2505.18298v1

Large language models (LLMs) have demonstrated strong reasoning abilities in
mathematical tasks, often enhanced through reinforcement learning (RL).
However, RL-trained models frequently produce unnecessarily long reasoning
traces -- even for simple queries -- leading to increased inference costs and
latency. While recent approaches attempt to control verbosity by adding length
penalties to the reward function, these methods rely on fixed penalty terms
that are hard to tune and cannot adapt as the model's reasoning capability
evolves, limiting their effectiveness. In this work, we propose an adaptive
reward-shaping method that enables LLMs to "think fast and right" -- producing
concise outputs without sacrificing correctness. Our method dynamically adjusts
the reward trade-off between accuracy and response length based on model
performance: when accuracy is high, the length penalty increases to encourage
faster length reduction; when accuracy drops, the penalty is relaxed to
preserve correctness. This adaptive reward accelerates early-stage length
reduction while avoiding over-compression in later stages. Experiments across
multiple datasets show that our approach consistently and dramatically reduces
reasoning length while largely maintaining accuracy, offering a new direction
for cost-efficient adaptive reasoning in large-scale language models.


## Einstein-Gauss-Bonnet-Myrzakulov Gravity from $R + F(T, G)$ Numerical Insights and Torsion-Gauss-Bonnet Dynamics in Weitzenböck Spacetime

>Authors: Davood Momeni, Ratbay Myrzakulov

>2025-05-23

> http://arxiv.org/abs/2505.18285v1

The study of modified gravity models has garnered significant attention
because of their potential to provide alternative explanations for cosmological
phenomena, such as the accelerated expansion of the universe and the nature of
dark energy. One such model, the Einstein-Gauss-Bonnet-Myrzakulov $R + F(T, G)$
gravity (EGBMG), which incorporates the curvature $R$, torsion $T$, and the
Gauss-Bonnet term $G$, offers a promising framework to explore the dynamics of
the universe and its evolution. This paper delves into the theoretical and
observational implications of the EGBMG model, focusing on its ability to
address long-standing challenges in cosmology, including the evolution of dark
energy and the transition from early-time inflationary behavior to late-time
**acceleration**. We review recent advancements in the model, including its
compatibility with observational data and its ability to provide new insights
into cosmic **acceleration**. Through a combination of theoretical models,
dynamical systems analysis, and cosmological diagnostics, we demonstrate the
robustness of the EGBMG framework in explaining the large-scale structure of
the universe and its accelerated expansion. This paper serves as a step toward
further exploring the potential of this model to understand the fundamental
forces driving Weitzenb$\"{o}$ck spacetime.


## Guided by Gut Efficient Test-Time Scaling with Reinforced Intrinsic Confidence

>Authors: Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu

>2025-05-23

> http://arxiv.org/abs/2505.20325v1

Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces **KV** cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.


## Accelerating Learned Image Compression Through Modeling Neural Training Dynamics

>Authors: Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu

>2025-05-23

> http://arxiv.org/abs/2505.18107v1

As learned image compression (LIC) methods become increasingly
computationally demanding, enhancing their training efficiency is crucial. This
paper takes a step forward in accelerating the training of LIC methods by
modeling the neural training dynamics. We first propose a Sensitivity-aware
True and Dummy Embedding Training mechanism (STDET) that clusters LIC model
parameters into few separate modes where parameters are expressed as affine
transformations of reference parameters within the same mode. By further
utilizing the stable intra-mode correlations throughout training and parameter
sensitivities, we gradually embed non-reference parameters, reducing the number
of trainable parameters. Additionally, we incorporate a Sampling-then-Moving
Average (SMA) technique, interpolating sampled weights from stochastic gradient
descent (SGD) training to obtain the moving average weights, ensuring smooth
temporal behavior and minimizing training state variances. Overall, our method
significantly reduces training space dimensions and the number of trainable
parameters without sacrificing model performance, thus accelerating model
convergence. We also provide a theoretical analysis on the Noisy quadratic
model, showing that the proposed method achieves a lower training variance than
standard SGD. Our approach offers valuable insights for further developing
efficient training methods for LICs.


## QwenLong-CPRS Towards $\infty$-LLMs with Dynamic Context Optimization

>Authors: Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan

>2025-05-23

> http://arxiv.org/abs/2505.18092v2

This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the "lost in the middle"
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and **sparse** attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.


## Reward Model Generalization for Compute-Aware Test-Time Reasoning

>Authors: Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua

>2025-05-23

> http://arxiv.org/abs/2505.18065v1

External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and **sparsity** statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.


## LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision

>Authors: Anthony Fuller, Yousef Yassin, Junfeng Wen, Daniel G. Kyrollos, Tarek Ibrahim, James R. Green, Evan Shelhamer

>2025-05-23

> http://arxiv.org/abs/2505.18051v1

Vision transformers are ever larger, more accurate, and more expensive to
compute. The expense is even more extreme at high resolution as the number of
tokens grows quadratically with the image size. We turn to adaptive computation
to cope with this cost by learning to predict where to compute. Our LookWhere
method divides the computation between a low-resolution selector and a
high-resolution extractor without ever processing the full high-resolution
input. We jointly pretrain the selector and extractor without task supervision
by distillation from a self-supervised teacher, in effect, learning where and
what to compute simultaneously. Unlike prior token reduction methods, which pay
to save by **pruning** already-computed tokens, and prior token selection methods,
which require complex and expensive per-task optimization, LookWhere
economically and accurately selects and extracts transferrable representations
of images. We show that LookWhere excels at **sparse** recognition on
high-resolution inputs (Traffic Signs), maintaining accuracy while reducing
FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks
that are global (ImageNet classification) or local (ADE20K segmentation),
improving accuracy while reducing time by 1.36x.


## Towards Analyzing and Understanding the Limitations of VAPO A Theoretical Perspective

>Authors: Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng

>2025-05-23

> http://arxiv.org/abs/2505.17997v2

The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and **sparse** reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.


## ADLGen Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling

>Authors: Weihang You, Hanqi Jiang, Zishuai Liu, Zihang Xie, Tianming Liu, Jin Lu, Fei Dou

>2025-05-23

> http://arxiv.org/abs/2505.17987v1

Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent **sparsity**
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.


## Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems

>Authors: Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths

>2025-05-23

> http://arxiv.org/abs/2505.17968v1

Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.


## NeuroTrails Training with Dynamic Sparse Heads as the Key to Effective Ensembling

>Authors: Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu

>2025-05-23

> http://arxiv.org/abs/2505.17909v1

Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a **sparse** multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic **sparsity** attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.


## Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization

>Authors: Wenjin Qin, Hailin Wang, Hao Shu, Feng Zhang, Jianjun Wang, Xiangyong Cao, Xi-Le Zhao, Gemine Vivone

>2025-05-23

> http://arxiv.org/abs/2505.17881v1

In recent years, tensor decomposition-based approaches for hyperspectral
anomaly detection (HAD) have gained significant attention in the field of
remote sensing. However, existing methods often fail to fully leverage both the
global correlations and local smoothness of the background components in
hyperspectral images (HSIs), which exist in both the spectral and spatial
domains. This limitation results in suboptimal detection performance. To
mitigate this critical issue, we put forward a novel HAD method named
HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)
factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first
decomposed into background and anomaly components. The TR decomposition is then
employed to capture the spatial-spectral correlations within the background
component. Additionally, we introduce a unified and efficient nonconvex
regularizer, induced by tensor singular value decomposition (TSVD), to
simultaneously encode the low-rankness and **sparsity** of the 3-D gradient TR
factors into a unique concise form. The above characterization scheme enables
the interpretable gradient TR factors to inherit the low-rankness and
smoothness of the original background. To further enhance anomaly detection, we
design a generalized nonconvex regularization term to exploit the group
**sparsity** of the anomaly component. To solve the resulting doubly nonconvex
model, we develop a highly efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) framework. Experimental
results on several benchmark datasets demonstrate that our proposed method
outperforms existing state-of-the-art (SOTA) approaches in terms of detection
accuracy.


## The emergence of sparse attention impact of data distribution and benefits of repetition

>Authors: Nicolas Zucchet, Francesco d'Angelo, Andrew K. Lampinen, Stephanie C. Y. Chan

>2025-05-23

> http://arxiv.org/abs/2505.17863v1

Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of **sparse** attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving **sparse** attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.


## Stochastic Weight Sharing for Bayesian Neural Networks

>Authors: Moule Lin, Shuhao Guan, Weipeng Jing, Goetz Botterweck, Andrea Patane

>2025-05-23

> http://arxiv.org/abs/2505.17856v1

While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing **quantization** techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.


## ELDeR Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning

>Authors: Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao

>2025-05-23

> http://arxiv.org/abs/2505.18232v1

The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit **sparsity**, which can be used for **pruning**. Previous **pruning**
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct **pruning**, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured **pruning** methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise **pruning** method, its end-to-end **acceleration** effect
is obvious, making it a promising technique for efficient LLMs.


## NSNQuant A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache

>Authors: Donghyun Son, Euntae Choi, Sungjoo Yoo

>2025-05-23

> http://arxiv.org/abs/2505.18231v1

Large Language Model (LLM) inference is typically memory-intensive,
especially when processing large batch sizes and long sequences, due to the
large size of key-value (**KV**) cache. Vector Quantization (VQ) is recently
adopted to alleviate this issue, but we find that the existing approach is
susceptible to distribution shift due to its reliance on calibration datasets.
To address this limitation, we introduce NSNQuant, a calibration-free Vector
Quantization (VQ) technique designed for **low-bit** compression of the **KV** cache.
By applying a three-step transformation-1) a token-wise normalization
(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise
normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns
the token distribution with the standard normal distribution. This alignment
enables robust, calibration-free vector **quantization** using a single reusable
codebook. Extensive experiments show that NSNQuant consistently outperforms
prior methods in both 1-bit and 2-bit settings, offering strong generalization
and up to 3$\times$ throughput gain over full-precision baselines.


## RECIPE-TKG From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion

>Authors: Ömer Faruk Akgül, Feiyu Zhu, Yuxin Yang, Rajgopal Kannan, Viktor Prasanna

>2025-05-23

> http://arxiv.org/abs/2505.17794v1

Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
**sparse** historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.


## Titanus Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration

>Authors: Peilin Chen, Xiaoxuan Yang

>2025-05-23

> http://arxiv.org/abs/2505.17787v1

Large language models (LLMs) have gained great success in various domains.
Existing systems cache Key and Value within the attention block to avoid
redundant computations. However, the size of key-value cache (**KV** cache) is
unpredictable and can even be tens of times larger than the weights in the long
context length scenario. In this work, we propose Titanus, a software-hardware
co-design to efficiently compress the **KV** cache on-the-fly. We first propose the
cascade **pruning**-**quantization** (CPQ) method to reduce the **KV** cache movement. The
hierarchical **quantization** extension strategy is introduced to tackle the
non-independent per-channel **quantization** issue. To further reduce **KV** cache
movement, we transfer only the non-zero **KV** cache between the accelerator and
off-chip memory. Moreover, we customize a two-stage design space exploration
framework for the CPQ method. A novel pipeline and parallelism dataflow is
designed to reduce the first token generation time. Experiments show that
Titanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency
(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code
for Titanus is available at
https://github.com/peilin-chen/Titanus-for-LLM-**acceleration**.


## Inference-Time Decomposition of Activations (ITDA) A Scalable Approach to Interpreting Large Language Models

>Authors: Patrick Leask, Neel Nanda, Noura Al Moubayed

>2025-05-23

> http://arxiv.org/abs/2505.17769v1

Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.


## Structured Linear CDEs Maximally Expressive and Parallel-in-Time Sequence Models

>Authors: Benjamin Walker, Lingyi Yang, Nicola Muca Cirone, Cristopher Salvi, Terry Lyons

>2025-05-23

> http://arxiv.org/abs/2505.17761v1

Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on **sparsity** and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, **sparse**, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.


## Slot-MLLM Object-Centric Visual Tokenization for Multimodal LLM

>Authors: Donghwan Chi, Hyomin Kim, Yoonjin Oh, Yongjin Kim, Donghoon Lee, Daejin Jo, Jongmin Kim, Junyeob Baek, Sungjin Ahn, Sungwoong Kim

>2025-05-23

> http://arxiv.org/abs/2505.17726v2

Recently, multimodal large language models (MLLMs) have emerged as a key
approach in achieving artificial general intelligence. In particular,
vision-language MLLMs have been developed to generate not only text but also
visual outputs from multimodal inputs. This advancement requires efficient
image tokens that LLMs can process effectively both in input and output.
However, existing image tokenization methods for MLLMs typically capture only
global abstract concepts or uniformly segmented image patches, restricting
MLLMs' capability to effectively understand or generate detailed visual
content, particularly at the object level. To address this limitation, we
propose an object-centric visual tokenizer based on Slot Attention specifically
for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and
residual vector **quantization**, our proposed discretized slot tokens can encode
local visual details while maintaining high-level semantics, and also align
with textual data to be integrated seamlessly within a unified next-token
prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant
performance improvements over baselines with previous visual tokenizers across
various vision-language tasks that entail local detailed comprehension and
generation. Notably, this work is the first demonstration of the feasibility of
object-centric slot attention performed with MLLMs and in-the-wild natural
images.


## FlashForge Ultra-Efficient Prefix-Aware Attention for LLM Decoding

>Authors: Zhibin Wang, Rui Ning, Chao Fang, Zhonghui Zhang, Xi Lin, Shaobo Ma, Mo Zhou, Xue Li, Zhongfeng Wang, Chengying Huan, Rong Gu, Kun Yang, Guihai Chen, Sheng Zhong, Chen Tian

>2025-05-23

> http://arxiv.org/abs/2505.17694v1

Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (**KV**)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared **KV** cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.


## Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection

>Authors: Dan Yuan, Yi Feng, Ziyun Tang

>2025-05-23

> http://arxiv.org/abs/2505.17683v1

Intraventricular hemorrhage (IVH) is a severe neurological complication among
premature infants, necessitating early and accurate detection from brain
ultrasound (US) images to improve clinical outcomes. While recent deep learning
methods offer promise for computer-aided diagnosis, challenges remain in
capturing both local spatial details and global contextual dependencies
critical for segmenting brain anatomies. In this work, we propose an enhanced
Residual U-Net architecture incorporating two complementary attention
mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse
Attention Layer (SAL). The CBAM improves the model's ability to refine spatial
and channel-wise features, while the SAL introduces a dual-branch design,
**sparse** attention filters out low-confidence query-key pairs to suppress noise,
and dense attention ensures comprehensive information propagation. Extensive
experiments on the Brain US dataset demonstrate that our method achieves
state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU
of 81.84% for ventricle region segmentation. These results highlight the
effectiveness of integrating spatial refinement and attention **sparsity** for
robust brain anatomy detection. Code is available at:
https://github.com/DanYuan001/BrainImgSegment.


## Rethinking Agent Design From Top-Down Workflows to Bottom-Up Skill Evolution

>Authors: Jiawei Du, Jinlong Wu, Yuzheng Chen, Yucheng Hu, Bing Li, Joey Tianyi Zhou

>2025-05-23

> http://arxiv.org/abs/2505.17673v1

Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose
tasks, define workflows, and assign agents to execute each step. While
effective on benchmark-style tasks, such systems rely on designer updates and
overlook agents' potential to learn from experience. Recently, Silver and
Sutton(2025) envision a shift into a new era, where agents could progress from
a stream of experiences. In this paper, we instantiate this vision of
experience-driven learning by introducing a bottom-up agent paradigm that
mirrors the human learning process. Agents acquire competence through a
trial-and-reasoning mechanism-exploring, reflecting on outcomes, and
abstracting skills over time. Once acquired, skills can be rapidly shared and
extended, enabling continual evolution rather than static replication. As more
agents are deployed, their diverse experiences accelerate this collective
process, making bottom-up design especially suited for open-ended environments.
We evaluate this paradigm in Slay the Spire and Civilization V, where agents
perceive through raw visual inputs and act via mouse outputs, the same as human
players. Using a unified, game-agnostic codebase without any game-specific
prompts or privileged APIs, our bottom-up agents acquire skills entirely
through autonomous interaction, demonstrating the potential of the bottom-up
paradigm in complex, real-world environments. Our code is available at
https://github.com/AngusDujw/Bottom-Up-Agent.


## Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs

>Authors: Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele

>2025-05-23

> http://arxiv.org/abs/2505.17662v1

Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines **quantization**-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).


## PreMoe Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval

>Authors: Zehua Pei, Ying Zhang, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu

>2025-05-23

> http://arxiv.org/abs/2505.17639v1

Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert **pruning** (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 **pruning**
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 **pruning**, while even more aggressive **pruning** to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.


## Causal Spatio-Temporal Prediction An Effective and Efficient Multi-Modal Approach

>Authors: Yuting Huang, Ziquan Fang, Zhihao Zeng, Lu Chen, Yunjun Gao

>2025-05-23

> http://arxiv.org/abs/2505.17637v1

Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.


## Navigate the Unknown Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration

>Authors: Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao

>2025-05-23

> http://arxiv.org/abs/2505.17621v2

Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
**sparse** outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, **sparse** reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.


## NeUQI Near-Optimal Uniform Quantization Parameter Initialization

>Authors: Li Lin, Xinyu Hu, Xiaojun Wan

>2025-05-23

> http://arxiv.org/abs/2505.17595v2

Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training **quantization** (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform **quantization** representation is favored for its efficiency and ease of
deployment since uniform **quantization** is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
**quantization** have led to noticeable improvements in post-**quantization** model
performance; however, they primarily focus on **quantization** methodologies, while
the initialization of **quantization** parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
**quantization**. NeUQI is orthogonal to prior **quantization** methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.


## H2Towards Efficient Large-Scale LLM Training on Hyper-Heterogeneous Cluster over 1,000 Chips

>Authors: Ding Tang, Jiecheng Zhou, Jiakai Hu, Shengwei Li, Huihuang Zheng, Zhilin Pei, Hui Wang, Xingcheng Zhang

>2025-05-23

> http://arxiv.org/abs/2505.17548v1

Recent advancements in large language models (LLMs) necessitate extensive
computational resources, prompting the use of diverse hardware accelerators
from multiple vendors. However, traditional distributed training frameworks
struggle to efficiently utilize hyper-heterogeneous clusters comprising
thousands of chips due to significant disparities in software stacks, operator
implementations, communication libraries, and hardware capabilities. To address
these challenges, we propose H2, which stands for HyperHetero and is a
systematic framework enabling efficient training of LLMs on clusters with over
1,000 heterogeneous chips. H2 incorporates DiTorch, a unified
PyTorch-compatible interface ensuring program consistency across chips, and
DiComm, a device-direct RDMA communication library optimized for heterogeneous
environments. Furthermore, we introduce HeteroPP with HeteroAuto, an adaptive
pipeline parallelism strategy that dynamically balances computational load,
memory limitations, and communication overhead. Evaluations on a
100-billion-parameter LLM demonstrate that our approach consistently achieves a
superlinear speedup, outperforming baseline homogeneous training solutions by
up to 16.37% in our experiments. These findings validate the feasibility and
efficiency of hyper-heterogeneous training at unprecedented scales.


## MEGADance Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation

>Authors: Kaixing Yang, Xulong Tang, Ziqiao Peng, Yuxuan Hu, Jun He, Hongyan Liu

>2025-05-23

> http://arxiv.org/abs/2505.17543v1

Music-driven 3D dance generation has attracted increasing attention in recent
years, with promising applications in choreography, virtual reality, and
creative content creation. Previous research has generated promising realistic
dance movement from audio signals. However, traditional methods underutilize
genre conditioning, often treating it as auxiliary modifiers rather than core
semantic drivers. This oversight compromises music-motion synchronization and
disrupts dance genre continuity, particularly during complex rhythmic
transitions, thereby leading to visually unsatisfactory effects. To address the
challenge, we propose MEGADance, a novel architecture for music-driven 3D dance
generation. By decoupling choreographic consistency into dance generality and
genre specificity, MEGADance demonstrates significant dance quality and strong
genre controllability. It consists of two stages: (1) High-Fidelity Dance
Quantization Stage (HFDQ), which encodes dance motions into a latent
representation by Finite Scalar Quantization (FSQ) and reconstructs them with
kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage
(GADG), which maps music into the latent representation by synergistic
utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid
backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate
the state-of-the-art performance of MEGADance both qualitatively and
quantitatively. Code will be released upon acceptance.


## L-MTP Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models

>Authors: Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua

>2025-05-23

> http://arxiv.org/abs/2505.17505v1

Large language models (LLMs) have achieved notable progress. Despite their
success, next-token prediction (NTP), the dominant method for LLM training and
inference, is constrained in both contextual coverage and inference efficiency
due to its inherently sequential process. To overcome these challenges, we
propose leap multi-token prediction~(L-MTP), an innovative token prediction
method that extends the capabilities of multi-token prediction (MTP) by
introducing a leap-based mechanism. Unlike conventional MTP, which generates
multiple tokens at adjacent positions, L-MTP strategically skips over
intermediate tokens, predicting non-sequential ones in a single forward pass.
This structured leap not only enhances the model's ability to capture
long-range dependencies but also enables a decoding strategy specially
optimized for non-sequential leap token generation, effectively accelerating
inference. We theoretically demonstrate the benefit of L-MTP in improving
inference efficiency. Experiments across diverse benchmarks validate its merit
in boosting both LLM performance and inference speed. The source code will be
publicly available.


## The GMRES method for solving the large indefinite least squares problem via an accelerated preconditioner

>Authors: Jun Li, Lingsheng Meng

>2025-05-23

> http://arxiv.org/abs/2505.17504v1

In this research, to solve the large indefinite least squares problem, we
firstly transform its normal equation into a **sparse** block three-by-three linear
systems, then use GMRES method with an accelerated preconditioner to solve it.
The construction idea of the preconditioner comes from the thought of Luo et.al
[Luo, WH., Gu, XM., Carpentieri, B., BIT 62, 1983-2004(2022)], and the
advantage of this is that the preconditioner is closer to the coefficient
matrix of the block three-by-three linear systems when the parameter approachs
zero. Theoretically, the iteration method under the preconditioner satisfies
the conditional convergence, and all eigenvalues of the preconditioned matrix
are real numbers and gathered at point $(1,0)$ as parameter is close to $0$. In
the end, numerical results reflect that the theoretical results is correct and
the proposed preconditioner is effective by comparing with serval existing
preconditioners.


## The Discovery Engine A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes

>Authors: Vladimir Baulin, Austin Cook, Daniel Friedman, Janna Lumiruusu, Andrew Pashea, Shagor Rahman, Benedikt Waldeck

>2025-05-23

> http://arxiv.org/abs/2505.17500v1

The prevailing model for disseminating scientific knowledge relies on
individual publications dispersed across numerous journals and archives. This
legacy system is ill suited to the recent exponential proliferation of
publications, contributing to insurmountable information overload, issues
surrounding reproducibility and retractions. We introduce the Discovery Engine,
a framework to address these challenges by transforming an array of
disconnected literature into a unified, computationally tractable
representation of a scientific domain. Central to our approach is the
LLM-driven distillation of publications into structured "knowledge artifacts,"
instances of a universal conceptual schema, complete with verifiable links to
source evidence. These artifacts are then encoded into a high-dimensional
Conceptual Tensor. This tensor serves as the primary, compressed representation
of the synthesized field, where its labeled modes index scientific components
(concepts, methods, parameters, relations) and its entries quantify their
interdependencies. The Discovery Engine allows dynamic "unrolling" of this
tensor into human-interpretable views, such as explicit knowledge graphs (the
CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI
agents operate directly on the graph using abstract mathematical and learned
operations to navigate the knowledge landscape, identify non-obvious
connections, pinpoint gaps, and assist researchers in generating novel
knowledge artifacts (hypotheses, designs). By converting literature into a
structured tensor and enabling agent-based interaction with this compact
representation, the Discovery Engine offers a new paradigm for AI-augmented
scientific inquiry and accelerated discovery.


## ProxySPEX Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs

>Authors: Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran

>2025-05-23

> http://arxiv.org/abs/2505.17495v1

Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction **sparsity** to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive **pruning** of heads than marginal approaches.


## HiLAB A Hybrid Inverse-Design Framework

>Authors: Reza Marzban, Hamed Abiri, Raphael Pestourie, Ali Adibi

>2025-05-23

> http://arxiv.org/abs/2505.17491v1

HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based
partial optimizations, and Bayesian optimization) is a new paradigm for inverse
design of nanophotonic structures. Combining early-terminated topological
optimization (TO) with a Vision Transformer-based variational autoencoder (VAE)
and a Bayesian search, HiLAB addresses multi-functional device design by
generating diverse freeform configurations at reduced simulation costs.
Shortened adjoint-driven TO runs, coupled with randomized physical parameters,
produce robust initial structures. These structures are compressed into a
compact latent space by the VAE, enabling Bayesian optimization to co-optimize
geometry and physical hyperparameters. Crucially, the trained VAE can be reused
for alternative objectives or constraints by adjusting only the acquisition
function. Compared to conventional TO pipelines prone to local optima, HiLAB
systematically explores near-global optima with considerably fewer
electromagnetic simulations. Even after accounting for training overhead, the
total number of full simulations decreases by over an order of magnitude,
accelerating the discovery of fabrication-friendly devices. Demonstrating its
efficacy, HiLAB is used to design an achromatic beam deflector for red, green,
and blue wavelengths, achieving balanced diffraction efficiencies of ~25% while
mitigating chromatic aberrations-a performance surpassing existing
demonstrations. Overall, HiLAB provides a flexible platform for robust,
multi-parameter photonic designs and rapid adaptation to next-generation
nanophotonic challenges.


## Hydra Structured Cross-Source Enhanced Large Language Model Reasoning

>Authors: Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang

>2025-05-23

> http://arxiv.org/abs/2505.17464v1

Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. Current hybrid RAG system retrieves evidence
from both knowledge graphs (KGs) and text documents to support LLM reasoning.
However, it faces challenges like handling multi-hop reasoning, multi-entity
questions, multi-source verification, and effective graph utilization. To
address these limitations, we present Hydra, a training-free framework that
unifies graph topology, document semantics, and source reliability to support
deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity
problems through agent-driven exploration that combines structured and
unstructured retrieval, increasing both diversity and precision of evidence. To
tackle multi-source verification, Hydra uses a tri-factor cross-source
verification (source trustworthiness assessment, cross-source corroboration,
and entity-path alignment), to balance topic relevance with cross-modal
agreement. By leveraging graph structure, Hydra fuses heterogeneous sources,
guides efficient exploration, and prunes noise early. Comprehensive experiments
on seven benchmark datasets show that Hydra achieves overall state-of-the-art
results on all benchmarks with GPT-3.5, outperforming the strong hybrid
baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra
enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance
comparable to that of GPT-4-Turbo.


## Discovering Forbidden Topics in Language Models

>Authors: Can Rager, Chris Wendler, Rohit Gandikota, David Bau

>2025-05-23

> http://arxiv.org/abs/2505.17441v2

Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits "thought suppression" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the **quantize**d model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.


## UniTTS An end-to-end TTS system without decoupling of acoustic and semantic information

>Authors: Rui Wang, Qianguo Sun, Tianrong Chen, Zhiyun Zeng, Junlong Wu, Jiaxing Zhang

>2025-05-23

> http://arxiv.org/abs/2505.17426v1

The emergence of multi-codebook neutral audio codecs such as Residual Vector
Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly
advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These
codecs are crucial in separating semantic and acoustic information while
efficiently harnessing semantic priors. However, since semantic and acoustic
information cannot be fully aligned, a significant drawback of these methods
when applied to LLM-based TTS is that large language models may have limited
access to comprehensive audio information. To address this limitation, we
propose DistilCodec and UniTTS, which collectively offer the following
advantages: 1) This method can distill a multi-codebook audio codec into a
single-codebook audio codec with 32,768 codes while achieving a near 100\%
utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a
large amount of high-quality unlabeled audio (such as audiobooks with sound
effects, songs, etc.) can be incorporated during training, further expanding
data diversity and broadening its applicability. 3) Leveraging the
comprehensive audio information modeling of DistilCodec, we integrated three
key tasks into UniTTS's pre-training framework: audio modality autoregression,
text modality autoregression, and speech-text cross-modal autoregression. This
allows UniTTS to accept interleaved text and speech/audio prompts while
substantially preserving LLM's text capabilities. 4) UniTTS employs a
three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and
Alignment. Source code and model checkpoints are publicly available at
https://github.com/IDEA-Emdoor-Lab/UniTTS and
https://github.com/IDEA-Emdoor-Lab/DistilCodec.


## DASH Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies

>Authors: Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang

>2025-05-23

> http://arxiv.org/abs/2505.17420v1

Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference **acceleration** while maintaining competitive task performance,
outperforming existing methods.


## Direct3D-S2 Gigascale 3D Generation Made Easy with Spatial Sparse Attention

>Authors: Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, Yao Yao

>2025-05-23

> http://arxiv.org/abs/2505.17412v2

Generating high-resolution 3D shapes using volumetric representations such as
Signed Distance Functions (SDFs) presents substantial computational and memory
challenges. We introduce Direct3D-S2, a scalable 3D generation framework based
on **sparse** volumes that achieves superior output quality with dramatically
reduced training costs. Our key innovation is the Spatial Sparse Attention
(SSA) mechanism, which greatly enhances the efficiency of Diffusion Transformer
(DiT) computations on **sparse** volumetric data. SSA allows the model to
effectively process large token sets within **sparse** volumes, substantially
reducing computational overhead and achieving a 3.9x speedup in the forward
pass and a 9.6x speedup in the backward pass. Our framework also includes a
variational autoencoder (VAE) that maintains a consistent **sparse** volumetric
format across input, latent, and output stages. Compared to previous methods
with heterogeneous representations in 3D VAE, this unified design significantly
improves training efficiency and stability. Our model is trained on public
available datasets, and experiments demonstrate that Direct3D-S2 not only
surpasses state-of-the-art methods in generation quality and efficiency, but
also enables training at 1024 resolution using only 8 GPUs, a task typically
requiring at least 32 GPUs for volumetric representations at 256 resolution,
thus making gigascale 3D generation both practical and accessible. Project
page: https://www.neural4d.com/research/direct3d-s2.


## CIM-NET A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures

>Authors: Shan Gao, Zhiqiang Wu, Yawen Niu, Xiaotao Li, Qingqing Xu

>2025-05-23

> http://arxiv.org/abs/2505.21522v1

While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their **acceleration**
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM **acceleration**; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

