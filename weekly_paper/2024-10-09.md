# 2024-10-09

# Table of Contents
* [PrefixQuant Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](#PrefixQuant-Static-Quantization-Beats-Dynamic-through-Prefixed-Outliers-in-LLMs)
* [Differential Transformer](#Differential-Transformer)
* [Variable Resolution Pixel Quantization for Low Power Machine Vision Application on Edge](#Variable-Resolution-Pixel-Quantization-for-Low-Power-Machine-Vision-Application-on-Edge)
* [Union Bound Analysis for Spin-Torque Transfer Magnetic Random Access Memory (STT-MRAM) With Channel Quantization](#Union-Bound-Analysis-for-Spin-Torque-Transfer-Magnetic-Random-Access-Memory-(STT-MRAM)-With-Channel-Quantization)
* [TidalDecode Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](#TidalDecode-Fast-and-Accurate-LLM-Decoding-with-Position-Persistent-Sparse-Attention)
* [Mastering Chinese Chess AI (Xiangqi) Without Search](#Mastering-Chinese-Chess-AI-(Xiangqi)-Without-Search)
* [Towards Measuring Goal-Directedness in AI Systems](#Towards-Measuring-Goal-Directedness-in-AI-Systems)
* [Deciphering Refactoring Branch Dynamics in Modern Code Review An Empirical Study on Qt](#Deciphering-Refactoring-Branch-Dynamics-in-Modern-Code-Review-An-Empirical-Study-on-Qt)
* [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](#Reasoning-Enhanced-Healthcare-Predictions-with-Knowledge-Graph-Community-Retrieval)
* [EnsemW2S Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?](#EnsemW2S-Can-an-Ensemble-of-LLMs-be-Leveraged-to-Obtain-a-Stronger-LLM?)
* [SparseVLM Visual Token Sparsification for Efficient Vision-Language Model Inference](#SparseVLM-Visual-Token-Sparsification-for-Efficient-Vision-Language-Model-Inference)
* [HALL-E Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis](#HALL-E-Hierarchical-Neural-Codec-Language-Model-for-Minute-Long-Zero-Shot-Text-to-Speech-Synthesis)
* [Accelerating Inference of Networks in the Frequency Domain](#Accelerating-Inference-of-Networks-in-the-Frequency-Domain)
* [Language Model-Driven Data Pruning Enables Efficient Active Learning](#Language-Model-Driven-Data-Pruning-Enables-Efficient-Active-Learning)
* [RoQLlama A Lightweight Romanian Adapted Language Model](#RoQLlama-A-Lightweight-Romanian-Adapted-Language-Model)
* [Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension](#Correlation-Aware-Select-and-Merge-Attention-for-Efficient-Fine-Tuning-and-Context-Length-Extension)
* [Equivariant Neural Functional Networks for Transformers](#Equivariant-Neural-Functional-Networks-for-Transformers)
* [Toxic Subword Pruning for Dialogue Response Generation on Large Language Models](#Toxic-Subword-Pruning-for-Dialogue-Response-Generation-on-Large-Language-Models)
* [LLM-TOPLA Efficient LLM Ensemble by Maximising Diversity](#LLM-TOPLA-Efficient-LLM-Ensemble-by-Maximising-Diversity)
* [Generative Semantic Communication for Text-to-Speech Synthesis](#Generative-Semantic-Communication-for-Text-to-Speech-Synthesis)
* [Exploring the Benefit of Activation Sparsity in Pre-training](#Exploring-the-Benefit-of-Activation-Sparsity-in-Pre-training)
* [Predictive Coding for Decision Transformer](#Predictive-Coding-for-Decision-Transformer)
* [Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization](#Mitigating-Adversarial-Perturbations-for-Deep-Reinforcement-Learning-via-Vector-Quantization)
* [Error Correction Code Transformer From Non-Unified to Unified](#Error-Correction-Code-Transformer-From-Non-Unified-to-Unified)
* [An X-Ray Is Worth 15 Features Sparse Autoencoders for Interpretable Radiology Report Generation](#An-X-Ray-Is-Worth-15-Features-Sparse-Autoencoders-for-Interpretable-Radiology-Report-Generation)
* [Strict quantization for compact pseudo-Kähler manifolds and group actions](#Strict-quantization-for-compact-pseudo-Kähler-manifolds-and-group-actions)
* [Resource-aware Mixed-precision Quantization for Enhancing Deployability of Transformers for Time-series Forecasting on Embedded FPGAs](#Resource-aware-Mixed-precision-Quantization-for-Enhancing-Deployability-of-Transformers-for-Time-series-Forecasting-on-Embedded-FPGAs)
* [ALR$^2$ A Retrieve-then-Reason Framework for Long-context Question Answering](#ALR$^2$-A-Retrieve-then-Reason-Framework-for-Long-context-Question-Answering)
* [EXAQ Exponent Aware Quantization For LLMs Acceleration](#EXAQ-Exponent-Aware-Quantization-For-LLMs-Acceleration)
* [ARB-LLM Alternating Refined Binarizations for Large Language Models](#ARB-LLM-Alternating-Refined-Binarizations-for-Large-Language-Models)
* [Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs](#Enhancing-Short-Text-Topic-Modeling-with-LLM-Driven-Context-Expansion-and-Prefix-Tuned-VAEs)
* [Adaptive Inference-Time Compute LLMs Can Predict if They Can Do Better, Even Mid-Generation](#Adaptive-Inference-Time-Compute-LLMs-Can-Predict-if-They-Can-Do-Better,-Even-Mid-Generation)
* [Deconstructing Recurrence, Attention, and Gating Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems](#Deconstructing-Recurrence,-Attention,-and-Gating-Investigating-the-transferability-of-Transformers-and-Gated-Recurrent-Neural-Networks-in-forecasting-of-dynamical-systems)
* [Cut the Crap An Economical Communication Pipeline for LLM-based Multi-Agent Systems](#Cut-the-Crap-An-Economical-Communication-Pipeline-for-LLM-based-Multi-Agent-Systems)
* [Overcoming Representation Bias in Fairness-Aware data Repair using Optimal Transport](#Overcoming-Representation-Bias-in-Fairness-Aware-data-Repair-using-Optimal-Transport)
* [SageAttention Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](#SageAttention-Accurate-8-Bit-Attention-for-Plug-and-play-Inference-Acceleration)
* [Llama SLayer 8B Shallow Layers Hold the Key to Knowledge Injection](#Llama-SLayer-8B-Shallow-Layers-Hold-the-Key-to-Knowledge-Injection)
* [Jailbreak Antidote Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](#Jailbreak-Antidote-Runtime-Safety-Utility-Balance-via-Sparse-Representation-Adjustment-in-Large-Language-Models)
* [Language Models are Graph Learners](#Language-Models-are-Graph-Learners)
* [SEAL SEmantic-Augmented Imitation Learning via Language Model](#SEAL-SEmantic-Augmented-Imitation-Learning-via-Language-Model)
* [Aligning with Logic Measuring, Evaluating and Improving Logical Consistency in Large Language Models](#Aligning-with-Logic-Measuring,-Evaluating-and-Improving-Logical-Consistency-in-Large-Language-Models)
* [Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices](#Searching-for-Efficient-Linear-Layers-over-a-Continuous-Space-of-Structured-Matrices)
* [Deep Generative Modeling for Identification of Noisy, Non-Stationary Dynamical Systems](#Deep-Generative-Modeling-for-Identification-of-Noisy,-Non-Stationary-Dynamical-Systems)
* [FLAG Financial Long Document Classification via AMR-based GNN](#FLAG-Financial-Long-Document-Classification-via-AMR-based-GNN)
* [A Spark of Vision-Language Intelligence 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation](#A-Spark-of-Vision-Language-Intelligence-2-Dimensional-Autoregressive-Transformer-for-Efficient-Finegrained-Image-Generation)
* [Locret Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads](#Locret-Enhancing-Eviction-in-Long-Context-LLM-Inference-with-Trained-Retaining-Heads)
* [DreamGarden A Designer Assistant for Growing Games from a Single Prompt](#DreamGarden-A-Designer-Assistant-for-Growing-Games-from-a-Single-Prompt)
* [Open-RAG Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models](#Open-RAG-Enhanced-Retrieval-Augmented-Reasoning-with-Open-Source-Large-Language-Models)
* [DRUPI Dataset Reduction Using Privileged Information](#DRUPI-Dataset-Reduction-Using-Privileged-Information)
* [Attention layers provably solve single-location regression](#Attention-layers-provably-solve-single-location-regression)
* [A Little Goes a Long Way Efficient Long Context Training and Inference with Partial Contexts](#A-Little-Goes-a-Long-Way-Efficient-Long-Context-Training-and-Inference-with-Partial-Contexts)
* [SurgPointTransformer Vertebrae Shape Completion with RGB-D Data](#SurgPointTransformer-Vertebrae-Shape-Completion-with-RGB-D-Data)
* [FlashMask Efficient and Rich Mask Extension of FlashAttention](#FlashMask-Efficient-and-Rich-Mask-Extension-of-FlashAttention)
* [Takin-VC Zero-shot Voice Conversion via Jointly Hybrid Content and Memory-Augmented Context-Aware Timbre Modeling](#Takin-VC-Zero-shot-Voice-Conversion-via-Jointly-Hybrid-Content-and-Memory-Augmented-Context-Aware-Timbre-Modeling)
* [Getting Free Bits Back from Rotational Symmetries in LLMs](#Getting-Free-Bits-Back-from-Rotational-Symmetries-in-LLMs)
* [Speculative Coreset Selection for Task-Specific Fine-tuning](#Speculative-Coreset-Selection-for-Task-Specific-Fine-tuning)
* [Mitigating Copy Bias in In-Context Learning through Neuron Pruning](#Mitigating-Copy-Bias-in-In-Context-Learning-through-Neuron-Pruning)
* [Uncertainty-aware Human Mobility Modeling and Anomaly Detection](#Uncertainty-aware-Human-Mobility-Modeling-and-Anomaly-Detection)
* [Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models](#Sparse-Autoencoders-Reveal-Temporal-Difference-Learning-in-Large-Language-Models)
* [Flex3D Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation](#Flex3D-Feed-Forward-3D-Generation-With-Flexible-Reconstruction-Model-And-Input-View-Curation)
* [Sparse Attention Decomposition Applied to Circuit Tracing](#Sparse-Attention-Decomposition-Applied-to-Circuit-Tracing)
* [End-to-end Piano Performance-MIDI to Score Conversion with Transformers](#End-to-end-Piano-Performance-MIDI-to-Score-Conversion-with-Transformers)
* [EEG Emotion Copilot Pruning LLMs for Emotional EEG Interpretation with Assisted Medical Record Generation](#EEG-Emotion-Copilot-Pruning-LLMs-for-Emotional-EEG-Interpretation-with-Assisted-Medical-Record-Generation)
* [Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models](#Fisher-Information-based-Efficient-Curriculum-Federated-Learning-with-Large-Language-Models)
* [Near-horizon aspects of black holes in quantum spacetime](#Near-horizon-aspects-of-black-holes-in-quantum-spacetime)
* [COLLAGE Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models](#COLLAGE-Collaborative-Human-Agent-Interaction-Generation-using-Hierarchical-Latent-Diffusion-and-Language-Models)
* [PersonalLLM Tailoring LLMs to Individual Preferences](#PersonalLLM-Tailoring-LLMs-to-Individual-Preferences)
* [Aggressive Post-Training Compression on Extremely Large Language Models](#Aggressive-Post-Training-Compression-on-Extremely-Large-Language-Models)
* [InfantCryNet A Data-driven Framework for Intelligent Analysis of Infant Cries](#InfantCryNet-A-Data-driven-Framework-for-Intelligent-Analysis-of-Infant-Cries)
* [Alternating Maximization Algorithm for Mismatch Capacity with Oblivious Relaying](#Alternating-Maximization-Algorithm-for-Mismatch-Capacity-with-Oblivious-Relaying)
* [Learning Attentional Mixture of LoRAs for Language Model Continual Learning](#Learning-Attentional-Mixture-of-LoRAs-for-Language-Model-Continual-Learning)
* [One Token to Seg Them All Language Instructed Reasoning Segmentation in Videos](#One-Token-to-Seg-Them-All-Language-Instructed-Reasoning-Segmentation-in-Videos)
* [Abstractive Summarization of Low resourced Nepali language using Multilingual Transformers](#Abstractive-Summarization-of-Low-resourced-Nepali-language-using-Multilingual-Transformers)
* [Efficient Federated Intrusion Detection in 5G ecosystem using optimized BERT-based model](#Efficient-Federated-Intrusion-Detection-in-5G-ecosystem-using-optimized-BERT-based-model)
* [MambaEviScrib Mamba and Evidence-Guided Consistency Make CNN Work Robustly for Scribble-Based Weakly Supervised Ultrasound Image Segmentation](#MambaEviScrib-Mamba-and-Evidence-Guided-Consistency-Make-CNN-Work-Robustly-for-Scribble-Based-Weakly-Supervised-Ultrasound-Image-Segmentation)
* [DelayPTC-LLM Metro Passenger Travel Choice Prediction under Train Delays with Large Language Models](#DelayPTC-LLM-Metro-Passenger-Travel-Choice-Prediction-under-Train-Delays-with-Large-Language-Models)
* [CLIP-MoE Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](#CLIP-MoE-Towards-Building-Mixture-of-Experts-for-CLIP-with-Diversified-Multiplet-Upcycling)
* [A GEN AI Framework for Medical Note Generation](#A-GEN-AI-Framework-for-Medical-Note-Generation)
* [On the Power of Decision Trees in Auto-Regressive Language Modeling](#On-the-Power-of-Decision-Trees-in-Auto-Regressive-Language-Modeling)
* [Pruning then Reweighting Towards Data-Efficient Training of Diffusion Models](#Pruning-then-Reweighting-Towards-Data-Efficient-Training-of-Diffusion-Models)
* [Voxel-CIM An Efficient Compute-in-Memory Accelerator for Voxel-based Point Cloud Neural Networks](#Voxel-CIM-An-Efficient-Compute-in-Memory-Accelerator-for-Voxel-based-Point-Cloud-Neural-Networks)
* [Localizing Memorization in SSL Vision Encoders](#Localizing-Memorization-in-SSL-Vision-Encoders)
* [Exploring Token Pruning in Vision State Space Models](#Exploring-Token-Pruning-in-Vision-State-Space-Models)
* [Mitigating Selection Bias with Node Pruning and Auxiliary Options](#Mitigating-Selection-Bias-with-Node-Pruning-and-Auxiliary-Options)
* [Asymptotic tracking control of dynamic reference over homomorphically encrypted data with finite modulus](#Asymptotic-tracking-control-of-dynamic-reference-over-homomorphically-encrypted-data-with-finite-modulus)
* [Sparse Signal Recovery via $L_1/L_2$ Minimization Bound Theory and Complexity](#Sparse-Signal-Recovery-via-$L_1/L_2$-Minimization-Bound-Theory-and-Complexity)
* [Token Caching for Diffusion Transformer Acceleration](#Token-Caching-for-Diffusion-Transformer-Acceleration)


## PrefixQuant Static Quantization Beats Dynamic through Prefixed Outliers in LLMs

>Authors: Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo

>2024-10-07

> http://arxiv.org/abs/2410.05265v1

Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.


## Differential Transformer

>Authors: Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei

>2024-10-07

> http://arxiv.org/abs/2410.05258v1

Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.


## Variable Resolution Pixel Quantization for Low Power Machine Vision Application on Edge

>Authors: Senorita Deb, Sai Sanjeet, Prabir Kumar Biswas, Bibhu Datta Sahoo

>2024-10-07

> http://arxiv.org/abs/2410.05189v1

This work describes an approach towards pixel quantization using variable
resolution which is made feasible using image transformation in the analog
domain. The main aim is to reduce the average bits-per-pixel (BPP) necessary
for representing an image while maintaining the classification accuracy of a
Convolutional Neural Network (CNN) that is trained for image classification.
The proposed algorithm is based on the Hadamard transform that leads to a
low-resolution variable quantization by the analog-to-digital converter (ADC)
thus reducing the power dissipation in hardware at the sensor node. Despite the
trade-offs inherent in image transformation, the proposed algorithm achieves
competitive accuracy levels across various image sizes and ADC configurations,
highlighting the importance of considering both accuracy and power consumption
in edge computing applications. The schematic of a novel 1.5 bit ADC that
incorporates the Hadamard transform is also proposed. A hardware implementation
of the analog transformation followed by software-based variable quantization
is done for the CIFAR-10 test dataset. The digitized data shows that the
network can still identify transformed images with a remarkable 90% accuracy
for 3-BPP transformed images following the proposed method.


## Union Bound Analysis for Spin-Torque Transfer Magnetic Random Access Memory (STT-MRAM) With Channel Quantization

>Authors: Xingwei Zhong, Kui Cai, Guanghui Song

>2024-10-07

> http://arxiv.org/abs/2410.05164v1

As an emerging non-volatile memory (NVM) technology, spin-torque transfer
magnetic random access memory (STT-MRAM) has received great attention in recent
years since it combines the features of low switching energy, fast write/read
speed, and high scalability. However, process variation and thermal fluctuation
severely affect the data integrity of STT-MRAM, resulting in both write errors
and read errors. Therefore, effective error correction codes (ECCs) are
necessary for correcting memory cell errors. Meanwhile, the design of channel
quantizer plays a critical role in supporting error correction coding for
STT-MRAM. In this work, we propose a union bound analysis which can accurately
predict the word error rates (WERs) of ECCs with maximum-likelihood (ML)
decoding over the quantized STT-MRAM channel. The derived bound provides a
theoretical tool for comparing the performance of ECCs with different
quantization schemes at very low error rate levels without resorting to lengthy
computer simulations. Moreover, we also propose a new criterion to design the
channel quantizer by minimizing the WERs of ECC decoding that are obtained from
the union bound analysis. Numerical results show that the proposed
union-bound-optimized (UBO) quantizer can achieve better error rate performance
than the state-of-art quantizers for STT-MRAM.


## TidalDecode Fast and Accurate LLM Decoding with Position Persistent Sparse Attention

>Authors: Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia

>2024-10-07

> http://arxiv.org/abs/2410.05076v1

Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.


## Mastering Chinese Chess AI (Xiangqi) Without Search

>Authors: Yu Chen, Juntong Lin, Zhichao Shu

>2024-10-07

> http://arxiv.org/abs/2410.04865v1

We have developed a high-performance Chinese Chess AI that operates without
reliance on search algorithms. This AI has demonstrated the capability to
compete at a level commensurate with the top 0.1\% of human players. By
eliminating the search process typically associated with such systems, this AI
achieves a Queries Per Second (QPS) rate that exceeds those of systems based on
the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and
surpasses those based on the AlphaBeta pruning algorithm by more than a
hundredfold. The AI training system consists of two parts: supervised learning
and reinforcement learning. Supervised learning provides an initial human-like
Chinese chess AI, while reinforcement learning, based on supervised learning,
elevates the strength of the entire AI to a new level. Based on this training
system, we carried out enough ablation experiments and discovered that 1. The
same parameter amount of Transformer architecture has a higher performance than
CNN on Chinese chess; 2. Possible moves of both sides as features can greatly
improve the training process; 3. Selective opponent pool, compared to pure
self-play training, results in a faster improvement curve and a higher strength
limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO
algorithm training process and we will give the explanation.


## Towards Measuring Goal-Directedness in AI Systems

>Authors: Dylan Xu, Juan-Pablo Rivera

>2024-10-07

> http://arxiv.org/abs/2410.04683v1

Recent advances in deep learning have brought attention to the possibility of
creating advanced, general AI systems that outperform humans across many tasks.
However, if these systems pursue unintended goals, there could be catastrophic
consequences. A key prerequisite for AI systems pursuing unintended goals is
whether they will behave in a coherent and goal-directed manner in the first
place, optimizing for some unknown goal; there exists significant research
trying to evaluate systems for said behaviors. However, the most rigorous
definitions of goal-directedness we currently have are difficult to compute in
real-world settings. Drawing upon this previous literature, we explore policy
goal-directedness within reinforcement learning (RL) environments. In our
findings, we propose a different family of definitions of the goal-directedness
of a policy that analyze whether it is well-modeled as near-optimal for many
(sparse) reward functions. We operationalize this preliminary definition of
goal-directedness and test it in toy Markov decision process (MDP)
environments. Furthermore, we explore how goal-directedness could be measured
in frontier large-language models (LLMs). Our contribution is a definition of
goal-directedness that is simpler and more easily computable in order to
approach the question of whether AI systems could pursue dangerous goals. We
recommend further exploration of measuring coherence and goal-directedness,
based on our findings.


## Deciphering Refactoring Branch Dynamics in Modern Code Review An Empirical Study on Qt

>Authors: Eman Abdullah AlOmar

>2024-10-07

> http://arxiv.org/abs/2410.04678v1

Context: Modern code review is a widely employed technique in both industrial
and open-source projects, serving to enhance software quality, share knowledge,
and ensure compliance with coding standards and guidelines. While code review
is extensively studied for its general challenges, best practices, outcomes,
and socio-technical aspects, little attention has been paid to how refactoring
is reviewed and what developers prioritize when reviewing refactored code in
the Refactor branch. Objective: The goal is to understand the review process
for refactoring changes in the Refactor branch and to identify what developers
care about when reviewing code in this branch. Method: In this study, we
present a quantitative and qualitative examination to understand the main
criteria developers use to decide whether to accept or reject refactored code
submissions and identify the challenges inherent in this process. Results:
Analyzing 2,154 refactoring and non-refactoring reviews across Qt open-source
projects, we find that reviews involving refactoring from the Refactor branch
take significantly less time to resolve in terms of code review efforts.
Additionally, documentation of developer intent is notably sparse within the
Refactor branch compared to other branches. Furthermore, through thematic
analysis of a substantial sample of refactoring code review discussions, we
construct a comprehensive taxonomy consisting of 12 refactoring review
criteria.


## Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval

>Authors: Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha Kass-Hout, Jimeng Sun, Jiawei Han

>2024-10-06

> http://arxiv.org/abs/2410.04585v1

Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.


## EnsemW2S Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?

>Authors: Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, John Langford, Furong Huang

>2024-10-06

> http://arxiv.org/abs/2410.04571v1

How can we harness the collective capabilities of multiple Large Language
Models (LLMs) to create an even more powerful model? This question forms the
foundation of our research, where we propose an innovative approach to
weak-to-strong (w2s) generalization-a critical problem in AI alignment. Our
work introduces an easy-to-hard (e2h) framework for studying the feasibility of
w2s generalization, where weak models trained on simpler tasks collaboratively
supervise stronger models on more complex tasks. This setup mirrors real-world
challenges, where direct human supervision is limited. To achieve this, we
develop a novel AdaBoost-inspired ensemble method, demonstrating that an
ensemble of weak supervisors can enhance the performance of stronger LLMs
across classification and generative tasks on difficult QA datasets. In several
cases, our ensemble approach matches the performance of models trained on
ground-truth data, establishing a new benchmark for w2s generalization. We
observe an improvement of up to 14% over existing baselines and average
improvements of 5% and 4% for binary classification and generative tasks,
respectively. This research points to a promising direction for enhancing AI
through collective supervision, especially in scenarios where labeled data is
sparse or insufficient.


## SparseVLM Visual Token Sparsification for Efficient Vision-Language Model Inference

>Authors: Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang

>2024-10-06

> http://arxiv.org/abs/2410.04417v1

In vision-language models (VLMs), visual tokens usually consume a significant
amount of computational overhead, despite their sparser information density
compared to text tokens. To address this, most existing methods learn a network
to prune redundant visual tokens and require additional training data.
Differently, we propose an efficient training-free token optimization mechanism
dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,
given that visual tokens complement text tokens in VLMs for linguistic
reasoning, we select visual-relevant text tokens to rate the significance of
vision tokens within the self-attention matrix extracted from the VLMs. Then we
progressively prune irrelevant tokens. To maximize sparsity while retaining
essential information, we introduce a rank-based strategy to adaptively
determine the sparsification ratio for each layer, alongside a token recycling
method that compresses pruned tokens into more compact representations.
Experimental results show that our SparseVLM improves the efficiency of various
VLMs across a range of image and video understanding tasks. In particular,
LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio
of 78% while maintaining 93% of the accuracy. Our code is available at
https://github.com/Gumpest/SparseVLMs.


## HALL-E Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis

>Authors: Yuto Nishimura, Takumi Hirose, Masanari Ohi, Hideki Nakayama, Nakamasa Inoue

>2024-10-06

> http://arxiv.org/abs/2410.04380v1

Recently, Text-to-speech (TTS) models based on large language models (LLMs)
that translate natural language text into sequences of discrete audio tokens
have gained great research attention, with advances in neural audio codec (NAC)
models using residual vector quantization (RVQ). However, long-form speech
synthesis remains a significant challenge due to the high frame rate, which
increases the length of audio tokens and makes it difficult for autoregressive
language models to generate audio tokens for even a minute of speech. To
address this challenge, this paper introduces two novel post-training
approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a
framework to reduce the frame rate of pre-trained NAC models. Specifically, it
incorporates multi-resolution residual vector quantization (MRVQ) module that
hierarchically reorganizes discrete audio tokens through teacher-student
distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical
tokens of MReQ. Specifically, it incorporates the technique of using MRVQ
sub-modules and continues training from a pre-trained LLM-based TTS model.
Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark
dataset consisting of 40k hours of filtered speech data for training and
evaluating speech synthesis ranging from 3s up to 180s. In experiments, we
demonstrated the effectiveness of our approaches by applying our post-training
framework to VALL-E. We achieved the frame rate down to as low as 8 Hz,
enabling the stable minitue-long speech synthesis in a single inference step.
Audio samples, dataset, codes and pre-trained models are available at
https://yutonishimura-v2.github.io/HALL-E_DEMO/.


## Accelerating Inference of Networks in the Frequency Domain

>Authors: Chenqiu Zhao, Guanfang Dong, Anup Basu

>2024-10-06

> http://arxiv.org/abs/2410.04342v1

It has been demonstrated that networks' parameters can be significantly
reduced in the frequency domain with a very small decrease in accuracy.
However, given the cost of frequency transforms, the computational complexity
is not significantly decreased. In this work, we propose performing network
inference in the frequency domain to speed up networks whose frequency
parameters are sparse. In particular, we propose a frequency inference chain
that is dual to the network inference in the spatial domain. In order to handle
the non-linear layers, we make a compromise to apply non-linear operations on
frequency data directly, which works effectively. Enabled by the frequency
inference chain and the strategy for non-linear layers, the proposed approach
completes the entire inference in the frequency domain. Unlike previous
approaches which require extra frequency or inverse transforms for all layers,
the proposed approach only needs the frequency transform and its inverse once
at the beginning and once at the end of a network. Comparisons with
state-of-the-art methods demonstrate that the proposed approach significantly
improves accuracy in the case of a high speedup ratio (over 100x). The source
code is available at \url{https://github.com/guanfangdong/FreqNet-Infer}.


## Language Model-Driven Data Pruning Enables Efficient Active Learning

>Authors: Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza

>2024-10-05

> http://arxiv.org/abs/2410.04275v1

Active learning (AL) optimizes data labeling efficiency by selecting the most
informative instances for annotation. A key component in this procedure is an
acquisition function that guides the selection process and identifies the
suitable instances for labeling from the unlabeled pool. However, these
acquisition methods suffer from high computational costs with large unlabeled
data pools, posing a roadblock to their applicability on large datasets. To
address this challenge and bridge this gap, we introduce a novel plug-and-play
unlabeled data pruning strategy, ActivePrune, which leverages language models
to prune the unlabeled pool. ActivePrune implements a two-stage pruning
process: an initial fast evaluation using perplexity scores from an n-gram
language model, followed by a high-quality selection using metrics for data
quality computed through a quantized LLM. Additionally, to enhance the
diversity in the unlabeled pool, we propose a novel perplexity reweighting
method that systematically brings forward underrepresented instances for
selection in subsequent labeling iterations. Experiments on translation,
sentiment analysis, topic classification, and summarization tasks on four
diverse datasets and four active learning strategies demonstrate that
ActivePrune outperforms existing data pruning methods. Finally, we compare the
selection quality $\leftrightarrow$ efficiency tradeoff of the data pruning
methods and demonstrate that ActivePrune is computationally more efficient than
other LLM score-based pruning methods, and provides up to 74% reduction in the
end-to-end time required for active learning.


## RoQLlama A Lightweight Romanian Adapted Language Model

>Authors: George-Andrei Dima, Andrei-Marius Avram, Cristian-George Crăciun, Dumitru-Clementin Cercel

>2024-10-05

> http://arxiv.org/abs/2410.04269v1

The remarkable achievements obtained by open-source large language models
(LLMs) in recent years have predominantly been concentrated on tasks involving
the English language. In this paper, we aim to advance the performance of
Llama2 models on Romanian tasks. We tackle the problem of reduced computing
resources by using QLoRA for training. We release RoQLlama-7b, a quantized LLM,
which shows equal or improved results compared to its full-sized counterpart
when tested on seven Romanian downstream tasks in the zero-shot setup. Also, it
consistently achieves higher average scores across all few-shot prompts.
Additionally, we introduce a novel Romanian dataset, namely RoMedQA, which
contains single-choice medical questions in Romanian.


## Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension

>Authors: Ning Wang, Zekun Li, Tongxin Bai, Guoqi Li

>2024-10-05

> http://arxiv.org/abs/2410.04211v1

Modeling long sequences is crucial for various large-scale models; however,
extending existing architectures to handle longer sequences presents
significant technical and resource challenges. In this paper, we propose an
efficient and flexible attention architecture that enables the extension of
context lengths in large language models with reduced computational resources
and fine-tuning time compared to other excellent methods. Specifically, we
introduce correlation-aware selection and merging mechanisms to facilitate
efficient sparse attention. In addition, we also propose a novel data
augmentation technique involving positional encodings to enhance generalization
to unseen positions. The results are as follows: First, using a single A100, we
achieve fine-tuning on Llama2-7B with a sequence length of 32K, which is more
efficient than other methods that rely on subsets for regression. Second, we
present a comprehensive method for extending context lengths across the
pre-training, fine-tuning, and inference phases. During pre-training, our
attention mechanism partially breaks translation invariance during token
selection, so we apply positional encodings only to the selected tokens. This
approach achieves relatively high performance and significant extrapolation
capabilities. For fine-tuning, we introduce Cyclic, Randomly Truncated, and
Dynamically Growing NTK Positional Embedding (CRD NTK). This design allows
fine-tuning with a sequence length of only 16K, enabling models such as
Llama2-7B and Mistral-7B to perform inference with context lengths of up to 1M
or even arbitrary lengths. Our method achieves 100\% accuracy on the passkey
task with a context length of 4M and maintains stable perplexity at a 1M
context length. This represents at least a 64-fold reduction in resource
requirements compared to traditional full-attention mechanisms, while still
achieving competitive performance.


## Equivariant Neural Functional Networks for Transformers

>Authors: Viet-Hoang Tran, Thieu N. Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, Tan Minh Nguyen

>2024-10-05

> http://arxiv.org/abs/2410.04209v1

This paper systematically explores neural functional networks (NFN) for
transformer architectures. NFN are specialized neural networks that treat the
weights, gradients, or sparsity patterns of a deep neural network (DNN) as
input data and have proven valuable for tasks such as learnable optimizers,
implicit data representations, and weight editing. While NFN have been
extensively developed for MLP and CNN, no prior work has addressed their design
for transformers, despite the importance of transformers in modern deep
learning. This paper aims to address this gap by providing a systematic study
of NFN for transformers. We first determine the maximal symmetric group of the
weights in a multi-head attention module as well as a necessary and sufficient
condition under which two sets of hyperparameters of the multi-head attention
module define the same function. We then define the weight space of transformer
architectures and its associated group action, which leads to the design
principles for NFN in transformers. Based on these, we introduce
Transformer-NFN, an NFN that is equivariant under this group action.
Additionally, we release a dataset of more than 125,000 Transformers model
checkpoints trained on two datasets with two different tasks, providing a
benchmark for evaluating Transformer-NFN and encouraging further research on
transformer training and performance.


## Toxic Subword Pruning for Dialogue Response Generation on Large Language Models

>Authors: Hongyuan Lu, Wai Lam

>2024-10-05

> http://arxiv.org/abs/2410.04155v1

How to defend large language models (LLMs) from generating toxic content is
an important research area. Yet, most research focused on various model
training techniques to remediate LLMs by updating their weights. A typical
related research area is safety alignment. This however is often costly and
tedious and can expose the model to even more problems such as catastrophic
forgetting if the trainings are not carefully handled by experienced NLP
practitioners. We thus propose a simple yet effective and novel algorithm,
namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword
contained by the toxic words from BPE in trained LLMs. In contrast to the
previous work that demonstrates pruning BPE tokens as harmful to the task of
machine translation, we surprisingly found its usefulness in preventing toxic
content from being generated on LLMs. Fortunately, our findings suggest that
ToxPrune simultaneously improves the toxic language model NSFW-3B on the task
of dialogue response generation obviously. We surprisingly found that ToxPrune
can even obviously improve official Llama-3.1-6B in the metric of dialogue
diversity. Extensive automatic results and human evaluation indicate that
ToxPrune could be helpful for both remediating toxic LLMs and improving
non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to
release the resources to facilitate future work.}


## LLM-TOPLA Efficient LLM Ensemble by Maximising Diversity

>Authors: Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ling Liu

>2024-10-04

> http://arxiv.org/abs/2410.03953v1

Combining large language models during training or at inference time has
shown substantial performance gain over component LLMs. This paper presents
LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique
properties: (i) We introduce the focal diversity metric to capture the
diversity-performance correlation among component LLMs of an ensemble. (ii) We
develop a diversity-optimized ensemble pruning algorithm to select the top-k
sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends
top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii)
We generate new output for each prompt query by utilizing a learn-to-ensemble
approach, which learns to detect and resolve the output inconsistency among all
component LLMs of an ensemble. Extensive evaluation on four different
benchmarks shows good performance gain over the best LLM ensemble methods: (i)
In constrained solution set problems, LLM-TOPLA outperforms the best-performing
ensemble (Mixtral) by 2.2\% in accuracy on MMLU and the best-performing LLM
ensemble (MoreAgent) on GSM8k by 2.1\%. (ii) In generative tasks, LLM-TOPLA
outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by
$3.9\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and
dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available
at https://github.com/git-disl/llm-topla


## Generative Semantic Communication for Text-to-Speech Synthesis

>Authors: Jiahao Zheng, Jinke Ren, Peng Xu, Zhihao Yuan, Jie Xu, Fangxin Wang, Gui Gui, Shuguang Cui

>2024-10-04

> http://arxiv.org/abs/2410.03459v1

Semantic communication is a promising technology to improve communication
efficiency by transmitting only the semantic information of the source data.
However, traditional semantic communication methods primarily focus on data
reconstruction tasks, which may not be efficient for emerging generative tasks
such as text-to-speech (TTS) synthesis. To address this limitation, this paper
develops a novel generative semantic communication framework for TTS synthesis,
leveraging generative artificial intelligence technologies. Firstly, we utilize
a pre-trained large speech model called WavLM and the residual vector
quantization method to construct two semantic knowledge bases (KBs) at the
transmitter and receiver, respectively. The KB at the transmitter enables
effective semantic extraction, while the KB at the receiver facilitates
lifelike speech synthesis. Then, we employ a transformer encoder and a
diffusion model to achieve efficient semantic coding without introducing
significant communication overhead. Finally, numerical results demonstrate that
our framework achieves much higher fidelity for the generated speech than four
baselines, in both cases with additive white Gaussian noise channel and
Rayleigh fading channel.


## Exploring the Benefit of Activation Sparsity in Pre-training

>Authors: Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou

>2024-10-04

> http://arxiv.org/abs/2410.03440v1

Pre-trained Transformers inherently possess the characteristic of sparse
activation, where only a small fraction of the neurons are activated for each
token. While sparse activation has been explored through post-training methods,
its potential in pre-training remains untapped. In this work, we first study
how activation properties change during pre-training. Our examination reveals
that Transformers exhibit sparse activation throughout the majority of the
pre-training process while the activation correlation keeps evolving as
training progresses. Leveraging this observation, we propose Switchable
Sparse-Dense Learning (SSD). SSD adaptively switches between the
Mixtures-of-Experts (MoE) based sparse training and the conventional dense
training during the pre-training process, leveraging the efficiency of sparse
training and avoiding the static activation correlation of sparse training.
Compared to dense training, SSD achieves comparable performance with identical
model size and reduces pre-training costs. Moreover, the models trained with
SSD can be directly used as MoE models for sparse inference and achieve the
same performance as dense models with up to $2\times$ faster inference speed.
Codes are available at https://github.com/thunlp/moefication.


## Predictive Coding for Decision Transformer

>Authors: Tung M. Luu, Donghoon Lee, Chang D. Yoo

>2024-10-04

> http://arxiv.org/abs/2410.03408v1

Recent work in offline reinforcement learning (RL) has demonstrated the
effectiveness of formulating decision-making as return-conditioned supervised
learning. Notably, the decision transformer (DT) architecture has shown promise
across various domains. However, despite its initial success, DTs have
underperformed on several challenging datasets in goal-conditioned RL. This
limitation stems from the inefficiency of return conditioning for guiding
policy learning, particularly in unstructured and suboptimal datasets,
resulting in DTs failing to effectively learn temporal compositionality.
Moreover, this problem might be further exacerbated in long-horizon
sparse-reward tasks. To address this challenge, we propose the Predictive
Coding for Decision Transformer (PCDT) framework, which leverages generalized
future conditioning to enhance DT methods. PCDT utilizes an architecture that
extends the DT framework, conditioned on predictive codings, enabling
decision-making based on both past and future factors, thereby improving
generalization. Through extensive experiments on eight datasets from the
AntMaze and FrankaKitchen environments, our proposed method achieves
performance on par with or surpassing existing popular value-based and
transformer-based methods in offline goal-conditioned RL. Furthermore, we also
evaluate our method on a goal-reaching task with a physical robot.


## Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization

>Authors: Tung M. Luu, Thanh Nguyen, Tee Joshua Tian Jin, Sungwoon Kim, Chang D. Yoo

>2024-10-04

> http://arxiv.org/abs/2410.03376v1

Recent studies reveal that well-performing reinforcement learning (RL) agents
in training often lack resilience against adversarial perturbations during
deployment. This highlights the importance of building a robust agent before
deploying it in the real world. Most prior works focus on developing robust
training-based procedures to tackle this problem, including enhancing the
robustness of the deep neural network component itself or adversarially
training the agent on strong attacks. In this work, we instead study an input
transformation-based defense for RL. Specifically, we propose using a variant
of vector quantization (VQ) as a transformation for input observations, which
is then used to reduce the space of adversarial attacks during testing,
resulting in the transformed observations being less affected by attacks. Our
method is computationally efficient and seamlessly integrates with adversarial
training, further enhancing the robustness of RL agents against adversarial
attacks. Through extensive experiments in multiple environments, we demonstrate
that using VQ as the input transformation effectively defends against
adversarial attacks on the agent's observations.


## Error Correction Code Transformer From Non-Unified to Unified

>Authors: Yongli Yan, Jieao Zhu, Tianyue Zheng, Jiaqi He, Linglong Dai

>2024-10-04

> http://arxiv.org/abs/2410.03364v1

Channel coding is vital for reliable data transmission in modern wireless
systems, and its significance will increase with the emergence of
sixth-generation (6G) networks, which will need to support various error
correction codes. However, traditional decoders were typically designed as
fixed hardware circuits tailored to specific decoding algorithms, leading to
inefficiencies and limited flexibility. To address these challenges, this paper
proposes a unified, code-agnostic Transformer-based decoding architecture
capable of handling multiple linear block codes, including Polar, Low-Density
Parity-Check (LDPC), and Bose-Chaudhuri-Hocquenghem (BCH), within a single
framework. To achieve this, standardized units are employed to harmonize
parameters across different code types, while the redesigned unified attention
module compresses the structural information of various codewords.
Additionally, a sparse mask, derived from the sparsity of the parity-check
matrix, is introduced to enhance the model's ability to capture inherent
constraints between information and parity-check bits, resulting in improved
decoding accuracy and robustness. Extensive experimental results demonstrate
that the proposed unified Transformer-based decoder not only outperforms
existing methods but also provides a flexible, efficient, and high-performance
solution for next-generation wireless communication systems.


## An X-Ray Is Worth 15 Features Sparse Autoencoders for Interpretable Radiology Report Generation

>Authors: Ahmed Abdulaal, Hugo Fry, Nina Montaña-Brown, Ayodeji Ijishakin, Jack Gao, Stephanie Hyland, Daniel C. Alexander, Daniel C. Castro

>2024-10-04

> http://arxiv.org/abs/2410.03334v1

Radiological services are experiencing unprecedented demand, leading to
increased interest in automating radiology report generation. Existing
Vision-Language Models (VLMs) suffer from hallucinations, lack
interpretability, and require expensive fine-tuning. We introduce SAE-Rad,
which uses sparse autoencoders (SAEs) to decompose latent representations from
a pre-trained vision transformer into human-interpretable features. Our hybrid
architecture combines state-of-the-art SAE advancements, achieving accurate
latent reconstructions while maintaining sparsity. Using an off-the-shelf
language model, we distil ground-truth reports into radiological descriptions
for each SAE feature, which we then compile into a full report for each image,
eliminating the need for fine-tuning large models for this task. To the best of
our knowledge, SAE-Rad represents the first instance of using mechanistic
interpretability techniques explicitly for a downstream multi-modal reasoning
task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific
metrics compared to state-of-the-art models while using significantly fewer
computational resources for training. Qualitative analysis reveals that SAE-Rad
learns meaningful visual concepts and generates reports aligning closely with
expert interpretations. Our results suggest that SAEs can enhance multimodal
reasoning in healthcare, providing a more interpretable alternative to existing
VLMs.


## Strict quantization for compact pseudo-Kähler manifolds and group actions

>Authors: Andrea Galasso

>2024-10-04

> http://arxiv.org/abs/2410.03322v1

The asymptotic results for Berezin-Toeplitz operators yield a strict
quantization for the algebra of smooth functions on a given Hodge manifold. It
seems natural to generalize this picture for quantizable pseudo-K\"ahler
manifolds in presence of a group action. Thus, in this setting we introduce a
Berezin transform which has a complete asymptotic expansion on the preimage of
the zero set of the moment map. It leads in a natural way to prove that certain
quantization maps are strict.


## Resource-aware Mixed-precision Quantization for Enhancing Deployability of Transformers for Time-series Forecasting on Embedded FPGAs

>Authors: Tianheng Ling, Chao Qian, Gregor Schiele

>2024-10-04

> http://arxiv.org/abs/2410.03294v1

This study addresses the deployment challenges of integer-only quantized
Transformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15).
We enhanced the flexibility of our VHDL template by introducing a selectable
resource type for storing intermediate results across model layers, thereby
breaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we
developed a resource-aware mixed-precision quantization approach that enables
researchers to explore hardware-level quantization strategies without requiring
extensive expertise in Neural Architecture Search. This method provides
accurate resource utilization estimates with a precision discrepancy as low as
3%, compared to actual deployment metrics. Compared to previous work, our
approach has successfully facilitated the deployment of model configurations
utilizing mixed-precision quantization, thus overcoming the limitations
inherent in five previously non-deployable configurations with uniform
quantization bitwidths. Consequently, this research enhances the applicability
of Transformers in embedded systems, facilitating a broader range of
Transformer-powered applications on edge devices.


## ALR$^2$ A Retrieve-then-Reason Framework for Long-context Question Answering

>Authors: Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang, Vijay Viswanathan, Patrick Lewis, Taro Watanabe, Yixuan Su

>2024-10-04

> http://arxiv.org/abs/2410.03227v1

The context window of large language models (LLMs) has been extended
significantly in recent years. However, while the context length that the LLM
can process has grown, the capability of the model to accurately reason over
that context degrades noticeably. This occurs because modern LLMs often become
overwhelmed by the vast amount of information in the context; when answering
questions, the model must identify and reason over relevant evidence sparsely
distributed throughout the text. To alleviate the challenge of long-context
reasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason
over relevant evidence collected during an intermediate retrieval step. We find
that modern LLMs struggle to accurately retrieve relevant facts and instead,
often hallucinate "retrieved facts", resulting in flawed reasoning and the
production of incorrect answers. To address these issues, we introduce ALR$^2$,
a method that augments the long-context reasoning capability of LLMs via an
explicit two-stage procedure, i.e., aligning LLMs with the objectives of both
retrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating
performance degradation in long-context reasoning tasks. Through extensive
experiments on long-context QA benchmarks, we find our method to outperform
competitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains
on the long-context versions of HotpotQA and SQuAD datasets, respectively.


## EXAQ Exponent Aware Quantization For LLMs Acceleration

>Authors: Moran Shkolnik, Maxim Fishman, Brian Chmiel, Hilla Ben-Yaacov, Ron Banner, Kfir Yehuda Levy

>2024-10-04

> http://arxiv.org/abs/2410.03185v1

Quantization has established itself as the primary approach for decreasing
the computational and storage expenses associated with Large Language Models
(LLMs) inference. The majority of current research emphasizes quantizing
weights and activations to enable low-bit general-matrix-multiply (GEMM)
operations, with the remaining non-linear operations executed at higher
precision. In our study, we discovered that following the application of these
techniques, the primary bottleneck in LLMs inference lies in the softmax layer.
The softmax operation comprises three phases: exponent calculation,
accumulation, and normalization, Our work focuses on optimizing the first two
phases. We propose an analytical approach to determine the optimal clipping
value for the input to the softmax function, enabling sub-4-bit quantization
for LLMs inference. This method accelerates the calculations of both $e^x$ and
$\sum(e^x)$ with minimal to no accuracy degradation. For example, in
LLaMA1-30B, we achieve baseline performance with 2-bit quantization on the
well-known "Physical Interaction: Question Answering" (PIQA) dataset
evaluation. This ultra-low bit quantization allows, for the first time, an
acceleration of approximately 4x in the accumulation phase. The combination of
accelerating both $e^x$ and $\sum(e^x)$ results in a 36.9% acceleration in the
softmax operation.


## ARB-LLM Alternating Refined Binarizations for Large Language Models

>Authors: Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, zhongchao shi, Linghe Kong, Yulun Zhang, Xiaokang Yang

>2024-10-04

> http://arxiv.org/abs/2410.03129v1

Large Language Models (LLMs) have greatly pushed forward advancements in
natural language processing, yet their high memory and computational demands
hinder practical deployment. Binarization, as an effective compression
technique, can shrink model weights to just 1 bit, significantly reducing the
high demands on computation and memory. However, current binarization methods
struggle to narrow the distribution gap between binarized and full-precision
weights, while also overlooking the column deviation in LLM weight
distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit
post-training quantization (PTQ) technique tailored for LLMs. To narrow the
distribution shift between binarized and full-precision weights, we first
design an alternating refined binarization (ARB) algorithm to progressively
update the binarization parameters, which significantly reduces the
quantization error. Moreover, considering the pivot role of calibration data
and the column deviation in LLM weights, we further extend ARB to ARB-X and
ARB-RC. In addition, we refine the weight partition strategy with column-group
bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC
with CGB, we obtain ARB-LLM$_\text{X}$ and ARB-LLM$_\text{RC}$ respectively,
which significantly outperform state-of-the-art (SOTA) binarization methods for
LLMs. As a binary PTQ method, our ARB-LLM$_\text{RC}$ is the first to surpass
FP16 models of the same size. The code and models will be available at
https://github.com/ZHITENGLI/ARB-LLM.


## Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs

>Authors: Pritom Saha Akash, Kevin Chen-Chuan Chang

>2024-10-04

> http://arxiv.org/abs/2410.03071v1

Topic modeling is a powerful technique for uncovering hidden themes within a
collection of documents. However, the effectiveness of traditional topic models
often relies on sufficient word co-occurrence, which is lacking in short texts.
Therefore, existing approaches, whether probabilistic or neural, frequently
struggle to extract meaningful patterns from such data, resulting in incoherent
topics. To address this challenge, we propose a novel approach that leverages
large language models (LLMs) to extend short texts into more detailed sequences
before applying topic modeling. To further improve the efficiency and solve the
problem of semantic inconsistency from LLM-generated texts, we propose to use
prefix tuning to train a smaller language model coupled with a variational
autoencoder for short-text topic modeling. Our method significantly improves
short-text topic modeling performance, as demonstrated by extensive experiments
on real-world datasets with extreme data sparsity, outperforming current
state-of-the-art topic models.


## Adaptive Inference-Time Compute LLMs Can Predict if They Can Do Better, Even Mid-Generation

>Authors: Rohin Manvi, Anikait Singh, Stefano Ermon

>2024-10-03

> http://arxiv.org/abs/2410.02725v1

Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.


## Deconstructing Recurrence, Attention, and Gating Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems

>Authors: Hunter S. Heidenreich, Pantelis R. Vlachas, Petros Koumoutsakos

>2024-10-03

> http://arxiv.org/abs/2410.02654v1

Machine learning architectures, including transformers and recurrent neural
networks (RNNs) have revolutionized forecasting in applications ranging from
text processing to extreme weather. Notably, advanced network architectures,
tuned for applications such as natural language processing, are transferable to
other tasks such as spatiotemporal forecasting tasks. However, there is a
scarcity of ablation studies to illustrate the key components that enable this
forecasting accuracy. The absence of such studies, although explainable due to
the associated computational cost, intensifies the belief that these models
ought to be considered as black boxes. In this work, we decompose the key
architectural components of the most powerful neural architectures, namely
gating and recurrence in RNNs, and attention mechanisms in transformers. Then,
we synthesize and build novel hybrid architectures from the standard blocks,
performing ablation studies to identify which mechanisms are effective for each
task. The importance of considering these components as hyper-parameters that
can augment the standard architectures is exhibited on various forecasting
datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96
system, the Kuramoto-Sivashinsky equation, as well as standard real world
time-series benchmarks. A key finding is that neural gating and attention
improves the performance of all standard RNNs in most tasks, while the addition
of a notion of recurrence in transformers is detrimental. Furthermore, our
study reveals that a novel, sparsely used, architecture which integrates
Recurrent Highway Networks with neural gating and attention mechanisms, emerges
as the best performing architecture in high-dimensional spatiotemporal
forecasting of dynamical systems.


## Cut the Crap An Economical Communication Pipeline for LLM-based Multi-Agent Systems

>Authors: Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, Tianlong Chen

>2024-10-03

> http://arxiv.org/abs/2410.02506v1

Recent advancements in large language model (LLM)-powered agents have shown
that collective intelligence can significantly outperform individual
capabilities, largely attributed to the meticulously designed inter-agent
communication topologies. Though impressive in performance, existing
multi-agent pipelines inherently introduce substantial token overhead, as well
as increased economic costs, which pose challenges for their large-scale
deployments. In response to this challenge, we propose an economical, simple,
and robust multi-agent communication framework, termed $\texttt{AgentPrune}$,
which can seamlessly integrate into mainstream multi-agent systems and prunes
redundant or even malicious communication messages. Technically,
$\texttt{AgentPrune}$ is the first to identify and formally define the
\textit{communication redundancy} issue present in current LLM-based
multi-agent pipelines, and efficiently performs one-shot pruning on the
spatial-temporal message-passing graph, yielding a token-economic and
high-performing communication topology. Extensive experiments across six
benchmarks demonstrate that $\texttt{AgentPrune}$ \textbf{(I)} achieves
comparable results as state-of-the-art topologies at merely $\$5.6$ cost
compared to their $\$43.7$, \textbf{(II)} integrates seamlessly into existing
multi-agent frameworks with $28.1\%\sim72.8\%\downarrow$ token reduction, and
\textbf{(III)} successfully defend against two types of agent-based adversarial
attacks with $3.5\%\sim10.8\%\uparrow$ performance boost.


## Overcoming Representation Bias in Fairness-Aware data Repair using Optimal Transport

>Authors: Abigail Langbridge, Anthony Quinn, Robert Shorten

>2024-10-03

> http://arxiv.org/abs/2410.02840v1

Optimal transport (OT) has an important role in transforming data
distributions in a manner which engenders fairness. Typically, the OT operators
are learnt from the unfair attribute-labelled data, and then used for their
repair. Two significant limitations of this approach are as follows: (i) the OT
operators for underrepresented subgroups are poorly learnt (i.e. they are
susceptible to representation bias); and (ii) these OT repairs cannot be
effected on identically distributed but out-of-sample (i.e.\ archival) data. In
this paper, we address both of these problems by adopting a Bayesian
nonparametric stopping rule for learning each attribute-labelled component of
the data distribution. The induced OT-optimal quantization operators can then
be used to repair the archival data. We formulate a novel definition of the
fair distributional target, along with quantifiers that allow us to trade
fairness against damage in the transformed data. These are used to reveal
excellent performance of our representation-bias-tolerant scheme in simulated
and benchmark data sets.


## SageAttention Accurate 8-Bit Attention for Plug-and-play Inference Acceleration

>Authors: Jintao Zhang, Jia wei, Pengle Zhang, Jun Zhu, Jianfei Chen

>2024-10-03

> http://arxiv.org/abs/2410.02367v1

The transformer architecture predominates across various models. As the heart
of the transformer, attention has a computational complexity of O(N^2),
compared to O(N) for linear transformations. When handling large sequence
lengths, attention becomes the primary time-consuming component. Although
quantization has proven to be an effective method for accelerating model
inference, existing quantization methods primarily focus on optimizing the
linear layer. In response, we first analyze the feasibility of quantization in
attention detailedly. Following that, we propose SageAttention, a highly
efficient and accurate quantization method for attention. The OPS (operations
per second) of our approach outperforms FlashAttention2 and xformers by about
2.1 times and 2.7 times, respectively. SageAttention also achieves superior
accuracy performance over FlashAttention3. Comprehensive experiments confirm
that our approach incurs almost no end-to-end metrics loss across diverse
models, including those for large language processing, image generation, and
video generation.


## Llama SLayer 8B Shallow Layers Hold the Key to Knowledge Injection

>Authors: Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, Nenghai Yu

>2024-10-03

> http://arxiv.org/abs/2410.02330v1

As a manner to augment pre-trained large language models (LLM), knowledge
injection is critical to develop vertical domain large models and has been
widely studied. Although most current approaches, including parameter-efficient
fine-tuning (PEFT) and block expansion methods, uniformly apply knowledge
across all LLM layers, it raises the question: are all layers equally crucial
for knowledge injection? We begin by evaluating the importance of each layer in
finding the optimal layer range for knowledge injection. Intuitively, the more
important layers should play a more critical role in knowledge injection and
deserve a denser injection. We observe performance dips in question-answering
benchmarks after the removal or expansion of the shallow layers, and the
degradation shrinks as the layer gets deeper, indicating that the shallow
layers hold the key to knowledge injection. This insight leads us to propose
the S strategy, a post-pretraining strategy of selectively enhancing shallow
layers while pruning the less effective deep ones. Based on this strategy, we
introduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the
corpus of code $\&$ math and demonstrated the effectiveness of our strategy.
Further experiments across different LLM, Mistral-7B, and a legal corpus
confirmed the general applicability of the approach, underscoring its
wide-ranging efficacy. Our code is available at:
\https://github.com/txchen-USTC/Llama-Slayer


## Jailbreak Antidote Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models

>Authors: Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng

>2024-10-03

> http://arxiv.org/abs/2410.02298v2

As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.


## Language Models are Graph Learners

>Authors: Zhe Xu, Kaveh Hassani, Si Zhang, Hanqing Zeng, Michihiro Yasunaga, Limei Wang, Dongqi Fu, Ning Yao, Bo Long, Hanghang Tong

>2024-10-03

> http://arxiv.org/abs/2410.02296v1

Language Models (LMs) are increasingly challenging the dominance of
domain-specific models, including Graph Neural Networks (GNNs) and Graph
Transformers (GTs), in graph learning tasks. Following this trend, we propose a
novel approach that empowers off-the-shelf LMs to achieve performance
comparable to state-of-the-art GNNs on node classification tasks, without
requiring any architectural modification. By preserving the LM's original
architecture, our approach retains a key benefit of LM instruction tuning: the
ability to jointly train on diverse datasets, fostering greater flexibility and
efficiency. To achieve this, we introduce two key augmentation strategies: (1)
Enriching LMs' input using topological and semantic retrieval methods, which
provide richer contextual information, and (2) guiding the LMs' classification
process through a lightweight GNN classifier that effectively prunes class
candidates. Our experiments on real-world datasets show that backbone Flan-T5
models equipped with these augmentation strategies outperform state-of-the-art
text-output node classifiers and are comparable to top-performing vector-output
node classifiers. By bridging the gap between specialized task-specific node
classifiers and general LMs, this work paves the way for more versatile and
widely applicable graph learning models. We will open-source the code upon
publication.


## SEAL SEmantic-Augmented Imitation Learning via Language Model

>Authors: Chengyang Gu, Yuxin Pan, Haotian Bai, Hui Xiong, Yize Chen

>2024-10-03

> http://arxiv.org/abs/2410.02231v1

Hierarchical Imitation Learning (HIL) is a promising approach for tackling
long-horizon decision-making tasks. While it is a challenging task due to the
lack of detailed supervisory labels for sub-goal learning, and reliance on
hundreds to thousands of expert demonstrations. In this work, we introduce
SEAL, a novel framework that leverages Large Language Models (LLMs)'s powerful
semantic and world knowledge for both specifying sub-goal space and
pre-labeling states to semantically meaningful sub-goal representations without
prior knowledge of task hierarchies. SEAL employs a dual-encoder structure,
combining supervised LLM-guided sub-goal learning with unsupervised Vector
Quantization (VQ) for more robust sub-goal representations. Additionally, SEAL
incorporates a transition-augmented low-level planner for improved adaptation
to sub-goal transitions. Our experiments demonstrate that SEAL outperforms
state-of-the-art HIL methods and LLM-based planning approaches, particularly in
settings with small expert datasets and complex long-horizon tasks.


## Aligning with Logic Measuring, Evaluating and Improving Logical Consistency in Large Language Models

>Authors: Yinhong Liu, Zhijiang Guo, Tianya Liang, Ehsan Shareghi, Ivan Vulić, Nigel Collier

>2024-10-03

> http://arxiv.org/abs/2410.02205v2

Recent research in Large Language Models (LLMs) has shown promising progress
related to LLM alignment with human preferences. LLM-empowered decision-making
systems are expected to be predictable, reliable and trustworthy, which implies
being free from paradoxes or contradictions that could undermine their
credibility and validity. However, LLMs still exhibit inconsistent and biased
behaviour when making decisions or judgements. In this work, we focus on
studying logical consistency of LLMs as a prerequisite for more reliable and
trustworthy systems. Logical consistency ensures that decisions are based on a
stable and coherent understanding of the problem, reducing the risk of erratic
or contradictory outputs. We first propose a universal framework to quantify
the logical consistency via three fundamental proxies: transitivity,
commutativity and negation invariance. We then evaluate logical consistency,
using the defined measures, of a wide range of LLMs, demonstrating that it can
serve as a strong proxy for overall robustness. Additionally, we introduce a
data refinement and augmentation technique that enhances the logical
consistency of LLMs without sacrificing alignment to human preferences. It
augments noisy and sparse pairwise-comparison annotations by estimating a
partially or totally ordered preference rankings using rank aggregation
methods. Finally, we show that logical consistency impacts the performance of
LLM-based logic-dependent algorithms, where LLMs serve as logical operators.


## Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices

>Authors: Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen, Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson

>2024-10-03

> http://arxiv.org/abs/2410.02117v2

Dense linear layers are the dominant computational bottleneck in large neural
networks, presenting a critical need for more efficient alternatives. Previous
efforts focused on a small number of hand-crafted structured matrices and
neglected to investigate whether these structures can surpass dense layers in
terms of compute-optimal scaling laws when both the model size and training
examples are optimally allocated. In this work, we present a unifying framework
that enables searching among all linear operators expressible via an Einstein
summation. This framework encompasses many previously proposed structures, such
as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,
along with many novel structures. To analyze the framework, we develop a
taxonomy of all such operators based on their computational and algebraic
properties and show that differences in the compute-optimal scaling laws are
mostly governed by a small number of variables that we introduce. Namely, a
small $\omega$ (which measures parameter sharing) and large $\psi$ (which
measures the rank) reliably led to better scaling laws. Guided by the insight
that full-rank structures that maximize parameters per unit of compute perform
the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture
obtained by sparsifying computation in the BTT structure. In contrast to the
standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE
in every single linear layer of the model, including the projection matrices in
the attention blocks. We find BTT-MoE provides a substantial compute-efficiency
gain over dense layers and standard MoE.


## Deep Generative Modeling for Identification of Noisy, Non-Stationary Dynamical Systems

>Authors: Doris Voina, Steven Brunton, J. Nathan Kutz

>2024-10-02

> http://arxiv.org/abs/2410.02079v1

A significant challenge in many fields of science and engineering is making
sense of time-dependent measurement data by recovering governing equations in
the form of differential equations. We focus on finding parsimonious ordinary
differential equation (ODE) models for nonlinear, noisy, and non-autonomous
dynamical systems and propose a machine learning method for data-driven system
identification. While many methods tackle noisy and limited data,
non-stationarity - where differential equation parameters change over time -
has received less attention. Our method, dynamic SINDy, combines variational
inference with SINDy (sparse identification of nonlinear dynamics) to model
time-varying coefficients of sparse ODEs. This framework allows for uncertainty
quantification of ODE coefficients, expanding on previous methods for
autonomous systems. These coefficients are then interpreted as latent variables
and added to the system to obtain an autonomous dynamical model. We validate
our approach using synthetic data, including nonlinear oscillators and the
Lorenz system, and apply it to neuronal activity data from C. elegans. Dynamic
SINDy uncovers a global nonlinear model, showing it can handle real, noisy, and
chaotic datasets. We aim to apply our method to a variety of problems,
specifically dynamic systems with complex time-dependent parameters.


## FLAG Financial Long Document Classification via AMR-based GNN

>Authors: Bolun, Xia, Mohammed J. Zaki, Aparna Gupta

>2024-10-02

> http://arxiv.org/abs/2410.02024v1

The advent of large language models (LLMs) has initiated much research into
their various financial applications. However, in applying LLMs on long
documents, semantic relations are not explicitly incorporated, and a full or
arbitrarily sparse attention operation is employed. In recent years, progress
has been made in Abstract Meaning Representation (AMR), which is a graph-based
representation of text to preserve its semantic relations. Since AMR can
represent semantic relationships at a deeper level, it can be beneficially
utilized by graph neural networks (GNNs) for constructing effective
document-level graph representations built upon LLM embeddings to predict
target metrics in the financial domain. We propose FLAG: Financial Long
document classification via AMR-based GNN, an AMR graph based framework to
generate document-level embeddings for long financial document classification.
We construct document-level graphs from sentence-level AMR graphs, endow them
with specialized LLM word embeddings in the financial domain, apply a deep
learning mechanism that utilizes a GNN, and examine the efficacy of our
AMR-based approach in predicting labeled target data from long financial
documents. Extensive experiments are conducted on a dataset of quarterly
earnings calls transcripts of companies in various sectors of the economy, as
well as on a corpus of more recent earnings calls of companies in the S&P 1500
Composite Index. We find that our AMR-based approach outperforms fine-tuning
LLMs directly on text in predicting stock price movement trends at different
time horizons in both datasets. Our work also outperforms previous work
utilizing document graphs and GNNs for text classification.


## A Spark of Vision-Language Intelligence 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation

>Authors: Liang Chen, Sinan Tan, Zefan Cai, Weichu Xie, Haozhe Zhao, Yichi Zhang, Junyang Lin, Jinze Bai, Tianyu Liu, Baobao Chang

>2024-10-02

> http://arxiv.org/abs/2410.01912v1

This work tackles the information loss bottleneck of vector-quantization (VQ)
autoregressive image generation by introducing a novel model architecture
called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer
predicts more codes for an image by introducing a new autoregression direction,
\textit{model depth}, along with the sequence length direction. Compared to
traditional 1D autoregression and previous work utilizing similar 2D image
decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end
model that can generate higher quality images with the same backbone model size
and sequence length, opening a new optimization perspective for autoregressive
image generation. Furthermore, our experiments reveal that the
DnD-Transformer's potential extends beyond generating natural images. It can
even generate images with rich text and graphical elements in a self-supervised
manner, demonstrating an understanding of these combined modalities. This has
not been previously demonstrated for popular vision generative models such as
diffusion models, showing a spark of vision-language intelligence when trained
solely on images. Code, datasets and models are open at
https://github.com/chenllliang/DnD-Transformer.


## Locret Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads

>Authors: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

>2024-10-02

> http://arxiv.org/abs/2410.01805v1

Large language models (LLMs) have shown remarkable advances in supporting
long-context comprehension and processing tasks. However, scaling the
generation inference of LLMs to such long contexts incurs significant
additional computation load, and demands a substantial GPU memory footprint to
maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache
compression methods, such as quantization, face memory bottlenecks as context
length increases, while static-sized caches, such as eviction, suffer from
inefficient policies. These limitations restrict deployment on consumer-grade
devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a
framework for long-context LLM inference that introduces retaining heads to
evaluate the causal importance of KV cache units, allowing for more accurate
eviction within a fixed cache size. Locret is fine-tuned on top of the frozen
backbone LLM using a minimal amount of data from standard long-context SFT
datasets. During inference, we evict low-importance cache units along with a
chunked prefill pattern, significantly reducing peak GPU memory usage. We
conduct an extensive empirical study to evaluate Locret, where the experimental
results show that Locret outperforms the recent competitive approaches,
including InfLLM, Quantization, SirLLM, and MInference, in terms of memory
efficiency and the quality of generated contents -- Locret achieves over a 20x
and 8x KV cache compression ratio compared to the full KV cache for
Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined
with other methods, such as quantization and token merging. To our knowledge,
Locret is the first framework capable of deploying Llama-3.1-8B or similar
models on a single Nvidia 4090 GPU, enabling 128K long-context inference
without compromising generation quality, and requiring little additional system
optimizations.


## DreamGarden A Designer Assistant for Growing Games from a Single Prompt

>Authors: Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey

>2024-10-02

> http://arxiv.org/abs/2410.01791v1

Coding assistants are increasingly leveraged in game design, both generating
code and making high-level plans. To what degree can these tools align with
developer workflows, and what new modes of human-computer interaction can
emerge from their use? We present DreamGarden, an AI system capable of
assisting with the development of diverse game environments in Unreal Engine.
At the core of our method is an LLM-driven planner, capable of breaking down a
single, high-level prompt -- a dream, memory, or imagined scenario provided by
a human user -- into a hierarchical action plan, which is then distributed
across specialized submodules facilitating concrete implementation. This system
is presented to the user as a garden of plans and actions, both growing
independently and responding to user intervention via seed prompts, pruning,
and feedback. Through a user study, we explore design implications of this
system, charting courses for future work in semi-autonomous assistants and
open-ended simulation design.


## Open-RAG Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models

>Authors: Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez

>2024-10-02

> http://arxiv.org/abs/2410.01782v1

Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/


## DRUPI Dataset Reduction Using Privileged Information

>Authors: Shaobo Wang, Yantai Yang, Shuaiyu Zhang, Chenghao Sun, Weiya Li, Xuming Hu, Linfeng Zhang

>2024-10-02

> http://arxiv.org/abs/2410.01611v1

Dataset reduction (DR) seeks to select or distill samples from large datasets
into smaller subsets while preserving performance on target tasks. Existing
methods primarily focus on pruning or synthesizing data in the same format as
the original dataset, typically the input data and corresponding labels.
However, in DR settings, we find it is possible to synthesize more information
beyond the data-label pair as an additional learning target to facilitate model
training. In this paper, we introduce Dataset Reduction Using Privileged
Information (DRUPI), which enriches DR by synthesizing privileged information
alongside the reduced dataset. This privileged information can take the form of
feature labels or attention labels, providing auxiliary supervision to improve
model learning. Our findings reveal that effective feature labels must balance
between being overly discriminative and excessively diverse, with a moderate
level proving optimal for improving the reduced dataset's efficacy. Extensive
experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI
integrates seamlessly with existing dataset reduction methods, offering
significant performance gains.


## Attention layers provably solve single-location regression

>Authors: Pierre Marion, Raphaël Berthier, Gérard Biau, Claire Boyer

>2024-10-02

> http://arxiv.org/abs/2410.01537v1

Attention-based models, such as Transformer, excel across various tasks but
lack a comprehensive theoretical understanding, especially regarding token-wise
sparsity and internal linear representations. To address this gap, we introduce
the single-location regression task, where only one token in a sequence
determines the output, and its position is a latent random variable,
retrievable via a linear projection of the input. To solve this task, we
propose a dedicated predictor, which turns out to be a simplified version of a
non-linear self-attention layer. We study its theoretical properties, by
showing its asymptotic Bayes optimality and analyzing its training dynamics. In
particular, despite the non-convex nature of the problem, the predictor
effectively learns the underlying structure. This work highlights the capacity
of attention mechanisms to handle sparse token information and internal linear
structures.


## A Little Goes a Long Way Efficient Long Context Training and Inference with Partial Contexts

>Authors: Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng

>2024-10-02

> http://arxiv.org/abs/2410.01485v1

Training and serving long-context large language models (LLMs) incurs
substantial overhead. To address this, two critical steps are often required: a
pretrained LLM typically undergoes a separate stage for context length
extension by training on long-context data, followed by architectural
modifications to reduce the overhead of KV cache during serving. This paper
argues that integrating length extension with a GPU-friendly KV cache reduction
architecture not only reduces training overhead during length extension, but
also achieves better long-context performance. This leads to our proposed
LongGen, which finetunes a pretrained LLM into an efficient architecture during
length extension. LongGen builds on three key insights: (1) Sparse attention
patterns, such as window attention (attending to recent tokens), attention sink
(initial ones), and blockwise sparse attention (strided token blocks) are
well-suited for building efficient long-context models, primarily due to their
GPU-friendly memory access patterns, enabling efficiency gains not just
theoretically but in practice as well. (2) It is essential for the model to
have direct access to all tokens. A hybrid architecture with 1/3 full attention
layers and 2/3 efficient ones achieves a balanced trade-off between efficiency
and long-context performance. (3) Lightweight training on 5B long-context data
is sufficient to extend the hybrid model's context length from 4K to 128K.
  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its
effectiveness across different scales. During training with 128K-long contexts,
LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,
compared to a full-attention baseline. During inference, LongGen reduces KV
cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding
speedup.


## SurgPointTransformer Vertebrae Shape Completion with RGB-D Data

>Authors: Aidana Massalimova, Florentin Liebmann, Sascha Jecklin, Fabio Carrillo, Farshad Mazda, Philipp Fürnstahl

>2024-10-02

> http://arxiv.org/abs/2410.01443v2

State-of-the-art computer- and robot-assisted surgery systems heavily depend
on intraoperative imaging technologies such as CT and fluoroscopy to generate
detailed 3D visualization of the patient's anatomy. While imaging techniques
are highly accurate, they are based on ionizing radiation and expose patients
and clinicians. This study introduces an alternative, radiation-free approach
for reconstructing the 3D spine anatomy using RGB-D data. Drawing inspiration
from the 3D "mental map" that surgeons form during surgeries, we introduce
SurgPointTransformer, a shape completion approach for surgical applications
that can accurately reconstruct the unexposed spine regions from sparse
observations of the exposed surface.
  Our method involves two main steps: segmentation and shape completion. The
segmentation step includes spinal column localization and segmentation,
followed by vertebra-wise segmentation. The segmented vertebra point clouds are
then subjected to SurgPointTransformer, which leverages an attention mechanism
to learn patterns between visible surface features and the underlying anatomy.
For evaluation, we utilize an ex-vivo dataset of nine specimens. Their CT data
is used to establish ground truth data that were used to compare to the outputs
of our methods. Our method significantly outperforms the state-of-the-art
baselines, achieving an average Chamfer Distance of 5.39, an F-Score of 0.85,
an Earth Mover's Distance of 0.011, and a Signal-to-Noise Ratio of 22.90 dB.
  This study demonstrates the potential of our reconstruction method for 3D
vertebral shape completion. It enables 3D reconstruction of the entire lumbar
spine and surgical guidance without ionizing radiation or invasive imaging. Our
work contributes to computer-aided and robot-assisted surgery, advancing the
perception and intelligence of these systems.


## FlashMask Efficient and Rich Mask Extension of FlashAttention

>Authors: Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, Haifeng Wang

>2024-10-02

> http://arxiv.org/abs/2410.01359v1

The computational and memory demands of vanilla attention scale quadratically
with the sequence length $N$, posing significant challenges for processing long
sequences in Transformer models. FlashAttention alleviates these challenges by
eliminating the $O(N^2)$ memory dependency and reducing attention latency
through IO-aware memory optimizations. However, its native support for certain
attention mask types is limited, and it does not inherently accommodate more
complex masking requirements. Previous approaches resort to using dense masks
with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we
propose FlashMask, an extension of FlashAttention that introduces a column-wise
sparse representation of attention masks. This approach efficiently represents
a wide range of mask types and facilitates the development of optimized kernel
implementations. By adopting this novel representation, FlashMask achieves
linear memory complexity $O(N)$, suitable for modeling long-context sequences.
Moreover, this representation enables kernel optimizations that eliminate
unnecessary computations by leveraging sparsity in the attention mask, without
sacrificing computational accuracy, resulting in higher computational
efficiency. We evaluate FlashMask's performance in fine-tuning and alignment
training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant
throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x
compared to existing FlashAttention dense method. Additionally, our
kernel-level comparisons demonstrate that FlashMask surpasses the latest
counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s,
achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU.
The code is open-sourced on PaddlePaddle and integrated into PaddleNLP,
supporting models with over 100 billion parameters for contexts up to 128K
tokens.


## Takin-VC Zero-shot Voice Conversion via Jointly Hybrid Content and Memory-Augmented Context-Aware Timbre Modeling

>Authors: Yuguang Yang, Yu Pan, Jixun Yao, Xiang Zhang, Jianhao Ye, Hongbin Zhou, Lei Xie, Lei Ma, Jianjun Zhao

>2024-10-02

> http://arxiv.org/abs/2410.01350v1

Zero-shot voice conversion (VC) aims to transform the source speaker timbre
into an arbitrary unseen one without altering the original speech content.While
recent advancements in zero-shot VC methods have shown remarkable progress,
there still remains considerable potential for improvement in terms of
improving speaker similarity and speech naturalness.In this paper, we propose
Takin-VC, a novel zero-shot VC framework based on jointly hybrid content and
memory-augmented context-aware timbre modeling to tackle this challenge.
Specifically, an effective hybrid content encoder, guided by neural codec
training, that leverages quantized features from pre-trained WavLM and
HybridFormer is first presented to extract the linguistic content of the source
speech. Subsequently, we introduce an advanced cross-attention-based
context-aware timbre modeling approach that learns the fine-grained,
semantically associated target timbre features. To further enhance both speaker
similarity and real-time performance, we utilize a conditional flow matching
model to reconstruct the Mel-spectrogram of the source speech. Additionally, we
advocate an efficient memory-augmented module designed to generate high-quality
conditional target inputs for the flow matching process, thereby improving the
overall performance of the proposed system. Experimental results demonstrate
that the proposed Takin-VC method surpasses state-of-the-art zero-shot VC
systems, delivering superior performance in terms of both speech naturalness
and speaker similarity.


## Getting Free Bits Back from Rotational Symmetries in LLMs

>Authors: Jiajun He, Gergely Flamich, José Miguel Hernández-Lobato

>2024-10-02

> http://arxiv.org/abs/2410.01309v1

Current methods for compressing neural network weights, such as
decomposition, pruning, quantization, and channel simulation, often overlook
the inherent symmetries within these networks and thus waste bits on encoding
redundant information. In this paper, we propose a format based on bits-back
coding for storing rotationally symmetric Transformer weights more efficiently
than the usual array layout at the same floating-point precision. We evaluate
our method on Large Language Models (LLMs) pruned by SliceGPT (Ashkboos et al.,
2024) and achieve a 3-5% reduction in total bit usage for free across different
model sizes and architectures without impacting model performance within a
certain numerical precision.


## Speculative Coreset Selection for Task-Specific Fine-tuning

>Authors: Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Tianlin Li, Weipeng Jiang, Yang Liu

>2024-10-02

> http://arxiv.org/abs/2410.01296v1

Task-specific fine-tuning is essential for the deployment of large language
models (LLMs), but it requires significant computational resources and time.
Existing solutions have proposed coreset selection methods to improve data
efficiency and reduce model training overhead, but they still have limitations:
1) Overlooking valuable samples at high pruning rates, which degrades the
coreset's performance. 2) Requiring high time overhead during coreset selection
to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a
speculative coreset selection method. STAFF leverages a small model from the
same family as the target LLM to efficiently estimate data scores and then
verifies the scores on the target LLM to accurately identify and allocate more
selection budget to important regions while maintaining coverage of easy
regions. We evaluate STAFF on three LLMs and three downstream tasks and show
that STAFF improves the performance of SOTA methods by up to 54.3% and reduces
selection overhead by up to 70.5% at different pruning rates. Furthermore, we
observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can
even obtain better fine-tuning performance than the full dataset.


## Mitigating Copy Bias in In-Context Learning through Neuron Pruning

>Authors: Ameen Ali, Lior Wolf, Ivan Titov

>2024-10-02

> http://arxiv.org/abs/2410.01288v2

Large language models (LLMs) have demonstrated impressive few-shot in-context
learning (ICL) abilities. Still, we show that they are sometimes prone to a
`copying bias', where they copy answers from provided examples instead of
learning the underlying patterns. In this work, we propose a novel and simple
method to mitigate such copying bias. First, we create a synthetic task and use
the Integrated Gradients method to identify neurons that prioritize copying
over generalization. We demonstrate that pruning these neurons consistently
improves performance across a diverse set of ICL tasks. We also show that our
method is applicable across various LLM architectures, including Transformers
and State-Space Models, without requiring modifications. In our analysis, we
adopt a task-recognition perspective on ICL and examine task vectors (Hendel et
al., 2023) induced by the model. We find that pruning enhances the quality of
these vectors, suggesting that the pruned neurons previously hindered effective
task recognition.


## Uncertainty-aware Human Mobility Modeling and Anomaly Detection

>Authors: Haomin Wen, Shurui Cao, Leman Akoglu

>2024-10-02

> http://arxiv.org/abs/2410.01281v1

Given the GPS coordinates of a large collection of human agents over time,
how can we model their mobility behavior toward effective anomaly detection
(e.g. for bad-actor or malicious behavior detection) without any labeled data?
Human mobility and trajectory modeling have been studied extensively with
varying capacity to handle complex input, and performance-efficiency
trade-offs. With the arrival of more expressive models in machine learning, we
attempt to model GPS data as a sequence of stay-point events, each with a set
of characterizing spatiotemporal features, and leverage modern sequence models
such as Transformers for un/self-supervised training and inference. Notably,
driven by the inherent stochasticity of certain individuals' behavior, we equip
our model with aleatoric/data uncertainty estimation. In addition, to handle
data sparsity of a large variety of behaviors, we incorporate epistemic/model
uncertainty into our model. Together, aleatoric and epistemic uncertainty
enable a robust loss and training dynamics, as well as uncertainty-aware
decision making in anomaly scoring. Experiments on large expert-simulated
datasets with tens of thousands of agents demonstrate the effectiveness of our
model against both forecasting and anomaly detection baselines.


## Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models

>Authors: Can Demircan, Tankred Saanum, Akshay K. Jagadish, Marcel Binz, Eric Schulz

>2024-10-02

> http://arxiv.org/abs/2410.01280v1

In-context learning, the ability to adapt based on a few examples in the
input prompt, is a ubiquitous feature of large language models (LLMs). However,
as LLMs' in-context learning abilities continue to improve, understanding this
phenomenon mechanistically becomes increasingly important. In particular, it is
not well-understood how LLMs learn to solve specific classes of problems, such
as reinforcement learning (RL) problems, in-context. Through three different
tasks, we first show that Llama $3$ $70$B can solve simple RL problems
in-context. We then analyze the residual stream of Llama using Sparse
Autoencoders (SAEs) and find representations that closely match temporal
difference (TD) errors. Notably, these representations emerge despite the model
only being trained to predict the next token. We verify that these
representations are indeed causally involved in the computation of TD errors
and $Q$-values by performing carefully designed interventions on them. Taken
together, our work establishes a methodology for studying and manipulating
in-context learning with SAEs, paving the way for a more mechanistic
understanding.


## Flex3D Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation

>Authors: Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos

>2024-10-01

> http://arxiv.org/abs/2410.00890v2

Generating high-quality 3D content from text, single images, or sparse view
images remains a challenging task with broad applications. Existing methods
typically employ multi-view diffusion models to synthesize multi-view images,
followed by a feed-forward process for 3D reconstruction. However, these
approaches are often constrained by a small and fixed number of input views,
limiting their ability to capture diverse viewpoints and, even worse, leading
to suboptimal generation results if the synthesized views are of poor quality.
To address these limitations, we propose Flex3D, a novel two-stage framework
capable of leveraging an arbitrary number of high-quality input views. The
first stage consists of a candidate view generation and curation pipeline. We
employ a fine-tuned multi-view image diffusion model and a video diffusion
model to generate a pool of candidate views, enabling a rich representation of
the target 3D object. Subsequently, a view selection pipeline filters these
views based on quality and consistency, ensuring that only the high-quality and
reliable views are used for reconstruction. In the second stage, the curated
views are fed into a Flexible Reconstruction Model (FlexRM), built upon a
transformer architecture that can effectively process an arbitrary number of
inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane
representation, enabling efficient and detailed 3D generation. Through
extensive exploration of design and training strategies, we optimize FlexRM to
achieve superior performance in both reconstruction and generation tasks. Our
results demonstrate that Flex3D achieves state-of-the-art performance, with a
user study winning rate of over 92% in 3D generation tasks when compared to
several of the latest feed-forward 3D generative models.


## Sparse Attention Decomposition Applied to Circuit Tracing

>Authors: Gabriel Franco, Mark Crovella

>2024-10-01

> http://arxiv.org/abs/2410.00340v1

Many papers have shown that attention heads work in conjunction with each
other to perform complex tasks. It's frequently assumed that communication
between attention heads is via the addition of specific features to token
residuals. In this work we seek to isolate and identify the features used to
effect communication and coordination among attention heads in GPT-2 small. Our
key leverage on the problem is to show that these features are very often
sparsely coded in the singular vectors of attention head matrices. We
characterize the dimensionality and occurrence of these signals across the
attention heads in GPT-2 small when used for the Indirect Object Identification
(IOI) task. The sparse encoding of signals, as provided by attention head
singular vectors, allows for efficient separation of signals from the residual
background and straightforward identification of communication paths between
attention heads. We explore the effectiveness of this approach by tracing
portions of the circuits used in the IOI task. Our traces reveal considerable
detail not present in previous studies, shedding light on the nature of
redundant paths present in GPT-2. And our traces go beyond previous work by
identifying features used to communicate between attention heads when
performing IOI.


## End-to-end Piano Performance-MIDI to Score Conversion with Transformers

>Authors: Tim Beyer, Angela Dai

>2024-09-30

> http://arxiv.org/abs/2410.00210v1

The automated creation of accurate musical notation from an expressive human
performance is a fundamental task in computational musicology. To this end, we
present an end-to-end deep learning approach that constructs detailed musical
scores directly from real-world piano performance-MIDI files. We introduce a
modern transformer-based architecture with a novel tokenized representation for
symbolic music data. Framing the task as sequence-to-sequence translation
rather than note-wise classification reduces alignment requirements and
annotation costs, while allowing the prediction of more concise and accurate
notation. To serialize symbolic music data, we design a custom tokenization
stage based on compound tokens that carefully quantizes continuous values. This
technique preserves more score information while reducing sequence lengths by
$3.5\times$ compared to prior approaches. Using the transformer backbone, our
method demonstrates better understanding of note values, rhythmic structure,
and details such as staff assignment. When evaluated end-to-end using
transcription metrics such as MUSTER, we achieve significant improvements over
previous deep learning approaches and complex HMM-based state-of-the-art
pipelines. Our method is also the first to directly predict notational details
like trill marks or stem direction from performance data. Code and models are
available at https://github.com/TimFelixBeyer/MIDI2ScoreTransformer


## EEG Emotion Copilot Pruning LLMs for Emotional EEG Interpretation with Assisted Medical Record Generation

>Authors: Hongyu Chen, Weiming Zeng, Chengcheng Chen, Luhui Cai, Fei Wang, Lei Wang, Wei Zhang, Yueyang Li, Hongjie Yan, Wai Ting Siok, Nizhuan Wang

>2024-09-30

> http://arxiv.org/abs/2410.00166v1

In the fields of affective computing (AC) and brain-machine interface (BMI),
the analysis of physiological and behavioral signals to discern individual
emotional states has emerged as a critical research frontier. While deep
learning-based approaches have made notable strides in EEG emotion recognition,
particularly in feature extraction and pattern recognition, significant
challenges persist in achieving end-to-end emotion computation, including
real-time processing, individual adaptation, and seamless user interaction.
This paper presents the EEG Emotion Copilot, a system leveraging a lightweight
large language model (LLM) operating in a local setting. The system is designed
to first recognize emotional states directly from EEG signals, subsequently
generate personalized diagnostic and treatment suggestions, and finally support
the automation of electronic medical records. The proposed solution emphasizes
both the accuracy of emotion recognition and an enhanced user experience,
facilitated by an intuitive interface for participant interaction. We further
discuss the construction of the data framework, model pruning, training, and
deployment strategies aimed at improving real-time performance and
computational efficiency. Privacy concerns are also addressed, with a focus on
ethical data collection, processing, and the protection of users' personal
information. Through these efforts, we aim to advance the application of AC in
the medical domain, offering innovative approaches to mental health diagnostics
and treatment.


## Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models

>Authors: Ji Liu, Jiaxiang Ren, Ruoming Jin, Zijie Zhang, Yang Zhou, Patrick Valduriez, Dejing Dou

>2024-09-30

> http://arxiv.org/abs/2410.00131v1

As a promising paradigm to collaboratively train models with decentralized
data, Federated Learning (FL) can be exploited to fine-tune Large Language
Models (LLMs). While LLMs correspond to huge size, the scale of the training
data significantly increases, which leads to tremendous amounts of computation
and communication costs. The training data is generally non-Independent and
Identically Distributed (non-IID), which requires adaptive data processing
within each device. Although Low Rank Adaptation (LoRA) can significantly
reduce the scale of parameters to update in the fine-tuning process, it still
takes unaffordable time to transfer the low-rank parameters of all the layers
in LLMs. In this paper, we propose a Fisher Information-based Efficient
Curriculum Federated Learning framework (FibecFed) with two novel methods,
i.e., adaptive federated curriculum learning and efficient sparse parameter
update. First, we propose a fisher information-based method to adaptively
sample data within each device to improve the effectiveness of the FL
fine-tuning process. Second, we dynamically select the proper layers for global
aggregation and sparse parameters for local update with LoRA so as to improve
the efficiency of the FL fine-tuning process. Extensive experimental results
based on 10 datasets demonstrate that FibecFed yields excellent performance (up
to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%
faster) compared with 17 baseline approaches).


## Near-horizon aspects of black holes in quantum spacetime

>Authors: Nikola Herceg, Tajron Jurić, A. Naveena Kumara, Andjelo Samsarov, Ivica Smolić

>2024-09-30

> http://arxiv.org/abs/2410.00088v1

We give a short introduction to the formalism of noncommutative (twisted)
differential geometry that is used to derive the equations of motion for the
gravitational perturbation of the Schwarzschild black hole in quantized
spacetime. Special attention is given to quantum spacetime arising from $r -
\varphi$ noncommutativity. Tortoise coordinate and near-horizon regions of the
effective potentials are analyzed for both polar and axial modes. By carefully
examining the associated Schr\"odinger-type equations, we provide the
asymptotic solutions at the horizon and illustrate some differences between the
polar and axial modes. These findings give further insight into the polar-axial
isospectrality violation in the presence of the quantum structure of spacetime.


## COLLAGE Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models

>Authors: Divyanshu Daiya, Damon Conover, Aniket Bera

>2024-09-30

> http://arxiv.org/abs/2409.20502v1

We propose a novel framework COLLAGE for generating collaborative
agent-object-agent interactions by leveraging large language models (LLMs) and
hierarchical motion-specific vector-quantized variational autoencoders
(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by
incorporating the knowledge and reasoning abilities of LLMs to guide a
generative diffusion model. The hierarchical VQ-VAE architecture captures
different motion-specific characteristics at multiple levels of abstraction,
avoiding redundant concepts and enabling efficient multi-resolution
representation. We introduce a diffusion model that operates in the latent
space and incorporates LLM-generated motion planning cues to guide the
denoising process, resulting in prompt-specific motion generation with greater
control and diversity. Experimental results on the CORE-4D, and InterHuman
datasets demonstrate the effectiveness of our approach in generating realistic
and diverse collaborative human-object-human interactions, outperforming
state-of-the-art methods. Our work opens up new possibilities for modeling
complex interactions in various domains, such as robotics, graphics and
computer vision.


## PersonalLLM Tailoring LLMs to Individual Preferences

>Authors: Thomas P. Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, Hongseok Namkoong

>2024-09-30

> http://arxiv.org/abs/2409.20296v1

As LLMs become capable of complex tasks, there is growing potential for
personalized interactions tailored to the subtle and idiosyncratic preferences
of the user. We present a public benchmark, PersonalLLM, focusing on adapting
LLMs to provide maximal benefits for a particular user. Departing from existing
alignment benchmarks that implicitly assume uniform preferences, we curate
open-ended prompts paired with many high-quality answers over which users would
be expected to display heterogeneous latent preferences. Instead of
persona-prompting LLMs based on high-level attributes (e.g., user's race or
response length), which yields homogeneous preferences relative to humans, we
develop a method that can simulate a large user base with diverse preferences
from a set of pre-trained reward models. Our dataset and generated
personalities offer an innovative testbed for developing personalization
algorithms that grapple with continual data sparsity--few relevant feedback
from the particular user--by leveraging historical data from other (similar)
users. We explore basic in-context learning and meta-learning baselines to
illustrate the utility of PersonalLLM and highlight the need for future
methodological development. Our dataset is available at
https://huggingface.co/datasets/namkoong-lab/PersonalLLM


## Aggressive Post-Training Compression on Extremely Large Language Models

>Authors: Zining Zhang, Yao Chen, Bingsheng He, Zhenjie Zhang

>2024-09-30

> http://arxiv.org/abs/2409.20094v1

The increasing size and complexity of Large Language Models (LLMs) pose
challenges for their deployment on personal computers and mobile devices.
Aggressive post-training model compression is necessary to reduce the models'
size, but it often results in significant accuracy loss. To address this
challenge, we propose a novel network pruning technology that utilizes over 0.7
sparsity and less than 8 bits of quantization. Our approach enables the
compression of prevailing LLMs within a couple of hours while maintaining a
relatively small accuracy loss. In experimental evaluations, our method
demonstrates effectiveness and potential for practical deployment. By making
LLMs available on domestic devices, our work can facilitate a new era of
natural language processing applications with wide-ranging impacts.


## InfantCryNet A Data-driven Framework for Intelligent Analysis of Infant Cries

>Authors: Mengze Hong, Chen Jason Zhang, Lingxiao Yang, Yuanfeng Song, Di Jiang

>2024-09-29

> http://arxiv.org/abs/2409.19689v1

Understanding the meaning of infant cries is a significant challenge for
young parents in caring for their newborns. The presence of background noise
and the lack of labeled data present practical challenges in developing systems
that can detect crying and analyze its underlying reasons. In this paper, we
present a novel data-driven framework, "InfantCryNet," for accomplishing these
tasks. To address the issue of data scarcity, we employ pre-trained audio
models to incorporate prior knowledge into our model. We propose the use of
statistical pooling and multi-head attention pooling techniques to extract
features more effectively. Additionally, knowledge distillation and model
quantization are applied to enhance model efficiency and reduce the model size,
better supporting industrial deployment in mobile devices. Experiments on
real-life datasets demonstrate the superior performance of the proposed
framework, outperforming state-of-the-art baselines by 4.4% in classification
accuracy. The model compression effectively reduces the model size by 7%
without compromising performance and by up to 28% with only an 8% decrease in
accuracy, offering practical insights for model selection and system design.


## Alternating Maximization Algorithm for Mismatch Capacity with Oblivious Relaying

>Authors: Xinwei Li, Lingyi Chen, Shitong Wu, Huihui Wu, Hao Wu, Wenyi Zhang

>2024-09-29

> http://arxiv.org/abs/2409.19674v3

Reliable communication over a discrete memoryless channel with the help of a
relay has aroused interest due to its widespread applications in practical
scenarios. By considering the system with a mismatched decoder, previous works
have provided optimization models to evaluate the mismatch capacity in these
scenarios. The proposed models, however, are difficult due to the complicated
structure of the mismatched decoding problem with the information flows in hops
given by the relay. Existing methods, such as the grid search, become
impractical as they involve finding all roots of a nonlinear system, with the
growing size of the alphabet. To address this problem, we reformulate the
max-min optimization model as a consistent maximization form, by considering
the dual form of the inner minimization problem and the Lagrangian with a fixed
multiplier. Based on the proposed formulation, an alternating maximization
framework is designed, which provides the closed-form solution with simple
iterations in each step by introducing a suitable variable transformation. The
effectiveness of the proposed approach is demonstrated by the simulations over
practical scenarios, including Quaternary and Gaussian channels. Moreover, the
simulation results of the transitional probability also shed light on the
promising application attribute to the quantizer design in the relay node.


## Learning Attentional Mixture of LoRAs for Language Model Continual Learning

>Authors: Jialin Liu, Jianhua Wu, Jie Liu, Yutai Duan

>2024-09-29

> http://arxiv.org/abs/2409.19611v1

Fine-tuning large language models (LLMs) with Low-Rank adaption (LoRA) is
widely acknowledged as an effective approach for continual learning for new
tasks. However, it often suffers from catastrophic forgetting when dealing with
multiple tasks sequentially. To this end, we propose Attentional Mixture of
LoRAs (AM-LoRA), a continual learning approach tailored for LLMs. Specifically,
AM-LoRA learns a sequence of LoRAs for a series of tasks to continually learn
knowledge from different tasks. The key of our approach is that we devise an
attention mechanism as a knowledge mixture module to adaptively integrate
information from each LoRA. With the attention mechanism, AM-LoRA can
efficiently leverage the distinctive contributions of each LoRA, while
mitigating the risk of mutually negative interactions among them that may lead
to catastrophic forgetting. Moreover, we further introduce $L1$ norm in the
learning process to make the attention vector more sparse. The sparse
constraints can enable the model to lean towards selecting a few highly
relevant LoRAs, rather than aggregating and weighting all LoRAs collectively,
which can further reduce the impact stemming from mutual interference.
Experimental results on continual learning benchmarks indicate the superiority
of our proposed method.


## One Token to Seg Them All Language Instructed Reasoning Segmentation in Videos

>Authors: Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou

>2024-09-29

> http://arxiv.org/abs/2409.19603v1

We introduce VideoLISA, a video-based multimodal large language model
designed to tackle the problem of language-instructed reasoning segmentation in
videos. Leveraging the reasoning capabilities and world knowledge of large
language models, and augmented by the Segment Anything Model, VideoLISA
generates temporally consistent segmentation masks in videos based on language
instructions. Existing image-based methods, such as LISA, struggle with video
tasks due to the additional temporal dimension, which requires temporal dynamic
understanding and consistent segmentation across frames. VideoLISA addresses
these challenges by integrating a Sparse Dense Sampling strategy into the
video-LLM, which balances temporal context and spatial detail within
computational constraints. Additionally, we propose a One-Token-Seg-All
approach using a specially designed <TRK> token, enabling the model to segment
and track objects across multiple frames. Extensive evaluations on diverse
benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate
VideoLISA's superior performance in video object segmentation tasks involving
complex reasoning, temporal understanding, and object tracking. While optimized
for videos, VideoLISA also shows promising generalization to image
segmentation, revealing its potential as a unified foundation model for
language-instructed object segmentation. Code and model will be available at:
https://github.com/showlab/VideoLISA.


## Abstractive Summarization of Low resourced Nepali language using Multilingual Transformers

>Authors: Prakash Dhakal, Daya Sagar Baral

>2024-09-29

> http://arxiv.org/abs/2409.19566v1

Automatic text summarization in Nepali language is an unexplored area in
natural language processing (NLP). Although considerable research has been
dedicated to extractive summarization, the area of abstractive summarization,
especially for low-resource languages such as Nepali, remains largely
unexplored. This study explores the use of multilingual transformer models,
specifically mBART and mT5, for generating headlines for Nepali news articles
through abstractive summarization. The research addresses key challenges
associated with summarizing texts in Nepali by first creating a summarization
dataset through web scraping from various Nepali news portals. These
multilingual models were then fine-tuned using different strategies. The
performance of the fine-tuned models were then assessed using ROUGE scores and
human evaluation to ensure the generated summaries were coherent and conveyed
the original meaning. During the human evaluation, the participants were asked
to select the best summary among those generated by the models, based on
criteria such as relevance, fluency, conciseness, informativeness, factual
accuracy, and coverage. During the evaluation with ROUGE scores, the 4-bit
quantized mBART with LoRA model was found to be effective in generating better
Nepali news headlines in comparison to other models and also it was selected
34.05% of the time during the human evaluation, outperforming all other
fine-tuned models created for Nepali News headline generation.


## Efficient Federated Intrusion Detection in 5G ecosystem using optimized BERT-based model

>Authors: Frederic Adjewa, Moez Esseghir, Leila Merghem-Boulahia

>2024-09-28

> http://arxiv.org/abs/2409.19390v1

The fifth-generation (5G) offers advanced services, supporting applications
such as intelligent transportation, connected healthcare, and smart cities
within the Internet of Things (IoT). However, these advancements introduce
significant security challenges, with increasingly sophisticated cyber-attacks.
This paper proposes a robust intrusion detection system (IDS) using federated
learning and large language models (LLMs). The core of our IDS is based on
BERT, a transformer model adapted to identify malicious network flows. We
modified this transformer to optimize performance on edge devices with limited
resources. Experiments were conducted in both centralized and federated
learning contexts. In the centralized setup, the model achieved an inference
accuracy of 97.79%. In a federated learning context, the model was trained
across multiple devices using both IID (Independent and Identically
Distributed) and non-IID data, based on various scenarios, ensuring data
privacy and compliance with regulations. We also leveraged linear quantization
to compress the model for deployment on edge devices. This reduction resulted
in a slight decrease of 0.02% in accuracy for a model size reduction of 28.74%.
The results underscore the viability of LLMs for deployment in IoT ecosystems,
highlighting their ability to operate on devices with constrained computational
and storage resources.


## MambaEviScrib Mamba and Evidence-Guided Consistency Make CNN Work Robustly for Scribble-Based Weakly Supervised Ultrasound Image Segmentation

>Authors: Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Qiaohong Liu, Qi Zhang

>2024-09-28

> http://arxiv.org/abs/2409.19370v1

Segmenting anatomical structures and lesions from ultrasound images
contributes to disease assessment, diagnosis, and treatment. Weakly supervised
learning (WSL) based on sparse annotation has achieved encouraging performance
and demonstrated the potential to reduce annotation costs. However, ultrasound
images often suffer from issues such as poor contrast, unclear edges, as well
as varying sizes and locations of lesions. This makes it challenging for
convolutional networks with local receptive fields to extract global
morphological features from the sparse information provided by scribble
annotations. Recently, the visual Mamba based on state space sequence models
(SSMs) has significantly reduced computational complexity while ensuring
long-range dependencies compared to Transformers. Consequently, for the first
time, we apply scribble-based WSL to ultrasound image segmentation and propose
a novel hybrid CNN-Mamba framework. Furthermore, due to the characteristics of
ultrasound images and insufficient supervision signals, existing consistency
regularization often filters out predictions near decision boundaries, leading
to unstable predictions of edges. Hence, we introduce the Dempster-Shafer
theory (DST) of evidence to devise an Evidence-Guided Consistency (EGC)
strategy, which leverages high-evidence predictions more likely to occur near
high-density regions to guide low-evidence predictions potentially present near
decision boundaries for optimization. During training, the collaboration
between the CNN branch and the Mamba branch in the proposed framework draws
inspiration from each other based on the EGC strategy. Extensive experiments on
four ultrasound public datasets for binary-class and multi-class segmentation
demonstrate the competitiveness of the proposed method. The scribble-annotated
dataset and code will be made available on
https://github.com/GtLinyer/MambaEviScrib.


## DelayPTC-LLM Metro Passenger Travel Choice Prediction under Train Delays with Large Language Models

>Authors: Chen Chen, Yuxin He, Hao Wang, Jingjing Chen, Qin Luo

>2024-09-28

> http://arxiv.org/abs/2410.00052v1

Train delays can propagate rapidly throughout the Urban Rail Transit (URT)
network under networked operation conditions, posing significant challenges to
operational departments. Accurately predicting passenger travel choices under
train delays can provide interpretable insights into the redistribution of
passenger flow, offering crucial decision support for emergency response and
service recovery. However, the diversity of travel choices due to passenger
heterogeneity and the sparsity of delay events leads to issues of data sparsity
and sample imbalance in the travel choices dataset under metro delays. It is
challenging to model this problem using traditional machine learning
approaches, which typically rely on large, balanced datasets. Given the
strengths of large language models (LLMs) in text processing, understanding,
and their capabilities in small-sample and even zero-shot learning, this paper
proposes a novel Passenger Travel Choice prediction framework under metro
delays with the Large Language Model (DelayPTC-LLM). The well-designed
prompting engineering is developed to guide the LLM in making and rationalizing
predictions about travel choices, taking into account passenger heterogeneity
and features of the delay events. Utilizing real-world data from Shenzhen
Metro, including Automated Fare Collection (AFC) data and detailed delay logs,
a comparative analysis of DelayPTC-LLM with traditional prediction models
demonstrates the superior capability of LLMs in handling complex, sparse
datasets commonly encountered under disruption of transportation systems. The
results validate the advantages of DelayPTC-LLM in terms of predictive accuracy
and its potential to provide actionable insights for big traffic data.


## CLIP-MoE Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling

>Authors: Jihai Zhang, Xiaoye Qu, Tong Zhu, Yu Cheng

>2024-09-28

> http://arxiv.org/abs/2409.19291v2

In recent years, Contrastive Language-Image Pre-training (CLIP) has become a
cornerstone in multimodal intelligence. However, recent studies have identified
that the information loss in the CLIP encoding process is substantial, and CLIP
tends to capture only coarse-grained features from the input. This deficiency
significantly limits the ability of a single CLIP model to handle images rich
in visual detail. In this work, we propose a simple yet effective
model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU
efficiently fine-tunes a series of CLIP models that capture different feature
spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for
the Feed-Forward Network (FFN). These models can then be transformed into a
CLIP-MoE with a larger model capacity, leading to significantly enhanced
performance with minimal computational overhead. To the best of our knowledge,
Diversified Multiplet Upcycling is the first approach to introduce sparsely
activated MoE into CLIP foundation models. Extensive experiments demonstrate
the significant performance of CLIP-MoE across various zero-shot retrieval,
zero-shot image classification tasks, and downstream Multimodal Large Language
Model (MLLM) benchmarks by serving as a vision encoder. Furthermore,
Diversified Multiplet Upcycling enables the conversion of any dense CLIP model
into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner
without requiring further adaptation in downstream frameworks. Through
Diversified Multiplet Upcycling, we aim to provide valuable insights for future
research on developing more efficient and effective multimodal learning
systems.


## A GEN AI Framework for Medical Note Generation

>Authors: Hui Yi Leong, Yi Fan Gao, Shuai Ji, Bora Kalaycioglu, Uktu Pamuksuz

>2024-09-27

> http://arxiv.org/abs/2410.01841v1

The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.


## On the Power of Decision Trees in Auto-Regressive Language Modeling

>Authors: Yulu Gan, Tomer Galanti, Tomaso Poggio, Eran Malach

>2024-09-27

> http://arxiv.org/abs/2409.19150v1

Originally proposed for handling time series data, Auto-regressive Decision
Trees (ARDTs) have not yet been explored for language modeling. This paper
delves into both the theoretical and practical applications of ARDTs in this
new context. We theoretically demonstrate that ARDTs can compute complex
functions, such as simulating automata, Turing machines, and sparse circuits,
by leveraging "chain-of-thought" computations. Our analysis provides bounds on
the size, depth, and computational efficiency of ARDTs, highlighting their
surprising computational power. Empirically, we train ARDTs on simple language
generation tasks, showing that they can learn to generate coherent and
grammatically correct text on par with a smaller Transformer model.
Additionally, we show that ARDTs can be used on top of transformer
representations to solve complex reasoning tasks. This research reveals the
unique computational abilities of ARDTs, aiming to broaden the architectural
diversity in language model development.


## Pruning then Reweighting Towards Data-Efficient Training of Diffusion Models

>Authors: Yize Li, Yihua Zhang, Sijia Liu, Xue Lin

>2024-09-27

> http://arxiv.org/abs/2409.19128v2

Despite the remarkable generation capabilities of Diffusion Models (DMs),
conducting training and inference remains computationally expensive. Previous
works have been devoted to accelerating diffusion sampling, but achieving
data-efficient diffusion training has often been overlooked. In this work, we
investigate efficient diffusion training from the perspective of dataset
pruning. Inspired by the principles of data-efficient training for generative
models such as generative adversarial networks (GANs), we first extend the data
selection scheme used in GANs to DM training, where data features are encoded
by a surrogate model, and a score criterion is then applied to select the
coreset. To further improve the generation performance, we employ a class-wise
reweighting approach, which derives class weights through distributionally
robust optimization (DRO) over a pre-trained reference DM. For a pixel-wise DM
(DDPM) on CIFAR-10, experiments demonstrate the superiority of our methodology
over existing approaches and its effectiveness in image synthesis comparable to
that of the original full-data model while achieving the speed-up between 2.34
times and 8.32 times. Additionally, our method could be generalized to latent
DMs (LDMs), e.g., Masked Diffusion Transformer (MDT) and Stable Diffusion (SD),
and achieves competitive generation capability on ImageNet. Code is available
here
(https://github.com/Yeez-lee/Data-Selection-and-Reweighting-for-Diffusion-Models).


## Voxel-CIM An Efficient Compute-in-Memory Accelerator for Voxel-based Point Cloud Neural Networks

>Authors: Xipeng Lin, Shanshi Huang, Hongwu Jiang

>2024-09-27

> http://arxiv.org/abs/2409.19077v1

The 3D point cloud perception has emerged as a fundamental role for a wide
range of applications. In particular, with the rapid development of neural
networks, the voxel-based networks attract great attention due to their
excellent performance. Various accelerator designs have been proposed to
improve the hardware performance of voxel-based networks, especially to speed
up the map search process. However, several challenges still exist including:
(1) massive off-chip data access volume caused by map search operations,
notably for high resolution and dense distribution cases, (2) frequent data
movement for data-intensive convolution operations, (3) imbalanced workload
caused by irregular sparsity of point data.
  To address the above challenges, we propose Voxel-CIM, an efficient
Compute-in-Memory based accelerator for voxel-based neural network processing.
To reduce off-chip memory access for map search, a depth-encoding-based output
major search approach is introduced to maximize data reuse, achieving stable
$O(N)$-level data access volume in various situations. Voxel-CIM also employs
the in-memory computing paradigm and designs innovative weight mapping
strategies to efficiently process Sparse 3D convolutions and 2D convolutions.
Implemented on 22 nm technology and evaluated on representative benchmarks, the
Voxel-CIM achieves averagely 4.5~7.0$\times$ higher energy efficiency (10.8
TOPS/w), and 2.4~5.4$\times$ speed up in detection task and 1.2~8.1$\times$
speed up in segmentation task compared to the state-of-the-art point cloud
accelerators and powerful GPUs.


## Localizing Memorization in SSL Vision Encoders

>Authors: Wenhao Wang, Adam Dziedzic, Michael Backes, Franziska Boenisch

>2024-09-27

> http://arxiv.org/abs/2409.19069v1

Recent work on studying memorization in self-supervised learning (SSL)
suggests that even though SSL encoders are trained on millions of images, they
still memorize individual data points. While effort has been put into
characterizing the memorized data and linking encoder memorization to
downstream utility, little is known about where the memorization happens inside
SSL encoders. To close this gap, we propose two metrics for localizing
memorization in SSL encoders on a per-layer (layermem) and per-unit basis
(unitmem). Our localization methods are independent of the downstream task, do
not require any label information, and can be performed in a forward pass. By
localizing memorization in various encoder architectures (convolutional and
transformer-based) trained on diverse datasets with contrastive and
non-contrastive SSL frameworks, we find that (1) while SSL memorization
increases with layer depth, highly memorizing units are distributed across the
entire encoder, (2) a significant fraction of units in SSL encoders experiences
surprisingly high memorization of individual data points, which is in contrast
to models trained under supervision, (3) atypical (or outlier) data points
cause much higher layer and unit memorization than standard data points, and
(4) in vision transformers, most memorization happens in the fully-connected
layers. Finally, we show that localizing memorization in SSL has the potential
to improve fine-tuning and to inform pruning strategies.


## Exploring Token Pruning in Vision State Space Models

>Authors: Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu, Zichong Meng, Hangyu Zheng, Xuan Shen, Stratis Ioannidis, Wei Niu, Pu Zhao, Yanzhi Wang

>2024-09-27

> http://arxiv.org/abs/2409.18962v1

State Space Models (SSMs) have the advantage of keeping linear computational
complexity compared to attention modules in transformers, and have been applied
to vision tasks as a new type of powerful vision foundation model. Inspired by
the observations that the final prediction in vision transformers (ViTs) is
only based on a subset of most informative tokens, we take the novel step of
enhancing the efficiency of SSM-based vision models through token-based
pruning. However, direct applications of existing token pruning techniques
designed for ViTs fail to deliver good performance, even with extensive
fine-tuning. To address this issue, we revisit the unique computational
characteristics of SSMs and discover that naive application disrupts the
sequential token positions. This insight motivates us to design a novel and
general token pruning method specifically for SSM-based vision models. We first
introduce a pruning-aware hidden state alignment method to stabilize the
neighborhood of remaining tokens for performance enhancement. Besides, based on
our detailed analysis, we propose a token importance evaluation method adapted
for SSM models, to guide the token pruning. With efficient implementation and
practical acceleration methods, our method brings actual speedup. Extensive
experiments demonstrate that our approach can achieve significant computation
reduction with minimal impact on performance across different tasks. Notably,
we achieve 81.7\% accuracy on ImageNet with a 41.6\% reduction in the FLOPs for
pruned PlainMamba-L3. Furthermore, our work provides deeper insights into
understanding the behavior of SSM-based vision models for future research.


## Mitigating Selection Bias with Node Pruning and Auxiliary Options

>Authors: Hyeong Kyu Choi, Weijie Xu, Chi Xue, Stephanie Eckman, Chandan K. Reddy

>2024-09-27

> http://arxiv.org/abs/2409.18857v1

Large language models (LLMs) often show unwarranted preference for certain
choice options when responding to multiple-choice questions, posing significant
reliability concerns in LLM-automated systems. To mitigate this selection bias
problem, previous solutions utilized debiasing methods to adjust the model's
input and/or output. Our work, in contrast, investigates the model's internal
representation of the selection bias. Specifically, we introduce a novel
debiasing approach, Bias Node Pruning (BNP), which eliminates the linear layer
parameters that contribute to the bias. Furthermore, we present Auxiliary
Option Injection (AOI), a simple yet effective input modification technique for
debiasing, which is compatible even with black-box LLMs. To provide a more
systematic evaluation of selection bias, we review existing metrics and
introduce Choice Kullback-Leibler Divergence (CKLD), which addresses the
insensitivity of the commonly used metrics to label imbalance. Experiments show
that our methods are robust and adaptable across various datasets when applied
to three LLMs.


## Asymptotic tracking control of dynamic reference over homomorphically encrypted data with finite modulus

>Authors: Shuai Feng, Junsoo Kim

>2024-09-27

> http://arxiv.org/abs/2409.18787v1

This paper considers a tracking control problem, in which the dynamic
controller is encrypted with an additively homomorphic encryption scheme and
the output of a process tracks a dynamic reference asymptotically. Our paper is
motivated by the following problem: When dealing with both asymptotic tracking
and dynamic reference, we find that the control input is generally subject to
overflow issues under a finite modulus, though the dynamic controller consists
of only integer coefficients. First, we provide a new controller design method
such that the coefficients of the tracking controller can be transformed into
integers leveraging the zooming-in factor of dynamic quantization.
  By the Cayley-Hamilton theorem, we represent the control input as linear
combination of the previous control inputs. Leveraging the property above, we
design an algorithm on the actuator side such that it can restore the control
input from the lower bits under a finite modulus. A lower bound of the modulus
is also provided.
  As an extension of the first result, we further solve the problem of
unbounded internal state taking place in the actuator. In particular, the
actuator can restore the correct control input under the same modulus.
  A simulation example is provided to verify the control schemes proposed in
our paper.


## Sparse Signal Recovery via $L_1/L_2$ Minimization Bound Theory and Complexity

>Authors: Min Tao, Xiao-Ping Zhang, Yun-Bin Zhao

>2024-09-27

> http://arxiv.org/abs/2409.18748v1

The \(L_1/L_2\) norm ratio has gained significant attention as a measure of
sparsity due to three primal advantages: sharper approximation to the \(L_0\)
norm compared to the widely-used \(L_1\) norm, being parameter-free and
scale-invariant, and exceptional performance with highly coherent matrices. In
this note, we establish uniform upper bounds in $L_2$ norm for any local
minimizer of constrained and unconstrained \(L_1/L_2\)-minimization models.
Furthermore, we derive some upper/lower bound for the magnitudes of nonzero
entries in any local minimizer of the unconstrained \(L_1/L_2\) minimization
problem. Moreover, we prove that finding the global minimum of both constrained
and unconstrained \(L_1/L_2\) models is strongly NP-Hard. Lastly, we point out
that finding the global minimum of constrained and unconstrained \(L_p\) (\(0 <
p \leq 1\)) over \(L_q\) (\(1 < q < +\infty\)) models is also strongly NP-Hard.


## Token Caching for Diffusion Transformer Acceleration

>Authors: Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, Chenguang Ma

>2024-09-27

> http://arxiv.org/abs/2409.18523v1

Diffusion transformers have gained substantial interest in diffusion
generative modeling due to their outstanding performance. However, their high
computational cost, arising from the quadratic computational complexity of
attention mechanisms and multi-step inference, presents a significant
bottleneck. To address this challenge, we propose TokenCache, a novel
post-training acceleration method that leverages the token-based multi-block
architecture of transformers to reduce redundant computations among tokens
across inference steps. TokenCache specifically addresses three critical
questions in the context of diffusion transformers: (1) which tokens should be
pruned to eliminate redundancy, (2) which blocks should be targeted for
efficient pruning, and (3) at which time steps caching should be applied to
balance speed and quality. In response to these challenges, TokenCache
introduces a Cache Predictor that assigns importance scores to tokens, enabling
selective pruning without compromising model performance. Furthermore, we
propose an adaptive block selection strategy to focus on blocks with minimal
impact on the network's output, along with a Two-Phase Round-Robin (TPRR)
scheduling policy to optimize caching intervals throughout the denoising
process. Experimental results across various models demonstrate that TokenCache
achieves an effective trade-off between generation quality and inference speed
for diffusion transformers. Our code will be publicly available.

