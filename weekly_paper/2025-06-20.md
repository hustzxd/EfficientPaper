# 2025-06-20

# Table of Contents
* [Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model](#Evolutionary-Caching-to-Accelerate-Your-Off-the-Shelf-Diffusion-Model)
* [Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention](#Automated-MRI-Tumor-Segmentation-using-hybrid-U-Net-with-Transformer-and-Efficient-Attention)
* [Intrinsic and Extrinsic Organized Attention Softmax Invariance and Network Sparsity](#Intrinsic-and-Extrinsic-Organized-Attention-Softmax-Invariance-and-Network-Sparsity)
* [GenHOI Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects](#GenHOI-Generalizing-Text-driven-4D-Human-Object-Interaction-Synthesis-for-Unseen-Objects)
* [Exploring Fast Fourier Transforms on the Tenstorrent Wormhole](#Exploring-Fast-Fourier-Transforms-on-the-Tenstorrent-Wormhole)
* [What is a good use case for quantum computers?](#What-is-a-good-use-case-for-quantum-computers?)
* [Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation](#Active-Learning-Guided-Seq2Seq-Variational-Autoencoder-for-Multi-target-Inhibitor-Generation)
* [New Physics Opportunities at Neutrino Facilities BSM Physics at Accelerator, Atmospheric, and Reactor Neutrino Experiments](#New-Physics-Opportunities-at-Neutrino-Facilities-BSM-Physics-at-Accelerator,-Atmospheric,-and-Reactor-Neutrino-Experiments)
* [Classical-quantum systems breaking conservation laws](#Classical-quantum-systems-breaking-conservation-laws)
* [Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models](#Human-Motion-Capture-from-Loose-and-Sparse-Inertial-Sensors-with-Garment-aware-Diffusion-Models)
* [Singular Value Decomposition on Kronecker Adaptation for Large Language Model](#Singular-Value-Decomposition-on-Kronecker-Adaptation-for-Large-Language-Model)
* [TopClustRAG at SIGIR 2025 LiveRAG Challenge](#TopClustRAG-at-SIGIR-2025-LiveRAG-Challenge)
* [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](#A-Novel-Compiler-Transformation-for-Fast-Sparse-Matrix-Multiplication-in-GPUs)
* [eLLM Elastic Memory Management Framework for Efficient LLM Serving](#eLLM-Elastic-Memory-Management-Framework-for-Efficient-LLM-Serving)
* [ChatModel Automating Reference Model Design and Verification with LLMs](#ChatModel-Automating-Reference-Model-Design-and-Verification-with-LLMs)
* [Truncated Proximal Policy Optimization](#Truncated-Proximal-Policy-Optimization)
* [Bosonic Spin-1 SOPHY](#Bosonic-Spin-1-SOPHY)
* [Scaling Intelligence Designing Data Centers for Next-Gen Language Models](#Scaling-Intelligence-Designing-Data-Centers-for-Next-Gen-Language-Models)
* [Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks](#Early-Prediction-of-Multiple-Sclerosis-Disability-Progression-via-Multimodal-Foundation-Model-Benchmarks)
* [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](#Massive-Supervised-Fine-tuning-Experiments-Reveal-How-Data,-Layer,-and-Training-Factors-Shape-LLM-Alignment-Quality)
* [250 Magnetic Tunnel Junctions-Based Probabilistic Ising Machine](#250-Magnetic-Tunnel-Junctions-Based-Probabilistic-Ising-Machine)
* [LingoLoop Attack Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](#LingoLoop-Attack-Trapping-MLLMs-via-Linguistic-Context-and-State-Entrapment-into-Endless-Loops)
* [Detecting fast-variation pulsations in solar hard X-ray and radio emissions](#Detecting-fast-variation-pulsations-in-solar-hard-X-ray-and-radio-emissions)
* [RAGtifier Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](#RAGtifier-Evaluating-RAG-Generation-Approaches-of-State-of-the-Art-RAG-Systems-for-the-SIGIR-LiveRAG-Competition)
* [Don't Make It Up Preserving Ignorance Awareness in LLM Fine-Tuning](#Don't-Make-It-Up-Preserving-Ignorance-Awareness-in-LLM-Fine-Tuning)
* [Compressed Video Super-Resolution based on Hierarchical Encoding](#Compressed-Video-Super-Resolution-based-on-Hierarchical-Encoding)
* [Tensor Manipulation Unit (TMU) Reconfigurable, Near-Memory Tensor Manipulation for High-Throughput AI SoC](#Tensor-Manipulation-Unit-(TMU)-Reconfigurable,-Near-Memory-Tensor-Manipulation-for-High-Throughput-AI-SoC)
* [Improving LoRA with Variational Learning](#Improving-LoRA-with-Variational-Learning)
* [High computational density nanophotonic media for machine learning inference](#High-computational-density-nanophotonic-media-for-machine-learning-inference)
* [Physics-Informed Neural Networks for the Korteweg-de Vries Equation for Internal Solitary Wave Problem Forward Simulation and Inverse Parameter Estimation](#Physics-Informed-Neural-Networks-for-the-Korteweg-de-Vries-Equation-for-Internal-Solitary-Wave-Problem-Forward-Simulation-and-Inverse-Parameter-Estimation)
* [S$^4$C Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](#S$^4$C-Speculative-Sampling-with-Syntactic-and-Semantic-Coherence-for-Efficient-Inference-of-Large-Language-Models)
* [Déjà Vu Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse](#Déjà-Vu-Efficient-Video-Language-Query-Engine-with-Learning-based-Inter-Frame-Computation-Reuse)
* [Transformers Learn Faster with Semantic Focus](#Transformers-Learn-Faster-with-Semantic-Focus)
* [Embedding physical symmetries into machine-learned reduced plasma physics models via data augmentation](#Embedding-physical-symmetries-into-machine-learned-reduced-plasma-physics-models-via-data-augmentation)
* [MultiFinBen A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](#MultiFinBen-A-Multilingual,-Multimodal,-and-Difficulty-Aware-Benchmark-for-Financial-LLM-Evaluation)
* [Beyond Black Boxes Enhancing Interpretability of Transformers Trained on Neural Data](#Beyond-Black-Boxes-Enhancing-Interpretability-of-Transformers-Trained-on-Neural-Data)
* [Taming Polysemanticity in LLMs Provable Feature Recovery via Sparse Autoencoders](#Taming-Polysemanticity-in-LLMs-Provable-Feature-Recovery-via-Sparse-Autoencoders)
* [HierVL Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](#HierVL-Semi-Supervised-Segmentation-leveraging-Hierarchical-Vision-Language-Synergy-with-Dynamic-Text-Spatial-Query-Alignment)
* [Discrete Diffusion in Large Language and Multimodal Models A Survey](#Discrete-Diffusion-in-Large-Language-and-Multimodal-Models-A-Survey)
* [Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs](#Attribution-guided-Pruning-for-Compression,-Circuit-Discovery,-and-Targeted-Correction-in-LLMs)
* [CAMS A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation](#CAMS-A-CityGPT-Powered-Agentic-Framework-for-Urban-Human-Mobility-Simulation)
* [Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization](#Mixture-of-Weight-shared-Heterogeneous-Group-Attention-Experts-for-Dynamic-Token-wise-KV-Optimization)
* [ROSAQ Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models](#ROSAQ-Rotation-based-Saliency-Aware-Weight-Quantization-for-Efficiently-Compressing-Large-Language-Models)
* [Block-wise Adaptive Caching for Accelerating Diffusion Policy](#Block-wise-Adaptive-Caching-for-Accelerating-Diffusion-Policy)
* [Delving Into the Psychology of Machines Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses](#Delving-Into-the-Psychology-of-Machines-Exploring-the-Structure-of-Self-Regulated-Learning-via-LLM-Generated-Survey-Responses)
* [DicFace Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration](#DicFace-Dirichlet-Constrained-Variational-Codebook-Learning-for-Temporally-Coherent-Video-Face-Restoration)
* [GreedyPrune Retenting Critical Visual Token Set for Large Vision Language Models](#GreedyPrune-Retenting-Critical-Visual-Token-Set-for-Large-Vision-Language-Models)
* [Thermodynamics of black and white holes in ensemble of Planckons](#Thermodynamics-of-black-and-white-holes-in-ensemble-of-Planckons)
* [EnhanceGraph A Continuously Enhanced Graph-based Index for High-dimensional Approximate Nearest Neighbor Search](#EnhanceGraph-A-Continuously-Enhanced-Graph-based-Index-for-High-dimensional-Approximate-Nearest-Neighbor-Search)
* [STAGE A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation](#STAGE-A-Stream-Centric-Generative-World-Model-for-Long-Horizon-Driving-Scene-Simulation)
* [AlphaEvolve A coding agent for scientific and algorithmic discovery](#AlphaEvolve-A-coding-agent-for-scientific-and-algorithmic-discovery)
* [Multipole Attention for Efficient Long Context Reasoning](#Multipole-Attention-for-Efficient-Long-Context-Reasoning)
* [NeuVAS Neural Implicit Surfaces for Variational Shape Modeling](#NeuVAS-Neural-Implicit-Surfaces-for-Variational-Shape-Modeling)
* [DETRPose Real-time end-to-end transformer model for multi-person pose estimation](#DETRPose-Real-time-end-to-end-transformer-model-for-multi-person-pose-estimation)
* [Combining Fault Tolerance Techniques and COTS SoC Accelerators for Payload Processing in Space](#Combining-Fault-Tolerance-Techniques-and-COTS-SoC-Accelerators-for-Payload-Processing-in-Space)
* [MaskPro Linear-Space Probabilistic Learning for Strict (NM)-Sparsity on Large Language Models](#MaskPro-Linear-Space-Probabilistic-Learning-for-Strict-(NM)-Sparsity-on-Large-Language-Models)
* [EraserDiT Fast Video Inpainting with Diffusion Transformer Model](#EraserDiT-Fast-Video-Inpainting-with-Diffusion-Transformer-Model)
* [A Review of the Long Horizon Forecasting Problem in Time Series Analysis](#A-Review-of-the-Long-Horizon-Forecasting-Problem-in-Time-Series-Analysis)
* [SP-VLA A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration](#SP-VLA-A-Joint-Model-Scheduling-and-Token-Pruning-Approach-for-VLA-Model-Acceleration)
* [Serving Large Language Models on Huawei CloudMatrix384](#Serving-Large-Language-Models-on-Huawei-CloudMatrix384)
* [Get on the Train or be Left on the Station Using LLMs for Software Engineering Research](#Get-on-the-Train-or-be-Left-on-the-Station-Using-LLMs-for-Software-Engineering-Research)
* [Watermarking Quantum Neural Networks Based on Sample Grouped and Paired Training](#Watermarking-Quantum-Neural-Networks-Based-on-Sample-Grouped-and-Paired-Training)
* [Social Media Reactions to Open Source Promotions AI-Powered GitHub Projects on Hacker News](#Social-Media-Reactions-to-Open-Source-Promotions-AI-Powered-GitHub-Projects-on-Hacker-News)
* [Towards Neural Audio Codec Source Parsing](#Towards-Neural-Audio-Codec-Source-Parsing)
* [OpenUnlearning Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics](#OpenUnlearning-Accelerating-LLM-Unlearning-via-Unified-Benchmarking-of-Methods-and-Metrics)
* [An Exploration of Mamba for Speech Self-Supervised Models](#An-Exploration-of-Mamba-for-Speech-Self-Supervised-Models)
* [Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts](#Automatic-Expert-Discovery-in-LLM-Upcycling-via-Sparse-Interpolated-Mixture-of-Experts)
* [GNSS Spoofing Detection Based on Opportunistic Position Information](#GNSS-Spoofing-Detection-Based-on-Opportunistic-Position-Information)
* [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](#Enabling-Precise-Topic-Alignment-in-Large-Language-Models-Via-Sparse-Autoencoders)
* [BSA Ball Sparse Attention for Large-scale Geometries](#BSA-Ball-Sparse-Attention-for-Large-scale-Geometries)
* [Quantizing Small-Scale State-Space Models for Edge AI](#Quantizing-Small-Scale-State-Space-Models-for-Edge-AI)
* [Recent Advances and Future Directions in Literature-Based Discovery](#Recent-Advances-and-Future-Directions-in-Literature-Based-Discovery)
* [Training-free LLM Merging for Multi-task Learning](#Training-free-LLM-Merging-for-Multi-task-Learning)
* [QiMeng-Attention SOTA Attention Operator is generated by SOTA Attention Algorithm](#QiMeng-Attention-SOTA-Attention-Operator-is-generated-by-SOTA-Attention-Algorithm)
* [GroupNL Low-Resource and Robust CNN Design over Cloud and Device](#GroupNL-Low-Resource-and-Robust-CNN-Design-over-Cloud-and-Device)
* [The Budget AI Researcher and the Power of RAG Chains](#The-Budget-AI-Researcher-and-the-Power-of-RAG-Chains)
* [Two heads are better than one simulating large transformers with small ones](#Two-heads-are-better-than-one-simulating-large-transformers-with-small-ones)
* [How Visual Representations Map to Language Feature Space in Multimodal LLMs](#How-Visual-Representations-Map-to-Language-Feature-Space-in-Multimodal-LLMs)
* [Beyond Homogeneous Attention Memory-Efficient LLMs via Fourier-Approximated KV Cache](#Beyond-Homogeneous-Attention-Memory-Efficient-LLMs-via-Fourier-Approximated-KV-Cache)
* [Structural Similarity-Inspired Unfolding for Lightweight Image Super-Resolution](#Structural-Similarity-Inspired-Unfolding-for-Lightweight-Image-Super-Resolution)
* [Learning to Integrate](#Learning-to-Integrate)
* [GPLQ A General, Practical, and Lightning QAT Method for Vision Transformers](#GPLQ-A-General,-Practical,-and-Lightning-QAT-Method-for-Vision-Transformers)
* [MambaVSR Content-Aware Scanning State Space Model for Video Super-Resolution](#MambaVSR-Content-Aware-Scanning-State-Space-Model-for-Video-Super-Resolution)
* [DeepResearch Bench A Comprehensive Benchmark for Deep Research Agents](#DeepResearch-Bench-A-Comprehensive-Benchmark-for-Deep-Research-Agents)
* [Fusion of multi-source precipitation records via coordinate-based generative model](#Fusion-of-multi-source-precipitation-records-via-coordinate-based-generative-model)
* [FieldFormer Self-supervised Reconstruction of Physical Fields via Tensor Attention Prior](#FieldFormer-Self-supervised-Reconstruction-of-Physical-Fields-via-Tensor-Attention-Prior)
* [SecONNds Secure Outsourced Neural Network Inference on ImageNet](#SecONNds-Secure-Outsourced-Neural-Network-Inference-on-ImageNet)
* [FIMA-Q Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation](#FIMA-Q-Post-Training-Quantization-for-Vision-Transformers-by-Fisher-Information-Matrix-Approximation)
* [Vectorized Sparse Second-Order Forward Automatic Differentiation for Optimal Control Direct Methods](#Vectorized-Sparse-Second-Order-Forward-Automatic-Differentiation-for-Optimal-Control-Direct-Methods)
* [Lag-Relative Sparse Attention In Long Context Training](#Lag-Relative-Sparse-Attention-In-Long-Context-Training)
* [SemanticST Spatially Informed Semantic Graph Learning for Clustering, Integration, and Scalable Analysis of Spatial Transcriptomics](#SemanticST-Spatially-Informed-Semantic-Graph-Learning-for-Clustering,-Integration,-and-Scalable-Analysis-of-Spatial-Transcriptomics)
* [Agent-RLVR Training Software Engineering Agents via Guidance and Environment Rewards](#Agent-RLVR-Training-Software-Engineering-Agents-via-Guidance-and-Environment-Rewards)
* [Efficient Long-Context LLM Inference via KV Cache Clustering](#Efficient-Long-Context-LLM-Inference-via-KV-Cache-Clustering)


## Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model

>Authors: Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam

>2025-06-18

> http://arxiv.org/abs/2506.15682v1

Diffusion-based image generation models excel at producing high-quality
synthetic content, but suffer from slow and computationally expensive
inference. Prior work has attempted to mitigate this by caching and reusing
features within diffusion transformers across inference steps. These methods,
however, often rely on rigid heuristics that result in limited **acceleration** or
poor generalization across architectures. We propose Evolutionary Caching to
Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,
per-model, caching schedules forming a Pareto frontier, using only a small set
of calibration prompts. ECAD requires no modifications to network parameters or
reference images. It offers significant inference speedups, enables
fine-grained control over the quality-latency trade-off, and adapts seamlessly
to different diffusion models. Notably, ECAD's learned schedules can generalize
effectively to resolutions and model variants not seen during calibration. We
evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple
metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,
PartiPrompts), demonstrating consistent improvements over previous approaches.
On PixArt-alpha, ECAD identifies a schedule that outperforms the previous
state-of-the-art method by 4.47 COCO FID while increasing inference speedup
from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable
approach for accelerating diffusion inference. Our project website is available
at https://aniaggarwal.github.io/ecad and our code is available at
https://github.com/aniaggarwal/ecad.


## Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention

>Authors: Syed Haider Ali, Asrar Ahmad, Muhammad Ali, Asifullah Khan, Muhammad Shahban, Nadeem Shaukat

>2025-06-18

> http://arxiv.org/abs/2506.15562v1

Cancer is an abnormal growth with potential to invade locally and metastasize
to distant organs. Accurate auto-segmentation of the tumor and surrounding
normal tissues is required for radiotherapy treatment plan optimization. Recent
AI-based segmentation models are generally trained on large public datasets,
which lack the heterogeneity of local patient populations. While these studies
advance AI-based medical image segmentation, research on local datasets is
necessary to develop and integrate AI tumor segmentation models directly into
hospital software for efficient and accurate oncology treatment planning and
execution. This study enhances tumor segmentation using computationally
efficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)
datasets acquired from a local hospital under strict privacy protection. We
developed a robust data pipeline for seamless DICOM extraction and
preprocessing, followed by extensive image augmentation to ensure model
generalization across diverse clinical settings, resulting in a total dataset
of 6080 images for training. Our novel architecture integrates UNet-based
convolutional neural networks with a transformer bottleneck and complementary
attention modules, including efficient attention, Squeeze-and-Excitation (SE)
blocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To
accelerate convergence and reduce computational demands, we used a maximum
batch size of 8 and initialized the encoder with pretrained ImageNet weights,
training the model on dual NVIDIA T4 GPUs via checkpointing to overcome
Kaggle's runtime limits. Quantitative evaluation on the local MRI dataset
yielded a Dice similarity coefficient of 0.764 and an Intersection over Union
(IoU) of 0.736, demonstrating competitive performance despite limited data and
underscoring the importance of site-specific model development for clinical
deployment.


## Intrinsic and Extrinsic Organized Attention Softmax Invariance and Network Sparsity

>Authors: Oluwadamilola Fasina, Ruben V. C. Pohle, Pei-Chun Su, Ronald R. Coifman

>2025-06-18

> http://arxiv.org/abs/2506.15541v1

We examine the intrinsic (within the attention head) and extrinsic (amongst
the attention heads) structure of the self-attention mechanism in transformers.
Theoretical evidence for invariance of the self-attention mechanism to softmax
activation is obtained by appealing to paradifferential calculus, (and is
supported by computational examples), which relies on the intrinsic
organization of the attention heads. Furthermore, we use an existing
methodology for hierarchical organization of tensors to examine network
structure by constructing hierarchal partition trees with respect to the query,
key, and head axes of network 3-tensors. Such an organization is consequential
since it allows one to profitably execute common signal processing tasks on a
geometry where the organized network 3-tensors exhibit regularity. We exemplify
this qualitatively, by visualizing the hierarchical organization of the tree
comprised of attention heads and the diffusion map embeddings, and
quantitatively by investigating network **sparsity** with the expansion
coefficients of individual attention heads and the entire network with respect
to the bi and tri-haar bases (respectively) on the space of queries, keys, and
heads of the network. To showcase the utility of our theoretical and
methodological findings, we provide computational examples using vision and
language transformers. The ramifications of these findings are two-fold: (1) a
subsequent step in interpretability analysis is theoretically admitted, and can
be exploited empirically for downstream interpretability tasks (2) one can use
the network 3-tensor organization for empirical network applications such as
model **pruning** (by virtue of network **sparsity**) and network architecture
comparison.


## GenHOI Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects

>Authors: Shujia Li, Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Yutong Ban

>2025-06-18

> http://arxiv.org/abs/2506.15483v1

While diffusion models and large-scale motion datasets have advanced
text-driven human motion synthesis, extending these advances to 4D human-object
interaction (HOI) remains challenging, mainly due to the limited availability
of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel
two-stage framework aimed at achieving two key objectives: 1) generalization to
unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the
initial stage of our framework, we employ an Object-AnchorNet to reconstruct
**sparse** 3D HOI keyframes for unseen objects, learning solely from 3D HOI
datasets, thereby mitigating the dependence on large-scale 4D HOI datasets.
Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the
second stage to seamlessly interpolate **sparse** 3D HOI keyframes into densely
temporally coherent 4D HOI sequences. To enhance the quality of generated 4D
HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to
extract human-object contact patterns and a novel Contact-Aware HOI Attention
to effectively integrate the contact signals into diffusion models.
Experimental results show that we achieve state-of-the-art results on the
publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong
generalization abilities to unseen objects, while enabling high-fidelity 4D HOI
generation.


## Exploring Fast Fourier Transforms on the Tenstorrent Wormhole

>Authors: Nick Brown, Jake Davies, Felix LeClair

>2025-06-18

> http://arxiv.org/abs/2506.15437v1

Whilst numerous areas of computing have adopted the RISC-V Instruction Set
Architecture (ISA) wholesale in recent years, it is yet to become widespread in
HPC. RISC-V accelerators offer a compelling option where the HPC community can
benefit from the specialisation offered by the open nature of the standard but
without the extensive ecosystem changes required when adopting RISC-V CPUs. In
this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)
algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon
Tenstorrent's Tensix architecture, this technology decouples the movement of
data from compute, potentially offering increased control to the programmer.
Exploring different optimisation techniques to address the bottlenecks inherent
in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is
slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around
8 times less power and consumes around 2.8 times less energy than the CPU when
computing the Fourier transform.


## What is a good use case for quantum computers?

>Authors: Michael Marthaler, Peter Pinski, Pascal Stadler, Vladimir Rybkin, Marina Walt

>2025-06-18

> http://arxiv.org/abs/2506.15426v1

Identify, Transform, Benchmark, Show Quantum Advantage (ITBQ): Evaluating use
cases for quantum computers. We introduce a four-step framework for assessing
quantum computing applications -- from identifying relevant industry problems
to demonstrating quantum advantage -- addressing steps often overlooked in the
literature, such as rigorous benchmarking against classical solutions and the
challenge of translating real-world tasks onto quantum hardware. Applying this
framework to cases like NMR, multireference chemistry, and radicals reveals
both significant opportunities and key barriers on the path to practical
advantage. Our results highlight the need for transparent, structured criteria
to focus research, guide investment, and accelerate meaningful quantum
progress.


## Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation

>Authors: Júlia Vilalta-Mor, Alexis Molina, Laura Ortega Varga, Isaac Filella-Merce, Victor Guallar

>2025-06-18

> http://arxiv.org/abs/2506.15309v1

Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to **sparse**
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the **sparse**-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.


## New Physics Opportunities at Neutrino Facilities BSM Physics at Accelerator, Atmospheric, and Reactor Neutrino Experiments

>Authors: Koun Choi, Doojin Kim, Jong-Chul Park, Seodong Shin, Pouya Bakhti, Ki-Young Choi, Chang Hyon Ha, Kazumi Hata, Wooyoung Jang, Yu Seon Jeong, Young Ju Ko, Hyun Su Lee, Weijun Li, Yu-Feng Li, Mehedi Masud, Kenny C. Y. Ng, Jungsic Park, Min-Gwa Park, Komninos-John Plows, Meshkat Rajaee, Eunil Won, Byeongsu Yang, Seong Moon Yoo, Jaehoon Yu, Seokhoon Yun

>2025-06-18

> http://arxiv.org/abs/2506.15306v1

Since the discovery of the Higgs boson, the long-standing task at hand in
particle physics is the search for new physics beyond the Standard Model, which
accounts for only about 5\% of the Universe.
  In light of this situation, the neutrino sector has drawn significant
attention due to neutrino oscillations, which require physics beyond the
Standard Model and have prompted a wide array of active and planned
experimental programs.
  Notably, neutrino facilities offer substantial potential to search for new
physics beyond neutrino oscillations, owing to their precision measurement
capabilities, diverse experimental configurations, and various neutrino
sources.
  This paper provides a review of the landscape of new physics that can be
probed at current and future neutrino experiments, categorized into
laboratory-produced and cosmogenic signals.
  We discuss recent experimental results interpreted through the lens of new
physics, as well as detailed plans and projected sensitivities of
next-generation facilities.
  This review is based on presentations from the 4th Workshop on New Physics
Opportunities in Neutrino Facilities (NPN 2024), held at IBS in Daejeon, Korea,
on June 3-5, 2024.
  Particular emphasis is placed on accelerator-based neutrino experiments and a
range of neutrino programs in East Asia.
  We also outline key tasks necessary to realize the promising new physics
opportunities ahead.


## Classical-quantum systems breaking conservation laws

>Authors: Masahiro Hotta, Sebastian Murk, Daniel R. Terno

>2025-06-18

> http://arxiv.org/abs/2506.15291v1

Whether gravity must be **quantize**d remains one of the biggest open problems in
fundamental physics. Classical-quantum hybrid theories have recently attracted
attention as a possible framework in which gravity is treated classically yet
interacts consistently with quantum matter. Schemes based on completely
positive dynamics satisfy most formal consistency requirements and enable a
systematic treatment of quantum backreaction, but they also give rise to
features that challenge conventional physical intuition, such as the breakdown
of conservation laws. To illustrate this issue, we consider a qubit interacting
with a classical particle and demonstrate that the corresponding hybrid system
violates angular momentum conservation despite the rotational symmetry of the
underlying equations of motion.


## Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models

>Authors: Andela Ilic, Jiaxi Jiang, Paul Streli, Xintong Liu, Christian Holz

>2025-06-18

> http://arxiv.org/abs/2506.15290v1

Motion capture using **sparse** inertial sensors has shown great promise due to
its portability and lack of occlusion issues compared to camera-based tracking.
Existing approaches typically assume that IMU sensors are tightly attached to
the human body. However, this assumption often does not hold in real-world
scenarios. In this paper, we present a new task of full-body human pose
estimation using **sparse**, loosely attached IMU sensors. To solve this task, we
simulate IMU recordings from an existing garment-aware human motion dataset. We
developed transformer-based diffusion models to synthesize loose IMU data and
estimate human poses based on this challenging loose IMU data. In addition, we
show that incorporating garment-related parameters while training the model on
simulated loose data effectively maintains expressiveness and enhances the
ability to capture variations introduced by looser or tighter garments.
Experiments show that our proposed diffusion methods trained on simulated and
synthetic data outperformed the state-of-the-art methods quantitatively and
qualitatively, opening up a promising direction for future research.


## Singular Value Decomposition on Kronecker Adaptation for Large Language Model

>Authors: Yee Hin Chong, Peng Qu

>2025-06-18

> http://arxiv.org/abs/2506.15251v1

Large pre-trained Transformer models achieve state-of-the-art results across
diverse language and reasoning tasks, but full fine-tuning incurs substantial
storage, memory, and computational overhead. Parameter-efficient fine-tuning
(PEFT) methods mitigate these costs by learning only a small subset of
task-specific parameters, yet existing approaches either introduce
inference-time latency (adapter modules), suffer from suboptimal convergence
(randomly initialized low-rank updates), or rely on fixed rank choices that may
not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that
combines Kronecker-product tensor factorization with SVD-driven initialization
and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)
procedure extracts principal components of the full weight update into compact
Kronecker factors, while an adaptive rank selection algorithm uses
energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal
mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires
only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or
exceeding baseline performance. Moreover, SoKA exhibits faster convergence and
more stable gradients, highlighting its robustness and efficiency for
large-scale model adaptation.


## TopClustRAG at SIGIR 2025 LiveRAG Challenge

>Authors: Juli Bakagianni, John Pavlopoulos, Aristidis Likas

>2025-06-18

> http://arxiv.org/abs/2506.15246v1

We present TopClustRAG, a retrieval-augmented generation (RAG) system
developed for the LiveRAG Challenge, which evaluates end-to-end question
answering over large-scale web corpora. Our system employs a hybrid retrieval
strategy combining **sparse** and dense indices, followed by K-Means clustering to
group semantically similar passages. Representative passages from each cluster
are used to construct cluster-specific prompts for a large language model
(LLM), generating intermediate answers that are filtered, reranked, and finally
synthesized into a single, comprehensive response. This multi-stage pipeline
enhances answer diversity, relevance, and faithfulness to retrieved evidence.
Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in
faithfulness and 7th in correctness on the official leaderboard, demonstrating
the effectiveness of clustering-based context filtering and prompt aggregation
in large-scale RAG systems.


## A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs

>Authors: Hossein Albakri, Kazem Cheshmi

>2025-06-18

> http://arxiv.org/abs/2506.15174v1

Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-**sparse**-coarsen, that accelerates **sparse**
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on **sparse**
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.


## eLLM Elastic Memory Management Framework for Efficient LLM Serving

>Authors: Jiale Xu, Rui Zhang, Yi Xiong, Cong Guo, Zihan Liu, Yangjie Zhou, Weiming Hu, Hao Wu, Changxu Shao, Ziqing Wang, Yongjie Yuan, Junping Zhao, Minyi Guo, Jingwen Leng

>2025-06-18

> http://arxiv.org/abs/2506.15155v1

Large Language Models are increasingly being deployed in datacenters. Serving
these models requires careful memory management, as their memory usage includes
static weights, dynamic activations, and key-value caches. While static weights
are constant and predictable, dynamic components such as activations and **KV**
caches change frequently during runtime, presenting significant challenges for
efficient memory management. Modern LLM serving systems typically handle
runtime memory and **KV** caches at distinct abstraction levels: runtime memory
management relies on static tensor abstractions, whereas **KV** caches utilize a
page table-based virtualization layer built on top of the tensor abstraction.
This virtualization dynamically manages **KV** caches to mitigate memory
fragmentation. However, this dual-level approach fundamentally isolates runtime
memory and **KV** cache management, resulting in suboptimal memory utilization
under dynamic workloads, which can lead to a nearly 20% drop in throughput.
  To address these limitations, we propose eLLM, an elastic memory management
framework inspired by the classical memory ballooning mechanism in operating
systems. The core components of eLLM include: (1) Virtual Tensor Abstraction,
which decouples the virtual address space of tensors from the physical GPU
memory, creating a unified and flexible memory pool; (2) an Elastic Memory
Mechanism that dynamically adjusts memory allocation through runtime memory
inflation and deflation, leveraging CPU memory as an extensible buffer; and (3)
a Lightweight Scheduling Strategy employing SLO-aware policies to optimize
memory utilization and effectively balance performance trade-offs under
stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM
significantly outperforms state-of-the-art systems, 2.32x higher decoding
throughput, and supporting 3x larger batch sizes for 128K-token inputs.


## ChatModel Automating Reference Model Design and Verification with LLMs

>Authors: Jianmin Ye, Tianyang Liu, Qi Tian, Shengchu Su, Zhe Jiang, Xi Wang

>2025-06-18

> http://arxiv.org/abs/2506.15066v1

As the complexity of integrated circuit designs continues to escalate, the
functional verification becomes increasingly challenging. Reference models,
critical for accelerating the verification process, are themselves becoming
more intricate and time-consuming to develop. Despite the promise shown by
large language models (LLMs) in code programming, effectively generating
complex reference models remains a significant hurdle. To address these
challenges, we introduce ChatModel, the first LLM-aided agile reference model
generation and verification platform. ChatModel streamlines the transition from
design specifications to fully functional reference models by integrating
design standardization and hierarchical agile modeling. Employing a
building-block generation strategy, it not only enhances the design
capabilities of LLMs for reference models but also significantly boosts
verification efficiency. We evaluated ChatModel on 300 designs of varying
complexity, demonstrating substantial improvements in both efficiency and
quality of reference model generation. ChatModel achieved a peak performance
improvement of 55.02% compared to alternative methods, with notable
enhancements in generation stability, and delivered a 9.18x increase in its
capacity to produce reference model designs. Furthermore, it accelerated the
iterative process of reference model design and validation by an average of
5.90x compared to traditional approaches. These results highlight the potential
of ChatModel to significantly advance the automation of reference model
generation and validation.


## Truncated Proximal Policy Optimization

>Authors: Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu

>2025-06-18

> http://arxiv.org/abs/2506.15050v1

Recently, test-time scaling Large Language Models (LLMs) have demonstrated
exceptional reasoning capabilities across scientific and professional tasks by
generating long chains-of-thought (CoT). As a crucial component for developing
these reasoning models, reinforcement learning (RL), exemplified by Proximal
Policy Optimization (PPO) and its variants, allows models to learn through
trial and error. However, PPO can be time-consuming due to its inherent
on-policy nature, which is further exacerbated by increasing response lengths.
In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a
novel extension to PPO that improves training efficiency by streamlining policy
update and length-restricted response generation. T-PPO mitigates the issue of
low hardware utilization, an inherent drawback of fully synchronized
long-generation procedures, where resources often sit idle during the waiting
periods for complete rollouts. Our contributions are two-folds. First, we
propose Extended Generalized Advantage Estimation (EGAE) for advantage
estimation derived from incomplete responses while maintaining the integrity of
policy learning. Second, we devise a computationally optimized mechanism that
allows for the independent optimization of the policy and value models. By
selectively filtering prompt and truncated tokens, this mechanism reduces
redundant computations and accelerates the training process without sacrificing
convergence performance. We demonstrate the effectiveness and efficacy of T-PPO
on AIME 2024 with a 32B base model. The experimental results show that T-PPO
improves the training efficiency of reasoning LLMs by up to 2.5x and
outperforms its existing competitors.


## Bosonic Spin-1 SOPHY

>Authors: Armando de la C. Rangel-Pantoja, I. Díaz-Saldaña, Carlos A. Vaquera-Araujo

>2025-06-17

> http://arxiv.org/abs/2506.15017v1

In this work we study the canonical **quantization** of a second-order
pseudo-Hermitian field theory for massive spin-1 bosons transforming under the
$(1,0)\oplus(0,1)$ representation of the restricted Lorentz Group and
satisfying the Klein-Gordon equation.


## Scaling Intelligence Designing Data Centers for Next-Gen Language Models

>Authors: Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini

>2025-06-17

> http://arxiv.org/abs/2506.15006v1

The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8
trillion parameters - demands a radical rethinking of data center architecture
to ensure scalability, efficiency, and cost-effectiveness. Our work provides a
comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth
and capacity, multiple network topologies (two-tier vs. FullFlat optical), the
size of the scale-out domain, and popular parallelism/optimization strategies
used in LLMs. We introduce and evaluate FullFlat network architectures, which
provide uniform high-bandwidth, low-latency connectivity between all nodes, and
demonstrate their transformative impact on performance and scalability. Through
detailed sensitivity analyses, we quantify the benefits of overlapping compute
and communication, leveraging hardware-accelerated collectives, wider scale-out
domains, and larger memory capacity. Our study spans both **sparse** (mixture of
experts) and dense transformer-based LLMs, revealing how system design choices
affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens
per sec / Peak flops of the hardware) and overall throughput. For the co-design
study, we extended and validated a performance modeling tool capable of
predicting LLM runtime within 10% of real-world measurements. Our findings
offer actionable insights and a practical roadmap for designing AI data centers
that can efficiently support trillion-parameter models, reduce optimization
complexity, and sustain the rapid evolution of AI capabilities.


## Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks

>Authors: Maxime Usdin, Lito Kriara, Licinio Craveiro

>2025-06-17

> http://arxiv.org/abs/2506.14986v1

Early multiple sclerosis (MS) disability progression prediction is
challenging due to disease heterogeneity. This work predicts 48- and 72-week
disability using **sparse** baseline clinical data and 12 weeks of daily digital
Floodlight data from the CONSONANCE clinical trial. We employed
state-of-the-art tabular and time-series foundation models (FMs), a custom
multimodal attention-based transformer, and machine learning methods. Despite
the difficulty of early prediction (AUROC 0.63), integrating digital data via
advanced models improved performance over clinical data alone. A transformer
model using unimodal embeddings from the Moment FM yielded the best result, but
our multimodal transformer consistently outperformed its unimodal counterpart,
confirming the advantages of combining clinical with digital data. Our findings
demonstrate the promise of FMs and multimodal approaches to extract predictive
signals from complex and diverse clinical and digital life sciences data (e.g.,
imaging, omics), enabling more accurate prognostics for MS and potentially
other complex diseases.


## Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality

>Authors: Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi

>2025-06-17

> http://arxiv.org/abs/2506.14681v1

Supervised fine-tuning (SFT) is a critical step in aligning large language
models (LLMs) with human instructions and values, yet many aspects of SFT
remain poorly understood. We trained a wide range of base models on a variety
of datasets including code generation, mathematical reasoning, and
general-domain tasks, resulting in 1,000+ SFT models under controlled
conditions. We then identified the dataset properties that matter most and
examined the layer-wise modifications introduced by SFT. Our findings reveal
that some training-task synergies persist across all models while others vary
substantially, emphasizing the importance of model-specific strategies.
Moreover, we demonstrate that perplexity consistently predicts SFT
effectiveness--often surpassing superficial similarity between trained data and
benchmark--and that mid-layer weight changes correlate most strongly with
performance gains. We will release these 1,000+ SFT models and benchmark
results to accelerate further research.


## 250 Magnetic Tunnel Junctions-Based Probabilistic Ising Machine

>Authors: Shuhan Yang, Andrea Grimaldi, Youwei Bao, Eleonora Raimondo, Jia Si, Giovanni Finocchio, Hyunsoo Yang

>2025-06-17

> http://arxiv.org/abs/2506.14590v1

In combinatorial optimization, probabilistic Ising machines (PIMs) have
gained significant attention for their **acceleration** of Monte Carlo sampling
with the potential to reduce time-to-solution in finding approximate ground
states. However, to be viable in real applications, further improvements in
scalability and energy efficiency are necessary. One of the promising paths
toward achieving this objective is the development of a co-design approach
combining different technology layers including device, circuits and
algorithms. Here, we experimentally demonstrate a fully connected PIM
architecture based on 250 spin-transfer torque magnetic tunnel junctions
(STT-MTJs), interfaced with an FPGA. Our computing approach integrates
STT-MTJ-based tunable true random number generators with advanced annealing
techniques, enabling the solution of problems with any topology and size. For
**sparse**ly connected graphs, the massive parallel architecture of our PIM enables
a cluster parallel update method that overcomes the serial limitations of Gibbs
sampling, leading to a 10 times **acceleration** without hardware changes.
Furthermore, we prove experimentally that the simulated quantum annealing
boosts solution quality 20 times over conventional simulated annealing while
also increasing robustness to MTJ variability. Short pulse switching
measurements indicate that STT-MTJ-based PIMs can potentially be 10 times
faster and 10 times more energy-efficient than graphic processing units, which
paves the way for future large-scale, high-performance, and energy-efficient
unconventional computing hardware implementations.


## LingoLoop Attack Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops

>Authors: Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, Wenqiang Zhang

>2025-06-17

> http://arxiv.org/abs/2506.14493v1

Multimodal Large Language Models (MLLMs) have shown great promise but require
substantial computational resources during inference. Attackers can exploit
this by inducing excessive output, leading to resource exhaustion and service
degradation. Prior energy-latency attacks aim to increase generation time by
broadly shifting the output token distribution away from the EOS token, but
they neglect the influence of token-level Part-of-Speech (POS) characteristics
on EOS and sentence-level structural patterns on output counts, limiting their
efficacy. To address this, we propose LingoLoop, an attack designed to induce
MLLMs to generate excessively verbose and repetitive sequences. First, we find
that the POS tag of a token strongly affects the likelihood of generating an
EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to
postpone EOS token generation by adjusting attention weights guided by POS
information. Second, we identify that constraining output diversity to induce
repetitive loops is effective for sustained generation. We introduce a
Generative Path Pruning Mechanism that limits the magnitude of hidden states,
encouraging the model to produce persistent loops. Extensive experiments
demonstrate LingoLoop can increase generated tokens by up to 30 times and
energy consumption by a comparable factor on models like Qwen2.5-VL-3B,
consistently driving MLLMs towards their maximum generation limits. These
findings expose significant MLLMs' vulnerabilities, posing challenges for their
reliable deployment. The code will be released publicly following the paper's
acceptance.


## Detecting fast-variation pulsations in solar hard X-ray and radio emissions

>Authors: Dong Li

>2025-06-17

> http://arxiv.org/abs/2506.14433v1

Quasi-periodic pulsations (QPPs) at sub-second periods are frequently
detected in the time series of X-rays during stellar flares. However, such
rapid pulsations are rarely reported in the hard X-ray (HXR) emission of the
small solar flare. We explored the QPP patterns with fast-time variations in
HXR and radio emissions produced in a small solar flare on 2025 January 19. By
applying the Fast Fourier Transform, the fast-variation pulsations at a
quasi-period of about 1 s are identified in the HXR channel of 20-80 keV, which
were simultaneously measured by the Hard X-ray Imager and the Konus-Wind. The
rapid pulsations with a same quasi-period were also detected in the radio
emission at a lower frequency range of about 40-100 MHz. The restructured HXR
images show that the QPP patterns mainly locate in footpoint areas that connect
by hot plasma loops, and they appear in the flare impulsive phase. Our
observations suggest that the fast-variation pulsations could be associated
with nonthermal electrons that are periodically accelerated by the intermittent
magnetic reconnection, and the 1-s period may be modulated by the coalescence
instability between current-carrying loops and magnetic islands.


## RAGtifier Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition

>Authors: Tim Cofala, Oleh Astappiev, William Xion, Hailay Teklehaymanot

>2025-06-17

> http://arxiv.org/abs/2506.14412v1

Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by
combining their internal, parametric knowledge with external, non-parametric
sources, with the goal of improving factual correctness and minimizing
hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize
accuracy on DataMorgana's QA pairs, which are composed of single-hop and
multi-hop questions. The challenge provides access to **sparse** OpenSearch and
dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to
LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A
judge-LLM assesses the submitted answers along with human evaluators. By
exploring distinct retriever combinations and RAG solutions under the challenge
conditions, our final solution emerged using InstructRAG in combination with a
Pinecone retriever and a BGE reranker. Our solution achieved a correctness
score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR
2025 LiveRAG Challenge.


## Don't Make It Up Preserving Ignorance Awareness in LLM Fine-Tuning

>Authors: William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane

>2025-06-17

> http://arxiv.org/abs/2506.14387v1

Existing work on mitigating catastrophic forgetting in large language model
(LLM) fine-tuning has primarily focused on preserving specific data or tasks,
while critically overlooking the degradation of essential capabilities
instilled through safety alignment, particularly the model's ability to
faithfully express ignorance. In this work, we show that this capability is
significantly degraded during conventional fine-tuning, leading to undesired
behaviors such as hallucinations. To address this novel but highly practical
problem, we propose SEAT, a simple and effective fine-tuning approach that
preserves both fine-tuning performance and the model's inherent ability to
acknowledge its ignorance. SEAT integrates two key components: (1) **sparse**
training that constrains activation drift, and (2) a novel entity perturbation
method with KL-divergence regularization, designed to counter knowledge
entanglement. Experimental results demonstrate that SEAT significantly
outperforms baselines in preserving ignorance awareness while retaining
fine-tuning performance, offering a more robust solution for LLM fine-tuning.


## Compressed Video Super-Resolution based on Hierarchical Encoding

>Authors: Yuxuan Jiang, Siyue Teng, Qiang Zhu, Chen Feng, Chengxi Zeng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull

>2025-06-17

> http://arxiv.org/abs/2506.14381v1

This paper presents a general-purpose video super-resolution (VSR) method,
dubbed VSR-HE, specifically designed to enhance the perceptual quality of
compressed content. Targeting scenarios characterized by heavy compression, the
method upscales low-resolution videos by a ratio of four, from 180p to 720p or
from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and
has been sophisticatedly optimized to eliminate a wide range of compression
artifacts commonly introduced by H.265/HEVC encoding across various
**quantization** parameter (QP) levels. To ensure robustness and generalization,
the model is trained and evaluated under diverse compression settings, allowing
it to effectively restore fine-grained details and preserve visual fidelity.
The proposed VSR-HE has been officially submitted to the ICME 2025 Grand
Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1
(General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).


## Tensor Manipulation Unit (TMU) Reconfigurable, Near-Memory Tensor Manipulation for High-Throughput AI SoC

>Authors: Weiyu Zhou, Zheng Wang, Chao Chen, Yike Li, Yongkui Yang, Zhuoyu Wu, Anupam Chattopadhyay

>2025-06-17

> http://arxiv.org/abs/2506.14364v1

While recent advances in AI SoC design have focused heavily on accelerating
tensor computation, the equally critical task of tensor manipulation, centered
on high,volume data movement with minimal computation, remains underexplored.
This work addresses that gap by introducing the Tensor Manipulation Unit (TMU),
a reconfigurable, near-memory hardware block designed to efficiently execute
data-movement-intensive operators. TMU manipulates long datastreams in a
memory-to-memory fashion using a RISC-inspired execution model and a unified
addressing abstraction, enabling broad support for both coarse- and
fine-grained tensor transformations. Integrated alongside a TPU within a
high-throughput AI SoC, the TMU leverages double buffering and output
forwarding to improve pipeline utilization. Fabricated in SMIC 40nm technology,
the TMU occupies only 0.019 mm2 while supporting over 10 representative tensor
manipulation operators. Benchmarking shows that TMU alone achieves up to 1413
and 8.54 operator-level latency reduction compared to ARM A72 and NVIDIA Jetson
TX2, respectively. When integrated with the in-house TPU, the complete system
achieves a 34.6% reduction in end-to-end inference latency, demonstrating the
effectiveness and scalability of reconfigurable tensor manipulation in modern
AI SoCs.


## Improving LoRA with Variational Learning

>Authors: Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff

>2025-06-17

> http://arxiv.org/abs/2506.14280v1

Bayesian methods have recently been used to improve LoRA finetuning and,
although they improve calibration, their effect on other metrics (such as
accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian
methods also increase computational overheads and require additional tricks for
them to work well. Here, we fix these issues by using a recently proposed
variational algorithm called IVON. We show that IVON is easy to implement and
has similar costs to AdamW, and yet it can also drastically improve many
metrics by using a simple posterior **pruning** technique. We present extensive
results on billion-scale LLMs (Llama and Qwen series) going way beyond the
scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B
model on a set of commonsense reasoning tasks and improve accuracy over AdamW
by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian
methods like Laplace-LoRA and BLoB. Overall, our results show that variational
learning with IVON can effectively improve LoRA finetuning.


## High computational density nanophotonic media for machine learning inference

>Authors: Zhenyu Zhao, Yichen Pan, Jinlong Xiang, Yujia Zhang, An He, Yaotian Zhao, Youlve Chen, Yu He, Xinyuan Fang, Yikai Su, Min Gu, Xuhan Guo

>2025-06-17

> http://arxiv.org/abs/2506.14269v1

Efficient machine learning inference is essential for the rapid adoption of
artificial intelligence across various domains.On-chip optical computing has
emerged as a transformative solution for accelerating machine learning tasks,
owing to its ultra-low power consumption. However, enhancing the computational
density of on-chip optical systems remains a significant challenge, primarily
due to the difficulties in miniaturizing and integrating key optical
interference components.In this work, we harness the potential of
fabrication-constrained scattering optical computing within nanophotonic media
to address these limitations.Central to our approach is the use of
fabrication-aware inverse design techniques, which enable the realization of
manufacturable on-chip scattering structures under practical constraints.This
results in an ultra-compact optical neural computing architecture with an area
of just 64 um2,representing a remarkable three orders of magnitude reduction in
footprint compared to traditional optical neural networks. Our prototype,
tested on the Iris flower dataset, achieved an experimental accuracy of 86.7%,
closely matching the simulation benchmark.This breakthrough showcases a
promising pathway toward ultra-dense, energy-efficient optical processors for
scalable machine learning inference, significantly reducing both the hardware
footprint, latency, and power consumption of next-generation AI applications.


## Physics-Informed Neural Networks for the Korteweg-de Vries Equation for Internal Solitary Wave Problem Forward Simulation and Inverse Parameter Estimation

>Authors: Ming Kang, Hang Li, Qiwen Tan, Zhan Wang, Ruipeng Li, Junfang Zhao, Hui Xiang, Dixia Fan

>2025-06-17

> http://arxiv.org/abs/2506.14236v1

Physics-informed neural networks (PINNs) have emerged as a transformative
framework for addressing operator learning and inverse problems involving the
Korteweg-de Vries (KdV) equation for internal solitary waves. By integrating
physical constraints with data-driven optimization, PINNs overcome the critical
challenges of parameter unmeasurability in the KdV equation for internal
solitary waves in two-layer fluid systems. This work addresses two problems:
(1) Operator learning constructs a mapping from parameters to solutions,
enabling wave evolution predictions from unknown parameters. Comparative
studies demonstrate prediction errors as low as $10^{-4}$ when using 1000
training points. (2) Inverse problem solving leverages **sparse** and potentially
noisy observational data with physics-regularized constraints to invert
nonlinear coefficients successfully. Compared to conventional approaches, this
end-to-end differentiable paradigm unifies operator learning and inverse
problem-solving while overcoming mesh discretization errors and
high-dimensional parameter space iteration costs. The method shows
effectiveness for internal wave problems in stratified fluids, providing both
accurate forward modeling and robust parameter inversion capabilities, even
under noise.


## S$^4$C Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models

>Authors: Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian

>2025-06-17

> http://arxiv.org/abs/2506.14158v1

Large language models (LLMs) exhibit remarkable reasoning capabilities across
diverse downstream tasks. However, their autoregressive nature leads to
substantial inference latency, posing challenges for real-time applications.
Speculative sampling mitigates this issue by introducing a drafting phase
followed by a parallel validation phase, enabling faster token generation and
verification. Existing approaches, however, overlook the inherent coherence in
text generation, limiting their efficiency. To address this gap, we propose a
Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,
which extends speculative sampling by leveraging multi-head drafting for rapid
token generation and a continuous verification tree for efficient candidate
validation and feature reuse. Experimental results demonstrate that S$^4$C
surpasses baseline methods across mainstream tasks, offering enhanced
efficiency, parallelism, and the ability to generate more valid tokens with
fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an
**acceleration** ratio of 2.26x-2.60x, outperforming state-of-the-art methods.


## Déjà Vu Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse

>Authors: Jinwoo Hwang, Daeun Kim, Sangyeop Lee, Yoonsung Kim, Guseul Heo, Hojoon Kim, Yunseok Jeong, Tadiwos Meaza, Eunhyeok Park, Jeongseob Ahn, Jongse Park

>2025-06-17

> http://arxiv.org/abs/2506.14107v1

Recently, Video-Language Models (VideoLMs) have demonstrated remarkable
capabilities, offering significant potential for flexible and powerful video
query systems. These models typically rely on Vision Transformers (ViTs), which
process video frames individually to extract visual embeddings. However,
generating embeddings for large-scale videos requires ViT inferencing across
numerous frames, posing a major hurdle to real-world deployment and
necessitating solutions for integration into scalable video data management
systems. This paper introduces D\'ej\`a Vu, a video-language query engine that
accelerates ViT-based VideoLMs by reusing computations across consecutive
frames. At its core is ReuseViT, a modified ViT model specifically designed for
VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking
an effective balance between accuracy and reuse. Although ReuseViT
significantly reduces computation, these savings do not directly translate into
performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates
memory-compute joint compaction techniques that convert the FLOP savings into
tangible performance gains. Evaluations on three VideoLM tasks show that
D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error
bound, dramatically enhancing the practicality of VideoLMs for large-scale
video analytics.


## Transformers Learn Faster with Semantic Focus

>Authors: Parikshit Ram, Kenneth L. Clarkson, Tim Klinger, Shashanka Ubaru, Alexander G. Gray

>2025-06-17

> http://arxiv.org/abs/2506.14095v2

Various forms of **sparse** attention have been explored to mitigate the
quadratic computational and memory cost of the attention mechanism in
transformers. We study **sparse** transformers not through a lens of efficiency but
rather in terms of learnability and generalization. Empirically studying a
range of attention mechanisms, we find that input-dependent **sparse** attention
models appear to converge faster and generalize better than standard attention
models, while input-agnostic **sparse** attention models show no such benefits -- a
phenomenon that is robust across architectural and optimization hyperparameter
choices. This can be interpreted as demonstrating that concentrating a model's
"semantic focus" with respect to the tokens currently being considered (in the
form of input-dependent **sparse** attention) accelerates learning. We develop a
theoretical characterization of the conditions that explain this behavior. We
establish a connection between the stability of the standard softmax and the
loss function's Lipschitz properties, then show how **sparsity** affects the
stability of the softmax and the subsequent convergence and generalization
guarantees resulting from the attention mechanism. This allows us to
theoretically establish that input-agnostic **sparse** attention does not provide
any benefits. We also characterize conditions when semantic focus
(input-dependent **sparse** attention) can provide improved guarantees, and we
validate that these conditions are in fact met in our empirical evaluations.


## Embedding physical symmetries into machine-learned reduced plasma physics models via data augmentation

>Authors: Madox C. McGrae-Menge, Jacob R. Pierce, Frederico Fiuza, E. Paulo Alves

>2025-06-16

> http://arxiv.org/abs/2506.14048v2

Machine learning is offering powerful new tools for the development and
discovery of reduced models of nonlinear, multiscale plasma dynamics from the
data of first-principles kinetic simulations. However, ensuring the physical
consistency of such models requires embedding fundamental symmetries of plasma
dynamics. In this work, we explore a symmetry-embedding strategy based on data
augmentation, where symmetry-preserving transformations (e.g., Lorentz and
Galilean boosts) are applied to simulation data. Using both **sparse** regression
and neural networks, we show that models trained on symmetry-augmented data
more accurately infer the plasma fluid equations and pressure tensor closures
from fully kinetic particle-in-cell simulations of magnetic reconnection. We
show that this approach suppresses spurious inertial-frame-dependent
correlations between dynamical variables, improves data efficiency, and
significantly outperforms models trained without symmetry-augmented data, as
well as commonly used theoretical pressure closure models. Our results
establish symmetry-based data augmentation as a broadly applicable method for
incorporating physical structure into machine-learned reduced plasma models.


## MultiFinBen A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation

>Authors: Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie

>2025-06-16

> http://arxiv.org/abs/2506.14028v1

Recent advances in large language models (LLMs) have accelerated progress in
financial NLP and applications, yet existing benchmarks remain limited to
monolingual and unimodal settings, often over-relying on simple tasks and
failing to reflect the complexity of real-world financial communication. We
introduce MultiFinBen, the first multilingual and multimodal benchmark tailored
to the global financial domain, evaluating LLMs across modalities (text,
vision, audio) and linguistic settings (monolingual, bilingual, multilingual)
on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy
and PolyFiQA-Expert, the first multilingual financial benchmarks requiring
models to perform complex reasoning over mixed-language inputs; and EnglishOCR
and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to
extract and reason over information from visual-text financial documents.
Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate
a compact, balanced benchmark rather than simple aggregation existing datasets.
Extensive evaluation of 22 state-of-the-art models reveals that even the
strongest models, despite their general multimodal and multilingual
capabilities, struggle dramatically when faced with complex cross-lingual and
multimodal tasks in financial domain. MultiFinBen is publicly released to
foster transparent, reproducible, and inclusive progress in financial studies
and applications.


## Beyond Black Boxes Enhancing Interpretability of Transformers Trained on Neural Data

>Authors: Laurence Freeman, Philip Shamash, Vinam Arora, Caswell Barry, Tiago Branco, Eva Dyer

>2025-06-16

> http://arxiv.org/abs/2506.14014v1

Transformer models have become state-of-the-art in decoding stimuli and
behavior from neural activity, significantly advancing neuroscience research.
Yet greater transparency in their decision-making processes would substantially
enhance their utility in scientific and clinical contexts. Sparse autoencoders
offer a promising solution by producing hidden units that respond selectively
to specific variables, enhancing interpretability. Here, we introduce SAEs into
a neural decoding framework by augmenting a transformer trained to predict
visual stimuli from calcium imaging in the mouse visual cortex. The enhancement
of the transformer model with an SAE preserved its original performance while
yielding hidden units that selectively responded to interpretable features,
such as stimulus orientation and genetic background. Furthermore, ablating
units associated with a given variable impaired the model's ability to process
that variable, revealing how specific internal representations support
downstream computations. Together, these results demonstrate that integrating
SAEs with transformers combines the power of modern deep learning with the
interpretability essential for scientific understanding and clinical
translation.


## Taming Polysemanticity in LLMs Provable Feature Recovery via Sparse Autoencoders

>Authors: Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang

>2025-06-16

> http://arxiv.org/abs/2506.14002v1

We study the challenge of achieving theoretically grounded feature recovery
using Sparse Autoencoders (SAEs) for the interpretation of Large Language
Models. Existing SAE training algorithms often lack rigorous mathematical
guarantees and suffer from practical limitations such as hyperparameter
sensitivity and instability. To address these issues, we first propose a novel
statistical framework for the feature recovery problem, which includes a new
notion of feature identifiability by modeling polysemantic features as **sparse**
mixtures of underlying monosemantic concepts. Building on this framework, we
introduce a new SAE training algorithm based on ``bias adaptation'', a
technique that adaptively adjusts neural network bias parameters to ensure
appropriate activation **sparsity**. We theoretically \highlight{prove that this
algorithm correctly recovers all monosemantic features} when input data is
sampled from our proposed statistical model. Furthermore, we develop an
improved empirical variant, Group Bias Adaptation (GBA), and
\highlight{demonstrate its superior performance against benchmark methods when
applied to LLMs with up to 1.5 billion parameters}. This work represents a
foundational step in demystifying SAE training by providing the first SAE
algorithm with theoretical recovery guarantees, thereby advancing the
development of more transparent and trustworthy AI systems through enhanced
mechanistic interpretability.


## HierVL Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment

>Authors: Numair Nadeem, Saeed Anwar, Muhammad Hamza Asad, Abdul Bais

>2025-06-16

> http://arxiv.org/abs/2506.13925v1

Semi-supervised semantic segmentation remains challenging under severe label
scarcity and domain variability. Vision-only methods often struggle to
generalize, resulting in pixel misclassification between similar classes, poor
generalization and boundary localization. Vision-Language Models offer robust,
domain-invariant semantics but lack the spatial grounding required for dense
prediction. We introduce HierVL, a unified framework that bridges this gap by
integrating abstract text embeddings into a mask-transformer architecture
tailored for semi-supervised segmentation. HierVL features three novel
components: a Hierarchical Semantic Query Generator that filters and projects
abstract class embeddings into multi-scale queries to suppress irrelevant
classes and handle intra-class variability; a Cross-Modal Spatial Alignment
Module that aligns semantic queries with pixel features for sharper boundaries
under **sparse** supervision; and a Dual-Query Transformer Decoder that fuses
semantic and instance-level queries to prevent instance collapse. We also
introduce targeted regularization losses that maintain vision-language
alignment throughout training to reinforce semantic grounding. HierVL
establishes a new state-of-the-art by achieving a +4.4% mean improvement of the
intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal
VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes
(with 100 labels), demonstrating better performance under 1% supervision on
four benchmark datasets. Our results show that language-guided segmentation
closes the label efficiency gap and unlocks new levels of fine-grained,
instance-aware generalization.


## Discrete Diffusion in Large Language and Multimodal Models A Survey

>Authors: Runpeng Yu, Qi Li, Xinchao Wang

>2025-06-16

> http://arxiv.org/abs/2506.13759v1

In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
**acceleration** in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey


## Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs

>Authors: Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin

>2025-06-16

> http://arxiv.org/abs/2506.13727v1

Large Language Models (LLMs) are central to many contemporary AI
applications, yet their extensive parameter counts pose significant challenges
for deployment in memory- and compute-constrained environments. Recent works in
eXplainable AI (XAI), particularly on attribution methods, suggest that
interpretability can also enable model compression by identifying and removing
components irrelevant to inference. In this paper, we leverage Layer-wise
Relevance Propagation (LRP) to perform attribution-guided **pruning** of LLMs.
While LRP has shown promise in structured **pruning** for vision models, we extend
it to unstructured **pruning** in LLMs and demonstrate that it can substantially
reduce model size with minimal performance loss. Our method is especially
effective in extracting task-relevant subgraphs -- so-called ``circuits'' --
which can represent core functions (e.g., indirect object identification).
Building on this, we introduce a technique for model correction, by selectively
removing circuits responsible for spurious behaviors (e.g., toxic outputs). All
in all, we gather these techniques as a uniform holistic framework and showcase
its effectiveness and limitations through extensive experiments for
compression, circuit discovery and model correction on Llama and OPT models,
highlighting its potential for improving both model efficiency and safety. Our
code is publicly available at https://github.com/erfanhatefi/SparC3.


## CAMS A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation

>Authors: Yuwei Du, Jie Feng, Jian Yuan, Yong Li

>2025-06-16

> http://arxiv.org/abs/2506.13599v1

Human mobility simulation plays a crucial role in various real-world
applications. Recently, to address the limitations of traditional data-driven
approaches, researchers have explored leveraging the commonsense knowledge and
reasoning capabilities of large language models (LLMs) to accelerate human
mobility simulation. However, these methods suffer from several critical
shortcomings, including inadequate modeling of urban spaces and poor
integration with both individual mobility patterns and collective mobility
distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered
\textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation
(\textbf{CAMS}), an agentic framework that leverages the language based urban
foundation model to simulate human mobility in urban space. \textbf{CAMS}
comprises three core modules, including MobExtractor to extract template
mobility patterns and synthesize new ones based on user profiles, GeoGenerator
to generate anchor points considering collective knowledge and generate
candidate urban geospatial knowledge using an enhanced version of CityGPT,
TrajEnhancer to retrieve spatial knowledge based on mobility patterns and
generate trajectories with real trajectory preference alignment via DPO.
Experiments on real-world datasets show that \textbf{CAMS} achieves superior
performance without relying on externally provided geospatial information.
Moreover, by holistically modeling both individual mobility patterns and
collective mobility constraints, \textbf{CAMS} generates more realistic and
plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm
that integrates the agentic framework with urban-knowledgeable LLMs for human
mobility simulation.


## Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization

>Authors: Guanghui Song, Dongping Liao, Yiren Zhao, Kejiang Ye, Cheng-zhong Xu, Xitong Gao

>2025-06-16

> http://arxiv.org/abs/2506.13541v1

Transformer models face scalability challenges in causal language modeling
(CLM) due to inefficient memory allocation for growing key-value (**KV**) caches,
which strains compute and storage resources. Existing methods like Grouped
Query Attention (GQA) and token-level **KV** optimization improve efficiency but
rely on rigid resource allocation, often discarding "low-priority" tokens or
statically grouping them, failing to address the dynamic spectrum of token
importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that
dynamically optimizes token-wise computation and memory allocation. Unlike
prior approaches, mixSGA retains all tokens while adaptively routing them to
specialized experts with varying **KV** group sizes, balancing granularity and
efficiency. Our key novelties include: (1) a token-wise expert-choice routing
mechanism guided by learned importance scores, enabling proportional resource
allocation without token discard; (2) weight-sharing across grouped attention
projections to minimize parameter overhead; and (3) an auxiliary loss to ensure
one-hot routing decisions for training-inference consistency in CLMs. Extensive
evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show
mixSGA's superiority over static baselines. On instruction-following and
continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower
perplexity under the same **KV** budgets.


## ROSAQ Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models

>Authors: Junho Yoon, Geom Lee, Donghyeon Jeon, Inho Kang, Seung-Hoon Na

>2025-06-16

> http://arxiv.org/abs/2506.13472v2

Quantization has been widely studied as an effective technique for reducing
the memory requirement of large language models (LLMs), potentially improving
the latency time as well. Utilizing the characteristic of rotational invariance
of transformer, we propose the rotation-based saliency-aware weight
**quantization** (ROSAQ), which identifies salient channels in the projection
feature space, not in the original feature space, where the projected
"principal" dimensions are naturally considered as "salient" features. The
proposed ROSAQ consists of 1) PCA-based projection, which first performs
principal component analysis (PCA) on a calibration set and transforms via the
PCA projection, 2) Salient channel dentification, which selects dimensions
corresponding to the K-largest eigenvalues as salient channels, and 3)
Saliency-aware **quantization** with mixed-precision, which uses FP16 for salient
dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ
shows improvements over the baseline saliency-aware **quantization** on the
original feature space and other existing **quantization** methods. With kernel
fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in
generating 256 tokens with a batch size of 64.


## Block-wise Adaptive Caching for Accelerating Diffusion Policy

>Authors: Kangye Ji, Yuan Meng, Hanyun Cui, Ye Li, Shengjia Hua, Lei Chen, Zhi Wang

>2025-06-16

> http://arxiv.org/abs/2506.13456v1

Diffusion Policy has demonstrated strong visuomotor modeling capabilities,
but its high computational cost renders it impractical for real-time robotic
control. Despite huge redundancy across repetitive denoising steps, existing
diffusion **acceleration** techniques fail to generalize to Diffusion Policy due to
fundamental architectural and data divergences. In this paper, we propose
Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by
caching intermediate action features. BAC achieves lossless action generation
**acceleration** by adaptively updating and reusing cached features at the block
level, based on a key observation that feature similarities vary non-uniformly
across timesteps and locks. To operationalize this insight, we first propose
the Adaptive Caching Scheduler, designed to identify optimal update timesteps
by maximizing the global feature similarities between cached and skipped
features. However, applying this scheduler for each block leads to signiffcant
error surges due to the inter-block propagation of caching errors, particularly
within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop
the Bubbling Union Algorithm, which truncates these errors by updating the
upstream blocks with signiffcant caching errors before downstream FFNs. As a
training-free plugin, BAC is readily integrable with existing transformer-based
Diffusion Policy and vision-language-action models. Extensive experiments on
multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference
speedup for free.


## Delving Into the Psychology of Machines Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses

>Authors: Leonie V. D. E. Vogelsmeier, Eduardo Oliveira, Kamila Misiejuk, Sonsoles López-Pernas, Mohammed Saqr

>2025-06-16

> http://arxiv.org/abs/2506.13384v1

Large language models (LLMs) offer the potential to simulate human-like
responses and behaviors, creating new opportunities for psychological science.
In the context of self-regulated learning (SRL), if LLMs can reliably simulate
survey responses at scale and speed, they could be used to test intervention
scenarios, refine theoretical models, augment **sparse** datasets, and represent
hard-to-reach populations. However, the validity of LLM-generated survey
responses remains uncertain, with limited research focused on SRL and existing
studies beyond SRL yielding mixed results. Therefore, in this study, we
examined LLM-generated responses to the 44-item Motivated Strategies for
Learning Questionnaire (MSLQ; Pintrich \& De Groot, 1990), a widely used
instrument assessing students' learning strategies and academic motivation.
Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA
3.1-8B, and Mistral Large. We analyzed item distributions, the psychological
network of the theoretical SRL dimensions, and psychometric validity based on
the latent factor structure. Our results suggest that Gemini 2 Flash was the
most promising LLM, showing considerable sampling variability and producing
underlying dimensions and theoretical relationships that align with prior
theory and empirical findings. At the same time, we observed discrepancies and
limitations, underscoring both the potential and current constraints of using
LLMs for simulating psychological survey data and applying it in educational
contexts.


## DicFace Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration

>Authors: Yan Chen, Hanlin Shang, Ce Liu, Yuxuan Chen, Hui Li, Weihao Yuan, Hao Zhu, Zilong Dong, Siyu Zhu

>2025-06-16

> http://arxiv.org/abs/2506.13355v1

Video face restoration faces a critical challenge in maintaining temporal
consistency while recovering fine facial details from degraded inputs. This
paper presents a novel approach that extends Vector-Quantized Variational
Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a
video restoration framework through variational latent space modeling. Our key
innovation lies in reformulating discrete codebook representations as
Dirichlet-distributed continuous variables, enabling probabilistic transitions
between facial features across frames. A spatio-temporal Transformer
architecture jointly models inter-frame dependencies and predicts latent
distributions, while a Laplacian-constrained reconstruction loss combined with
perceptual (LPIPS) regularization enhances both pixel accuracy and visual
quality. Comprehensive evaluations on blind face restoration, video inpainting,
and facial colorization tasks demonstrate state-of-the-art performance. This
work establishes an effective paradigm for adapting intensive image priors,
pretrained on high-quality images, to video restoration while addressing the
critical challenge of flicker artifacts. The source code has been open-sourced
and is available at https://github.com/fudan-generative-vision/DicFace.


## GreedyPrune Retenting Critical Visual Token Set for Large Vision Language Models

>Authors: Ruiguang Pei, Weiqing Sun, Zhihui Fu, Jun Wang

>2025-06-16

> http://arxiv.org/abs/2506.13166v1

Although Large Vision Language Models (LVLMs) have demonstrated remarkable
performance in image understanding tasks, their computational efficiency
remains a significant challenge, particularly on resource-constrained devices
due to the high cost of processing large numbers of visual tokens. Recently,
training-free visual token **pruning** methods have gained popularity as a low-cost
solution to this issue. However, existing approaches suffer from two key
limitations: semantic saliency-based strategies primarily focus on high
cross-attention visual tokens, often neglecting visual diversity, whereas
visual diversity-based methods risk inadvertently discarding semantically
important tokens, especially under high compression ratios. In this paper, we
introduce GreedyPrune, a training-free plug-and-play visual token **pruning**
algorithm designed to jointly optimize semantic saliency and visual diversity.
We formalize the token **pruning** process as a combinatorial optimization problem
and demonstrate that greedy algorithms effectively balance computational
efficiency with model accuracy. Extensive experiments validate the
effectiveness of our approach, showing that GreedyPrune achieves
state-of-the-art accuracy across various multimodal tasks and models while
significantly reducing end-to-end inference latency.


## Thermodynamics of black and white holes in ensemble of Planckons

>Authors: G. E. Volovik

>2025-06-16

> http://arxiv.org/abs/2506.13145v1

The Tsallis-Cirto non-extensive statistics with $\delta=2$ describes the
processes of splitting and merging of black holes and their thermodynamics.
Here we consider a toy model, which matches this generalized statistics and
extends it by providing the integer valued entropy of the black hole, $S_{\rm
BH}(N)=N(N-1)/2$. In this model the black hole consists of $N$ the so-called
Planckons -- objects with reduced Planck mass $m_{\rm P}=1/\sqrt{8\pi G}$ -- so
that its mass is **quantize**d, $M=Nm_{\rm P}$. The entropy of each Planckon is
zero, but the entropy of black hole with $N$ Planckons is provided by the
$N(N-1)/2$ degrees of freedom -- the correlations between the gravitationally
attracted Planckons. This toy model can be extended to a charged
Reissner-Nordstr\"om (RN) black hole, which consists of charged Planckons.
Despite the charge, the statistical ensemble of Planckons remains the same, and
the RN black hole with $N$ Planckons has the same entropy as the electrically
neutral hole, $S_{\rm RNBH}(N)=N(N-1)/2$. This is supported by the adiabatic
process of transformation from the RN to Schwarzschild black hole by varying
the fine structure constant. The adiabaticity is violated in the extreme limit,
when the gravitational interaction between two Planckons is compensated by the
repulsion between their electric charges, and the RN black hole loses
stability. The entropy of a white hole formed by the same $N$ Planckons has
negative entropy, $S_{\rm WH}(N)=-N(N-1)/2$.


## EnhanceGraph A Continuously Enhanced Graph-based Index for High-dimensional Approximate Nearest Neighbor Search

>Authors: Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Mingyu Yang, Lei Chen, Haoyang Li, Zhitao Shen, Xuemin Lin, Heng Tao Shen, Jingkuan Song

>2025-06-16

> http://arxiv.org/abs/2506.13144v1

Recently, Approximate Nearest Neighbor Search in high-dimensional vector
spaces has garnered considerable attention due to the rapid advancement of deep
learning techniques. We observed that a substantial amount of search and
construction logs are generated throughout the lifespan of a graph-based index.
However, these two types of valuable logs are not fully exploited due to the
static nature of existing indexes. We present the EnhanceGraph framework, which
integrates two types of logs into a novel structure called a conjugate graph.
The conjugate graph is then used to improve search quality. Through theoretical
analyses and observations of the limitations of graph-based indexes, we propose
several optimization methods. For the search logs, the conjugate graph stores
the edges from local optima to global optima to enhance routing to the nearest
neighbor. For the construction logs, the conjugate graph stores the pruned
edges from the proximity graph to enhance retrieving of k nearest neighbors.
Our experimental results on several public and real-world industrial datasets
show that EnhanceGraph significantly improves search accuracy with the greatest
improvement on recall from 41.74% to 93.42%, but does not sacrifices search
efficiency. In addition, our EnhanceGraph algorithm has been integrated into
Ant Group's open-source vector library, VSAG.


## STAGE A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation

>Authors: Jiamin Wang, Yichen Yao, Xiang Feng, Hang Wu, Yaming Wang, Qingqiu Huang, Yuexin Ma, Xinge Zhu

>2025-06-16

> http://arxiv.org/abs/2506.13138v1

The generation of temporally consistent, high-fidelity driving videos over
extended horizons presents a fundamental challenge in autonomous driving world
modeling. Existing approaches often suffer from error accumulation and feature
misalignment due to inadequate decoupling of spatio-temporal dynamics and
limited cross-frame feature propagation mechanisms. To address these
limitations, we present STAGE (Streaming Temporal Attention Generative Engine),
a novel auto-regressive framework that pioneers hierarchical feature
coordination and multi-phase optimization for sustainable video synthesis. To
achieve high-quality long-horizon driving video generation, we introduce
Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training
strategy. HTFT enhances temporal consistency between video frames throughout
the video generation process by modeling the temporal and denoising process
separately and transferring denoising features between frames. The multi-stage
training strategy is to divide the training into three stages, through model
decoupling and auto-regressive inference process simulation, thereby
accelerating model convergence and reducing error accumulation. Experiments on
the Nuscenes dataset show that STAGE has significantly surpassed existing
methods in the long-horizon driving video generation task. In addition, we also
explored STAGE's ability to generate unlimited-length driving videos. We
generated 600 frames of high-quality driving videos on the Nuscenes dataset,
which far exceeds the maximum length achievable by existing methods.


## AlphaEvolve A coding agent for scientific and algorithmic discovery

>Authors: Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, Matej Balog

>2025-06-16

> http://arxiv.org/abs/2506.13131v1

In this white paper, we present AlphaEvolve, an evolutionary coding agent
that substantially enhances capabilities of state-of-the-art LLMs on highly
challenging tasks such as tackling open scientific problems or optimizing
critical pieces of computational infrastructure. AlphaEvolve orchestrates an
autonomous pipeline of LLMs, whose task is to improve an algorithm by making
direct changes to the code. Using an evolutionary approach, continuously
receiving feedback from one or more evaluators, AlphaEvolve iteratively
improves the algorithm, potentially leading to new scientific and practical
discoveries. We demonstrate the broad applicability of this approach by
applying it to a number of important computational problems. When applied to
optimizing critical components of large-scale computational stacks at Google,
AlphaEvolve developed a more efficient scheduling algorithm for data centers,
found a functionally equivalent simplification in the circuit design of
hardware accelerators, and accelerated the training of the LLM underpinning
AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct
algorithms that surpass state-of-the-art solutions on a spectrum of problems in
mathematics and computer science, significantly expanding the scope of prior
automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve
developed a search algorithm that found a procedure to multiply two $4 \times
4$ complex-valued matrices using $48$ scalar multiplications; offering the
first improvement, after 56 years, over Strassen's algorithm in this setting.
We believe AlphaEvolve and coding agents like it can have a significant impact
in improving solutions of problems across many areas of science and
computation.


## Multipole Attention for Efficient Long Context Reasoning

>Authors: Coleman Hooper, Sebastian Zhao, Luca Manolache, Sehoon Kim, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

>2025-06-16

> http://arxiv.org/abs/2506.13059v1

Large Reasoning Models (LRMs) have shown promising accuracy improvements on
complex problem-solving tasks. While these models have attained high accuracy
by leveraging additional computation at test time, they need to generate long
chain-of-thought reasoning in order to think before answering, which requires
generating thousands of tokens. While **sparse** attention methods can help reduce
the **KV** cache pressure induced by this long autoregressive reasoning, these
methods can introduce errors which disrupt the reasoning process. Additionally,
prior methods often pre-process the input to make it easier to identify the
important prompt tokens when computing attention during generation, and this
pre-processing is challenging to perform online for newly generated reasoning
tokens. Our work addresses these challenges by introducing Multipole Attention,
which accelerates autoregressive reasoning by only computing exact attention
for the most important tokens, while maintaining approximate representations
for the remaining tokens. Our method first performs clustering to group
together semantically similar key vectors, and then uses the cluster centroids
both to identify important key vectors and to approximate the remaining key
vectors in order to retain high accuracy. We design a fast cluster update
process to quickly re-cluster the input and previously generated tokens,
thereby allowing for accelerating attention to the previous output tokens. We
evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our
approach can maintain accuracy on complex reasoning tasks even with aggressive
attention **sparsity** settings. We also provide kernel implementations to
demonstrate the practical efficiency gains from our method, achieving up to
4.5$\times$ speedup for attention in long-context reasoning applications. Our
code is available at https://github.com/SqueezeAILab/MultipoleAttention.


## NeuVAS Neural Implicit Surfaces for Variational Shape Modeling

>Authors: Pengfei Wang, Qiujie Dong, Fangtian Liang, Hao Pan, Lei Yang, Congyi Zhang, Guying Lin, Caiming Zhang, Yuanfeng Zhou, Changhe Tu, Shiqing Xin, Alla Sheffer, Xin Li, Wenping Wang

>2025-06-16

> http://arxiv.org/abs/2506.13050v1

Neural implicit shape representation has drawn significant attention in
recent years due to its smoothness, differentiability, and topological
flexibility. However, directly modeling the shape of a neural implicit surface,
especially as the zero-level set of a neural signed distance function (SDF),
with **sparse** geometric control is still a challenging task. Sparse input shape
control typically includes 3D curve networks or, more generally, 3D curve
sketches, which are unstructured and cannot be connected to form a curve
network, and therefore more difficult to deal with. While 3D curve networks or
curve sketches provide intuitive shape control, their **sparsity** and varied
topology pose challenges in generating high-quality surfaces to meet such curve
constraints. In this paper, we propose NeuVAS, a variational approach to shape
modeling using neural implicit surfaces constrained under **sparse** input shape
control, including unstructured 3D curve sketches as well as connected 3D curve
networks. Specifically, we introduce a smoothness term based on a functional of
surface curvatures to minimize shape variation of the zero-level set surface of
a neural SDF. We also develop a new technique to faithfully model G0 sharp
feature curves as specified in the input curve sketches. Comprehensive
comparisons with the state-of-the-art methods demonstrate the significant
advantages of our method.


## DETRPose Real-time end-to-end transformer model for multi-person pose estimation

>Authors: Sebastian Janampa, Marios Pattichis

>2025-06-16

> http://arxiv.org/abs/2506.13027v1

Multi-person pose estimation (MPPE) estimates keypoints for all individuals
present in an image. MPPE is a fundamental task for several applications in
computer vision and virtual reality. Unfortunately, there are currently no
transformer-based models that can perform MPPE in real time. The paper presents
a family of transformer-based models capable of performing multi-person 2D pose
estimation in real-time. Our approach utilizes a modified decoder architecture
and keypoint similarity metrics to generate both positive and negative queries,
thereby enhancing the quality of the selected queries within the architecture.
Compared to state-of-the-art models, our proposed models train much faster,
using 5 to 10 times fewer epochs, with competitive inference times without
requiring **quantization** libraries to speed up the model. Furthermore, our
proposed models provide competitive results or outperform alternative models,
often using significantly fewer parameters.


## Combining Fault Tolerance Techniques and COTS SoC Accelerators for Payload Processing in Space

>Authors: Vasileios Leon, Elissaios Alexios Papatheofanous, George Lentaris, Charalampos Bezaitis, Nikolaos Mastorakis, Georgios Bampilis, Dionysios Reisis, Dimitrios Soudris

>2025-06-15

> http://arxiv.org/abs/2506.12971v1

The ever-increasing demand for computational power and I/O throughput in
space applications is transforming the landscape of on-board computing. A
variety of Commercial-Off-The-Shelf (COTS) accelerators emerges as an
attractive solution for payload processing to outperform the traditional
radiation-hardened devices. Towards increasing the reliability of such COTS
accelerators, the current paper explores and evaluates fault-tolerance
techniques for the Zynq FPGA and the Myriad VPU, which are two device families
being integrated in industrial space avionics architectures/boards, such as
Ubotica's CogniSat, Xiphos' Q7S, and Cobham Gaisler's GR-VPX-XCKU060. On the
FPGA side, we combine techniques such as memory scrubbing, partial
reconfiguration, triple modular redundancy, and watchdogs. On the VPU side, we
detect and correct errors in the instruction and data memories, as well as we
apply redundancy at processor level (SHAVE cores). When considering FPGA with
VPU co-processing, we also develop a fault-tolerant interface between the two
devices based on the CIF/LCD protocols and our custom CRC error-detecting code.


## MaskPro Linear-Space Probabilistic Learning for Strict (NM)-Sparsity on Large Language Models

>Authors: Yan Sun, Qixin Zhang, Zhiyuan Yu, Xikun Zhang, Li Shen, Dacheng Tao

>2025-06-15

> http://arxiv.org/abs/2506.12876v1

The rapid scaling of large language models (LLMs) has made inference
efficiency a primary bottleneck in the practical deployment. To address this,
semi-structured **sparsity** offers a promising solution by strategically retaining
$N$ elements out of every $M$ weights, thereby enabling hardware-friendly
**acceleration** and reduced memory. However, existing (N:M)-compatible approaches
typically fall into two categories: rule-based layerwise greedy search, which
suffers from considerable errors, and gradient-driven combinatorial learning,
which incurs prohibitive training costs. To tackle these challenges, we propose
a novel linear-space probabilistic framework named MaskPro, which aims to learn
a prior categorical distribution for every $M$ consecutive weights and
subsequently leverages this distribution to generate the (N:M)-**sparsity**
throughout an $N$-way sampling without replacement. Furthermore, to mitigate
the training instability induced by the high variance of policy gradients in
the super large combinatorial space, we propose a novel update method by
introducing a moving average tracker of loss residuals instead of vanilla loss.
Finally, we conduct comprehensive theoretical analysis and extensive
experiments to validate the superior performance of MaskPro, as well as its
excellent scalability in memory efficiency and exceptional robustness to data
samples. Our code is available at https://github.com/woodenchild95/Maskpro.git.


## EraserDiT Fast Video Inpainting with Diffusion Transformer Model

>Authors: Jie Liu, Zheng Hui

>2025-06-15

> http://arxiv.org/abs/2506.12853v1

Video object removal and inpainting are critical tasks in the fields of
computer vision and multimedia processing, aimed at restoring missing or
corrupted regions in video sequences. Traditional methods predominantly rely on
flow-based propagation and spatio-temporal Transformers, but these approaches
face limitations in effectively leveraging long-term temporal features and
ensuring temporal consistency in the completion results, particularly when
dealing with large masks. Consequently, performance on extensive masked areas
remains suboptimal. To address these challenges, this paper introduces a novel
video inpainting approach leveraging the Diffusion Transformer (DiT). DiT
synergistically combines the advantages of diffusion models and transformer
architectures to maintain long-term temporal consistency while ensuring
high-quality inpainting results. We propose a Circular Position-Shift strategy
to further enhance long-term temporal consistency during the inference stage.
Additionally, the proposed method automatically detects objects within videos,
interactively removes specified objects, and generates corresponding prompts.
In terms of processing speed, it takes only 180 seconds (testing on one NVIDIA
A100 GPU) to complete a video with a resolution of $1080 \times 1920$ with 121
frames without any **acceleration** method. Experimental results indicate that the
proposed method demonstrates superior performance in content fidelity, texture
restoration, and temporal consistency. Project page:
https://jieliu95.github.io/EraserDiT_demo.


## A Review of the Long Horizon Forecasting Problem in Time Series Analysis

>Authors: Hans Krupakar, Kandappan V A

>2025-06-15

> http://arxiv.org/abs/2506.12809v1

The long horizon forecasting (LHF) problem has come up in the time series
literature for over the last 35 years or so. This review covers aspects of LHF
in this period and how deep learning has incorporated variants of trend,
seasonality, fourier and wavelet transforms, misspecification bias reduction
and bandpass filters while contributing using convolutions, residual
connections, **sparsity** reduction, strided convolutions, attention masks, SSMs,
normalization methods, low-rank approximations and gating mechanisms. We
highlight time series decomposition techniques, input data preprocessing and
dataset windowing schemes that improve performance. Multi-layer perceptron
models, recurrent neural network hybrids, self-attention models that improve
and/or address the performances of the LHF problem are described, with an
emphasis on the feature space construction. Ablation studies are conducted over
the ETTm2 dataset in the multivariate and univariate high useful load (HUFL)
forecasting contexts, evaluated over the last 4 months of the dataset. The
heatmaps of MSE averages per time step over test set series in the horizon show
that there is a steady increase in the error proportionate to its length except
with xLSTM and Triformer models and motivate LHF as an error propagation
problem. The trained models are available here: https://bit.ly/LHFModelZoo


## SP-VLA A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration

>Authors: Ye Li, Yuan Meng, Zewen Sun, Kangye Ji, Chen Tang, Jiajun Fan, Xinzhu Ma, Shutao Xia, Zhi Wang, Wenwu Zhu

>2025-06-15

> http://arxiv.org/abs/2506.12723v1

Vision-Language-Action (VLA) models have attracted increasing attention for
their strong control capabilities. However, their high computational cost and
low execution frequency hinder their suitability for real-time tasks such as
robotic manipulation and autonomous navigation. Existing VLA **acceleration**
methods primarily focus on structural optimization, overlooking the fact that
these models operate in sequential decision-making environments. As a result,
temporal redundancy in sequential action generation and spatial redundancy in
visual input remain unaddressed. To this end, we propose SP-VLA, a unified
framework that accelerates VLA models by jointly scheduling models and **pruning**
tokens. Specifically, we design an action-aware model scheduling mechanism that
reduces temporal redundancy by dynamically switching between VLA model and a
lightweight generator. Inspired by the human motion pattern of focusing on key
decision points while relying on intuition for other actions, we categorize VLA
actions into deliberative and intuitive, assigning the former to the VLA model
and the latter to the lightweight generator, enabling frequency-adaptive
execution through collaborative model scheduling. To address spatial
redundancy, we further develop a spatio-semantic dual-aware token **pruning**
method. Tokens are classified into spatial and semantic types and pruned based
on their dual-aware importance to accelerate VLA inference. These two
mechanisms work jointly to guide the VLA in focusing on critical actions and
salient visual information, achieving effective **acceleration** while maintaining
high accuracy. Experimental results demonstrate that our method achieves up to
1.5$\times$ **acceleration** with less than 3% drop in accuracy, outperforming
existing approaches in multiple tasks.


## Serving Large Language Models on Huawei CloudMatrix384

>Authors: Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao

>2025-06-15

> http://arxiv.org/abs/2506.12708v2

The rapid evolution of large language models (LLMs), driven by growing
parameter scales, adoption of mixture-of-experts (MoE) architectures, and
expanding context lengths, imposes unprecedented demands on AI infrastructure.
Traditional AI clusters face limitations in compute intensity, memory
bandwidth, inter-chip communication, and latency, compounded by variable
workloads and strict service-level objectives. Addressing these issues requires
fundamentally redesigned hardware-software integration. This paper introduces
Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in
the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C
NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified
Bus (UB) network, enabling direct all-to-all communication and dynamic pooling
of resources. These features optimize performance for communication-intensive
operations, such as large-scale MoE expert parallelism and distributed
key-value cache access. To fully leverage CloudMatrix384, we propose
CloudMatrix-Infer, an advanced LLM serving solution incorporating three core
innovations: a peer-to-peer serving architecture that independently scales
prefill, decode, and caching; a large-scale expert parallelism strategy
supporting EP320 via efficient UB-based token dispatch; and hardware-aware
optimizations including specialized operators, microbatch-based pipelining, and
INT8 **quantization**. Evaluation with the DeepSeek-R1 model shows
CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of
6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms
TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s
per NPU even under stringent 15 ms latency constraints, while INT8 **quantization**
maintains model accuracy across benchmarks.


## Get on the Train or be Left on the Station Using LLMs for Software Engineering Research

>Authors: Bianca Trinkenreich, Fabio Calefato, Geir Hanssen, Kelly Blincoe, Marcos Kalinowski, Mauro Pezzè, Paolo Tell, Margaret-Anne Storey

>2025-06-15

> http://arxiv.org/abs/2506.12691v1

The adoption of Large Language Models (LLMs) is not only transforming
software engineering (SE) practice but is also poised to fundamentally disrupt
how research is conducted in the field. While perspectives on this
transformation range from viewing LLMs as mere productivity tools to
considering them revolutionary forces, we argue that the SE research community
must proactively engage with and shape the integration of LLMs into research
practices, emphasizing human agency in this transformation. As LLMs rapidly
become integral to SE research - both as tools that support investigations and
as subjects of study - a human-centric perspective is essential. Ensuring human
oversight and interpretability is necessary for upholding scientific rigor,
fostering ethical responsibility, and driving advancements in the field.
Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI
in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze
the impact of LLMs on SE research. Through this theoretical lens, we examine
how LLMs enhance research capabilities through accelerated ideation and
automated processes, make some traditional research practices obsolete,
retrieve valuable aspects of historical research approaches, and risk reversal
effects when taken to extremes. Our analysis reveals opportunities for
innovation and potential pitfalls that require careful consideration. We
conclude with a call to action for the SE research community to proactively
harness the benefits of LLMs while developing frameworks and guidelines to
mitigate their risks, to ensure continued rigor and impact of research in an
AI-augmented future.


## Watermarking Quantum Neural Networks Based on Sample Grouped and Paired Training

>Authors: Limengnan Zhou, Hanzhou Wu

>2025-06-15

> http://arxiv.org/abs/2506.12675v1

Quantum neural networks (QNNs) leverage quantum computing to create powerful
and efficient artificial intelligence models capable of solving complex
problems significantly faster than traditional computers. With the fast
development of quantum hardware technology, such as superconducting qubits,
trapped ions, and integrated photonics, quantum computers may become reality,
accelerating the applications of QNNs. However, preparing quantum circuits and
optimizing parameters for QNNs require quantum hardware support, expertise, and
high-quality data. How to protect intellectual property (IP) of QNNs becomes an
urgent problem to be solved in the era of quantum computing. We make the first
attempt towards IP protection of QNNs by watermarking. To this purpose, we
collect classical clean samples and trigger ones, each of which is generated by
adding a perturbation to a clean sample, associated with a label different from
the ground-truth one. The host QNN, consisting of quantum encoding, quantum
state transformation, and quantum measurement, is then trained from scratch
with the clean samples and trigger ones, resulting in a watermarked QNN model.
During training, we introduce sample grouped and paired training to ensure that
the performance on the downstream task can be maintained while achieving good
performance for watermark extraction. When disputes arise, by collecting a
mini-set of trigger samples, the hidden watermark can be extracted by analyzing
the prediction results of the target model corresponding to the trigger
samples, without accessing the internal details of the target QNN model,
thereby verifying the ownership of the model. Experiments have verified the
superiority and applicability of this work.


## Social Media Reactions to Open Source Promotions AI-Powered GitHub Projects on Hacker News

>Authors: Prachnachai Meakpaiboonwattana, Warittha Tarntong, Thai Mekratanavorakul, Chaiyong Ragkhitwetsagul, Pattaraporn Sangaroonsilp, Raula Kula, Morakot Choetkiertikul, Kenichi Matsumoto, Thanwadee Sunetnanta

>2025-06-14

> http://arxiv.org/abs/2506.12643v1

Social media platforms have become more influential than traditional news
sources, shaping public discourse and accelerating the spread of information.
With the rapid advancement of artificial intelligence (AI), open-source
software (OSS) projects can leverage these platforms to gain visibility and
attract contributors. In this study, we investigate the relationship between
Hacker News, a social news site focused on computer science and
entrepreneurship, and the extent to which it influences developer activity on
the promoted GitHub AI projects.
  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments
over a two-year period. Our findings reveal that at least 19\% of AI developers
promoted their GitHub projects on Hacker News, often receiving positive
engagement from the community. By tracking activity on the associated 1,814
GitHub repositories after they were shared on Hacker News, we observed a
significant increase in forks, stars, and contributors. These results suggest
that Hacker News serves as a viable platform for AI-powered OSS projects, with
the potential to gain attention, foster community engagement, and accelerate
software development.


## Towards Neural Audio Codec Source Parsing

>Authors: Orchid Chetia Phukan, Girish, Mohd Mujtaba Akhtar, Arun Balaji Buduru, Rajesh Sharma

>2025-06-14

> http://arxiv.org/abs/2506.12627v1

A new class of audio deepfakes-codecfakes (CFs)-has recently caught
attention, synthesized by Audio Language Models that leverage neural audio
codecs (NACs) in the backend. In response, the community has introduced
dedicated benchmarks and tailored detection strategies. As the field advances,
efforts have moved beyond binary detection toward source attribution, including
open-set attribution, which aims to identify the NAC responsible for generation
and flag novel, unseen ones during inference. This shift toward source
attribution improves forensic interpretability and accountability. However,
open-set attribution remains fundamentally limited: while it can detect that a
NAC is unfamiliar, it cannot characterize or identify individual unseen codecs.
It treats such inputs as generic ``unknowns'', lacking insight into their
internal configuration. This leads to major shortcomings: limited
generalization to new NACs and inability to resolve fine-grained variations
within NAC families. To address these gaps, we propose Neural Audio Codec
Source Parsing (NACSP) - a paradigm shift that reframes source attribution for
CFs as structured regression over generative NAC parameters such as **quantize**rs,
bandwidth, and sampling rate. We formulate NACSP as a multi-task regression
task for predicting these NAC parameters and establish the first comprehensive
benchmark using various state-of-the-art speech pre-trained models (PTMs). To
this end, we propose HYDRA, a novel framework that leverages hyperbolic
geometry to disentangle complex latent properties from PTM representations. By
employing task-specific attention over multiple curvature-aware hyperbolic
subspaces, HYDRA enables superior multi-task generalization. Our extensive
experiments show HYDRA achieves top results on benchmark CFs datasets compared
to baselines operating in Euclidean space.


## OpenUnlearning Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics

>Authors: Vineeth Dorna, Anmol Mekala, Wenlong Zhao, Andrew McCallum, Zachary C. Lipton, J. Zico Kolter, Pratyush Maini

>2025-06-14

> http://arxiv.org/abs/2506.12618v1

Robust unlearning is crucial for safely deploying large language models
(LLMs) in environments where data privacy, model safety, and regulatory
compliance must be ensured. Yet the task is inherently challenging, partly due
to difficulties in reliably measuring whether unlearning has truly occurred.
Moreover, fragmentation in current methodologies and inconsistent evaluation
metrics hinder comparative analysis and reproducibility. To unify and
accelerate research efforts, we introduce OpenUnlearning, a standardized and
extensible framework designed explicitly for benchmarking both LLM unlearning
methods and metrics. OpenUnlearning integrates 9 unlearning algorithms and 16
diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also
enables analyses of forgetting behaviors across 450+ checkpoints we publicly
release. Leveraging OpenUnlearning, we propose a novel meta-evaluation
benchmark focused specifically on assessing the faithfulness and robustness of
evaluation metrics themselves. We also benchmark diverse unlearning methods and
provide a comparative analysis against an extensive evaluation suite. Overall,
we establish a clear, community-driven pathway toward rigorous development in
LLM unlearning research.


## An Exploration of Mamba for Speech Self-Supervised Models

>Authors: Tzu-Quan Lin, Heng-Cheng Kuo, Tzu-Chieh Wei, Hsi-Chun Cheng, Chun-Wei Chen, Hsien-Fu Hsiao, Yu Tsao, Hung-yi Lee

>2025-06-14

> http://arxiv.org/abs/2506.12606v1

While Mamba has demonstrated strong performance in language modeling, its
potential as a speech self-supervised (SSL) model remains underexplored, with
prior studies limited to isolated tasks. To address this, we explore
Mamba-based HuBERT models as alternatives to Transformer-based SSL
architectures. Leveraging the linear-time Selective State Space, these models
enable fine-tuning on long-context ASR with significantly lower compute.
Moreover, they show superior performance when fine-tuned for streaming ASR.
Beyond fine-tuning, these models show competitive performance on SUPERB probing
benchmarks, particularly in causal settings. Our analysis shows that they yield
higher-quality **quantize**d representations and capture speaker-related features
more distinctly than Transformer-based models. These findings highlight
Mamba-based SSL as a promising and complementary direction for long-sequence
modeling, real-time speech modeling, and speech unit extraction.


## Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts

>Authors: Shengzhuang Chen, Ying Wei, Jonathan Richard Schwarz

>2025-06-14

> http://arxiv.org/abs/2506.12597v1

We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning,
an end-to-end algorithm designed to fine-tune a dense pre-trained Large
Language Model (LLM) into a MoE-style model that possesses capabilities in
multiple specialized domains. During instruction-tuning, SIMoE automatically
identifies multiple specialized experts under a specified **sparsity** constraint,
with each expert representing a structurally **sparse** subset of the seed LLM's
parameters that correspond to domain-specific knowledge within the data. SIMoE
simultaneously learns an input-dependent expert merging strategy via a router
network, leveraging rich cross-expert knowledge for superior downstream
generalization that surpasses existing baselines. Empirically, SIMoE
consistently achieves state-of-the-art performance on common instruction-tuning
benchmarks while maintaining an optimal performance-compute trade-off compared
to all baselines.


## GNSS Spoofing Detection Based on Opportunistic Position Information

>Authors: Wenjie Liu, Panos Papadimitratos

>2025-06-14

> http://arxiv.org/abs/2506.12580v1

The limited or no protection for civilian Global Navigation Satellite System
(GNSS) signals makes spoofing attacks relatively easy. With modern mobile
devices often featuring network interfaces, state-of-the-art signals of
opportunity (SOP) schemes can provide accurate network positions in replacement
of GNSS. The use of onboard inertial sensors can also assist in the absence of
GNSS, possibly in the presence of jammers. The combination of SOP and inertial
sensors has received limited attention, yet it shows strong results on fully
custom-built platforms. We do not seek to improve such special-purpose schemes.
Rather, we focus on countering GNSS attacks, notably detecting them, with
emphasis on deployment with consumer-grade platforms, notably smartphones, that
provide off-the-shelf opportunistic information (i.e., network position and
inertial sensor data). Our Position-based Attack Detection Scheme (PADS) is a
probabilistic framework that uses regression and uncertainty analysis for
positions. The regression optimization problem is a weighted mean square error
of polynomial fitting, with constraints that the fitted positions satisfy the
device velocity and **acceleration**. Then, uncertainty is modeled by a Gaussian
process, which provides more flexibility to analyze how sure or unsure we are
about position estimations. In the detection process, we combine all
uncertainty information with the position estimations into a fused test
statistic, which is the input utilized by an anomaly detector based on outlier
ensembles. The evaluation shows that the PADS outperforms a set of baseline
methods that rely on SOP or inertial sensor-based or statistical tests,
achieving up to 3 times the true positive rate at a low false positive rate.


## Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders

>Authors: Ananya Joshi, Celia Cintas, Skyler Speakman

>2025-06-14

> http://arxiv.org/abs/2506.12576v1

Recent work shows that Sparse Autoencoders (SAE) applied to large language
model (LLM) layers have neurons corresponding to interpretable concepts. These
SAE neurons can be modified to align generated outputs, but only towards
pre-identified topics and with some parameter tuning. Our approach leverages
the observational and modification properties of SAEs to enable alignment for
any topic. This method 1) scores each SAE neuron by its semantic similarity to
an alignment text and uses them to 2) modify SAE-layer-level outputs by
emphasizing topic-aligned neurons. We assess the alignment capabilities of this
approach on diverse public topic datasets including Amazon reviews, Medicine,
and Sycophancy, across the currently available open-source LLMs and SAE pairs
(GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to
medical prompts reveal several benefits over fine-tuning, including increased
average language acceptability (0.25 vs. 0.5), reduced training time across
multiple alignment topics (333.6s vs. 62s), and acceptable inference time for
many applications (+0.00092s/token). Our open-source code is available at
github.com/IBM/sae-steering.


## BSA Ball Sparse Attention for Large-scale Geometries

>Authors: Catalin E. Brita, Hieu Nguyen, Lohithsai Yadala Chanchu, Domonkos Nagy, Maksim Zhdanov

>2025-06-14

> http://arxiv.org/abs/2506.12541v1

Self-attention scales quadratically with input size, limiting its use for
large-scale physical systems. Although **sparse** attention mechanisms provide a
viable alternative, they are primarily designed for regular structures such as
text or images, making them inapplicable for irregular geometries. In this
work, we present Ball Sparse Attention (BSA), which adapts Native Sparse
Attention (NSA) (Yuan et al., 2025) to unordered point sets by imposing
regularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et
al., 2025). We modify NSA's components to work with ball-based neighborhoods,
yielding a global receptive field at sub-quadratic cost. On an airflow pressure
prediction task, we achieve accuracy comparable to Full Attention while
significantly reducing the theoretical computational complexity. Our
implementation is available at https://github.com/britacatalin/bsa.


## Quantizing Small-Scale State-Space Models for Edge AI

>Authors: Leo Zhao, Tristan Torchet, Melika Payvand, Laura Kriener, Filippo Moro

>2025-06-14

> http://arxiv.org/abs/2506.12480v1

State-space models (SSMs) have recently gained attention in deep learning for
their ability to efficiently model long-range dependencies, making them
promising candidates for edge-AI applications. In this paper, we analyze the
effects of **quantization** on small-scale SSMs with a focus on reducing memory and
computational costs while maintaining task performance. Using the S4D
architecture, we first investigate post-training **quantization** (PTQ) and show
that the state matrix A and internal state x are particularly sensitive to
**quantization**. Furthermore, we analyze the impact of different **quantization**
techniques applied to the parameters and activations in the S4D architecture.
To address the observed performance drop after Post-training Quantization
(PTQ), we apply Quantization-aware Training (QAT), significantly improving
performance from 40% (PTQ) to 96% on the sequential MNIST benchmark at 8-bit
precision. We further demonstrate the potential of QAT in enabling sub-8-bit
precisions and evaluate different parameterization schemes for QAT stability.
Additionally, we propose a heterogeneous **quantization** strategy that assigns
different precision levels to model components, reducing the overall memory
footprint by a factor of 6x without sacrificing performance. Our results
provide actionable insights for deploying **quantize**d SSMs in
resource-constrained environments.


## Recent Advances and Future Directions in Literature-Based Discovery

>Authors: Andrej Kastrin, Bojan Cestnik, Nada Lavrač

>2025-06-14

> http://arxiv.org/abs/2506.12385v1

The explosive growth of scientific publications has created an urgent need
for automated methods that facilitate knowledge synthesis and hypothesis
generation. Literature-based discovery (LBD) addresses this challenge by
uncovering previously unknown associations between disparate domains. This
article surveys recent methodological advances in LBD, focusing on developments
from 2000 to the present. We review progress in three key areas: knowledge
graph construction, deep learning approaches, and the integration of
pre-trained and large language models (LLMs). While LBD has made notable
progress, several fundamental challenges remain unresolved, particularly
concerning scalability, reliance on structured data, and the need for extensive
manual curation. By examining ongoing advances and outlining promising future
directions, this survey underscores the transformative role of LLMs in
enhancing LBD and aims to support researchers and practitioners in harnessing
these technologies to accelerate scientific innovation.


## Training-free LLM Merging for Multi-task Learning

>Authors: Zichuan Fu, Xian Wu, Yejing Wang, Wanyu Wang, Shanshan Ye, Hongzhi Yin, Yi Chang, Yefeng Zheng, Xiangyu Zhao

>2025-06-14

> http://arxiv.org/abs/2506.12379v1

Large Language Models (LLMs) have demonstrated exceptional capabilities
across diverse natural language processing (NLP) tasks. The release of
open-source LLMs like LLaMA and Qwen has triggered the development of numerous
fine-tuned models tailored for various tasks and languages. In this paper, we
explore an important question: is it possible to combine these specialized
models to create a unified model with multi-task capabilities. We introduces
Hierarchical Iterative Merging (Hi-Merging), a training-free method for
unifying different specialized LLMs into a single model. Specifically,
Hi-Merging employs model-wise and layer-wise **pruning** and scaling, guided by
contribution analysis, to mitigate parameter conflicts. Extensive experiments
on multiple-choice and question-answering tasks in both Chinese and English
validate Hi-Merging's ability for multi-task learning. The results demonstrate
that Hi-Merging consistently outperforms existing merging techniques and
surpasses the performance of models fine-tuned on combined datasets in most
scenarios. Code is available at:
https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.


## QiMeng-Attention SOTA Attention Operator is generated by SOTA Attention Algorithm

>Authors: Qirui Zhou, Shaohui Peng, Weiqiang Xiong, Haixin Chen, Yuanbo Wen, Haochen Li, Ling Li, Qi Guo, Yongwei Zhao, Ke Gao, Ruizhi Chen, Yanjun Wu, Chen Zhao, Yunji Chen

>2025-06-14

> http://arxiv.org/abs/2506.12355v1

The attention operator remains a critical performance bottleneck in large
language models (LLMs), particularly for long-context scenarios. While
FlashAttention is the most widely used and effective GPU-aware **acceleration**
algorithm, it must require time-consuming and hardware-specific manual
implementation, limiting adaptability across GPU architectures. Existing LLMs
have shown a lot of promise in code generation tasks, but struggle to generate
high-performance attention code. The key challenge is it cannot comprehend the
complex data flow and computation process of the attention operator and utilize
low-level primitive to exploit GPU performance.
  To address the above challenge, we propose an LLM-friendly Thinking Language
(LLM-TL) to help LLMs decouple the generation of high-level optimization logic
and low-level implementation on GPU, and enhance LLMs' understanding of
attention operator. Along with a 2-stage reasoning workflow, TL-Code generation
and translation, the LLMs can automatically generate FlashAttention
implementation on diverse GPUs, establishing a self-optimizing paradigm for
generating high-performance attention operators in attention-centric
algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our
methods significantly outshines that of vanilla LLMs, achieving a speed-up of
up to 35.16x. Besides, our method not only surpasses human-optimized libraries
(cuDNN and official library) in most scenarios but also extends support to
unsupported hardware and data types, reducing development time from months to
minutes compared with human experts.


## GroupNL Low-Resource and Robust CNN Design over Cloud and Device

>Authors: Chuntao Ding, Jianhang Xie, Junna Zhang, Salman Raza, Shangguang Wang, Jiannong Cao

>2025-06-14

> http://arxiv.org/abs/2506.12335v1

It has become mainstream to deploy Convolutional Neural Network (CNN) models
on ubiquitous Internet of Things (IoT) devices with the help of the cloud to
provide users with a variety of high-quality services. Most existing methods
have two limitations: (i) low robustness in handling corrupted image data
collected by IoT devices; and (ii) high consumption of computational and
transmission resources. To this end, we propose the Grouped NonLinear
transformation generation method (GroupNL), which generates diversified feature
maps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to
improve the robustness of the CNN model. Specifically, partial convolution
filters are designated as seed filters in a convolutional layer, and a small
set of feature maps, i.e., seed feature maps, are first generated based on
vanilla convolution operation. Then, we split seed feature maps into several
groups, each with a set of different NLFs, to generate corresponding diverse
feature maps with in-place nonlinear processing. Moreover, GroupNL effectively
reduces the parameter transmission between multiple nodes during model training
by setting the hyperparameters of NLFs to random initialization and not
updating them during model training, and reduces the computing resources by
using NLFs to generate feature maps instead of most feature maps generated
based on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,
Icons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the
proposed GroupNL outperforms other state-of-the-art methods in model robust and
training **acceleration**. Specifically, on the Icons-50 dataset, the accuracy of
GroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla
ResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN
when trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.


## The Budget AI Researcher and the Power of RAG Chains

>Authors: Franklin Lee, Tengfei Ma

>2025-06-14

> http://arxiv.org/abs/2506.12317v1

Navigating the vast and rapidly growing body of scientific literature is a
formidable challenge for aspiring researchers. Current approaches to supporting
research idea generation often rely on generic large language models (LLMs).
While LLMs are effective at aiding comprehension and summarization, they often
fall short in guiding users toward practical research ideas due to their
limitations. In this study, we present a novel structural framework for
research ideation. Our framework, The Budget AI Researcher, uses
retrieval-augmented generation (RAG) chains, vector databases, and topic-guided
pairing to recombine concepts from hundreds of machine learning papers. The
system ingests papers from nine major AI conferences, which collectively span
the vast subfields of machine learning, and organizes them into a hierarchical
topic tree. It uses the tree to identify distant topic pairs, generate novel
research abstracts, and refine them through iterative self-evaluation against
relevant literature and peer reviews, generating and refining abstracts that
are both grounded in real-world research and demonstrably interesting.
Experiments using LLM-based metrics indicate that our method significantly
improves the concreteness of generated research ideas relative to standard
prompting approaches. Human evaluations further demonstrate a substantial
enhancement in the perceived interestingness of the outputs. By bridging the
gap between academic data and creative generation, the Budget AI Researcher
offers a practical, free tool for accelerating scientific discovery and
lowering the barrier for aspiring researchers. Beyond research ideation, this
approach inspires solutions to the broader challenge of generating
personalized, context-aware outputs grounded in evolving real-world knowledge.


## Two heads are better than one simulating large transformers with small ones

>Authors: Hantao Yu, Josh Alman

>2025-06-13

> http://arxiv.org/abs/2506.12220v1

The quadratic complexity of self-attention prevents transformers from scaling
effectively to long input sequences. On the other hand, modern GPUs and other
specialized hardware accelerators are well-optimized for processing small input
sequences in transformers during both training and inference. A natural
question arises: can we take advantage of the efficiency of small transformers
to deal with long input sequences?
  In this paper, we show that transformers with long input sequences (large
transformers) can be efficiently simulated by transformers that can only take
short input sequences (small transformers). Specifically, we prove that any
transformer with input length $N$ can be efficiently simulated by only
$O((N/M)^2)$ transformers with input length $M \ll N$, and that this cannot be
improved in the worst case. However, we then prove that in various natural
scenarios including average-case inputs, sliding window masking and attention
sinks, the optimal number $O(N/M)$ of small transformers suffice.


## How Visual Representations Map to Language Feature Space in Multimodal LLMs

>Authors: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda

>2025-06-13

> http://arxiv.org/abs/2506.11976v1

Effective multimodal reasoning depends on the alignment of visual and
linguistic representations, yet the mechanisms by which vision-language models
(VLMs) achieve this alignment remain poorly understood. We introduce a
methodological framework that deliberately maintains a frozen large language
model (LLM) and a frozen vision transformer (ViT), connected solely by training
a linear adapter during visual instruction tuning. This design is fundamental
to our approach: by keeping the language model frozen, we ensure it maintains
its original language representations without adaptation to visual data.
Consequently, the linear adapter must map visual features directly into the
LLM's existing representational space rather than allowing the language model
to develop specialized visual understanding through fine-tuning. Our
experimental design uniquely enables the use of pre-trained **sparse** autoencoders
(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned
with the unchanged language model and serve as a snapshot of the learned
language feature-representations. Through systematic analysis of SAE
reconstruction error, **sparsity** patterns, and feature SAE descriptions, we
reveal the layer-wise progression through which visual representations
gradually align with language feature representations, converging in
middle-to-later layers. This suggests a fundamental misalignment between ViT
outputs and early LLM layers, raising important questions about whether current
adapter-based architectures optimally facilitate cross-modal representation
learning.


## Beyond Homogeneous Attention Memory-Efficient LLMs via Fourier-Approximated KV Cache

>Authors: Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

>2025-06-13

> http://arxiv.org/abs/2506.11886v1

Large Language Models struggle with memory demands from the growing Key-Value
(**KV**) cache as context lengths increase. Existing compression methods homogenize
head dimensions or rely on attention-guided token **pruning**, often sacrificing
accuracy or introducing computational overhead. We propose FourierAttention, a
training-free framework that exploits the heterogeneous roles of transformer
head dimensions: lower dimensions prioritize local context, while upper ones
capture long-range dependencies. By projecting the long-context-insensitive
dimensions onto orthogonal Fourier bases, FourierAttention approximates their
temporal evolution with fixed-length spectral coefficients. Evaluations on
LLaMA models show that FourierAttention achieves the best long-context accuracy
on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,
FlashFourierAttention, is designed to optimize memory via streamlined
read-write operations, enabling efficient deployment without performance
compromise.


## Structural Similarity-Inspired Unfolding for Lightweight Image Super-Resolution

>Authors: Zhangkai Ni, Yang Zhang, Wenhan Yang, Hanli Wang, Shiqi Wang, Sam Kwong

>2025-06-13

> http://arxiv.org/abs/2506.11823v1

Major efforts in data-driven image super-resolution (SR) primarily focus on
expanding the receptive field of the model to better capture contextual
information. However, these methods are typically implemented by stacking
deeper networks or leveraging transformer-based attention mechanisms, which
consequently increases model complexity. In contrast, model-driven methods
based on the unfolding paradigm show promise in improving performance while
effectively maintaining model compactness through sophisticated module design.
Based on these insights, we propose a Structural Similarity-Inspired Unfolding
(SSIU) method for efficient image SR. This method is designed through unfolding
an SR optimization function constrained by structural similarity, aiming to
combine the strengths of both data-driven and model-driven approaches. Our
model operates progressively following the unfolding paradigm. Each iteration
consists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse
Attention Module (ESAM). The former implements comprehensive constraints on
features, including a structural similarity constraint, while the latter aims
to achieve **sparse** activation. In addition, we design a Mixture-of-Experts-based
Feature Selector (MoE-FS) that fully utilizes multi-level feature information
by combining features from different steps. Extensive experiments validate the
efficacy and efficiency of our unfolding-inspired network. Our model
outperforms current state-of-the-art models, boasting lower parameter counts
and reduced memory consumption. Our code will be available at:
https://github.com/eezkni/SSIU


## Learning to Integrate

>Authors: Oliver G. Ernst, Hanno Gottschalk, Toni Kowalewitz, Patrick Krüger

>2025-06-13

> http://arxiv.org/abs/2506.11801v1

This work deals with uncertainty quantification for a generic input
distribution to some resource-intensive simulation, e.g., requiring the
solution of a partial differential equation. While efficient numerical methods
exist to compute integrals for high-dimensional Gaussian and other separable
distributions based on **sparse** grids (SG), input data arising in practice often
does not fall into this class. We therefore employ transport maps to transform
complex distributions to multivatiate standard normals. In generative learning,
a number of neural network architectures have been introduced that accomplish
this task approximately. Examples are affine coupling flows (ACF) and ordinary
differential equation-based networks such as conditional flow matching (CFM).
To compute the expectation of a quantity of interest, we numerically integrate
the composition of the inverse of the learned transport map with the simulation
code output. As this map is integrated over a multivariate Gaussian
distribution, SG techniques can be applied. Viewing the images of the SG
quadrature nodes as learned quadrature nodes for a given complex distribution
motivates our title. We demonstrate our method for monomials of total degrees
for which the unmapped SG rules are exact. We also apply our approach to the
stationary diffusion equation with coefficients modeled by exponentiated L\'evy
random fields, using a Karhunen-Lo\`eve-like modal expansions with 9 and 25
modes. In a series of numerical experiments, we investigate errors due to
learning accuracy, quadrature, statistical estimation, truncation of the modal
series of the input random field, and training data size for three normalizing
flows (ACF, conditional Flow Matching and Optimal transport Flow Matching) We
discuss the mathematical assumptions on which our approach is based and
demonstrate its shortcomings when these are violated.


## GPLQ A General, Practical, and Lightning QAT Method for Vision Transformers

>Authors: Guang Liang, Xinyao Liu, Jianxin Wu

>2025-06-13

> http://arxiv.org/abs/2506.11784v1

Vision Transformers (ViTs) are essential in computer vision but are
computationally intensive, too. Model **quantization**, particularly to low
bit-widths like 4-bit, aims to alleviate this difficulty, yet existing
Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) methods
exhibit significant limitations. PTQ often incurs substantial accuracy drop,
while QAT achieves high accuracy but suffers from prohibitive computational
costs, limited generalization to downstream tasks, training instability, and
lacking of open-source codebase. To address these challenges, this paper
introduces General, Practical, and Lightning Quantization (GPLQ), a novel
framework designed for efficient and effective ViT **quantization**. GPLQ is
founded on two key empirical insights: the paramount importance of activation
**quantization** and the necessity of preserving the model's original optimization
``basin'' to maintain generalization. Consequently, GPLQ employs a sequential
``activation-first, weights-later'' strategy. Stage 1 keeps weights in FP32
while quantizing activations with a feature mimicking loss in only 1 epoch to
keep it stay in the same ``basin'', thereby preserving generalization. Stage 2
**quantize**s weights using a PTQ method. As a result, GPLQ is 100x faster than
existing QAT methods, lowers memory footprint to levels even below FP32
training, and achieves 4-bit model performance that is highly competitive with
FP32 models in terms of both accuracy on ImageNet and generalization to diverse
downstream tasks, including fine-grained visual classification and object
detection. We will release an easy-to-use open-source toolkit supporting
multiple vision tasks.


## MambaVSR Content-Aware Scanning State Space Model for Video Super-Resolution

>Authors: Linfeng He, Meiqin Liu, Qi Tang, Chao Yao, Yao Zhao

>2025-06-13

> http://arxiv.org/abs/2506.11768v1

Video super-resolution (VSR) faces critical challenges in effectively
modeling non-local dependencies across misaligned frames while preserving
computational efficiency. Existing VSR methods typically rely on optical flow
strategies or transformer architectures, which struggle with large motion
displacements and long video sequences. To address this, we propose MambaVSR,
the first state-space model framework for VSR that incorporates an innovative
content-aware scanning mechanism. Unlike rigid 1D sequential processing in
conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal
interactions through the Shared Compass Construction (SCC) and the
Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs
intra-frame semantic connectivity graphs via efficient **sparse** attention and
generates adaptive spatial scanning sequences through spectral clustering.
Building upon SCC, the CAS module effectively aligns and aggregates non-local
similar content across multiple frames by interleaving temporal features along
the learned spatial order. To bridge global dependencies with local details,
the Global-Local State Space Block (GLSSB) synergistically integrates window
self-attention operations with SSM-based feature propagation, enabling
high-frequency detail recovery under global dependency guidance. Extensive
experiments validate MambaVSR's superiority, outperforming the
Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer
parameters.


## DeepResearch Bench A Comprehensive Benchmark for Deep Research Agents

>Authors: Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao

>2025-06-13

> http://arxiv.org/abs/2506.11763v1

Deep Research Agents are a prominent category of LLM-based agents. By
autonomously orchestrating multistep web exploration, targeted retrieval, and
higher-order synthesis, they transform vast amounts of online information into
analyst-grade, citation-rich reports--compressing hours of manual desk research
into minutes. However, a comprehensive benchmark for systematically evaluating
the capabilities of these agents remains absent. To bridge this gap, we present
DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,
each meticulously crafted by domain experts across 22 distinct fields.
Evaluating DRAs is inherently complex and labor-intensive. We therefore propose
two novel methodologies that achieve strong alignment with human judgment. The
first is a reference-based method with adaptive criteria to assess the quality
of generated research reports. The other framework is introduced to evaluate
DRA's information retrieval and collection capabilities by assessing its
effective citation count and overall citation accuracy. We have open-sourced
DeepResearch Bench and key components of these frameworks at
https://github.com/Ayanami0730/deep_research_bench to accelerate the
development of practical LLM-based agents.


## Fusion of multi-source precipitation records via coordinate-based generative model

>Authors: Sencan Sun, Congyi Nai, Baoxiang Pan, Wentao Li, Xin Li, Efi Foufoula-Georgiou, Yanluan Lin

>2025-06-13

> http://arxiv.org/abs/2506.11698v1

Precipitation remains one of the most challenging climate variables to
observe and predict accurately. Existing datasets face intricate trade-offs:
gauge observations are relatively trustworthy but **sparse**, satellites provide
global coverage with retrieval uncertainties, and numerical models offer
physical consistency but are biased and computationally intensive. Here we
introduce PRIMER (Precipitation Record Infinite MERging), a deep generative
framework that fuses these complementary sources to produce accurate,
high-resolution, full-coverage precipitation estimates. PRIMER employs a
coordinate-based diffusion model that learns from arbitrary spatial locations
and associated precipitation values, enabling seamless integration of gridded
data and irregular gauge observations. Through two-stage training--first
learning large-scale patterns, then refining with accurate gauge
measurements--PRIMER captures both large-scale climatology and local precision.
Once trained, it can downscale forecasts, interpolate **sparse** observations, and
correct systematic biases within a principled Bayesian framework. Using gauge
observations as ground truth, PRIMER effectively corrects biases in existing
datasets, yielding statistically significant error reductions at most stations
and furthermore enhancing the spatial coherence of precipitation fields.
Crucially, it generalizes without retraining, correcting biases in operational
forecasts it has never seen. This demonstrates how generative AI can transform
Earth system science by combining imperfect data, providing a scalable solution
for global precipitation monitoring and prediction.


## FieldFormer Self-supervised Reconstruction of Physical Fields via Tensor Attention Prior

>Authors: Panqi Chen, Siyuan Li, Lei Cheng, Xiao Fu, Yik-Chung Wu, Sergios Theodoridis

>2025-06-13

> http://arxiv.org/abs/2506.11629v1

Reconstructing physical field tensors from \textit{in situ} observations,
such as radio maps and ocean sound speed fields, is crucial for enabling
environment-aware decision making in various applications, e.g., wireless
communications and underwater acoustics. Field data reconstruction is often
challenging, due to the limited and noisy nature of the observations,
necessitating the incorporation of prior information to aid the reconstruction
process. Deep neural network-based data-driven structural constraints (e.g.,
``deeply learned priors'') have showed promising performance. However, this
family of techniques faces challenges such as model mismatches between training
and testing phases. This work introduces FieldFormer, a self-supervised neural
prior learned solely from the limited {\it in situ} observations without the
need of offline training. Specifically, the proposed framework starts with
modeling the fields of interest using the tensor Tucker model of a high
multilinear rank, which ensures a universal approximation property for all
fields. In the sequel, an attention mechanism is incorporated to learn the
**sparsity** pattern that underlies the core tensor in order to reduce the solution
space.
  In this way, a ``complexity-adaptive'' neural representation, grounded in the
Tucker decomposition, is obtained that can flexibly represent
  various types of fields. A theoretical analysis is provided to support the
recoverability of the proposed design. Moreover, extensive experiments, using
various physical field tensors, demonstrate the superiority of the proposed
approach compared to state-of-the-art baselines.


## SecONNds Secure Outsourced Neural Network Inference on ImageNet

>Authors: Shashank Balla

>2025-06-13

> http://arxiv.org/abs/2506.11586v1

The widespread adoption of outsourced neural network inference presents
significant privacy challenges, as sensitive user data is processed on
untrusted remote servers. Secure inference offers a privacy-preserving
solution, but existing frameworks suffer from high computational overhead and
communication costs, rendering them impractical for real-world deployment. We
introduce SecONNds, a non-intrusive secure inference framework optimized for
large ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel
fully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison
-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit
triples generated from Silent Random Oblivious Transfer. Our novel protocol
achieves an online speedup of 17$\times$ in nonlinear operations compared to
state-of-the-art solutions while reducing communication overhead. To further
enhance performance, SecONNds employs Number Theoretic Transform (NTT)
preprocessing and leverages GPU **acceleration** for homomorphic encryption
operations, resulting in speedups of 1.6$\times$ on CPU and 2.2$\times$ on GPU
for linear operations. We also present SecONNds-P, a bit-exact variant that
ensures verifiable full-precision results in secure computation, matching the
results of plaintext computations. Evaluated on a 37-bit **quantize**d SqueezeNet
model, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s
on CPU, with a total communication of just 420 MiB. SecONNds' efficiency and
reduced computational load make it well-suited for deploying privacy-sensitive
applications in resource-constrained environments. SecONNds is open source and
can be accessed from: https://github.com/shashankballa/SecONNds.


## FIMA-Q Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation

>Authors: Zhuguanyu Wu, Shihe Wang, Jiayi Zhang, Jiaxin Chen, Yunhong Wang

>2025-06-13

> http://arxiv.org/abs/2506.11543v1

Post-training **quantization** (PTQ) has stood out as a cost-effective and
promising model compression paradigm in recent years, as it avoids
computationally intensive model retraining. Nevertheless, current PTQ methods
for Vision Transformers (ViTs) still suffer from significant accuracy
degradation, especially under **low-bit** **quantization**. To address these
shortcomings, we analyze the prevailing Hessian-guided **quantization** loss, and
uncover certain limitations of conventional Hessian approximations. By
following the block-wise reconstruction framework, we propose a novel PTQ
method for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the
connection between KL divergence and FIM, which enables fast computation of the
**quantization** loss during reconstruction. We further propose an efficient FIM
approximation method, namely DPLR-FIM, by employing the diagonal plus low-rank
principle, and formulate the ultimate **quantization** loss. Our extensive
experiments, conducted across various vision tasks with representative
ViT-based architectures on public datasets, demonstrate that our method
substantially promotes the accuracy compared to the state-of-the-art
approaches, especially in the case of **low-bit** **quantization**. The source code is
available at https://github.com/ShiheWang/FIMA-Q.


## Vectorized Sparse Second-Order Forward Automatic Differentiation for Optimal Control Direct Methods

>Authors: Yilin Zou, Fanghua Jiang

>2025-06-13

> http://arxiv.org/abs/2506.11537v1

Direct collocation methods are widely used numerical techniques for solving
optimal control problems. The discretization of continuous-time optimal control
problems transforms them into large-scale nonlinear programming problems, which
require efficient computation of first- and second-order derivatives. To
achieve computational efficiency, these derivatives must be computed in **sparse**
and vectorized form, exploiting the problem's inherent **sparsity** structure. This
paper presents a vectorized **sparse** second-order forward automatic
differentiation framework designed for direct collocation methods in optimal
control. The method exploits the problem's **sparse** structure to efficiently
compute derivatives across multiple mesh points. By incorporating both scalar
and vector nodes within the expression graph, the approach enables effective
parallelization and optimized memory access patterns while maintaining
flexibility for complex problems. The methodology is demonstrated through
application to a prototype optimal control problem. A complete implementation
for multi-phase optimal control problems is available as an open-source
package, supporting both theoretical research and practical applications.


## Lag-Relative Sparse Attention In Long Context Training

>Authors: Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li

>2025-06-13

> http://arxiv.org/abs/2506.11498v1

Large Language Models (LLMs) have made significant strides in natural
language processing and generation, yet their ability to handle long-context
input remains constrained by the quadratic complexity of attention computation
and linear-increasing key-value memory footprint. To reduce computational costs
and memory, key-value cache compression techniques are commonly applied at
inference time, but this often leads to severe performance degradation, as
models are not trained to handle compressed context. Although there are more
sophisticated compression methods, they are typically unsuitable for
post-training because of their incompatibility with gradient-based optimization
or high computation overhead. To fill this gap with no additional parameter and
little computation overhead, we propose Lag-Relative Sparse Attention(LRSA)
anchored by the Lag**KV** compression method for long context post-training. Our
method performs chunk-by-chunk prefilling, which selects the top K most
relevant key-value pairs in a fixed-size lagging window, allowing the model to
focus on salient historical context while maintaining efficiency. Experimental
results show that our approach significantly enhances the robustness of the LLM
with key-value compression and achieves better fine-tuned results in the
question-answer tuning task.


## SemanticST Spatially Informed Semantic Graph Learning for Clustering, Integration, and Scalable Analysis of Spatial Transcriptomics

>Authors: Roxana Zahedi, Ahmadreza Argha, Nona Farbehi, Ivan Bakhshayeshi, Youqiong Ye, Nigel H. Lovell, Hamid Alinejad-Rokny

>2025-06-13

> http://arxiv.org/abs/2506.11491v2

Spatial transcriptomics (ST) technologies enable gene expression profiling
with spatial resolution, offering unprecedented insights into tissue
organization and disease heterogeneity. However, current analysis methods often
struggle with noisy data, limited scalability, and inadequate modelling of
complex cellular relationships. We present SemanticST, a biologically informed,
graph-based deep learning framework that models diverse cellular contexts
through multi-semantic graph construction. SemanticST builds multiple
context-specific graphs capturing spatial proximity, gene expression
similarity, and tissue domain structure, and learns disentangled embeddings for
each. These are fused using an attention-inspired strategy to yield a unified,
biologically meaningful representation. A community-aware min-cut loss improves
robustness over contrastive learning, particularly in **sparse** ST data.
SemanticST supports mini-batch training, making it the first graph neural
network scalable to large-scale datasets such as Xenium (500,000 cells).
Benchmarking across four platforms (Visium, Slide-seq, Stereo-seq, Xenium) and
multiple human and mouse tissues shows consistent 20 percentage gains in ARI,
NMI, and trajectory fidelity over DeepST, GraphST, and IRIS. In re-analysis of
breast cancer Xenium data, SemanticST revealed rare and clinically significant
niches, including triple receptor-positive clusters, spatially distinct
DCIS-to-IDC transition zones, and FOXC2 tumour-associated myoepithelial cells,
suggesting non-canonical EMT programs with stem-like features. SemanticST thus
provides a scalable, interpretable, and biologically grounded framework for
spatial transcriptomics analysis, enabling robust discovery across tissue types
and diseases, and paving the way for spatially resolved tissue atlases and
next-generation precision medicine.


## Agent-RLVR Training Software Engineering Agents via Guidance and Environment Rewards

>Authors: Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx

>2025-06-13

> http://arxiv.org/abs/2506.11425v1

Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too **sparse** for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.


## Efficient Long-Context LLM Inference via KV Cache Clustering

>Authors: Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan

>2025-06-13

> http://arxiv.org/abs/2506.11418v1

Large language models (LLMs) with extended context windows have become
increasingly prevalent for tackling complex tasks. However, the substantial
Key-Value (**KV**) cache required for long-context LLMs poses significant
deployment challenges. Existing approaches either discard potentially critical
information needed for future generations or offer limited efficiency gains due
to high computational overhead. In this paper, we introduce Chelsea, a simple
yet effective framework for online **KV** cache clustering. Our approach is based
on the observation that key states exhibit high similarity along the sequence
dimension. To enable efficient clustering, we divide the sequence into chunks
and propose Chunked Soft Matching, which employs an alternating partition
strategy within each chunk and identifies clusters based on similarity. Chelsea
then merges the **KV** cache within each cluster into a single centroid.
Additionally, we provide a theoretical analysis of the computational complexity
and the optimality of the intra-chunk partitioning strategy. Extensive
experiments across various models and long-context benchmarks demonstrate that
Chelsea achieves up to 80% reduction in **KV** cache memory usage while maintaining
comparable model performance. Moreover, with minimal computational overhead,
Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and
reduces end-to-end latency by up to 2.72$\times$.

