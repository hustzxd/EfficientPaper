# 2024-11-06

# Table of Contents
* [Adaptive Caching for Faster Video Generation with Diffusion Transformers](#Adaptive-Caching-for-Faster-Video-Generation-with-Diffusion-Transformers)
* ["Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization](#"Give-Me-BF16-or-Give-Me-Death"?-Accuracy-Performance-Trade-Offs-in-LLM-Quantization)
* [WebRL Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning](#WebRL-Training-LLM-Web-Agents-via-Self-Evolving-Online-Curriculum-Reinforcement-Learning)
* [Sparsing Law Towards Large Language Models with Greater Activation Sparsity](#Sparsing-Law-Towards-Large-Language-Models-with-Greater-Activation-Sparsity)
* [Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning](#Provably-Transformers-Harness-Multi-Concept-Word-Semantics-for-Efficient-In-Context-Learning)
* [AVSS Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis](#AVSS-Layer-Importance-Evaluation-in-Large-Language-Models-via-Activation-Variance-Sparsity-Analysis)
* [FedMoE-DA Federated Mixture of Experts via Domain Aware Fine-grained Aggregation](#FedMoE-DA-Federated-Mixture-of-Experts-via-Domain-Aware-Fine-grained-Aggregation)
* [Addressing Representation Collapse in Vector Quantized Models with One Linear Layer](#Addressing-Representation-Collapse-in-Vector-Quantized-Models-with-One-Linear-Layer)
* [V-CAS A Realtime Vehicle Anti Collision System Using Vision Transformer on Multi-Camera Streams](#V-CAS-A-Realtime-Vehicle-Anti-Collision-System-Using-Vision-Transformer-on-Multi-Camera-Streams)
* [Learning Where to Edit Vision Transformers](#Learning-Where-to-Edit-Vision-Transformers)
* [Context Parallelism for Scalable Million-Token Inference](#Context-Parallelism-for-Scalable-Million-Token-Inference)
* [TabSec A Collaborative Framework for Novel Insider Threat Detection](#TabSec-A-Collaborative-Framework-for-Novel-Insider-Threat-Detection)
* [LES-SINDy Laplace-Enhanced Sparse Identification of Nonlinear Dynamical Systems](#LES-SINDy-Laplace-Enhanced-Sparse-Identification-of-Nonlinear-Dynamical-Systems)


## Adaptive Caching for Faster Video Generation with Diffusion Transformers

>Authors: Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Michael S. Ryoo, Tian Xie

>2024-11-04

> http://arxiv.org/abs/2411.02397v1

Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.


## "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization

>Authors: Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh

>2024-11-04

> http://arxiv.org/abs/2411.02355v1

Despite the popularity of large language model (LLM) **quantization** for
inference **acceleration**, significant uncertainty remains regarding the
accuracy-performance trade-offs associated with various **quantization** formats.
We present a comprehensive empirical study of **quantize**d accuracy, evaluating
popular **quantization** formats (FP8, INT8, INT4) across academic benchmarks and
real-world tasks, on the entire Llama-3.1 model family. Additionally, our study
examines the difference in text generated by **quantize**d models versus their
uncompressed counterparts. Beyond benchmarks, we also present a couple of
**quantization** improvements which allowed us to obtain state-of-the-art accuracy
recovery results. Our investigation, encompassing over 500,000 individual
evaluations, yields several key findings: (1) FP8 weight and activation
**quantization** (W8A8-FP) is lossless across all model scales, (2) INT8 weight and
activation **quantization** (W8A8-INT), when properly tuned, incurs surprisingly
low 1-3% accuracy degradation, and (3) INT4 weight-only **quantization**
(W4A16-INT) is competitive with 8-bit integer weight and activation
**quantization**. To address the question of the "best" format for a given
deployment environment, we conduct inference performance analysis using the
popular open-source vLLM framework on various GPU architectures. We find that
W4A16 offers the best cost-efficiency for synchronous deployments, and for
asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel
in asynchronous "continuous batching" deployment of mid- and large-size models
on high-end GPUs. Our results provide a set of practical guidelines for
deploying **quantize**d LLMs across scales and performance requirements.


## WebRL Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning

>Authors: Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong

>2024-11-04

> http://arxiv.org/abs/2411.02337v1

Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, **sparse** feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.


## Sparsing Law Towards Large Language Models with Greater Activation Sparsity

>Authors: Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun

>2024-11-04

> http://arxiv.org/abs/2411.02335v1

Activation **sparsity** denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation **sparsity** within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation **sparsity** and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation **sparsity** within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ **sparsity**, a precise
and performance-aware activation **sparsity** metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time **sparsity** trends. The activation ratio
(i.e., $1-\mathrm{**sparsity**\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation **sparsity**. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation **sparsity** varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation **sparsity** have important implications for making LLMs more
efficient and interpretable.


## Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning

>Authors: Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong

>2024-11-04

> http://arxiv.org/abs/2411.02199v1

Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise **sparse** coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.


## AVSS Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis

>Authors: Zichen Song, Yuxin Wu, Sitan Huang, Zhongfeng Kang

>2024-11-04

> http://arxiv.org/abs/2411.02117v1

The evaluation of layer importance in deep learning has been an active area
of research, with significant implications for model optimization and
interpretability. Recently, large language models (LLMs) have gained prominence
across various domains, yet limited studies have explored the functional
importance and performance contributions of individual layers within LLMs,
especially from the perspective of activation distribution. In this work, we
propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining
normalized activation variance and **sparsity** to assess each layer's contribution
to model performance. By identifying and removing approximately the lowest 25%
of layers based on AVSS, we achieve over 90% of original model performance
across tasks such as question answering, language modeling, and sentiment
classification, indicating that these layers may be non-essential. Our approach
provides a systematic method for identifying less critical layers, contributing
to efficient large language model architectures.


## FedMoE-DA Federated Mixture of Experts via Domain Aware Fine-grained Aggregation

>Authors: Ziwei Zhan, Wenkuan Zhao, Yuanqing Li, Weijie Liu, Xiaoxi Zhang, Chee Wei Tan, Chuan Wu, Deke Guo, Xu Chen

>2024-11-04

> http://arxiv.org/abs/2411.02115v1

Federated learning (FL) is a collaborative machine learning approach that
enables multiple clients to train models without sharing their private data.
With the rise of deep learning, large-scale models have garnered significant
attention due to their exceptional performance. However, a key challenge in FL
is the limitation imposed by clients with constrained computational and
communication resources, which hampers the deployment of these large models.
The Mixture of Experts (MoE) architecture addresses this challenge with its
**sparse** activation property, which reduces computational workload and
communication demands during inference and updates. Additionally, MoE
facilitates better personalization by allowing each expert to specialize in
different subsets of the data distribution. To alleviate the communication
burdens between the server and clients, we propose FedMoE-DA, a new FL model
training framework that leverages the MoE architecture and incorporates a novel
domain-aware, fine-grained aggregation strategy to enhance the robustness,
personalizability, and communication efficiency simultaneously. Specifically,
the correlation between both intra-client expert models and inter-client data
heterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)
communication between clients for selective expert model synchronization, thus
significantly reducing the server-client transmissions. Experiments demonstrate
that our FedMoE-DA achieves excellent performance while reducing the
communication pressure on the server.


## Addressing Representation Collapse in Vector Quantized Models with One Linear Layer

>Authors: Yongxin Zhu, Bocheng Li, Yifei Xin, Linli Xu

>2024-11-04

> http://arxiv.org/abs/2411.02038v1

Vector Quantization (VQ) is a widely used method for converting continuous
representations into discrete codes, which has become fundamental in
unsupervised representation learning and latent generative models. However, VQ
models are often hindered by the problem of representation collapse in the
latent space, which leads to low codebook utilization and limits the
scalability of the codebook for large-scale training. Existing methods designed
to mitigate representation collapse typically reduce the dimensionality of
latent space at the expense of model capacity, which do not fully resolve the
core issue. In this study, we conduct a theoretical analysis of representation
collapse in VQ models and identify its primary cause as the disjoint
optimization of the codebook, where only a small subset of code vectors are
updated through gradient descent. To address this issue, we propose
\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a
linear transformation layer based on a learnable latent basis. This
transformation optimizes the \textit{entire linear space} spanned by the
codebook, rather than merely updating \textit{the code vector} selected by the
nearest-neighbor search in vanilla VQ models. Although it is commonly
understood that the multiplication of two linear matrices is equivalent to
applying a single linear layer, our approach works surprisingly well in
resolving the collapse issue in VQ models with just one linear layer. We
validate the efficacy of SimVQ through extensive experiments across various
modalities, including image and audio data with different model architectures.
Our code is available at \url{https://github.com/youngsheen/SimVQ}.


## V-CAS A Realtime Vehicle Anti Collision System Using Vision Transformer on Multi-Camera Streams

>Authors: Muhammad Waqas Ashraf, Ali Hassan, Imad Ali Shah

>2024-11-04

> http://arxiv.org/abs/2411.01963v1

This paper introduces a real-time Vehicle Collision Avoidance System (V-CAS)
designed to enhance vehicle safety through adaptive braking based on
environmental perception. V-CAS leverages the advanced vision-based transformer
model RT-DETR, DeepSORT tracking, speed estimation, brake light detection, and
an adaptive braking mechanism. It computes a composite collision risk score
based on vehicles' relative **acceleration**s, distances, and detected braking
actions, using brake light signals and trajectory data from multiple camera
streams to improve scene perception. Implemented on the Jetson Orin Nano, V-CAS
enables real-time collision risk assessment and proactive mitigation through
adaptive braking. A comprehensive training process was conducted on various
datasets for comparative analysis, followed by fine-tuning the selected object
detection model using transfer learning. The system's effectiveness was
rigorously evaluated on the Car Crash Dataset (CCD) from YouTube and through
real-time experiments, achieving over 98% accuracy with an average proactive
alert time of 1.13 seconds. Results indicate significant improvements in object
detection and tracking, enhancing collision avoidance compared to traditional
single-camera methods. This research demonstrates the potential of low-cost,
multi-camera embedded vision transformer systems to advance automotive safety
through enhanced environmental perception and proactive collision avoidance
mechanisms.


## Learning Where to Edit Vision Transformers

>Authors: Yunqiao Yang, Long-Kai Huang, Shengzhuang Chen, Kede Ma, Ying Wei

>2024-11-04

> http://arxiv.org/abs/2411.01948v1

Model editing aims to data-efficiently correct predictive errors of large
pre-trained models while ensuring generalization to neighboring failures and
locality to minimize unintended effects on unrelated examples. While
significant progress has been made in editing Transformer-based large language
models, effective strategies for editing vision Transformers (ViTs) in computer
vision remain largely untapped. In this paper, we take initial steps towards
correcting predictive errors of ViTs, particularly those arising from
subpopulation shifts. Taking a locate-then-edit approach, we first address the
where-to-edit challenge by meta-learning a hypernetwork on CutMix-augmented
data generated for editing reliability. This trained hypernetwork produces
generalizable binary masks that identify a **sparse** subset of structured model
parameters, responsive to real-world failure samples. Afterward, we solve the
how-to-edit problem by simply fine-tuning the identified parameters using a
variant of gradient descent to achieve successful edits. To validate our
method, we construct an editing benchmark that introduces subpopulation shifts
towards natural underrepresented images and AI-generated images, thereby
revealing the limitations of pre-trained ViTs for object recognition. Our
approach not only achieves superior performance on the proposed benchmark but
also allows for adjustable trade-offs between generalization and locality. Our
code is available at https://github.com/hustyyq/Where-to-Edit.


## Context Parallelism for Scalable Million-Token Inference

>Authors: Amy, Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jongsoo Park, Jianyu Huang

>2024-11-04

> http://arxiv.org/abs/2411.01783v1

We present context parallelism for long-context large language model
inference, which achieves near-linear scaling for long-context prefill latency
with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M
context prefill with Llama3 405B model in 77s (93% parallelization efficiency,
63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two
lossless exact ring attention variants: pass-**KV** and pass-Q to cover a wide
range of use cases with the state-of-the-art performance: full prefill,
persistent **KV** prefill and decode. Benchmarks on H100 GPU hosts inter-connected
with RDMA and TCP both show similar scalability for long-context prefill,
demonstrating that our method scales well using common commercial data center
with medium-to-low inter-host bandwidth.


## TabSec A Collaborative Framework for Novel Insider Threat Detection

>Authors: Zilin Huang, Xiangyan Tang, Hongyu Li, Xinyi Cao, Jieren Cheng

>2024-11-04

> http://arxiv.org/abs/2411.01779v1

In the era of the Internet of Things (IoT) and data sharing, users frequently
upload their personal information to enterprise databases to enjoy enhanced
service experiences provided by various online services. However, the
widespread presence of system vulnerabilities, remote network intrusions, and
insider threats significantly increases the exposure of private enterprise data
on the internet. If such data is stolen or leaked by attackers, it can result
in severe asset losses and business operation disruptions. To address these
challenges, this paper proposes a novel threat detection framework, TabITD.
This framework integrates Intrusion Detection Systems (IDS) with User and
Entity Behavior Analytics (UEBA) strategies to form a collaborative detection
system that bridges the gaps in existing systems' capabilities. It effectively
addresses the blurred boundaries between external and insider threats caused by
the diversification of attack methods, thereby enhancing the model's learning
ability and overall detection performance. Moreover, the proposed method
leverages the TabNet architecture, which employs a **sparse** attention feature
selection mechanism that allows TabNet to select the most relevant features at
each decision step, thereby improving the detection of rare-class attacks. We
evaluated our proposed solution on two different datasets, achieving average
accuracies of 96.71% and 97.25%, respectively. The results demonstrate that
this approach can effectively detect malicious behaviors such as masquerade
attacks and external threats, significantly enhancing network security defenses
and the efficiency of network attack detection.


## LES-SINDy Laplace-Enhanced Sparse Identification of Nonlinear Dynamical Systems

>Authors: Haoyang Zheng, Guang Lin

>2024-11-04

> http://arxiv.org/abs/2411.01719v1

Sparse Identification of Nonlinear Dynamical Systems (SINDy) is a powerful
tool for the data-driven discovery of governing equations. However, it
encounters challenges when modeling complex dynamical systems involving
high-order derivatives or discontinuities, particularly in the presence of
noise. These limitations restrict its applicability across various fields in
applied mathematics and physics. To mitigate these, we propose Laplace-Enhanced
SparSe Identification of Nonlinear Dynamical Systems (LES-SINDy). By
transforming time-series measurements from the time domain to the Laplace
domain using the Laplace transform and integration by parts, LES-SINDy enables
more accurate approximations of derivatives and discontinuous terms. It also
effectively handles unbounded growth functions and accumulated numerical errors
in the Laplace domain, thereby overcoming challenges in the identification
process. The model evaluation process selects the most accurate and
parsimonious dynamical systems from multiple candidates. Experimental results
across diverse ordinary and partial differential equations show that LES-SINDy
achieves superior robustness, accuracy, and parsimony compared to existing
methods.

